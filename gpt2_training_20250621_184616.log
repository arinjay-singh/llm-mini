W0621 18:46:18.290000 359090 torch/distributed/run.py:791] 
W0621 18:46:18.290000 359090 torch/distributed/run.py:791] *****************************************
W0621 18:46:18.290000 359090 torch/distributed/run.py:791] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0621 18:46:18.290000 359090 torch/distributed/run.py:791] *****************************************
Total desired batch size: 524,288
=> calculated gradient accumulation steps: 1
found 99 shards for split train
found 1 shards for split val
Total parameters: 124,475,904
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of decayed parameter tensors: 74, with 124354560 total parametersnum of non-decayed parameter tensors: 122, with 121344 total parameters

num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: Trueusing fused AdamW: True

wandb: Currently logged in as: arinjay-singh (arinjay-singh-northeastern-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/ubuntu/llm/wandb/run-20250621_184634-by6p52jo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sponge-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining
wandb: üöÄ View run at https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining/runs/by6p52jo
Wandb logging initialized
Validation loss at step 0: 10.961261749267578
Step 0: loss=10.9660, lr=0.000024, tokens/sec=20918.23, grad_norm=2.8407, duration=25.06s
Step 1: loss=10.6926, lr=0.000024, tokens/sec=1320581.61, grad_norm=2.6768, duration=0.40s
Step 2: loss=10.4523, lr=0.000024, tokens/sec=1412043.44, grad_norm=2.6229, duration=0.37s
Step 3: loss=10.2479, lr=0.000024, tokens/sec=1414368.43, grad_norm=2.5387, duration=0.37s
Step 4: loss=10.0980, lr=0.000024, tokens/sec=1407169.55, grad_norm=2.4194, duration=0.37s
Step 5: loss=9.9744, lr=0.000024, tokens/sec=1414886.23, grad_norm=2.3614, duration=0.37s
Step 6: loss=9.8908, lr=0.000024, tokens/sec=1416174.65, grad_norm=2.2938, duration=0.37s
Step 7: loss=9.8196, lr=0.000024, tokens/sec=1412229.34, grad_norm=2.2162, duration=0.37s
Step 8: loss=9.7484, lr=0.000024, tokens/sec=1407360.47, grad_norm=2.2043, duration=0.37s
Step 9: loss=9.7104, lr=0.000024, tokens/sec=1416099.87, grad_norm=2.1462, duration=0.37s
Step 10: loss=9.6577, lr=0.000024, tokens/sec=1409102.78, grad_norm=2.1241, duration=0.37s
Step 11: loss=9.6363, lr=0.000024, tokens/sec=1414827.97, grad_norm=2.0712, duration=0.37s
Step 12: loss=9.6097, lr=0.000024, tokens/sec=1412468.81, grad_norm=2.0430, duration=0.37s
Step 13: loss=9.5867, lr=0.000024, tokens/sec=1410140.11, grad_norm=2.0209, duration=0.37s
Step 14: loss=9.5313, lr=0.000024, tokens/sec=1415955.80, grad_norm=2.0642, duration=0.37s
Step 15: loss=9.5145, lr=0.000024, tokens/sec=1412530.51, grad_norm=2.0520, duration=0.37s
Step 16: loss=9.5029, lr=0.000024, tokens/sec=1414142.86, grad_norm=2.0269, duration=0.37s
Step 17: loss=9.4763, lr=0.000025, tokens/sec=1410545.34, grad_norm=2.0256, duration=0.37s
Step 18: loss=9.5250, lr=0.000025, tokens/sec=1414182.87, grad_norm=1.9396, duration=0.37s
Step 19: loss=9.5190, lr=0.000025, tokens/sec=1412056.13, grad_norm=1.9597, duration=0.37s
Step 20: loss=9.4130, lr=0.000025, tokens/sec=1410375.26, grad_norm=2.0139, duration=0.37s
Step 21: loss=9.3783, lr=0.000025, tokens/sec=1410549.86, grad_norm=2.0436, duration=0.37s
Step 22: loss=9.3627, lr=0.000025, tokens/sec=1409355.65, grad_norm=2.0171, duration=0.37s
Step 23: loss=9.3560, lr=0.000025, tokens/sec=1410788.77, grad_norm=1.9927, duration=0.37s
Step 24: loss=9.2984, lr=0.000025, tokens/sec=1412365.39, grad_norm=2.0233, duration=0.37s
Step 25: loss=9.2734, lr=0.000025, tokens/sec=1413731.93, grad_norm=2.0040, duration=0.37s
Step 26: loss=9.2631, lr=0.000025, tokens/sec=1410755.28, grad_norm=1.9712, duration=0.37s
Step 27: loss=9.2233, lr=0.000025, tokens/sec=1413042.43, grad_norm=1.9751, duration=0.37s
Step 28: loss=9.2032, lr=0.000025, tokens/sec=1413840.09, grad_norm=1.9524, duration=0.37s
Step 29: loss=9.1640, lr=0.000025, tokens/sec=1412395.33, grad_norm=1.9528, duration=0.37s
Step 30: loss=9.2052, lr=0.000026, tokens/sec=1410937.22, grad_norm=1.8558, duration=0.37s
Step 31: loss=9.1415, lr=0.000026, tokens/sec=1412840.88, grad_norm=1.8921, duration=0.37s
Step 32: loss=9.1313, lr=0.000026, tokens/sec=1406290.35, grad_norm=1.8629, duration=0.37s
Step 33: loss=9.1048, lr=0.000026, tokens/sec=1412157.69, grad_norm=1.8865, duration=0.37s
Step 34: loss=9.0190, lr=0.000026, tokens/sec=1412252.92, grad_norm=1.9424, duration=0.37s
Step 35: loss=9.0408, lr=0.000026, tokens/sec=1412022.58, grad_norm=1.8466, duration=0.37s
Step 36: loss=9.0387, lr=0.000026, tokens/sec=1413373.92, grad_norm=1.8217, duration=0.37s
Step 37: loss=8.9801, lr=0.000026, tokens/sec=1409036.87, grad_norm=1.8414, duration=0.37s
Step 38: loss=8.9685, lr=0.000026, tokens/sec=1410001.77, grad_norm=1.8566, duration=0.37s
Step 39: loss=8.9639, lr=0.000027, tokens/sec=1411135.50, grad_norm=1.8100, duration=0.37s
Step 40: loss=8.9446, lr=0.000027, tokens/sec=1407771.31, grad_norm=1.7808, duration=0.37s
Step 41: loss=8.8821, lr=0.000027, tokens/sec=1409567.95, grad_norm=1.8420, duration=0.37s
Step 42: loss=8.8614, lr=0.000027, tokens/sec=1407013.79, grad_norm=1.8119, duration=0.37s
Step 43: loss=8.8578, lr=0.000027, tokens/sec=1407256.90, grad_norm=1.7878, duration=0.37s
Step 44: loss=8.8417, lr=0.000027, tokens/sec=1409243.65, grad_norm=1.7347, duration=0.37s
Step 45: loss=8.8375, lr=0.000027, tokens/sec=1406329.92, grad_norm=1.7016, duration=0.37s
Step 46: loss=8.7955, lr=0.000027, tokens/sec=1404998.30, grad_norm=1.7154, duration=0.37s
Step 47: loss=8.7809, lr=0.000028, tokens/sec=1404892.38, grad_norm=1.7004, duration=0.37s
Step 48: loss=8.7735, lr=0.000028, tokens/sec=1401805.34, grad_norm=1.6647, duration=0.37s
Step 49: loss=8.7190, lr=0.000028, tokens/sec=1405944.19, grad_norm=1.6893, duration=0.37s
Step 50: loss=8.7371, lr=0.000028, tokens/sec=1407153.34, grad_norm=1.6278, duration=0.37s
Step 51: loss=8.6971, lr=0.000028, tokens/sec=1404027.68, grad_norm=1.6305, duration=0.37s
Step 52: loss=8.6761, lr=0.000028, tokens/sec=1407754.19, grad_norm=1.5979, duration=0.37s
Step 53: loss=8.6424, lr=0.000029, tokens/sec=1404351.37, grad_norm=1.5968, duration=0.37s
Step 54: loss=8.6526, lr=0.000029, tokens/sec=1404571.13, grad_norm=1.5463, duration=0.37s
Step 55: loss=8.6232, lr=0.000029, tokens/sec=1403423.74, grad_norm=1.5539, duration=0.37s
Step 56: loss=8.6005, lr=0.000029, tokens/sec=1403239.26, grad_norm=1.5419, duration=0.37s
Step 57: loss=8.5885, lr=0.000029, tokens/sec=1403948.80, grad_norm=1.5206, duration=0.37s
Step 58: loss=8.5489, lr=0.000029, tokens/sec=1403698.76, grad_norm=1.5266, duration=0.37s
Step 59: loss=8.5291, lr=0.000030, tokens/sec=1397351.51, grad_norm=1.5369, duration=0.38s
Step 60: loss=8.5223, lr=0.000030, tokens/sec=1400749.00, grad_norm=1.5183, duration=0.37s
Step 61: loss=8.5238, lr=0.000030, tokens/sec=1402857.91, grad_norm=1.5031, duration=0.37s
Step 62: loss=8.4505, lr=0.000030, tokens/sec=1400069.43, grad_norm=1.5329, duration=0.37s
Step 63: loss=8.4272, lr=0.000030, tokens/sec=1402148.57, grad_norm=1.5048, duration=0.37s
Step 64: loss=8.4018, lr=0.000031, tokens/sec=1397066.54, grad_norm=1.4838, duration=0.38s
Step 65: loss=8.4021, lr=0.000031, tokens/sec=1396566.13, grad_norm=1.4733, duration=0.38s
Step 66: loss=8.3597, lr=0.000031, tokens/sec=1398242.68, grad_norm=1.4456, duration=0.37s
Step 67: loss=8.3605, lr=0.000031, tokens/sec=1396808.31, grad_norm=1.3903, duration=0.38s
Step 68: loss=8.3395, lr=0.000031, tokens/sec=1401063.15, grad_norm=1.3695, duration=0.37s
Step 69: loss=8.2719, lr=0.000032, tokens/sec=1401873.26, grad_norm=1.4107, duration=0.37s
Step 70: loss=8.2391, lr=0.000032, tokens/sec=1398834.16, grad_norm=1.3738, duration=0.37s
Step 71: loss=8.2592, lr=0.000032, tokens/sec=1399565.09, grad_norm=1.3051, duration=0.37s
Step 72: loss=8.2006, lr=0.000032, tokens/sec=1402291.63, grad_norm=1.3177, duration=0.37s
Step 73: loss=8.1803, lr=0.000033, tokens/sec=1398108.44, grad_norm=1.3303, duration=0.37s
Step 74: loss=8.1421, lr=0.000033, tokens/sec=1395664.70, grad_norm=1.3077, duration=0.38s
Step 75: loss=8.1557, lr=0.000033, tokens/sec=1399936.63, grad_norm=1.2984, duration=0.37s
Step 76: loss=8.1040, lr=0.000033, tokens/sec=1397505.14, grad_norm=1.3074, duration=0.38s
Step 77: loss=8.1189, lr=0.000033, tokens/sec=1397688.12, grad_norm=1.2685, duration=0.38s
Step 78: loss=8.0845, lr=0.000034, tokens/sec=1387381.50, grad_norm=1.2655, duration=0.38s
Step 79: loss=8.0518, lr=0.000034, tokens/sec=1396997.32, grad_norm=1.2711, duration=0.38s
Step 80: loss=8.0232, lr=0.000034, tokens/sec=1399319.28, grad_norm=1.2617, duration=0.37s
Step 81: loss=8.0302, lr=0.000034, tokens/sec=1394619.36, grad_norm=1.2122, duration=0.38s
Step 82: loss=7.9857, lr=0.000035, tokens/sec=1395327.29, grad_norm=1.2166, duration=0.38s
Step 83: loss=7.9436, lr=0.000035, tokens/sec=1396785.24, grad_norm=1.2181, duration=0.38s
Step 84: loss=7.9423, lr=0.000035, tokens/sec=1397067.43, grad_norm=1.1742, duration=0.38s
Step 85: loss=7.9071, lr=0.000036, tokens/sec=1397271.60, grad_norm=1.1694, duration=0.38s
Step 86: loss=7.9011, lr=0.000036, tokens/sec=1391924.30, grad_norm=1.1276, duration=0.38s
Step 87: loss=7.8952, lr=0.000036, tokens/sec=1394292.18, grad_norm=1.1175, duration=0.38s
Step 88: loss=7.8816, lr=0.000036, tokens/sec=1397825.83, grad_norm=1.1003, duration=0.38s
Step 89: loss=7.9068, lr=0.000037, tokens/sec=1390112.58, grad_norm=1.0691, duration=0.38s
Step 90: loss=7.8319, lr=0.000037, tokens/sec=1389270.36, grad_norm=1.0709, duration=0.38s
Step 91: loss=7.8208, lr=0.000037, tokens/sec=1392285.63, grad_norm=1.0359, duration=0.38s
Step 92: loss=7.8298, lr=0.000037, tokens/sec=1390905.67, grad_norm=1.0193, duration=0.38s
Step 93: loss=7.8690, lr=0.000038, tokens/sec=1389128.18, grad_norm=0.9713, duration=0.38s
Step 94: loss=7.8650, lr=0.000038, tokens/sec=1396222.97, grad_norm=0.9689, duration=0.38s
Step 95: loss=7.8564, lr=0.000038, tokens/sec=1391004.21, grad_norm=0.9424, duration=0.38s
Step 96: loss=7.8178, lr=0.000039, tokens/sec=1394283.34, grad_norm=0.9379, duration=0.38s
Step 97: loss=7.7682, lr=0.000039, tokens/sec=1394441.60, grad_norm=0.9470, duration=0.38s
Step 98: loss=7.7818, lr=0.000039, tokens/sec=1393336.30, grad_norm=0.9268, duration=0.38s
Step 99: loss=7.7369, lr=0.000040, tokens/sec=1393209.18, grad_norm=0.9008, duration=0.38s
Step 100/19073 (0.5%), Elapsed time: 62.45s, Steps per hour: 5764.17, Estimated hours remaining: 3.29
Step 100: loss=7.7795, lr=0.000040, tokens/sec=1388009.38, grad_norm=0.8221, duration=0.38s
Step 101: loss=7.7193, lr=0.000040, tokens/sec=1387660.78, grad_norm=0.8404, duration=0.38s
Step 102: loss=7.7017, lr=0.000040, tokens/sec=1386934.36, grad_norm=0.8159, duration=0.38s
Step 103: loss=7.6090, lr=0.000041, tokens/sec=1388742.19, grad_norm=0.8400, duration=0.38s
Step 104: loss=7.6791, lr=0.000041, tokens/sec=1388871.12, grad_norm=0.7967, duration=0.38s
Step 105: loss=7.6880, lr=0.000041, tokens/sec=1390654.98, grad_norm=0.7672, duration=0.38s
Step 106: loss=7.6324, lr=0.000042, tokens/sec=1389771.70, grad_norm=0.8048, duration=0.38s
Step 107: loss=7.5943, lr=0.000042, tokens/sec=1387251.09, grad_norm=0.7717, duration=0.38s
Step 108: loss=7.6321, lr=0.000042, tokens/sec=1388884.28, grad_norm=0.6935, duration=0.38s
Step 109: loss=7.6510, lr=0.000043, tokens/sec=1389009.73, grad_norm=0.6509, duration=0.38s
Step 110: loss=7.5785, lr=0.000043, tokens/sec=1387209.96, grad_norm=0.6689, duration=0.38s
Step 111: loss=7.5633, lr=0.000043, tokens/sec=1380339.66, grad_norm=0.6751, duration=0.38s
Step 112: loss=7.5294, lr=0.000044, tokens/sec=1385486.41, grad_norm=0.6842, duration=0.38s
Step 113: loss=7.5788, lr=0.000044, tokens/sec=1384517.28, grad_norm=0.6555, duration=0.38s
Step 114: loss=7.5682, lr=0.000044, tokens/sec=1381217.93, grad_norm=0.6181, duration=0.38s
Step 115: loss=7.5351, lr=0.000045, tokens/sec=1387021.84, grad_norm=0.6119, duration=0.38s
Step 116: loss=7.4632, lr=0.000045, tokens/sec=1382616.10, grad_norm=0.6229, duration=0.38s
Step 117: loss=7.3831, lr=0.000046, tokens/sec=1385571.09, grad_norm=0.6487, duration=0.38s
Step 118: loss=7.4132, lr=0.000046, tokens/sec=1386717.45, grad_norm=0.6299, duration=0.38s
Step 119: loss=7.4519, lr=0.000046, tokens/sec=1387312.35, grad_norm=0.5472, duration=0.38s
Step 120: loss=7.4094, lr=0.000047, tokens/sec=1387029.71, grad_norm=0.5508, duration=0.38s
Step 121: loss=7.3978, lr=0.000047, tokens/sec=1387495.29, grad_norm=0.5329, duration=0.38s
Step 122: loss=7.3802, lr=0.000047, tokens/sec=1383833.33, grad_norm=0.5626, duration=0.38s
Step 123: loss=7.3629, lr=0.000048, tokens/sec=1389711.10, grad_norm=0.5329, duration=0.38s
Step 124: loss=7.3826, lr=0.000048, tokens/sec=1379921.29, grad_norm=0.4823, duration=0.38s
Step 125: loss=7.3662, lr=0.000049, tokens/sec=1383884.71, grad_norm=0.4649, duration=0.38s
Step 126: loss=7.3623, lr=0.000049, tokens/sec=1386275.11, grad_norm=0.4783, duration=0.38s
Step 127: loss=7.3147, lr=0.000049, tokens/sec=1382425.75, grad_norm=0.4860, duration=0.38s
Step 128: loss=7.2863, lr=0.000050, tokens/sec=1385765.81, grad_norm=0.4729, duration=0.38s
Step 129: loss=7.2655, lr=0.000050, tokens/sec=1384786.69, grad_norm=0.4665, duration=0.38s
Step 130: loss=7.3192, lr=0.000050, tokens/sec=1381922.74, grad_norm=0.4214, duration=0.38s
Step 131: loss=7.2768, lr=0.000051, tokens/sec=1384098.99, grad_norm=0.4029, duration=0.38s
Step 132: loss=7.2775, lr=0.000051, tokens/sec=1385695.07, grad_norm=0.4341, duration=0.38s
Step 133: loss=7.1884, lr=0.000052, tokens/sec=1385509.11, grad_norm=0.4388, duration=0.38s
Step 134: loss=7.1856, lr=0.000052, tokens/sec=1383012.62, grad_norm=0.4674, duration=0.38s
Step 135: loss=7.2597, lr=0.000052, tokens/sec=1384902.68, grad_norm=0.3804, duration=0.38s
Step 136: loss=7.2347, lr=0.000053, tokens/sec=1386255.01, grad_norm=0.3944, duration=0.38s
Step 137: loss=7.2508, lr=0.000053, tokens/sec=1383929.13, grad_norm=0.3251, duration=0.38s
Step 138: loss=7.1867, lr=0.000054, tokens/sec=1381652.71, grad_norm=0.3770, duration=0.38s
Step 139: loss=7.2549, lr=0.000054, tokens/sec=1382683.04, grad_norm=0.4050, duration=0.38s
Step 140: loss=7.3457, lr=0.000055, tokens/sec=1384275.86, grad_norm=0.3148, duration=0.38s
Step 141: loss=7.2359, lr=0.000055, tokens/sec=1385636.57, grad_norm=0.3286, duration=0.38s
Step 142: loss=7.3019, lr=0.000055, tokens/sec=1378932.25, grad_norm=0.3484, duration=0.38s
Step 143: loss=7.2354, lr=0.000056, tokens/sec=1353817.43, grad_norm=0.3129, duration=0.39s
Step 144: loss=7.2395, lr=0.000056, tokens/sec=1387828.05, grad_norm=0.3245, duration=0.38s
Step 145: loss=7.2397, lr=0.000057, tokens/sec=1386130.06, grad_norm=0.2883, duration=0.38s
Step 146: loss=7.2365, lr=0.000057, tokens/sec=1383329.30, grad_norm=0.3392, duration=0.38s
Step 147: loss=7.2291, lr=0.000058, tokens/sec=1385798.99, grad_norm=0.2830, duration=0.38s
Step 148: loss=7.2222, lr=0.000058, tokens/sec=1382341.45, grad_norm=0.2928, duration=0.38s
Step 149: loss=7.2252, lr=0.000059, tokens/sec=1383006.53, grad_norm=0.2698, duration=0.38s
Step 150: loss=7.2119, lr=0.000059, tokens/sec=1384275.86, grad_norm=0.2553, duration=0.38s
Step 151: loss=7.1337, lr=0.000059, tokens/sec=1385735.24, grad_norm=0.3034, duration=0.38s
Step 152: loss=7.1500, lr=0.000060, tokens/sec=1386028.71, grad_norm=0.2994, duration=0.38s
Step 153: loss=7.2254, lr=0.000060, tokens/sec=1383647.87, grad_norm=0.3181, duration=0.38s
Step 154: loss=7.1247, lr=0.000061, tokens/sec=1388375.68, grad_norm=0.2898, duration=0.38s
Step 155: loss=7.0844, lr=0.000061, tokens/sec=1384553.02, grad_norm=0.3455, duration=0.38s
Step 156: loss=7.1626, lr=0.000062, tokens/sec=1381957.48, grad_norm=0.2855, duration=0.38s
Step 157: loss=7.1947, lr=0.000062, tokens/sec=1381152.87, grad_norm=0.3389, duration=0.38s
Step 158: loss=7.1676, lr=0.000063, tokens/sec=1382623.06, grad_norm=0.3263, duration=0.38s
Step 159: loss=7.1015, lr=0.000063, tokens/sec=1381633.61, grad_norm=0.2980, duration=0.38s
Step 160: loss=7.0512, lr=0.000064, tokens/sec=1382137.28, grad_norm=0.3163, duration=0.38s
Step 161: loss=7.1213, lr=0.000064, tokens/sec=1386234.91, grad_norm=0.3081, duration=0.38s
Step 162: loss=7.0204, lr=0.000065, tokens/sec=1379935.15, grad_norm=0.3070, duration=0.38s
Step 163: loss=6.9777, lr=0.000065, tokens/sec=1382126.85, grad_norm=0.2491, duration=0.38s
Step 164: loss=7.0234, lr=0.000066, tokens/sec=1378882.97, grad_norm=0.2704, duration=0.38s
Step 165: loss=7.0425, lr=0.000066, tokens/sec=1381526.85, grad_norm=0.3097, duration=0.38s
Step 166: loss=6.8907, lr=0.000067, tokens/sec=1384588.76, grad_norm=0.3809, duration=0.38s
Step 167: loss=6.9758, lr=0.000067, tokens/sec=1377543.25, grad_norm=0.3241, duration=0.38s
Step 168: loss=6.9441, lr=0.000068, tokens/sec=1383424.16, grad_norm=0.2744, duration=0.38s
Step 169: loss=6.9931, lr=0.000068, tokens/sec=1378663.39, grad_norm=0.4328, duration=0.38s
Step 170: loss=6.9552, lr=0.000069, tokens/sec=1379475.49, grad_norm=0.2646, duration=0.38s
Step 171: loss=6.9764, lr=0.000069, tokens/sec=1384835.52, grad_norm=0.3271, duration=0.38s
Step 172: loss=6.9717, lr=0.000070, tokens/sec=1379732.55, grad_norm=0.2706, duration=0.38s
Step 173: loss=6.9065, lr=0.000070, tokens/sec=1381145.06, grad_norm=0.2604, duration=0.38s
Step 174: loss=6.8845, lr=0.000071, tokens/sec=1384247.98, grad_norm=0.2264, duration=0.38s
Step 175: loss=6.9530, lr=0.000071, tokens/sec=1382025.22, grad_norm=0.2770, duration=0.38s
Step 176: loss=6.8892, lr=0.000072, tokens/sec=1380406.38, grad_norm=0.2515, duration=0.38s
Step 177: loss=6.9130, lr=0.000072, tokens/sec=1384099.86, grad_norm=0.2631, duration=0.38s
Step 178: loss=6.8540, lr=0.000073, tokens/sec=1380937.77, grad_norm=0.2601, duration=0.38s
Step 179: loss=6.8310, lr=0.000073, tokens/sec=1379794.88, grad_norm=0.3092, duration=0.38s
Step 180: loss=6.8207, lr=0.000074, tokens/sec=1386718.33, grad_norm=0.2093, duration=0.38s
Step 181: loss=6.8540, lr=0.000074, tokens/sec=1381470.43, grad_norm=0.2631, duration=0.38s
Step 182: loss=6.8403, lr=0.000075, tokens/sec=1382395.33, grad_norm=0.2403, duration=0.38s
Step 183: loss=6.8312, lr=0.000075, tokens/sec=1380956.85, grad_norm=0.2163, duration=0.38s
Step 184: loss=6.8426, lr=0.000076, tokens/sec=1383288.40, grad_norm=0.2756, duration=0.38s
Step 185: loss=6.8903, lr=0.000077, tokens/sec=1381624.06, grad_norm=0.2869, duration=0.38s
Step 186: loss=6.9041, lr=0.000077, tokens/sec=1380554.57, grad_norm=0.2354, duration=0.38s
Step 187: loss=6.9668, lr=0.000078, tokens/sec=1382446.61, grad_norm=0.2885, duration=0.38s
Step 188: loss=6.9212, lr=0.000078, tokens/sec=1384256.69, grad_norm=0.2559, duration=0.38s
Step 189: loss=6.8687, lr=0.000079, tokens/sec=1385257.75, grad_norm=0.2988, duration=0.38s
Step 190: loss=6.9253, lr=0.000079, tokens/sec=1382002.64, grad_norm=0.2962, duration=0.38s
Step 191: loss=6.9347, lr=0.000080, tokens/sec=1380236.56, grad_norm=0.3485, duration=0.38s
Step 192: loss=6.8797, lr=0.000080, tokens/sec=1383488.57, grad_norm=0.2454, duration=0.38s
Step 193: loss=6.8422, lr=0.000081, tokens/sec=1383802.85, grad_norm=0.3259, duration=0.38s
Step 194: loss=6.9004, lr=0.000082, tokens/sec=1382180.71, grad_norm=0.2281, duration=0.38s
Step 195: loss=6.8656, lr=0.000082, tokens/sec=1381997.43, grad_norm=0.3990, duration=0.38s
Step 196: loss=6.9095, lr=0.000083, tokens/sec=1378634.87, grad_norm=0.4043, duration=0.38s
Step 197: loss=6.8810, lr=0.000083, tokens/sec=1383693.14, grad_norm=0.2681, duration=0.38s
Step 198: loss=6.8604, lr=0.000084, tokens/sec=1380738.34, grad_norm=0.4541, duration=0.38s
Step 199: loss=6.7983, lr=0.000084, tokens/sec=1379459.91, grad_norm=0.3118, duration=0.38s
Step 200/19073 (1.0%), Elapsed time: 100.45s, Steps per hour: 7167.74, Estimated hours remaining: 2.63
Step 200: loss=6.8219, lr=0.000085, tokens/sec=1381126.84, grad_norm=0.3648, duration=0.38s
Step 201: loss=6.8219, lr=0.000086, tokens/sec=1385072.77, grad_norm=0.2490, duration=0.38s
Step 202: loss=6.8444, lr=0.000086, tokens/sec=1381868.03, grad_norm=0.4115, duration=0.38s
Step 203: loss=6.7376, lr=0.000087, tokens/sec=1379529.14, grad_norm=0.2981, duration=0.38s
Step 204: loss=6.7304, lr=0.000087, tokens/sec=1384302.87, grad_norm=0.3274, duration=0.38s
Step 205: loss=6.7746, lr=0.000088, tokens/sec=1382105.14, grad_norm=0.4124, duration=0.38s
Step 206: loss=6.7828, lr=0.000089, tokens/sec=1381325.52, grad_norm=0.2465, duration=0.38s
Step 207: loss=6.9406, lr=0.000089, tokens/sec=1380618.71, grad_norm=0.3857, duration=0.38s
Step 208: loss=6.7875, lr=0.000090, tokens/sec=1377394.84, grad_norm=0.3075, duration=0.38s
Step 209: loss=6.9667, lr=0.000090, tokens/sec=1380310.20, grad_norm=0.6411, duration=0.38s
Step 210: loss=6.7713, lr=0.000091, tokens/sec=1381277.79, grad_norm=0.3708, duration=0.38s
Step 211: loss=6.7241, lr=0.000092, tokens/sec=1377815.99, grad_norm=0.3693, duration=0.38s
Step 212: loss=6.7121, lr=0.000092, tokens/sec=1381063.52, grad_norm=0.2965, duration=0.38s
Step 213: loss=6.7396, lr=0.000093, tokens/sec=1381702.19, grad_norm=0.4298, duration=0.38s
Step 214: loss=6.6368, lr=0.000093, tokens/sec=1379505.78, grad_norm=0.3034, duration=0.38s
Step 215: loss=6.5950, lr=0.000094, tokens/sec=1384060.66, grad_norm=0.3739, duration=0.38s
Step 216: loss=6.6438, lr=0.000095, tokens/sec=1377706.36, grad_norm=0.3280, duration=0.38s
Step 217: loss=6.5770, lr=0.000095, tokens/sec=1378970.30, grad_norm=0.3976, duration=0.38s
Step 218: loss=6.5902, lr=0.000096, tokens/sec=1379550.78, grad_norm=0.2924, duration=0.38s
Step 219: loss=6.5605, lr=0.000097, tokens/sec=1382518.74, grad_norm=0.3372, duration=0.38s
Step 220: loss=6.6792, lr=0.000097, tokens/sec=1375271.43, grad_norm=0.2724, duration=0.38s
Step 221: loss=6.6453, lr=0.000098, tokens/sec=1380054.66, grad_norm=0.3896, duration=0.38s
Step 222: loss=6.6109, lr=0.000098, tokens/sec=1378780.09, grad_norm=0.2835, duration=0.38s
Step 223: loss=6.6163, lr=0.000099, tokens/sec=1379435.68, grad_norm=0.3572, duration=0.38s
Step 224: loss=6.5321, lr=0.000100, tokens/sec=1382248.48, grad_norm=0.2948, duration=0.38s
Step 225: loss=6.5290, lr=0.000100, tokens/sec=1379187.38, grad_norm=0.2857, duration=0.38s
Step 226: loss=6.5410, lr=0.000101, tokens/sec=1380128.28, grad_norm=0.4037, duration=0.38s
Step 227: loss=6.5439, lr=0.000102, tokens/sec=1382336.24, grad_norm=0.2717, duration=0.38s
Step 228: loss=6.4867, lr=0.000102, tokens/sec=1378472.40, grad_norm=0.3680, duration=0.38s
Step 229: loss=6.5391, lr=0.000103, tokens/sec=1378033.57, grad_norm=0.2671, duration=0.38s
Step 230: loss=6.5375, lr=0.000104, tokens/sec=1378471.53, grad_norm=0.3896, duration=0.38s
Step 231: loss=6.4860, lr=0.000104, tokens/sec=1377309.43, grad_norm=0.2822, duration=0.38s
Step 232: loss=6.4797, lr=0.000105, tokens/sec=1378109.57, grad_norm=0.5767, duration=0.38s
Step 233: loss=6.5076, lr=0.000106, tokens/sec=1380628.25, grad_norm=0.2693, duration=0.38s
Step 234: loss=6.5183, lr=0.000106, tokens/sec=1381037.50, grad_norm=0.3780, duration=0.38s
Step 235: loss=6.5687, lr=0.000107, tokens/sec=1379819.99, grad_norm=0.3024, duration=0.38s
Step 236: loss=6.4908, lr=0.000108, tokens/sec=1379822.59, grad_norm=0.3317, duration=0.38s
Step 237: loss=6.6159, lr=0.000108, tokens/sec=1374372.36, grad_norm=0.2882, duration=0.38s
Step 238: loss=6.6260, lr=0.000109, tokens/sec=1378957.33, grad_norm=0.3348, duration=0.38s
Step 239: loss=6.5665, lr=0.000110, tokens/sec=1378774.90, grad_norm=0.3265, duration=0.38s
Step 240: loss=6.6126, lr=0.000110, tokens/sec=1384837.27, grad_norm=0.3386, duration=0.38s
Step 241: loss=6.6038, lr=0.000111, tokens/sec=1379061.10, grad_norm=0.2913, duration=0.38s
Step 242: loss=6.5944, lr=0.000112, tokens/sec=1378540.67, grad_norm=0.2581, duration=0.38s
Step 243: loss=6.6082, lr=0.000112, tokens/sec=1377053.27, grad_norm=0.3171, duration=0.38s
Step 244: loss=6.5534, lr=0.000113, tokens/sec=1382231.97, grad_norm=0.2996, duration=0.38s
Step 245: loss=6.5924, lr=0.000114, tokens/sec=1377431.07, grad_norm=0.3487, duration=0.38s
Step 246: loss=6.5556, lr=0.000114, tokens/sec=1380093.63, grad_norm=0.3034, duration=0.38s
Step 247: loss=6.6030, lr=0.000115, tokens/sec=1379871.07, grad_norm=0.2784, duration=0.38s
Step 248: loss=6.5026, lr=0.000116, tokens/sec=1383627.84, grad_norm=0.2734, duration=0.38s
Step 249: loss=6.4770, lr=0.000116, tokens/sec=1379721.30, grad_norm=0.2625, duration=0.38s
Validation loss at step 250: 6.4884233474731445
Step 250: loss=6.5616, lr=0.000117, tokens/sec=153935.74, grad_norm=0.2581, duration=3.41s
Step 251: loss=6.5294, lr=0.000118, tokens/sec=1381344.60, grad_norm=0.2488, duration=0.38s
Step 252: loss=6.4798, lr=0.000119, tokens/sec=1378679.81, grad_norm=0.2628, duration=0.38s
Step 253: loss=6.4880, lr=0.000119, tokens/sec=1378864.81, grad_norm=0.2925, duration=0.38s
Step 254: loss=6.4898, lr=0.000120, tokens/sec=1383642.64, grad_norm=0.2530, duration=0.38s
Step 255: loss=6.5187, lr=0.000121, tokens/sec=1382398.81, grad_norm=0.2735, duration=0.38s
Step 256: loss=6.4596, lr=0.000121, tokens/sec=1377463.00, grad_norm=0.3218, duration=0.38s
Step 257: loss=6.5140, lr=0.000122, tokens/sec=1381466.09, grad_norm=0.2990, duration=0.38s
Step 258: loss=6.4780, lr=0.000123, tokens/sec=1378204.58, grad_norm=0.2663, duration=0.38s
Step 259: loss=6.3900, lr=0.000124, tokens/sec=1381060.05, grad_norm=0.3120, duration=0.38s
Step 260: loss=6.4326, lr=0.000124, tokens/sec=1379961.13, grad_norm=0.2588, duration=0.38s
Step 261: loss=6.4042, lr=0.000125, tokens/sec=1379406.26, grad_norm=0.2507, duration=0.38s
Step 262: loss=6.4320, lr=0.000126, tokens/sec=1381478.24, grad_norm=0.3809, duration=0.38s
Step 263: loss=6.4053, lr=0.000126, tokens/sec=1378617.58, grad_norm=0.4148, duration=0.38s
Step 264: loss=6.3526, lr=0.000127, tokens/sec=1377576.90, grad_norm=0.3312, duration=0.38s
Step 265: loss=6.4310, lr=0.000128, tokens/sec=1379604.44, grad_norm=0.3858, duration=0.38s
Step 266: loss=6.3543, lr=0.000129, tokens/sec=1380836.31, grad_norm=0.3201, duration=0.38s
Step 267: loss=6.3848, lr=0.000129, tokens/sec=1378359.21, grad_norm=0.3549, duration=0.38s
Step 268: loss=6.3642, lr=0.000130, tokens/sec=1377664.93, grad_norm=0.3224, duration=0.38s
Step 269: loss=6.3738, lr=0.000131, tokens/sec=1374395.55, grad_norm=0.3221, duration=0.38s
Step 270: loss=6.3649, lr=0.000132, tokens/sec=1377343.07, grad_norm=0.3771, duration=0.38s
Step 271: loss=6.3236, lr=0.000132, tokens/sec=1378502.64, grad_norm=0.3162, duration=0.38s
Step 272: loss=6.3115, lr=0.000133, tokens/sec=1379638.20, grad_norm=0.3128, duration=0.38s
Step 273: loss=6.3144, lr=0.000134, tokens/sec=1375732.59, grad_norm=0.2969, duration=0.38s
Step 274: loss=6.2854, lr=0.000135, tokens/sec=1380599.64, grad_norm=0.2842, duration=0.38s
Step 275: loss=6.2840, lr=0.000135, tokens/sec=1374588.85, grad_norm=0.2824, duration=0.38s
Step 276: loss=6.2793, lr=0.000136, tokens/sec=1373107.40, grad_norm=0.2780, duration=0.38s
Step 277: loss=6.2979, lr=0.000137, tokens/sec=1375040.10, grad_norm=0.3032, duration=0.38s
Step 278: loss=6.3515, lr=0.000138, tokens/sec=1379001.43, grad_norm=0.2525, duration=0.38s
Step 279: loss=6.2996, lr=0.000138, tokens/sec=1375889.25, grad_norm=0.3358, duration=0.38s
Step 280: loss=6.2855, lr=0.000139, tokens/sec=1378974.63, grad_norm=0.3473, duration=0.38s
Step 281: loss=6.3199, lr=0.000140, tokens/sec=1378378.22, grad_norm=0.4400, duration=0.38s
Step 282: loss=6.3852, lr=0.000141, tokens/sec=1373157.13, grad_norm=0.4551, duration=0.38s
Step 283: loss=6.4210, lr=0.000141, tokens/sec=1378704.88, grad_norm=0.4313, duration=0.38s
Step 284: loss=6.4182, lr=0.000142, tokens/sec=1375591.46, grad_norm=0.3453, duration=0.38s
Step 285: loss=6.4404, lr=0.000143, tokens/sec=1380341.39, grad_norm=0.3742, duration=0.38s
Step 286: loss=6.3900, lr=0.000144, tokens/sec=1380149.07, grad_norm=0.3945, duration=0.38s
Step 287: loss=6.3911, lr=0.000144, tokens/sec=1376986.01, grad_norm=0.5887, duration=0.38s
Step 288: loss=6.4075, lr=0.000145, tokens/sec=1377851.39, grad_norm=0.5771, duration=0.38s
Step 289: loss=6.3963, lr=0.000146, tokens/sec=1380084.11, grad_norm=0.3513, duration=0.38s
Step 290: loss=6.4024, lr=0.000147, tokens/sec=1379605.31, grad_norm=0.3322, duration=0.38s
Step 291: loss=6.3729, lr=0.000148, tokens/sec=1380350.06, grad_norm=0.5741, duration=0.38s
Step 292: loss=6.3340, lr=0.000148, tokens/sec=1383270.13, grad_norm=0.3680, duration=0.38s
Step 293: loss=6.2607, lr=0.000149, tokens/sec=1382858.68, grad_norm=0.4007, duration=0.38s
Step 294: loss=6.3714, lr=0.000150, tokens/sec=1380130.88, grad_norm=0.3512, duration=0.38s
Step 295: loss=6.3423, lr=0.000151, tokens/sec=1381680.49, grad_norm=0.3003, duration=0.38s
Step 296: loss=6.3267, lr=0.000151, tokens/sec=1375723.99, grad_norm=0.3520, duration=0.38s
Step 297: loss=6.2785, lr=0.000152, tokens/sec=1379306.76, grad_norm=0.2930, duration=0.38s
Step 298: loss=6.3373, lr=0.000153, tokens/sec=1376371.51, grad_norm=0.2897, duration=0.38s
Step 299: loss=6.3795, lr=0.000154, tokens/sec=1378763.66, grad_norm=0.3711, duration=0.38s
Step 300/19073 (1.6%), Elapsed time: 141.57s, Steps per hour: 7628.65, Estimated hours remaining: 2.46
Step 300: loss=6.2966, lr=0.000155, tokens/sec=1381646.63, grad_norm=0.3168, duration=0.38s
Step 301: loss=6.2915, lr=0.000155, tokens/sec=1377153.31, grad_norm=0.3776, duration=0.38s
Step 302: loss=6.2938, lr=0.000156, tokens/sec=1378694.51, grad_norm=0.3343, duration=0.38s
Step 303: loss=6.2923, lr=0.000157, tokens/sec=1381899.29, grad_norm=0.3396, duration=0.38s
Step 304: loss=6.3590, lr=0.000158, tokens/sec=1380996.74, grad_norm=0.3118, duration=0.38s
Step 305: loss=6.2813, lr=0.000159, tokens/sec=1379715.24, grad_norm=0.3389, duration=0.38s
Step 306: loss=6.2252, lr=0.000159, tokens/sec=1375169.94, grad_norm=0.3818, duration=0.38s
Step 307: loss=6.1971, lr=0.000160, tokens/sec=1376323.27, grad_norm=0.3110, duration=0.38s
Step 308: loss=6.2457, lr=0.000161, tokens/sec=1381832.43, grad_norm=0.3157, duration=0.38s
Step 309: loss=6.2378, lr=0.000162, tokens/sec=1376631.72, grad_norm=0.2956, duration=0.38s
Step 310: loss=6.2078, lr=0.000163, tokens/sec=1374221.19, grad_norm=0.3430, duration=0.38s
Step 311: loss=6.2106, lr=0.000163, tokens/sec=1373693.24, grad_norm=0.3411, duration=0.38s
Step 312: loss=6.1561, lr=0.000164, tokens/sec=1377766.79, grad_norm=0.3302, duration=0.38s
Step 313: loss=6.2516, lr=0.000165, tokens/sec=1370995.46, grad_norm=0.4204, duration=0.38s
Step 314: loss=6.2026, lr=0.000166, tokens/sec=1378874.32, grad_norm=0.3470, duration=0.38s
Step 315: loss=6.2188, lr=0.000167, tokens/sec=1377588.99, grad_norm=0.3727, duration=0.38s
Step 316: loss=6.2101, lr=0.000168, tokens/sec=1384841.63, grad_norm=0.3137, duration=0.38s
Step 317: loss=6.1451, lr=0.000168, tokens/sec=1376214.74, grad_norm=0.3759, duration=0.38s
Step 318: loss=6.1395, lr=0.000169, tokens/sec=1379240.15, grad_norm=0.3067, duration=0.38s
Step 319: loss=6.1473, lr=0.000170, tokens/sec=1378094.02, grad_norm=0.2773, duration=0.38s
Step 320: loss=6.1829, lr=0.000171, tokens/sec=1381539.87, grad_norm=0.3963, duration=0.38s
Step 321: loss=6.1670, lr=0.000172, tokens/sec=1381391.46, grad_norm=0.5261, duration=0.38s
Step 322: loss=6.1409, lr=0.000173, tokens/sec=1377139.51, grad_norm=0.4897, duration=0.38s
Step 323: loss=6.0755, lr=0.000173, tokens/sec=1377707.23, grad_norm=0.3735, duration=0.38s
Step 324: loss=6.1216, lr=0.000174, tokens/sec=1382242.40, grad_norm=0.4162, duration=0.38s
Step 325: loss=6.1516, lr=0.000175, tokens/sec=1378649.56, grad_norm=0.5190, duration=0.38s
Step 326: loss=6.1184, lr=0.000176, tokens/sec=1375977.93, grad_norm=0.3225, duration=0.38s
Step 327: loss=6.1436, lr=0.000177, tokens/sec=1377125.71, grad_norm=0.4347, duration=0.38s
Step 328: loss=6.1655, lr=0.000178, tokens/sec=1378691.05, grad_norm=0.4248, duration=0.38s
Step 329: loss=6.2003, lr=0.000178, tokens/sec=1379802.67, grad_norm=0.3937, duration=0.38s
Step 330: loss=6.3008, lr=0.000179, tokens/sec=1381466.09, grad_norm=0.4376, duration=0.38s
Step 331: loss=6.1823, lr=0.000180, tokens/sec=1376954.97, grad_norm=0.4760, duration=0.38s
Step 332: loss=6.2584, lr=0.000181, tokens/sec=1378486.22, grad_norm=0.4692, duration=0.38s
Step 333: loss=6.2142, lr=0.000182, tokens/sec=1380225.30, grad_norm=0.5725, duration=0.38s
Step 334: loss=6.1941, lr=0.000183, tokens/sec=1382504.84, grad_norm=0.4380, duration=0.38s
Step 335: loss=6.2284, lr=0.000183, tokens/sec=1377911.82, grad_norm=0.4567, duration=0.38s
Step 336: loss=6.2124, lr=0.000184, tokens/sec=1379114.73, grad_norm=0.3171, duration=0.38s
Step 337: loss=6.2280, lr=0.000185, tokens/sec=1376987.74, grad_norm=0.3924, duration=0.38s
Step 338: loss=6.2530, lr=0.000186, tokens/sec=1374756.42, grad_norm=0.3110, duration=0.38s
Step 339: loss=6.1943, lr=0.000187, tokens/sec=1381743.87, grad_norm=0.3505, duration=0.38s
Step 340: loss=6.2015, lr=0.000188, tokens/sec=1376206.13, grad_norm=0.2966, duration=0.38s
Step 341: loss=6.1363, lr=0.000189, tokens/sec=1377929.95, grad_norm=0.3180, duration=0.38s
Step 342: loss=6.1695, lr=0.000189, tokens/sec=1375928.86, grad_norm=0.3389, duration=0.38s
Step 343: loss=6.1987, lr=0.000190, tokens/sec=1378441.29, grad_norm=0.2949, duration=0.38s
Step 344: loss=6.1220, lr=0.000191, tokens/sec=1378551.90, grad_norm=0.4626, duration=0.38s
Step 345: loss=6.1169, lr=0.000192, tokens/sec=1375652.56, grad_norm=0.5096, duration=0.38s
Step 346: loss=6.1885, lr=0.000193, tokens/sec=1378316.88, grad_norm=0.3986, duration=0.38s
Step 347: loss=6.2098, lr=0.000194, tokens/sec=1374508.95, grad_norm=0.3676, duration=0.38s
Step 348: loss=6.2196, lr=0.000195, tokens/sec=1379208.14, grad_norm=0.3453, duration=0.38s
Step 349: loss=6.1256, lr=0.000195, tokens/sec=1378103.52, grad_norm=0.4220, duration=0.38s
Step 350: loss=6.1049, lr=0.000196, tokens/sec=1378624.50, grad_norm=0.3934, duration=0.38s
Step 351: loss=6.1666, lr=0.000197, tokens/sec=1378463.76, grad_norm=0.3808, duration=0.38s
Step 352: loss=6.0761, lr=0.000198, tokens/sec=1379745.53, grad_norm=0.3854, duration=0.38s
Step 353: loss=6.0502, lr=0.000199, tokens/sec=1374896.53, grad_norm=0.3281, duration=0.38s
Step 354: loss=6.0653, lr=0.000200, tokens/sec=1380841.52, grad_norm=0.3863, duration=0.38s
Step 355: loss=6.0548, lr=0.000201, tokens/sec=1382032.17, grad_norm=0.3354, duration=0.38s
Step 356: loss=6.0025, lr=0.000202, tokens/sec=1374413.59, grad_norm=0.3835, duration=0.38s
Step 357: loss=6.0530, lr=0.000202, tokens/sec=1377751.25, grad_norm=0.3199, duration=0.38s
Step 358: loss=6.0632, lr=0.000203, tokens/sec=1374037.44, grad_norm=0.3687, duration=0.38s
Step 359: loss=6.0522, lr=0.000204, tokens/sec=1376728.25, grad_norm=0.4194, duration=0.38s
Step 360: loss=6.0486, lr=0.000205, tokens/sec=1375463.26, grad_norm=0.4667, duration=0.38s
Step 361: loss=6.0661, lr=0.000206, tokens/sec=1376911.87, grad_norm=0.4778, duration=0.38s
Step 362: loss=6.0442, lr=0.000207, tokens/sec=1379038.62, grad_norm=0.5805, duration=0.38s
Step 363: loss=6.0133, lr=0.000208, tokens/sec=1379388.09, grad_norm=0.4108, duration=0.38s
Step 364: loss=5.9771, lr=0.000209, tokens/sec=1376128.62, grad_norm=0.4534, duration=0.38s
Step 365: loss=6.0282, lr=0.000210, tokens/sec=1374687.67, grad_norm=0.4494, duration=0.38s
Step 366: loss=5.9941, lr=0.000210, tokens/sec=1375133.83, grad_norm=0.4896, duration=0.38s
Step 367: loss=6.0014, lr=0.000211, tokens/sec=1375589.74, grad_norm=0.5436, duration=0.38s
Step 368: loss=5.9601, lr=0.000212, tokens/sec=1372584.59, grad_norm=0.4037, duration=0.38s
Step 369: loss=5.9574, lr=0.000213, tokens/sec=1375306.69, grad_norm=0.4231, duration=0.38s
Step 370: loss=5.9357, lr=0.000214, tokens/sec=1374345.73, grad_norm=0.4566, duration=0.38s
Step 371: loss=5.9802, lr=0.000215, tokens/sec=1379113.86, grad_norm=0.4850, duration=0.38s
Step 372: loss=5.9691, lr=0.000216, tokens/sec=1377074.83, grad_norm=0.5618, duration=0.38s
Step 373: loss=5.9720, lr=0.000217, tokens/sec=1377810.81, grad_norm=0.4513, duration=0.38s
Step 374: loss=6.0134, lr=0.000218, tokens/sec=1375506.27, grad_norm=0.4911, duration=0.38s
Step 375: loss=6.0447, lr=0.000218, tokens/sec=1373948.15, grad_norm=0.3765, duration=0.38s
Step 376: loss=6.0673, lr=0.000219, tokens/sec=1377842.75, grad_norm=0.4769, duration=0.38s
Step 377: loss=6.1480, lr=0.000220, tokens/sec=1375415.08, grad_norm=0.3716, duration=0.38s
Step 378: loss=6.0530, lr=0.000221, tokens/sec=1377264.57, grad_norm=0.5349, duration=0.38s
Step 379: loss=6.0625, lr=0.000222, tokens/sec=1378582.15, grad_norm=0.5312, duration=0.38s
Step 380: loss=6.0901, lr=0.000223, tokens/sec=1378054.30, grad_norm=0.5927, duration=0.38s
Step 381: loss=6.1164, lr=0.000224, tokens/sec=1374952.41, grad_norm=0.5774, duration=0.38s
Step 382: loss=6.0345, lr=0.000225, tokens/sec=1380506.04, grad_norm=0.4559, duration=0.38s
Step 383: loss=6.1006, lr=0.000226, tokens/sec=1373815.97, grad_norm=0.4724, duration=0.38s
Step 384: loss=6.0671, lr=0.000227, tokens/sec=1372646.27, grad_norm=0.4841, duration=0.38s
Step 385: loss=6.0687, lr=0.000227, tokens/sec=1375275.73, grad_norm=0.4163, duration=0.38s
Step 386: loss=6.1113, lr=0.000228, tokens/sec=1377120.54, grad_norm=0.4526, duration=0.38s
Step 387: loss=6.0686, lr=0.000229, tokens/sec=1372930.80, grad_norm=0.4145, duration=0.38s
Step 388: loss=6.0694, lr=0.000230, tokens/sec=1375810.92, grad_norm=0.3989, duration=0.38s
Step 389: loss=6.0200, lr=0.000231, tokens/sec=1381444.40, grad_norm=0.4234, duration=0.38s
Step 390: loss=6.0743, lr=0.000232, tokens/sec=1376153.59, grad_norm=0.3554, duration=0.38s
Step 391: loss=6.0308, lr=0.000233, tokens/sec=1377881.60, grad_norm=0.3870, duration=0.38s
Step 392: loss=5.9897, lr=0.000234, tokens/sec=1378798.24, grad_norm=0.4332, duration=0.38s
Step 393: loss=6.0101, lr=0.000235, tokens/sec=1371624.00, grad_norm=0.3626, duration=0.38s
Step 394: loss=5.9844, lr=0.000236, tokens/sec=1379049.00, grad_norm=0.3867, duration=0.38s
Step 395: loss=6.0055, lr=0.000237, tokens/sec=1375243.04, grad_norm=0.4934, duration=0.38s
Step 396: loss=6.1807, lr=0.000238, tokens/sec=1376925.66, grad_norm=0.7041, duration=0.38s
Step 397: loss=6.0145, lr=0.000238, tokens/sec=1377277.51, grad_norm=0.9096, duration=0.38s
Step 398: loss=6.0505, lr=0.000239, tokens/sec=1379273.89, grad_norm=0.5574, duration=0.38s
Step 399: loss=5.9955, lr=0.000240, tokens/sec=1377602.79, grad_norm=0.8219, duration=0.38s
Step 400/19073 (2.1%), Elapsed time: 179.73s, Steps per hour: 8011.83, Estimated hours remaining: 2.33
Step 400: loss=6.0077, lr=0.000241, tokens/sec=1376900.66, grad_norm=0.5773, duration=0.38s
Step 401: loss=5.9691, lr=0.000242, tokens/sec=1376262.11, grad_norm=0.5884, duration=0.38s
Step 402: loss=5.9750, lr=0.000243, tokens/sec=1376164.79, grad_norm=0.5869, duration=0.38s
Step 403: loss=5.9800, lr=0.000244, tokens/sec=1377180.05, grad_norm=0.5852, duration=0.38s
Step 404: loss=5.9333, lr=0.000245, tokens/sec=1374663.61, grad_norm=0.5916, duration=0.38s
Step 405: loss=5.8966, lr=0.000246, tokens/sec=1381318.57, grad_norm=0.6321, duration=0.38s
Step 406: loss=5.9055, lr=0.000247, tokens/sec=1378375.62, grad_norm=0.5603, duration=0.38s
Step 407: loss=5.8577, lr=0.000248, tokens/sec=1377816.85, grad_norm=0.5856, duration=0.38s
Step 408: loss=5.8952, lr=0.000249, tokens/sec=1373896.65, grad_norm=0.4850, duration=0.38s
Step 409: loss=5.8519, lr=0.000250, tokens/sec=1375740.34, grad_norm=0.5039, duration=0.38s
Step 410: loss=5.9809, lr=0.000250, tokens/sec=1380065.05, grad_norm=0.5530, duration=0.38s
Step 411: loss=5.9131, lr=0.000251, tokens/sec=1377180.91, grad_norm=0.6991, duration=0.38s
Step 412: loss=5.8827, lr=0.000252, tokens/sec=1375407.34, grad_norm=0.6774, duration=0.38s
Step 413: loss=5.9359, lr=0.000253, tokens/sec=1377961.90, grad_norm=0.8167, duration=0.38s
Step 414: loss=5.8403, lr=0.000254, tokens/sec=1375782.51, grad_norm=0.5346, duration=0.38s
Step 415: loss=5.7963, lr=0.000255, tokens/sec=1379940.34, grad_norm=0.4598, duration=0.38s
Step 416: loss=5.8921, lr=0.000256, tokens/sec=1377183.50, grad_norm=0.4397, duration=0.38s
Step 417: loss=5.8241, lr=0.000257, tokens/sec=1377717.58, grad_norm=0.5105, duration=0.38s
Step 418: loss=5.7714, lr=0.000258, tokens/sec=1379172.68, grad_norm=0.4703, duration=0.38s
Step 419: loss=5.8406, lr=0.000259, tokens/sec=1377387.07, grad_norm=0.4500, duration=0.38s
Step 420: loss=5.8152, lr=0.000260, tokens/sec=1380884.00, grad_norm=0.4374, duration=0.38s
Step 421: loss=5.7936, lr=0.000261, tokens/sec=1377892.83, grad_norm=0.5426, duration=0.38s
Step 422: loss=5.7661, lr=0.000262, tokens/sec=1378786.14, grad_norm=0.4617, duration=0.38s
Step 423: loss=5.8380, lr=0.000263, tokens/sec=1379808.73, grad_norm=0.4747, duration=0.38s
Step 424: loss=5.8755, lr=0.000263, tokens/sec=1376367.20, grad_norm=0.4836, duration=0.38s
Step 425: loss=5.8428, lr=0.000264, tokens/sec=1379559.43, grad_norm=0.4571, duration=0.38s
Step 426: loss=5.8323, lr=0.000265, tokens/sec=1378044.80, grad_norm=0.4828, duration=0.38s
Step 427: loss=5.9492, lr=0.000266, tokens/sec=1380112.69, grad_norm=0.5322, duration=0.38s
Step 428: loss=5.9553, lr=0.000267, tokens/sec=1379590.59, grad_norm=0.7700, duration=0.38s
Step 429: loss=5.9038, lr=0.000268, tokens/sec=1381728.24, grad_norm=0.9526, duration=0.38s
Step 430: loss=5.9390, lr=0.000269, tokens/sec=1377585.53, grad_norm=0.5483, duration=0.38s
Step 431: loss=5.9212, lr=0.000270, tokens/sec=1376727.39, grad_norm=0.6597, duration=0.38s
Step 432: loss=5.9847, lr=0.000271, tokens/sec=1375598.34, grad_norm=0.5952, duration=0.38s
Step 433: loss=5.9121, lr=0.000272, tokens/sec=1377489.75, grad_norm=0.6978, duration=0.38s
Step 434: loss=5.8741, lr=0.000273, tokens/sec=1377288.73, grad_norm=0.7154, duration=0.38s
Step 435: loss=5.9033, lr=0.000274, tokens/sec=1375989.12, grad_norm=0.6161, duration=0.38s
Step 436: loss=5.8956, lr=0.000275, tokens/sec=1375410.78, grad_norm=0.5613, duration=0.38s
Step 437: loss=5.9117, lr=0.000276, tokens/sec=1376757.56, grad_norm=0.5715, duration=0.38s
Step 438: loss=5.8181, lr=0.000277, tokens/sec=1378997.11, grad_norm=0.5551, duration=0.38s
Step 439: loss=5.8565, lr=0.000278, tokens/sec=1378890.75, grad_norm=0.7461, duration=0.38s
Step 440: loss=5.8702, lr=0.000279, tokens/sec=1382161.60, grad_norm=0.6941, duration=0.38s
Step 441: loss=5.8374, lr=0.000279, tokens/sec=1378358.35, grad_norm=0.6791, duration=0.38s
Step 442: loss=5.8336, lr=0.000280, tokens/sec=1378193.35, grad_norm=0.6098, duration=0.38s
Step 443: loss=5.8483, lr=0.000281, tokens/sec=1377690.83, grad_norm=0.5184, duration=0.38s
Step 444: loss=5.8292, lr=0.000282, tokens/sec=1379232.36, grad_norm=0.5161, duration=0.38s
Step 445: loss=5.8612, lr=0.000283, tokens/sec=1381174.55, grad_norm=0.4794, duration=0.38s
Step 446: loss=5.8149, lr=0.000284, tokens/sec=1380042.53, grad_norm=0.4994, duration=0.38s
Step 447: loss=5.8479, lr=0.000285, tokens/sec=1377923.05, grad_norm=0.4705, duration=0.38s
Step 448: loss=5.8061, lr=0.000286, tokens/sec=1376400.80, grad_norm=0.5006, duration=0.38s
Step 449: loss=5.7663, lr=0.000287, tokens/sec=1379166.62, grad_norm=0.6571, duration=0.38s
Step 450: loss=5.7623, lr=0.000288, tokens/sec=1381660.52, grad_norm=0.7595, duration=0.38s
Step 451: loss=5.7942, lr=0.000289, tokens/sec=1378659.93, grad_norm=0.9739, duration=0.38s
Step 452: loss=5.7658, lr=0.000290, tokens/sec=1379414.92, grad_norm=0.7034, duration=0.38s
Step 453: loss=5.7674, lr=0.000291, tokens/sec=1377615.74, grad_norm=0.7249, duration=0.38s
Step 454: loss=5.7444, lr=0.000292, tokens/sec=1379132.02, grad_norm=0.8628, duration=0.38s
Step 455: loss=5.7805, lr=0.000293, tokens/sec=1377869.52, grad_norm=0.5188, duration=0.38s
Step 456: loss=5.6959, lr=0.000294, tokens/sec=1377705.50, grad_norm=0.5190, duration=0.38s
Step 457: loss=5.7147, lr=0.000295, tokens/sec=1379898.78, grad_norm=0.5806, duration=0.38s
Step 458: loss=5.7523, lr=0.000296, tokens/sec=1378646.97, grad_norm=0.5691, duration=0.38s
Step 459: loss=5.7432, lr=0.000297, tokens/sec=1381604.10, grad_norm=0.5540, duration=0.38s
Step 460: loss=5.7043, lr=0.000297, tokens/sec=1378191.62, grad_norm=0.6277, duration=0.38s
Step 461: loss=5.6679, lr=0.000298, tokens/sec=1381351.55, grad_norm=0.7965, duration=0.38s
Step 462: loss=5.7099, lr=0.000299, tokens/sec=1381540.73, grad_norm=0.7623, duration=0.38s
Step 463: loss=5.6527, lr=0.000300, tokens/sec=1382594.37, grad_norm=0.5859, duration=0.38s
Step 464: loss=5.6430, lr=0.000301, tokens/sec=1378808.62, grad_norm=0.4784, duration=0.38s
Step 465: loss=5.6505, lr=0.000302, tokens/sec=1379844.23, grad_norm=0.6781, duration=0.38s
Step 466: loss=5.6232, lr=0.000303, tokens/sec=1383001.31, grad_norm=0.5396, duration=0.38s
Step 467: loss=5.6996, lr=0.000304, tokens/sec=1375684.40, grad_norm=0.4216, duration=0.38s
Step 468: loss=5.6528, lr=0.000305, tokens/sec=1376018.40, grad_norm=0.5068, duration=0.38s
Step 469: loss=5.6374, lr=0.000306, tokens/sec=1378376.49, grad_norm=0.4073, duration=0.38s
Step 470: loss=5.6579, lr=0.000307, tokens/sec=1377068.79, grad_norm=0.4644, duration=0.38s
Step 471: loss=5.7139, lr=0.000308, tokens/sec=1379349.16, grad_norm=0.5515, duration=0.38s
Step 472: loss=5.7362, lr=0.000309, tokens/sec=1379841.63, grad_norm=0.7845, duration=0.38s
Step 473: loss=5.7779, lr=0.000310, tokens/sec=1380143.87, grad_norm=1.1249, duration=0.38s
Step 474: loss=5.7994, lr=0.000311, tokens/sec=1379293.79, grad_norm=0.9475, duration=0.38s
Step 475: loss=5.7957, lr=0.000312, tokens/sec=1378603.75, grad_norm=0.7523, duration=0.38s
Step 476: loss=5.7834, lr=0.000313, tokens/sec=1376822.21, grad_norm=0.7976, duration=0.38s
Step 477: loss=5.7611, lr=0.000314, tokens/sec=1379416.65, grad_norm=0.8699, duration=0.38s
Step 478: loss=5.8041, lr=0.000315, tokens/sec=1378137.21, grad_norm=1.1447, duration=0.38s
Step 479: loss=5.7738, lr=0.000315, tokens/sec=1378461.16, grad_norm=0.8210, duration=0.38s
Step 480: loss=5.7824, lr=0.000316, tokens/sec=1380665.52, grad_norm=0.6385, duration=0.38s
Step 481: loss=5.7372, lr=0.000317, tokens/sec=1380862.33, grad_norm=0.6709, duration=0.38s
Step 482: loss=5.7217, lr=0.000318, tokens/sec=1379194.30, grad_norm=0.5944, duration=0.38s
Step 483: loss=5.6483, lr=0.000319, tokens/sec=1379324.07, grad_norm=0.5342, duration=0.38s
Step 484: loss=5.7437, lr=0.000320, tokens/sec=1379880.60, grad_norm=0.5817, duration=0.38s
Step 485: loss=5.7078, lr=0.000321, tokens/sec=1378619.31, grad_norm=0.5746, duration=0.38s
Step 486: loss=5.6831, lr=0.000322, tokens/sec=1380318.87, grad_norm=0.5016, duration=0.38s
Step 487: loss=5.6490, lr=0.000323, tokens/sec=1375516.60, grad_norm=0.5198, duration=0.38s
Step 488: loss=5.7261, lr=0.000324, tokens/sec=1378828.50, grad_norm=0.5092, duration=0.38s
Step 489: loss=5.7082, lr=0.000325, tokens/sec=1376154.45, grad_norm=0.6655, duration=0.38s
Step 490: loss=5.6712, lr=0.000326, tokens/sec=1381101.69, grad_norm=0.5855, duration=0.38s
Step 491: loss=5.6608, lr=0.000327, tokens/sec=1377305.12, grad_norm=0.5559, duration=0.38s
Step 492: loss=5.6507, lr=0.000328, tokens/sec=1379545.59, grad_norm=0.5559, duration=0.38s
Step 493: loss=5.6596, lr=0.000329, tokens/sec=1382674.35, grad_norm=0.5348, duration=0.38s
Step 494: loss=5.7047, lr=0.000330, tokens/sec=1372620.57, grad_norm=0.6698, duration=0.38s
Step 495: loss=5.6563, lr=0.000331, tokens/sec=1379440.01, grad_norm=0.8398, duration=0.38s
Step 496: loss=5.6466, lr=0.000332, tokens/sec=1381331.59, grad_norm=0.9334, duration=0.38s
Step 497: loss=5.6227, lr=0.000333, tokens/sec=1379202.95, grad_norm=0.7760, duration=0.38s
Step 498: loss=5.6364, lr=0.000334, tokens/sec=1380035.61, grad_norm=0.6683, duration=0.38s
Step 499: loss=5.6070, lr=0.000334, tokens/sec=1380565.84, grad_norm=0.7964, duration=0.38s
Step 500/19073 (2.6%), Elapsed time: 217.85s, Steps per hour: 8262.61, Estimated hours remaining: 2.25
Validation loss at step 500: 5.62709379196167
Step 500: loss=5.6154, lr=0.000335, tokens/sec=157459.63, grad_norm=0.8370, duration=3.33s
Step 501: loss=5.5447, lr=0.000336, tokens/sec=1378330.70, grad_norm=0.7660, duration=0.38s
Step 502: loss=5.5940, lr=0.000337, tokens/sec=1379472.03, grad_norm=0.8037, duration=0.38s
Step 503: loss=5.6097, lr=0.000338, tokens/sec=1379122.51, grad_norm=0.7171, duration=0.38s
Step 504: loss=5.5886, lr=0.000339, tokens/sec=1379543.86, grad_norm=0.6129, duration=0.38s
Step 505: loss=5.6088, lr=0.000340, tokens/sec=1379093.97, grad_norm=0.6715, duration=0.38s
Step 506: loss=5.5636, lr=0.000341, tokens/sec=1380421.98, grad_norm=0.8662, duration=0.38s
Step 507: loss=5.5474, lr=0.000342, tokens/sec=1376205.27, grad_norm=0.8412, duration=0.38s
Step 508: loss=5.5493, lr=0.000343, tokens/sec=1379190.84, grad_norm=0.8881, duration=0.38s
Step 509: loss=5.5173, lr=0.000344, tokens/sec=1378782.68, grad_norm=0.6801, duration=0.38s
Step 510: loss=5.5907, lr=0.000345, tokens/sec=1380942.11, grad_norm=0.6464, duration=0.38s
Step 511: loss=5.5316, lr=0.000346, tokens/sec=1382377.95, grad_norm=0.6866, duration=0.38s
Step 512: loss=5.5175, lr=0.000347, tokens/sec=1376846.34, grad_norm=0.7007, duration=0.38s
Step 513: loss=5.4771, lr=0.000348, tokens/sec=1378560.54, grad_norm=0.5961, duration=0.38s
Step 514: loss=5.5117, lr=0.000349, tokens/sec=1376459.39, grad_norm=0.4944, duration=0.38s
Step 515: loss=5.5316, lr=0.000350, tokens/sec=1378703.15, grad_norm=0.6103, duration=0.38s
Step 516: loss=5.4768, lr=0.000351, tokens/sec=1376620.52, grad_norm=0.5967, duration=0.38s
Step 517: loss=5.5612, lr=0.000351, tokens/sec=1376126.89, grad_norm=0.5528, duration=0.38s
Step 518: loss=5.5448, lr=0.000352, tokens/sec=1379520.49, grad_norm=0.5193, duration=0.38s
Step 519: loss=5.5743, lr=0.000353, tokens/sec=1378176.07, grad_norm=0.5183, duration=0.38s
Step 520: loss=5.6535, lr=0.000354, tokens/sec=1375508.86, grad_norm=0.5465, duration=0.38s
Step 521: loss=5.5757, lr=0.000355, tokens/sec=1378211.49, grad_norm=0.6387, duration=0.38s
Step 522: loss=5.6450, lr=0.000356, tokens/sec=1380094.50, grad_norm=0.6331, duration=0.38s
Step 523: loss=5.5868, lr=0.000357, tokens/sec=1375717.10, grad_norm=0.7775, duration=0.38s
Step 524: loss=5.6293, lr=0.000358, tokens/sec=1376999.81, grad_norm=1.0651, duration=0.38s
Step 525: loss=5.6035, lr=0.000359, tokens/sec=1380066.78, grad_norm=0.9746, duration=0.38s
Step 526: loss=5.5985, lr=0.000360, tokens/sec=1374967.02, grad_norm=0.7851, duration=0.38s
Step 527: loss=5.6327, lr=0.000361, tokens/sec=1376058.01, grad_norm=0.6507, duration=0.38s
Step 528: loss=5.6524, lr=0.000362, tokens/sec=1377165.38, grad_norm=0.5462, duration=0.38s
Step 529: loss=5.5862, lr=0.000363, tokens/sec=1376443.02, grad_norm=0.5018, duration=0.38s
Step 530: loss=5.5967, lr=0.000364, tokens/sec=1377586.40, grad_norm=0.4836, duration=0.38s
Step 531: loss=5.5262, lr=0.000365, tokens/sec=1375767.02, grad_norm=0.5063, duration=0.38s
Step 532: loss=5.5368, lr=0.000366, tokens/sec=1372128.95, grad_norm=0.4529, duration=0.38s
Step 533: loss=5.5574, lr=0.000366, tokens/sec=1375958.99, grad_norm=0.5368, duration=0.38s
Step 534: loss=5.4947, lr=0.000367, tokens/sec=1373784.21, grad_norm=0.7791, duration=0.38s
Step 535: loss=5.5639, lr=0.000368, tokens/sec=1376870.48, grad_norm=1.1017, duration=0.38s
Step 536: loss=5.5582, lr=0.000369, tokens/sec=1372933.37, grad_norm=0.9304, duration=0.38s
Step 537: loss=5.6110, lr=0.000370, tokens/sec=1379308.49, grad_norm=0.8524, duration=0.38s
Step 538: loss=5.6224, lr=0.000371, tokens/sec=1379774.97, grad_norm=0.6487, duration=0.38s
Step 539: loss=5.5500, lr=0.000372, tokens/sec=1374814.87, grad_norm=0.5729, duration=0.38s
Step 540: loss=5.4678, lr=0.000373, tokens/sec=1378398.09, grad_norm=0.5733, duration=0.38s
Step 541: loss=5.5545, lr=0.000374, tokens/sec=1378978.08, grad_norm=0.5901, duration=0.38s
Step 542: loss=5.4959, lr=0.000375, tokens/sec=1378807.75, grad_norm=0.6439, duration=0.38s
Step 543: loss=5.4715, lr=0.000376, tokens/sec=1379679.74, grad_norm=0.7361, duration=0.38s
Step 544: loss=5.4208, lr=0.000377, tokens/sec=1375757.55, grad_norm=0.8658, duration=0.38s
Step 545: loss=5.4984, lr=0.000378, tokens/sec=1380266.02, grad_norm=0.8707, duration=0.38s
Step 546: loss=5.4109, lr=0.000379, tokens/sec=1380321.47, grad_norm=0.6024, duration=0.38s
Step 547: loss=5.4657, lr=0.000379, tokens/sec=1380411.58, grad_norm=0.6764, duration=0.38s
Step 548: loss=5.4762, lr=0.000380, tokens/sec=1376058.87, grad_norm=0.6703, duration=0.38s
Step 549: loss=5.4564, lr=0.000381, tokens/sec=1373451.30, grad_norm=0.6647, duration=0.38s
Step 550: loss=5.4346, lr=0.000382, tokens/sec=1383316.25, grad_norm=0.4821, duration=0.38s
Step 551: loss=5.4346, lr=0.000383, tokens/sec=1375315.29, grad_norm=0.4711, duration=0.38s
Step 552: loss=5.3912, lr=0.000384, tokens/sec=1378027.53, grad_norm=0.4622, duration=0.38s
Step 553: loss=5.4083, lr=0.000385, tokens/sec=1380093.63, grad_norm=0.4830, duration=0.38s
Step 554: loss=5.3529, lr=0.000386, tokens/sec=1380478.31, grad_norm=0.6913, duration=0.38s
Step 555: loss=5.4088, lr=0.000387, tokens/sec=1376612.76, grad_norm=0.8634, duration=0.38s
Step 556: loss=5.3568, lr=0.000388, tokens/sec=1381570.25, grad_norm=0.7378, duration=0.38s
Step 557: loss=5.3941, lr=0.000389, tokens/sec=1377585.53, grad_norm=0.7647, duration=0.38s
Step 558: loss=5.3725, lr=0.000390, tokens/sec=1377824.62, grad_norm=0.7638, duration=0.38s
Step 559: loss=5.3344, lr=0.000390, tokens/sec=1382336.24, grad_norm=0.7033, duration=0.38s
Step 560: loss=5.3085, lr=0.000391, tokens/sec=1380312.80, grad_norm=0.6580, duration=0.38s
Step 561: loss=5.3591, lr=0.000392, tokens/sec=1377484.57, grad_norm=0.5705, duration=0.38s
Step 562: loss=5.3401, lr=0.000393, tokens/sec=1377241.28, grad_norm=0.5653, duration=0.38s
Step 563: loss=5.3768, lr=0.000394, tokens/sec=1376952.39, grad_norm=0.6247, duration=0.38s
Step 564: loss=5.4115, lr=0.000395, tokens/sec=1377763.33, grad_norm=0.7555, duration=0.38s
Step 565: loss=5.4482, lr=0.000396, tokens/sec=1377447.47, grad_norm=0.7336, duration=0.38s
Step 566: loss=5.4863, lr=0.000397, tokens/sec=1378727.36, grad_norm=0.6928, duration=0.38s
Step 567: loss=5.5215, lr=0.000398, tokens/sec=1380903.95, grad_norm=0.9997, duration=0.38s
Step 568: loss=5.4718, lr=0.000399, tokens/sec=1381017.56, grad_norm=0.9197, duration=0.38s
Step 569: loss=5.4709, lr=0.000400, tokens/sec=1377962.76, grad_norm=0.7178, duration=0.38s
Step 570: loss=5.4875, lr=0.000400, tokens/sec=1379490.20, grad_norm=0.7208, duration=0.38s
Step 571: loss=5.4872, lr=0.000401, tokens/sec=1377041.20, grad_norm=0.8413, duration=0.38s
Step 572: loss=5.5181, lr=0.000402, tokens/sec=1381175.42, grad_norm=0.7782, duration=0.38s
Step 573: loss=5.5008, lr=0.000403, tokens/sec=1380264.28, grad_norm=0.7003, duration=0.38s
Step 574: loss=5.4793, lr=0.000404, tokens/sec=1378029.25, grad_norm=0.6048, duration=0.38s
Step 575: loss=5.4854, lr=0.000405, tokens/sec=1376230.24, grad_norm=0.7186, duration=0.38s
Step 576: loss=5.5343, lr=0.000406, tokens/sec=1375565.64, grad_norm=0.7575, duration=0.38s
Step 577: loss=5.4660, lr=0.000407, tokens/sec=1374020.27, grad_norm=0.6277, duration=0.38s
Step 578: loss=5.4854, lr=0.000408, tokens/sec=1374624.08, grad_norm=0.7061, duration=0.38s
Step 579: loss=5.4375, lr=0.000409, tokens/sec=1377532.03, grad_norm=0.7516, duration=0.38s
Step 580: loss=5.4837, lr=0.000409, tokens/sec=1382009.59, grad_norm=0.6595, duration=0.38s
Step 581: loss=5.3598, lr=0.000410, tokens/sec=1380547.64, grad_norm=0.5100, duration=0.38s
Step 582: loss=5.4431, lr=0.000411, tokens/sec=1379754.19, grad_norm=0.5351, duration=0.38s
Step 583: loss=5.3855, lr=0.000412, tokens/sec=1377211.10, grad_norm=0.6310, duration=0.38s
Step 584: loss=5.3716, lr=0.000413, tokens/sec=1378203.71, grad_norm=0.6454, duration=0.38s
Step 585: loss=5.5853, lr=0.000414, tokens/sec=1377509.59, grad_norm=0.6949, duration=0.38s
Step 586: loss=5.3999, lr=0.000415, tokens/sec=1378729.08, grad_norm=0.7125, duration=0.38s
Step 587: loss=5.4438, lr=0.000416, tokens/sec=1377388.80, grad_norm=0.7903, duration=0.38s
Step 588: loss=5.4467, lr=0.000417, tokens/sec=1379484.14, grad_norm=0.8114, duration=0.38s
Step 589: loss=5.2703, lr=0.000417, tokens/sec=1380126.55, grad_norm=0.9406, duration=0.38s
Step 590: loss=5.3970, lr=0.000418, tokens/sec=1378405.00, grad_norm=0.7405, duration=0.38s
Step 591: loss=5.3570, lr=0.000419, tokens/sec=1377082.59, grad_norm=0.7874, duration=0.38s
Step 592: loss=5.3898, lr=0.000420, tokens/sec=1377111.91, grad_norm=0.7063, duration=0.38s
Step 593: loss=5.3730, lr=0.000421, tokens/sec=1378640.05, grad_norm=0.7758, duration=0.38s
Step 594: loss=5.3771, lr=0.000422, tokens/sec=1378635.73, grad_norm=0.9366, duration=0.38s
Step 595: loss=5.3333, lr=0.000423, tokens/sec=1377767.65, grad_norm=1.2518, duration=0.38s
Step 596: loss=5.3271, lr=0.000424, tokens/sec=1377424.17, grad_norm=0.9374, duration=0.38s
Step 597: loss=5.2961, lr=0.000424, tokens/sec=1378411.91, grad_norm=0.8569, duration=0.38s
Step 598: loss=5.3309, lr=0.000425, tokens/sec=1379389.82, grad_norm=0.9622, duration=0.38s
Step 599: loss=5.3208, lr=0.000426, tokens/sec=1378451.66, grad_norm=0.8491, duration=0.38s
Step 600/19073 (3.1%), Elapsed time: 258.93s, Steps per hour: 8341.97, Estimated hours remaining: 2.21
Step 600: loss=5.3621, lr=0.000427, tokens/sec=1379063.70, grad_norm=0.7098, duration=0.38s
Step 601: loss=5.3275, lr=0.000428, tokens/sec=1380399.45, grad_norm=0.7002, duration=0.38s
Step 602: loss=5.3193, lr=0.000429, tokens/sec=1380243.49, grad_norm=0.8907, duration=0.38s
Step 603: loss=5.3656, lr=0.000430, tokens/sec=1378716.98, grad_norm=0.6382, duration=0.38s
Step 604: loss=5.2194, lr=0.000431, tokens/sec=1379368.19, grad_norm=0.6112, duration=0.38s
Step 605: loss=5.2468, lr=0.000431, tokens/sec=1378532.02, grad_norm=0.6731, duration=0.38s
Step 606: loss=5.2886, lr=0.000432, tokens/sec=1381053.98, grad_norm=0.6612, duration=0.38s
Step 607: loss=5.2109, lr=0.000433, tokens/sec=1378530.30, grad_norm=0.5880, duration=0.38s
Step 608: loss=5.2081, lr=0.000434, tokens/sec=1376098.48, grad_norm=0.6854, duration=0.38s
Step 609: loss=5.2417, lr=0.000435, tokens/sec=1377494.92, grad_norm=0.7660, duration=0.38s
Step 610: loss=5.2106, lr=0.000436, tokens/sec=1374616.35, grad_norm=0.7985, duration=0.38s
Step 611: loss=5.2296, lr=0.000437, tokens/sec=1374961.86, grad_norm=0.8422, duration=0.38s
Step 612: loss=5.1952, lr=0.000437, tokens/sec=1375124.37, grad_norm=0.6447, duration=0.38s
Step 613: loss=5.2846, lr=0.000438, tokens/sec=1380404.65, grad_norm=0.4770, duration=0.38s
Step 614: loss=5.2602, lr=0.000439, tokens/sec=1379548.18, grad_norm=0.4818, duration=0.38s
Step 615: loss=5.2416, lr=0.000440, tokens/sec=1379403.67, grad_norm=0.5144, duration=0.38s
Step 616: loss=5.2497, lr=0.000441, tokens/sec=1378166.57, grad_norm=0.5469, duration=0.38s
Step 617: loss=5.3414, lr=0.000442, tokens/sec=1380791.23, grad_norm=0.5887, duration=0.38s
Step 618: loss=5.3747, lr=0.000443, tokens/sec=1379981.05, grad_norm=0.6058, duration=0.38s
Step 619: loss=5.3020, lr=0.000443, tokens/sec=1381394.07, grad_norm=0.5195, duration=0.38s
Step 620: loss=5.3296, lr=0.000444, tokens/sec=1380323.20, grad_norm=0.5534, duration=0.38s
Step 621: loss=5.3746, lr=0.000445, tokens/sec=1380123.95, grad_norm=0.6769, duration=0.38s
Step 622: loss=5.3672, lr=0.000446, tokens/sec=1380093.63, grad_norm=0.8177, duration=0.38s
Step 623: loss=5.3322, lr=0.000447, tokens/sec=1376803.24, grad_norm=0.8430, duration=0.38s
Step 624: loss=5.2772, lr=0.000448, tokens/sec=1379649.45, grad_norm=0.8291, duration=0.38s
Step 625: loss=5.3227, lr=0.000448, tokens/sec=1379452.99, grad_norm=0.8083, duration=0.38s
Step 626: loss=5.3253, lr=0.000449, tokens/sec=1375020.33, grad_norm=1.0293, duration=0.38s
Step 627: loss=5.3183, lr=0.000450, tokens/sec=1376312.07, grad_norm=0.7503, duration=0.38s
Step 628: loss=5.2754, lr=0.000451, tokens/sec=1375557.90, grad_norm=0.7955, duration=0.38s
Step 629: loss=5.2883, lr=0.000452, tokens/sec=1376069.20, grad_norm=0.7671, duration=0.38s
Step 630: loss=5.2865, lr=0.000453, tokens/sec=1378288.37, grad_norm=0.6518, duration=0.38s
Step 631: loss=5.2588, lr=0.000453, tokens/sec=1379789.69, grad_norm=0.6630, duration=0.38s
Step 632: loss=5.2814, lr=0.000454, tokens/sec=1377634.73, grad_norm=0.7354, duration=0.38s
Step 633: loss=5.2866, lr=0.000455, tokens/sec=1378782.68, grad_norm=0.7872, duration=0.38s
Step 634: loss=5.2700, lr=0.000456, tokens/sec=1378739.46, grad_norm=0.7318, duration=0.38s
Step 635: loss=5.3174, lr=0.000457, tokens/sec=1381683.09, grad_norm=0.7560, duration=0.38s
Step 636: loss=5.2151, lr=0.000458, tokens/sec=1378799.11, grad_norm=0.6707, duration=0.38s
Step 637: loss=5.2970, lr=0.000458, tokens/sec=1375878.06, grad_norm=0.6793, duration=0.38s
Step 638: loss=5.2678, lr=0.000459, tokens/sec=1379591.46, grad_norm=0.6941, duration=0.38s
Step 639: loss=5.1848, lr=0.000460, tokens/sec=1378998.84, grad_norm=0.8206, duration=0.38s
Step 640: loss=5.2234, lr=0.000461, tokens/sec=1378451.66, grad_norm=0.7588, duration=0.38s
Step 641: loss=5.2045, lr=0.000462, tokens/sec=1377390.52, grad_norm=0.6557, duration=0.38s
Step 642: loss=5.1935, lr=0.000462, tokens/sec=1378089.71, grad_norm=0.6160, duration=0.38s
Step 643: loss=5.2150, lr=0.000463, tokens/sec=1378935.71, grad_norm=0.6596, duration=0.38s
Step 644: loss=5.1777, lr=0.000464, tokens/sec=1381327.25, grad_norm=0.6692, duration=0.38s
Step 645: loss=5.1729, lr=0.000465, tokens/sec=1374049.46, grad_norm=0.6013, duration=0.38s
Step 646: loss=5.1088, lr=0.000466, tokens/sec=1382278.89, grad_norm=0.4641, duration=0.38s
Step 647: loss=5.1522, lr=0.000466, tokens/sec=1375676.65, grad_norm=0.5553, duration=0.38s
Step 648: loss=5.1761, lr=0.000467, tokens/sec=1378497.46, grad_norm=0.6052, duration=0.38s
Step 649: loss=5.1571, lr=0.000468, tokens/sec=1380373.45, grad_norm=0.6870, duration=0.38s
Step 650: loss=5.1301, lr=0.000469, tokens/sec=1381128.58, grad_norm=0.8753, duration=0.38s
Step 651: loss=5.1634, lr=0.000470, tokens/sec=1380952.51, grad_norm=1.2527, duration=0.38s
Step 652: loss=5.1632, lr=0.000470, tokens/sec=1377409.51, grad_norm=1.0693, duration=0.38s
Step 653: loss=5.1180, lr=0.000471, tokens/sec=1381379.31, grad_norm=0.9295, duration=0.38s
Step 654: loss=5.1269, lr=0.000472, tokens/sec=1378984.14, grad_norm=0.8696, duration=0.38s
Step 655: loss=5.0932, lr=0.000473, tokens/sec=1380719.27, grad_norm=1.0458, duration=0.38s
Step 656: loss=5.1159, lr=0.000474, tokens/sec=1379207.28, grad_norm=0.7264, duration=0.38s
Step 657: loss=5.1421, lr=0.000474, tokens/sec=1380065.92, grad_norm=0.7490, duration=0.38s
Step 658: loss=5.1180, lr=0.000475, tokens/sec=1379834.71, grad_norm=0.8979, duration=0.38s
Step 659: loss=5.1570, lr=0.000476, tokens/sec=1377033.44, grad_norm=1.3009, duration=0.38s
Step 660: loss=5.1737, lr=0.000477, tokens/sec=1379572.42, grad_norm=0.7777, duration=0.38s
Step 661: loss=5.1936, lr=0.000478, tokens/sec=1372516.91, grad_norm=0.8502, duration=0.38s
Step 662: loss=5.1921, lr=0.000478, tokens/sec=1376731.70, grad_norm=0.7998, duration=0.38s
Step 663: loss=5.2363, lr=0.000479, tokens/sec=1379793.15, grad_norm=0.8242, duration=0.38s
Step 664: loss=5.2590, lr=0.000480, tokens/sec=1373259.17, grad_norm=0.7087, duration=0.38s
Step 665: loss=5.2689, lr=0.000481, tokens/sec=1379706.58, grad_norm=0.7081, duration=0.38s
Step 666: loss=5.2701, lr=0.000481, tokens/sec=1380272.08, grad_norm=0.7479, duration=0.38s
Step 667: loss=5.2361, lr=0.000482, tokens/sec=1379670.22, grad_norm=0.7727, duration=0.38s
Step 668: loss=5.2476, lr=0.000483, tokens/sec=1376974.80, grad_norm=0.8473, duration=0.38s
Step 669: loss=5.2748, lr=0.000484, tokens/sec=1376616.21, grad_norm=0.7765, duration=0.38s
Step 670: loss=5.2141, lr=0.000484, tokens/sec=1374545.03, grad_norm=0.6199, duration=0.38s
Step 671: loss=5.2099, lr=0.000485, tokens/sec=1377607.11, grad_norm=0.5972, duration=0.38s
Step 672: loss=5.1741, lr=0.000486, tokens/sec=1369909.08, grad_norm=0.5527, duration=0.38s
Step 673: loss=5.1380, lr=0.000487, tokens/sec=1375942.63, grad_norm=0.5965, duration=0.38s
Step 674: loss=5.2029, lr=0.000487, tokens/sec=1381237.88, grad_norm=0.5769, duration=0.38s
Step 675: loss=5.1418, lr=0.000488, tokens/sec=1376343.08, grad_norm=0.5902, duration=0.38s
Step 676: loss=5.1382, lr=0.000489, tokens/sec=1380111.82, grad_norm=0.4847, duration=0.38s
Step 677: loss=5.1128, lr=0.000490, tokens/sec=1376612.76, grad_norm=0.4522, duration=0.38s
Step 678: loss=5.1624, lr=0.000490, tokens/sec=1381677.02, grad_norm=0.5353, duration=0.38s
Step 679: loss=5.1208, lr=0.000491, tokens/sec=1380420.25, grad_norm=0.6324, duration=0.38s
Step 680: loss=5.1281, lr=0.000492, tokens/sec=1380331.00, grad_norm=0.7797, duration=0.38s
Step 681: loss=5.1325, lr=0.000493, tokens/sec=1373916.39, grad_norm=0.9104, duration=0.38s
Step 682: loss=5.1380, lr=0.000493, tokens/sec=1377042.92, grad_norm=0.7522, duration=0.38s
Step 683: loss=5.0943, lr=0.000494, tokens/sec=1379569.82, grad_norm=0.6774, duration=0.38s
Step 684: loss=5.1741, lr=0.000495, tokens/sec=1377197.30, grad_norm=0.8331, duration=0.38s
Step 685: loss=5.1591, lr=0.000496, tokens/sec=1376509.36, grad_norm=1.1148, duration=0.38s
Step 686: loss=5.1753, lr=0.000496, tokens/sec=1380892.68, grad_norm=1.0292, duration=0.38s
Step 687: loss=5.1380, lr=0.000497, tokens/sec=1377208.51, grad_norm=0.8194, duration=0.38s
Step 688: loss=5.1049, lr=0.000498, tokens/sec=1373761.04, grad_norm=0.8497, duration=0.38s
Step 689: loss=5.1110, lr=0.000499, tokens/sec=1380357.86, grad_norm=0.7102, duration=0.38s
Step 690: loss=5.0651, lr=0.000499, tokens/sec=1379820.85, grad_norm=0.7883, duration=0.38s
Step 691: loss=5.0624, lr=0.000500, tokens/sec=1380327.53, grad_norm=0.6994, duration=0.38s
Step 692: loss=5.0737, lr=0.000501, tokens/sec=1377396.56, grad_norm=0.6727, duration=0.38s
Step 693: loss=5.0716, lr=0.000501, tokens/sec=1377136.06, grad_norm=0.5983, duration=0.38s
Step 694: loss=5.0868, lr=0.000502, tokens/sec=1378863.08, grad_norm=0.6500, duration=0.38s
Step 695: loss=5.0385, lr=0.000503, tokens/sec=1378339.34, grad_norm=0.5259, duration=0.38s
Step 696: loss=5.0375, lr=0.000504, tokens/sec=1380810.30, grad_norm=0.5331, duration=0.38s
Step 697: loss=5.0129, lr=0.000504, tokens/sec=1376567.09, grad_norm=0.5594, duration=0.38s
Step 698: loss=4.9871, lr=0.000505, tokens/sec=1379170.95, grad_norm=0.4908, duration=0.38s
Step 699: loss=4.9982, lr=0.000506, tokens/sec=1372632.57, grad_norm=0.5100, duration=0.38s
Step 700/19073 (3.7%), Elapsed time: 297.06s, Steps per hour: 8483.24, Estimated hours remaining: 2.17
Step 700: loss=5.0156, lr=0.000506, tokens/sec=1375897.86, grad_norm=0.5362, duration=0.38s
Step 701: loss=4.9496, lr=0.000507, tokens/sec=1375937.46, grad_norm=0.6001, duration=0.38s
Step 702: loss=4.9822, lr=0.000508, tokens/sec=1376814.45, grad_norm=0.7193, duration=0.38s
Step 703: loss=4.9364, lr=0.000508, tokens/sec=1379994.04, grad_norm=0.7094, duration=0.38s
Step 704: loss=4.9944, lr=0.000509, tokens/sec=1373854.59, grad_norm=0.6450, duration=0.38s
Step 705: loss=4.9858, lr=0.000510, tokens/sec=1376612.76, grad_norm=0.6280, duration=0.38s
Step 706: loss=5.0031, lr=0.000511, tokens/sec=1375976.21, grad_norm=0.7041, duration=0.38s
Step 707: loss=5.0373, lr=0.000511, tokens/sec=1375942.63, grad_norm=0.9407, duration=0.38s
Step 708: loss=5.0527, lr=0.000512, tokens/sec=1376717.05, grad_norm=1.0080, duration=0.38s
Step 709: loss=5.0722, lr=0.000513, tokens/sec=1376766.18, grad_norm=1.1037, duration=0.38s
Step 710: loss=5.1927, lr=0.000513, tokens/sec=1383023.93, grad_norm=0.9007, duration=0.38s
Step 711: loss=5.1246, lr=0.000514, tokens/sec=1378845.79, grad_norm=0.9624, duration=0.38s
Step 712: loss=5.1841, lr=0.000515, tokens/sec=1377707.23, grad_norm=1.0467, duration=0.38s
Step 713: loss=5.1610, lr=0.000515, tokens/sec=1377654.58, grad_norm=0.9454, duration=0.38s
Step 714: loss=5.1318, lr=0.000516, tokens/sec=1373466.74, grad_norm=0.7617, duration=0.38s
Step 715: loss=5.0895, lr=0.000517, tokens/sec=1376910.14, grad_norm=0.6429, duration=0.38s
Step 716: loss=5.0912, lr=0.000517, tokens/sec=1374378.37, grad_norm=0.4775, duration=0.38s
Step 717: loss=5.1185, lr=0.000518, tokens/sec=1375174.24, grad_norm=0.4785, duration=0.38s
Step 718: loss=5.1548, lr=0.000519, tokens/sec=1379823.45, grad_norm=0.4978, duration=0.38s
Step 719: loss=5.0927, lr=0.000519, tokens/sec=1378266.77, grad_norm=0.6148, duration=0.38s
Step 720: loss=5.1176, lr=0.000520, tokens/sec=1377847.93, grad_norm=0.7463, duration=0.38s
Step 721: loss=5.0150, lr=0.000521, tokens/sec=1377199.02, grad_norm=0.7282, duration=0.38s
Step 722: loss=5.0353, lr=0.000521, tokens/sec=1379523.95, grad_norm=0.6839, duration=0.38s
Step 723: loss=5.0405, lr=0.000522, tokens/sec=1376309.49, grad_norm=0.7262, duration=0.38s
Step 724: loss=5.0249, lr=0.000523, tokens/sec=1374734.08, grad_norm=0.7348, duration=0.38s
Step 725: loss=5.0413, lr=0.000523, tokens/sec=1380515.57, grad_norm=0.7095, duration=0.38s
Step 726: loss=5.0252, lr=0.000524, tokens/sec=1376443.88, grad_norm=0.5871, duration=0.38s
Step 727: loss=5.0862, lr=0.000525, tokens/sec=1374777.05, grad_norm=0.5376, duration=0.38s
Step 728: loss=5.1189, lr=0.000525, tokens/sec=1375820.39, grad_norm=0.4829, duration=0.38s
Step 729: loss=5.0091, lr=0.000526, tokens/sec=1377502.69, grad_norm=0.5030, duration=0.38s
Step 730: loss=4.9494, lr=0.000526, tokens/sec=1377589.85, grad_norm=0.5226, duration=0.38s
Step 731: loss=5.0345, lr=0.000527, tokens/sec=1372953.94, grad_norm=0.5008, duration=0.38s
Step 732: loss=4.9799, lr=0.000528, tokens/sec=1379527.41, grad_norm=0.5042, duration=0.38s
Step 733: loss=4.9108, lr=0.000528, tokens/sec=1378270.23, grad_norm=0.5689, duration=0.38s
Step 734: loss=4.9049, lr=0.000529, tokens/sec=1376778.24, grad_norm=0.6162, duration=0.38s
Step 735: loss=4.9643, lr=0.000530, tokens/sec=1378278.00, grad_norm=0.7516, duration=0.38s
Step 736: loss=4.9183, lr=0.000530, tokens/sec=1377765.06, grad_norm=0.8269, duration=0.38s
Step 737: loss=4.9600, lr=0.000531, tokens/sec=1377549.29, grad_norm=0.6867, duration=0.38s
Step 738: loss=4.9941, lr=0.000531, tokens/sec=1373903.52, grad_norm=0.5526, duration=0.38s
Step 739: loss=4.9349, lr=0.000532, tokens/sec=1376456.80, grad_norm=0.6423, duration=0.38s
Step 740: loss=4.9187, lr=0.000533, tokens/sec=1377005.85, grad_norm=0.6555, duration=0.38s
Step 741: loss=4.9276, lr=0.000533, tokens/sec=1376766.18, grad_norm=0.6576, duration=0.38s
Step 742: loss=4.8925, lr=0.000534, tokens/sec=1377237.83, grad_norm=0.7483, duration=0.38s
Step 743: loss=4.9245, lr=0.000534, tokens/sec=1379378.57, grad_norm=0.8626, duration=0.38s
Step 744: loss=4.8986, lr=0.000535, tokens/sec=1376710.15, grad_norm=0.8794, duration=0.38s
Step 745: loss=4.8721, lr=0.000536, tokens/sec=1378032.71, grad_norm=0.7175, duration=0.38s
Step 746: loss=4.8703, lr=0.000536, tokens/sec=1374358.61, grad_norm=0.8361, duration=0.38s
Step 747: loss=4.9438, lr=0.000537, tokens/sec=1380152.53, grad_norm=0.6894, duration=0.38s
Step 748: loss=4.8645, lr=0.000537, tokens/sec=1378481.04, grad_norm=0.7323, duration=0.38s
Step 749: loss=4.8682, lr=0.000538, tokens/sec=1379742.94, grad_norm=0.9376, duration=0.38s
Validation loss at step 750: 4.98995304107666
Step 750: loss=4.8449, lr=0.000539, tokens/sec=153621.23, grad_norm=0.8948, duration=3.41s
Step 751: loss=4.8668, lr=0.000539, tokens/sec=1380578.84, grad_norm=0.7351, duration=0.38s
Step 752: loss=4.8995, lr=0.000540, tokens/sec=1378469.81, grad_norm=0.6326, duration=0.38s
Step 753: loss=4.8921, lr=0.000540, tokens/sec=1376853.24, grad_norm=0.4536, duration=0.38s
Step 754: loss=4.9167, lr=0.000541, tokens/sec=1379553.38, grad_norm=0.4553, duration=0.38s
Step 755: loss=4.9731, lr=0.000542, tokens/sec=1380269.48, grad_norm=0.4512, duration=0.38s
Step 756: loss=4.9590, lr=0.000542, tokens/sec=1378303.05, grad_norm=0.4313, duration=0.38s
Step 757: loss=5.0223, lr=0.000543, tokens/sec=1375239.60, grad_norm=0.5252, duration=0.38s
Step 758: loss=4.9604, lr=0.000543, tokens/sec=1370728.83, grad_norm=0.7878, duration=0.38s
Step 759: loss=5.0298, lr=0.000544, tokens/sec=1376897.21, grad_norm=1.0966, duration=0.38s
Step 760: loss=4.9753, lr=0.000544, tokens/sec=1378353.16, grad_norm=0.7534, duration=0.38s
Step 761: loss=5.0808, lr=0.000545, tokens/sec=1375890.11, grad_norm=0.7785, duration=0.38s
Step 762: loss=5.0238, lr=0.000545, tokens/sec=1378832.82, grad_norm=0.7328, duration=0.38s
Step 763: loss=5.0480, lr=0.000546, tokens/sec=1377293.90, grad_norm=0.6931, duration=0.38s
Step 764: loss=5.0154, lr=0.000547, tokens/sec=1376318.96, grad_norm=0.6300, duration=0.38s
Step 765: loss=5.0055, lr=0.000547, tokens/sec=1376125.17, grad_norm=0.5299, duration=0.38s
Step 766: loss=5.0621, lr=0.000548, tokens/sec=1372758.53, grad_norm=0.5215, duration=0.38s
Step 767: loss=4.9951, lr=0.000548, tokens/sec=1373505.34, grad_norm=0.5671, duration=0.38s
Step 768: loss=5.0138, lr=0.000549, tokens/sec=1380980.26, grad_norm=0.7556, duration=0.38s
Step 769: loss=5.0056, lr=0.000549, tokens/sec=1376711.88, grad_norm=0.8462, duration=0.38s
Step 770: loss=4.9549, lr=0.000550, tokens/sec=1378323.79, grad_norm=0.6281, duration=0.38s
Step 771: loss=4.9484, lr=0.000550, tokens/sec=1372703.68, grad_norm=0.7293, duration=0.38s
Step 772: loss=4.9867, lr=0.000551, tokens/sec=1374588.85, grad_norm=0.7623, duration=0.38s
Step 773: loss=4.9053, lr=0.000551, tokens/sec=1376185.46, grad_norm=0.6744, duration=0.38s
Step 774: loss=5.0838, lr=0.000552, tokens/sec=1378221.85, grad_norm=0.7194, duration=0.38s
Step 775: loss=4.9697, lr=0.000552, tokens/sec=1375237.88, grad_norm=0.6956, duration=0.38s
Step 776: loss=4.9682, lr=0.000553, tokens/sec=1371837.06, grad_norm=0.7289, duration=0.38s
Step 777: loss=4.9559, lr=0.000554, tokens/sec=1375906.47, grad_norm=0.6641, duration=0.38s
Step 778: loss=4.9619, lr=0.000554, tokens/sec=1380577.11, grad_norm=0.6196, duration=0.38s
Step 779: loss=4.7782, lr=0.000555, tokens/sec=1375264.55, grad_norm=0.6932, duration=0.38s
Step 780: loss=4.9046, lr=0.000555, tokens/sec=1378281.46, grad_norm=0.6649, duration=0.38s
Step 781: loss=4.8694, lr=0.000556, tokens/sec=1377706.36, grad_norm=0.7265, duration=0.38s
Step 782: loss=4.9302, lr=0.000556, tokens/sec=1376729.98, grad_norm=0.5886, duration=0.38s
Step 783: loss=4.8937, lr=0.000557, tokens/sec=1380084.97, grad_norm=0.6020, duration=0.38s
Step 784: loss=4.8810, lr=0.000557, tokens/sec=1378379.08, grad_norm=0.6987, duration=0.38s
Step 785: loss=4.8423, lr=0.000558, tokens/sec=1374899.11, grad_norm=0.7316, duration=0.38s
Step 786: loss=4.8236, lr=0.000558, tokens/sec=1377752.97, grad_norm=0.6303, duration=0.38s
Step 787: loss=4.8171, lr=0.000559, tokens/sec=1376712.74, grad_norm=0.6693, duration=0.38s
Step 788: loss=4.8733, lr=0.000559, tokens/sec=1373180.28, grad_norm=0.5425, duration=0.38s
Step 789: loss=4.7952, lr=0.000560, tokens/sec=1374423.90, grad_norm=0.5053, duration=0.38s
Step 790: loss=4.8775, lr=0.000560, tokens/sec=1377732.26, grad_norm=0.6266, duration=0.38s
Step 791: loss=4.8327, lr=0.000561, tokens/sec=1375417.66, grad_norm=0.6996, duration=0.38s
Step 792: loss=4.8439, lr=0.000561, tokens/sec=1373206.86, grad_norm=0.7010, duration=0.38s
Step 793: loss=4.8639, lr=0.000561, tokens/sec=1380315.40, grad_norm=0.6544, duration=0.38s
Step 794: loss=4.8143, lr=0.000562, tokens/sec=1376917.90, grad_norm=0.6747, duration=0.38s
Step 795: loss=4.7422, lr=0.000562, tokens/sec=1378627.95, grad_norm=0.6399, duration=0.38s
Step 796: loss=4.7985, lr=0.000563, tokens/sec=1377633.86, grad_norm=0.6527, duration=0.38s
Step 797: loss=4.7833, lr=0.000563, tokens/sec=1378074.16, grad_norm=0.7825, duration=0.38s
Step 798: loss=4.7467, lr=0.000564, tokens/sec=1376478.34, grad_norm=0.6846, duration=0.38s
Step 799: loss=4.7508, lr=0.000564, tokens/sec=1376903.24, grad_norm=0.8522, duration=0.38s
Step 800/19073 (4.2%), Elapsed time: 338.25s, Steps per hour: 8514.29, Estimated hours remaining: 2.15
Step 800: loss=4.7594, lr=0.000565, tokens/sec=1379040.35, grad_norm=0.6806, duration=0.38s
Step 801: loss=4.7903, lr=0.000565, tokens/sec=1374405.86, grad_norm=0.7586, duration=0.38s
Step 802: loss=4.8120, lr=0.000566, tokens/sec=1377429.35, grad_norm=0.8366, duration=0.38s
Step 803: loss=4.8457, lr=0.000566, tokens/sec=1375920.25, grad_norm=0.7928, duration=0.38s
Step 804: loss=4.8391, lr=0.000567, tokens/sec=1378419.69, grad_norm=0.7265, duration=0.38s
Step 805: loss=4.8079, lr=0.000567, tokens/sec=1377641.63, grad_norm=0.7192, duration=0.38s
Step 806: loss=4.7951, lr=0.000567, tokens/sec=1377186.08, grad_norm=0.6255, duration=0.38s
Step 807: loss=4.9199, lr=0.000568, tokens/sec=1376123.45, grad_norm=0.5742, duration=0.38s
Step 808: loss=4.9361, lr=0.000568, tokens/sec=1376068.34, grad_norm=0.6121, duration=0.38s
Step 809: loss=4.8665, lr=0.000569, tokens/sec=1378813.80, grad_norm=0.6478, duration=0.38s
Step 810: loss=4.9470, lr=0.000569, tokens/sec=1377536.34, grad_norm=0.5793, duration=0.38s
Step 811: loss=4.8983, lr=0.000570, tokens/sec=1379208.14, grad_norm=0.6460, duration=0.38s
Step 812: loss=4.9237, lr=0.000570, tokens/sec=1375512.30, grad_norm=0.6071, duration=0.38s
Step 813: loss=4.8782, lr=0.000570, tokens/sec=1378345.39, grad_norm=0.6372, duration=0.38s
Step 814: loss=4.8038, lr=0.000571, tokens/sec=1378724.76, grad_norm=0.5235, duration=0.38s
Step 815: loss=4.8458, lr=0.000571, tokens/sec=1378493.14, grad_norm=0.5342, duration=0.38s
Step 816: loss=4.8376, lr=0.000572, tokens/sec=1382150.31, grad_norm=0.6435, duration=0.38s
Step 817: loss=4.8678, lr=0.000572, tokens/sec=1378965.98, grad_norm=0.7283, duration=0.38s
Step 818: loss=4.8222, lr=0.000572, tokens/sec=1379956.80, grad_norm=0.6712, duration=0.38s
Step 819: loss=4.7947, lr=0.000573, tokens/sec=1379123.38, grad_norm=0.5330, duration=0.38s
Step 820: loss=4.8367, lr=0.000573, tokens/sec=1380377.79, grad_norm=0.5251, duration=0.38s
Step 821: loss=4.8047, lr=0.000574, tokens/sec=1378456.84, grad_norm=0.5568, duration=0.38s
Step 822: loss=4.8021, lr=0.000574, tokens/sec=1371477.72, grad_norm=0.7266, duration=0.38s
Step 823: loss=4.8448, lr=0.000574, tokens/sec=1377160.21, grad_norm=0.6983, duration=0.38s
Step 824: loss=4.8164, lr=0.000575, tokens/sec=1377094.67, grad_norm=0.6658, duration=0.38s
Step 825: loss=4.8498, lr=0.000575, tokens/sec=1380206.24, grad_norm=0.6825, duration=0.38s
Step 826: loss=4.7896, lr=0.000576, tokens/sec=1376223.35, grad_norm=0.7472, duration=0.38s
Step 827: loss=4.8977, lr=0.000576, tokens/sec=1380098.83, grad_norm=0.7547, duration=0.38s
Step 828: loss=4.8224, lr=0.000576, tokens/sec=1379973.25, grad_norm=0.6988, duration=0.38s
Step 829: loss=4.7718, lr=0.000577, tokens/sec=1377879.88, grad_norm=0.5763, duration=0.38s
Step 830: loss=4.7639, lr=0.000577, tokens/sec=1378482.77, grad_norm=0.5691, duration=0.38s
Step 831: loss=4.7790, lr=0.000578, tokens/sec=1375981.37, grad_norm=0.5917, duration=0.38s
Step 832: loss=4.7614, lr=0.000578, tokens/sec=1379209.87, grad_norm=0.5193, duration=0.38s
Step 833: loss=4.7881, lr=0.000578, tokens/sec=1378578.69, grad_norm=0.5827, duration=0.38s
Step 834: loss=4.7089, lr=0.000579, tokens/sec=1381119.90, grad_norm=0.6927, duration=0.38s
Step 835: loss=4.7459, lr=0.000579, tokens/sec=1383570.39, grad_norm=0.6997, duration=0.38s
Step 836: loss=4.7248, lr=0.000579, tokens/sec=1381119.04, grad_norm=0.5594, duration=0.38s
Step 837: loss=4.7310, lr=0.000580, tokens/sec=1375626.74, grad_norm=0.6076, duration=0.38s
Step 838: loss=4.7386, lr=0.000580, tokens/sec=1379935.15, grad_norm=0.6042, duration=0.38s
Step 839: loss=4.6936, lr=0.000580, tokens/sec=1380241.76, grad_norm=0.5375, duration=0.38s
Step 840: loss=4.7290, lr=0.000581, tokens/sec=1380916.09, grad_norm=0.5844, duration=0.38s
Step 841: loss=4.6545, lr=0.000581, tokens/sec=1377797.86, grad_norm=0.6544, duration=0.38s
Step 842: loss=4.7209, lr=0.000581, tokens/sec=1380410.71, grad_norm=0.9081, duration=0.38s
Step 843: loss=4.6815, lr=0.000582, tokens/sec=1377825.49, grad_norm=0.8021, duration=0.38s
Step 844: loss=4.6612, lr=0.000582, tokens/sec=1378249.49, grad_norm=0.6431, duration=0.38s
Step 845: loss=4.6235, lr=0.000582, tokens/sec=1379395.01, grad_norm=0.5523, duration=0.38s
Step 846: loss=4.6402, lr=0.000583, tokens/sec=1374792.52, grad_norm=0.5868, duration=0.38s
Step 847: loss=4.6706, lr=0.000583, tokens/sec=1378381.67, grad_norm=0.6209, duration=0.38s
Step 848: loss=4.6903, lr=0.000583, tokens/sec=1377519.95, grad_norm=0.5940, duration=0.38s
Step 849: loss=4.7063, lr=0.000584, tokens/sec=1374795.96, grad_norm=0.5638, duration=0.38s
Step 850: loss=4.7050, lr=0.000584, tokens/sec=1378144.98, grad_norm=0.6501, duration=0.38s
Step 851: loss=4.7138, lr=0.000584, tokens/sec=1378163.98, grad_norm=0.6657, duration=0.38s
Step 852: loss=4.7330, lr=0.000585, tokens/sec=1374752.13, grad_norm=0.7174, duration=0.38s
Step 853: loss=4.7621, lr=0.000585, tokens/sec=1378660.80, grad_norm=0.6636, duration=0.38s
Step 854: loss=4.8092, lr=0.000585, tokens/sec=1374099.26, grad_norm=0.6188, duration=0.38s
Step 855: loss=4.8480, lr=0.000585, tokens/sec=1375936.60, grad_norm=0.6652, duration=0.38s
Step 856: loss=4.8356, lr=0.000586, tokens/sec=1376737.73, grad_norm=0.6819, duration=0.38s
Step 857: loss=4.7798, lr=0.000586, tokens/sec=1381306.43, grad_norm=0.6369, duration=0.38s
Step 858: loss=4.8164, lr=0.000586, tokens/sec=1374883.63, grad_norm=0.6895, duration=0.38s
Step 859: loss=4.8047, lr=0.000587, tokens/sec=1376519.70, grad_norm=0.8115, duration=0.38s
Step 860: loss=4.7976, lr=0.000587, tokens/sec=1377243.87, grad_norm=0.8229, duration=0.38s
Step 861: loss=4.7832, lr=0.000587, tokens/sec=1377116.22, grad_norm=0.6919, duration=0.38s
Step 862: loss=4.7769, lr=0.000588, tokens/sec=1376592.94, grad_norm=0.6460, duration=0.38s
Step 863: loss=4.7331, lr=0.000588, tokens/sec=1378196.80, grad_norm=0.4946, duration=0.38s
Step 864: loss=4.7455, lr=0.000588, tokens/sec=1380119.62, grad_norm=0.4376, duration=0.38s
Step 865: loss=4.7181, lr=0.000588, tokens/sec=1372624.85, grad_norm=0.3951, duration=0.38s
Step 866: loss=4.7215, lr=0.000589, tokens/sec=1376076.09, grad_norm=0.3843, duration=0.38s
Step 867: loss=4.6826, lr=0.000589, tokens/sec=1378150.16, grad_norm=0.4348, duration=0.38s
Step 868: loss=4.7312, lr=0.000589, tokens/sec=1374022.84, grad_norm=0.5363, duration=0.38s
Step 869: loss=4.6874, lr=0.000589, tokens/sec=1375121.79, grad_norm=0.5878, duration=0.38s
Step 870: loss=4.7074, lr=0.000590, tokens/sec=1380708.00, grad_norm=0.5432, duration=0.38s
Step 871: loss=4.7051, lr=0.000590, tokens/sec=1377336.17, grad_norm=0.6748, duration=0.38s
Step 872: loss=4.6804, lr=0.000590, tokens/sec=1375423.68, grad_norm=0.5832, duration=0.38s
Step 873: loss=4.6530, lr=0.000590, tokens/sec=1380366.52, grad_norm=0.4308, duration=0.38s
Step 874: loss=4.7266, lr=0.000591, tokens/sec=1377199.02, grad_norm=0.4434, duration=0.38s
Step 875: loss=4.7066, lr=0.000591, tokens/sec=1377483.71, grad_norm=0.5058, duration=0.38s
Step 876: loss=4.7256, lr=0.000591, tokens/sec=1380434.98, grad_norm=0.6129, duration=0.38s
Step 877: loss=4.6435, lr=0.000591, tokens/sec=1379552.51, grad_norm=0.6666, duration=0.38s
Step 878: loss=4.6946, lr=0.000592, tokens/sec=1377814.26, grad_norm=0.7716, duration=0.38s
Step 879: loss=4.6680, lr=0.000592, tokens/sec=1374472.00, grad_norm=0.7162, duration=0.38s
Step 880: loss=4.6617, lr=0.000592, tokens/sec=1379129.43, grad_norm=0.6866, duration=0.38s
Step 881: loss=4.6439, lr=0.000592, tokens/sec=1377660.62, grad_norm=0.7577, duration=0.38s
Step 882: loss=4.6882, lr=0.000592, tokens/sec=1379265.24, grad_norm=0.8508, duration=0.38s
Step 883: loss=4.6913, lr=0.000593, tokens/sec=1378740.32, grad_norm=0.7620, duration=0.38s
Step 884: loss=4.6567, lr=0.000593, tokens/sec=1376105.37, grad_norm=0.7076, duration=0.38s
Step 885: loss=4.6556, lr=0.000593, tokens/sec=1377710.68, grad_norm=0.6093, duration=0.38s
Step 886: loss=4.6609, lr=0.000593, tokens/sec=1378160.53, grad_norm=0.6153, duration=0.38s
Step 887: loss=4.6139, lr=0.000593, tokens/sec=1377895.42, grad_norm=0.5866, duration=0.38s
Step 888: loss=4.6621, lr=0.000594, tokens/sec=1378252.09, grad_norm=0.5590, duration=0.38s
Step 889: loss=4.5823, lr=0.000594, tokens/sec=1379959.40, grad_norm=0.5393, duration=0.38s
Step 890: loss=4.5970, lr=0.000594, tokens/sec=1375969.32, grad_norm=0.5238, duration=0.38s
Step 891: loss=4.5533, lr=0.000594, tokens/sec=1378819.86, grad_norm=0.5502, duration=0.38s
Step 892: loss=4.5755, lr=0.000594, tokens/sec=1380123.08, grad_norm=0.6599, duration=0.38s
Step 893: loss=4.5599, lr=0.000595, tokens/sec=1377545.84, grad_norm=0.7007, duration=0.38s
Step 894: loss=4.6013, lr=0.000595, tokens/sec=1377760.74, grad_norm=0.6296, duration=0.38s
Step 895: loss=4.6537, lr=0.000595, tokens/sec=1380571.91, grad_norm=0.6909, duration=0.38s
Step 896: loss=4.6214, lr=0.000595, tokens/sec=1376809.28, grad_norm=0.8123, duration=0.38s
Step 897: loss=4.6408, lr=0.000595, tokens/sec=1376013.23, grad_norm=0.9223, duration=0.38s
Step 898: loss=4.6098, lr=0.000596, tokens/sec=1381724.77, grad_norm=0.6660, duration=0.38s
Step 899: loss=4.6764, lr=0.000596, tokens/sec=1374327.69, grad_norm=0.7239, duration=0.38s
Step 900/19073 (4.7%), Elapsed time: 376.39s, Steps per hour: 8608.14, Estimated hours remaining: 2.11
Step 900: loss=4.7540, lr=0.000596, tokens/sec=1378081.07, grad_norm=0.6733, duration=0.38s
Step 901: loss=4.7121, lr=0.000596, tokens/sec=1380994.14, grad_norm=0.8114, duration=0.38s
Step 902: loss=4.7922, lr=0.000596, tokens/sec=1378714.39, grad_norm=0.8434, duration=0.38s
Step 903: loss=4.7153, lr=0.000596, tokens/sec=1377846.21, grad_norm=0.6288, duration=0.38s
Step 904: loss=4.7132, lr=0.000596, tokens/sec=1374524.41, grad_norm=0.5821, duration=0.38s
Step 905: loss=4.6824, lr=0.000597, tokens/sec=1379462.51, grad_norm=0.6309, duration=0.38s
Step 906: loss=4.7014, lr=0.000597, tokens/sec=1377840.16, grad_norm=0.6453, duration=0.38s
Step 907: loss=4.7450, lr=0.000597, tokens/sec=1375446.91, grad_norm=0.7042, duration=0.38s
Step 908: loss=4.8083, lr=0.000597, tokens/sec=1379168.35, grad_norm=0.8055, duration=0.38s
Step 909: loss=4.7443, lr=0.000597, tokens/sec=1370631.43, grad_norm=0.6147, duration=0.38s
Step 910: loss=4.7072, lr=0.000597, tokens/sec=1374979.06, grad_norm=0.5092, duration=0.38s
Step 911: loss=4.6046, lr=0.000597, tokens/sec=1376968.77, grad_norm=0.4737, duration=0.38s
Step 912: loss=4.6343, lr=0.000598, tokens/sec=1375907.33, grad_norm=0.4209, duration=0.38s
Step 913: loss=4.6565, lr=0.000598, tokens/sec=1377807.36, grad_norm=0.3622, duration=0.38s
Step 914: loss=4.5959, lr=0.000598, tokens/sec=1373982.49, grad_norm=0.3378, duration=0.38s
Step 915: loss=4.6450, lr=0.000598, tokens/sec=1377767.65, grad_norm=0.3619, duration=0.38s
Step 916: loss=4.6087, lr=0.000598, tokens/sec=1381028.83, grad_norm=0.4252, duration=0.38s
Step 917: loss=4.6882, lr=0.000598, tokens/sec=1378186.44, grad_norm=0.5635, duration=0.38s
Step 918: loss=4.7190, lr=0.000598, tokens/sec=1375643.09, grad_norm=0.6569, duration=0.38s
Step 919: loss=4.6269, lr=0.000598, tokens/sec=1376351.70, grad_norm=0.5679, duration=0.38s
Step 920: loss=4.5629, lr=0.000598, tokens/sec=1378836.28, grad_norm=0.5657, duration=0.38s
Step 921: loss=4.6371, lr=0.000599, tokens/sec=1368712.80, grad_norm=0.5099, duration=0.38s
Step 922: loss=4.5771, lr=0.000599, tokens/sec=1374009.96, grad_norm=0.5366, duration=0.38s
Step 923: loss=4.5575, lr=0.000599, tokens/sec=1375298.09, grad_norm=0.5919, duration=0.38s
Step 924: loss=4.5120, lr=0.000599, tokens/sec=1374168.81, grad_norm=0.6667, duration=0.38s
Step 925: loss=4.5740, lr=0.000599, tokens/sec=1371225.43, grad_norm=0.6087, duration=0.38s
Step 926: loss=4.5202, lr=0.000599, tokens/sec=1374006.53, grad_norm=0.5294, duration=0.38s
Step 927: loss=4.5818, lr=0.000599, tokens/sec=1379613.09, grad_norm=0.5037, duration=0.38s
Step 928: loss=4.6068, lr=0.000599, tokens/sec=1373689.81, grad_norm=0.4810, duration=0.38s
Step 929: loss=4.5148, lr=0.000599, tokens/sec=1366835.21, grad_norm=0.5247, duration=0.38s
Step 930: loss=4.5147, lr=0.000599, tokens/sec=1372426.96, grad_norm=0.4822, duration=0.38s
Step 931: loss=4.5219, lr=0.000599, tokens/sec=1372888.80, grad_norm=0.4775, duration=0.38s
Step 932: loss=4.4596, lr=0.000599, tokens/sec=1376462.83, grad_norm=0.5880, duration=0.38s
Step 933: loss=4.5315, lr=0.000599, tokens/sec=1370712.60, grad_norm=0.6135, duration=0.38s
Step 934: loss=4.4401, lr=0.000600, tokens/sec=1377645.95, grad_norm=0.5101, duration=0.38s
Step 935: loss=4.4407, lr=0.000600, tokens/sec=1378438.70, grad_norm=0.5462, duration=0.38s
Step 936: loss=4.4586, lr=0.000600, tokens/sec=1373833.13, grad_norm=0.5360, duration=0.38s
Step 937: loss=4.5013, lr=0.000600, tokens/sec=1376071.78, grad_norm=0.5050, duration=0.38s
Step 938: loss=4.4533, lr=0.000600, tokens/sec=1376695.50, grad_norm=0.5858, duration=0.38s
Step 939: loss=4.4518, lr=0.000600, tokens/sec=1376418.03, grad_norm=0.6917, duration=0.38s
Step 940: loss=4.3930, lr=0.000600, tokens/sec=1375778.21, grad_norm=0.6831, duration=0.38s
Step 941: loss=4.4851, lr=0.000600, tokens/sec=1375151.88, grad_norm=0.5645, duration=0.38s
Step 942: loss=4.5084, lr=0.000600, tokens/sec=1368542.44, grad_norm=0.5576, duration=0.38s
Step 943: loss=4.5061, lr=0.000600, tokens/sec=1378521.65, grad_norm=0.5608, duration=0.38s
Step 944: loss=4.5445, lr=0.000600, tokens/sec=1378729.95, grad_norm=0.5993, duration=0.38s
Step 945: loss=4.5849, lr=0.000600, tokens/sec=1372271.09, grad_norm=0.7361, duration=0.38s
Step 946: loss=4.6205, lr=0.000600, tokens/sec=1375563.92, grad_norm=0.6692, duration=0.38s
Step 947: loss=4.6485, lr=0.000600, tokens/sec=1373537.08, grad_norm=0.6837, duration=0.38s
Step 948: loss=4.6092, lr=0.000600, tokens/sec=1375016.89, grad_norm=0.7433, duration=0.38s
Step 949: loss=4.5940, lr=0.000600, tokens/sec=1372222.28, grad_norm=0.7677, duration=0.38s
Step 950: loss=4.6718, lr=0.000600, tokens/sec=1376380.99, grad_norm=0.9181, duration=0.38s
Step 951: loss=4.6961, lr=0.000600, tokens/sec=1373291.76, grad_norm=0.8540, duration=0.38s
Step 952: loss=4.6677, lr=0.000600, tokens/sec=1376783.42, grad_norm=0.6237, duration=0.38s
Step 953: loss=4.7012, lr=0.000600, tokens/sec=1374879.34, grad_norm=0.6029, duration=0.38s
Step 954: loss=4.6273, lr=0.000600, tokens/sec=1375193.16, grad_norm=0.5399, duration=0.38s
Step 955: loss=4.6411, lr=0.000600, tokens/sec=1375704.19, grad_norm=0.5441, duration=0.38s
Step 956: loss=4.7291, lr=0.000600, tokens/sec=1379120.78, grad_norm=0.7576, duration=0.38s
Step 957: loss=4.6368, lr=0.000600, tokens/sec=1375317.87, grad_norm=0.8044, duration=0.38s
Step 958: loss=4.7216, lr=0.000600, tokens/sec=1372392.70, grad_norm=1.0577, duration=0.38s
Step 959: loss=4.6192, lr=0.000600, tokens/sec=1370123.31, grad_norm=0.8528, duration=0.38s
Step 960: loss=4.6727, lr=0.000600, tokens/sec=1375342.82, grad_norm=0.6882, duration=0.38s
Step 961: loss=4.5950, lr=0.000600, tokens/sec=1377257.67, grad_norm=0.6917, duration=0.38s
Step 962: loss=4.6204, lr=0.000600, tokens/sec=1371006.58, grad_norm=0.7071, duration=0.38s
Step 963: loss=4.6867, lr=0.000600, tokens/sec=1376935.14, grad_norm=0.6621, duration=0.38s
Step 964: loss=4.5852, lr=0.000600, tokens/sec=1375420.24, grad_norm=0.6289, duration=0.38s
Step 965: loss=4.6377, lr=0.000600, tokens/sec=1374033.14, grad_norm=0.7063, duration=0.38s
Step 966: loss=4.5795, lr=0.000600, tokens/sec=1376101.06, grad_norm=0.7126, duration=0.38s
Step 967: loss=4.5978, lr=0.000600, tokens/sec=1375479.60, grad_norm=0.5031, duration=0.38s
Step 968: loss=4.5694, lr=0.000600, tokens/sec=1374996.25, grad_norm=0.5556, duration=0.38s
Step 969: loss=4.3989, lr=0.000600, tokens/sec=1376477.48, grad_norm=0.5324, duration=0.38s
Step 970: loss=4.5140, lr=0.000600, tokens/sec=1374405.86, grad_norm=0.3979, duration=0.38s
Step 971: loss=4.4945, lr=0.000600, tokens/sec=1375969.32, grad_norm=0.4058, duration=0.38s
Step 972: loss=4.5864, lr=0.000600, tokens/sec=1374459.12, grad_norm=0.3860, duration=0.38s
Step 973: loss=4.4561, lr=0.000600, tokens/sec=1377393.98, grad_norm=0.4321, duration=0.38s
Step 974: loss=4.4941, lr=0.000600, tokens/sec=1379454.72, grad_norm=0.4001, duration=0.38s
Step 975: loss=4.4555, lr=0.000600, tokens/sec=1375331.64, grad_norm=0.3954, duration=0.38s
Step 976: loss=4.4454, lr=0.000600, tokens/sec=1376425.78, grad_norm=0.4190, duration=0.38s
Step 977: loss=4.4662, lr=0.000600, tokens/sec=1377080.00, grad_norm=0.5317, duration=0.38s
Step 978: loss=4.4687, lr=0.000600, tokens/sec=1376626.55, grad_norm=0.6350, duration=0.38s
Step 979: loss=4.4417, lr=0.000600, tokens/sec=1376472.31, grad_norm=0.5830, duration=0.38s
Step 980: loss=4.4646, lr=0.000600, tokens/sec=1377503.55, grad_norm=0.5453, duration=0.38s
Step 981: loss=4.4645, lr=0.000600, tokens/sec=1378696.24, grad_norm=0.4983, duration=0.38s
Step 982: loss=4.4214, lr=0.000600, tokens/sec=1375523.48, grad_norm=0.5201, duration=0.38s
Step 983: loss=4.5391, lr=0.000600, tokens/sec=1380016.55, grad_norm=0.6413, duration=0.38s
Step 984: loss=4.4163, lr=0.000600, tokens/sec=1378846.66, grad_norm=0.6401, duration=0.38s
Step 985: loss=4.3426, lr=0.000600, tokens/sec=1374700.56, grad_norm=0.5302, duration=0.38s
Step 986: loss=4.4493, lr=0.000600, tokens/sec=1375625.02, grad_norm=0.5024, duration=0.38s
Step 987: loss=4.3528, lr=0.000600, tokens/sec=1380997.61, grad_norm=0.5223, duration=0.38s
Step 988: loss=4.3017, lr=0.000600, tokens/sec=1375415.94, grad_norm=0.6617, duration=0.38s
Step 989: loss=4.3837, lr=0.000600, tokens/sec=1376804.10, grad_norm=0.6160, duration=0.38s
Step 990: loss=4.3954, lr=0.000600, tokens/sec=1379800.94, grad_norm=0.6863, duration=0.38s
Step 991: loss=4.4607, lr=0.000600, tokens/sec=1379158.84, grad_norm=0.6943, duration=0.38s
Step 992: loss=4.4040, lr=0.000600, tokens/sec=1377983.49, grad_norm=0.4937, duration=0.38s
Step 993: loss=4.4515, lr=0.000600, tokens/sec=1378136.34, grad_norm=0.5010, duration=0.38s
Step 994: loss=4.4153, lr=0.000600, tokens/sec=1376145.84, grad_norm=0.4331, duration=0.38s
Step 995: loss=4.3794, lr=0.000600, tokens/sec=1375093.41, grad_norm=0.4037, duration=0.38s
Step 996: loss=4.4171, lr=0.000600, tokens/sec=1374110.42, grad_norm=0.4212, duration=0.38s
Step 997: loss=4.5236, lr=0.000600, tokens/sec=1375779.93, grad_norm=0.4674, duration=0.38s
Step 998: loss=4.5441, lr=0.000600, tokens/sec=1379768.04, grad_norm=0.5038, duration=0.38s
Step 999: loss=4.5327, lr=0.000600, tokens/sec=1377799.59, grad_norm=0.6203, duration=0.38s
Step 1000/19073 (5.2%), Elapsed time: 414.58s, Steps per hour: 8683.42, Estimated hours remaining: 2.08
Validation loss at step 1000: 4.476228713989258
Step 1000: loss=4.5336, lr=0.000600, tokens/sec=156766.40, grad_norm=0.6651, duration=3.34s
Step 1001: loss=4.5244, lr=0.000600, tokens/sec=1377882.47, grad_norm=0.5451, duration=0.38s
Step 1002: loss=4.5433, lr=0.000600, tokens/sec=1381016.69, grad_norm=0.5828, duration=0.38s
Step 1003: loss=4.4970, lr=0.000600, tokens/sec=1378026.66, grad_norm=0.6157, duration=0.38s
Step 1004: loss=4.4435, lr=0.000600, tokens/sec=1379967.19, grad_norm=0.5664, duration=0.38s
Step 1005: loss=4.4690, lr=0.000600, tokens/sec=1375940.91, grad_norm=0.5183, duration=0.38s
Step 1006: loss=4.4846, lr=0.000600, tokens/sec=1376598.98, grad_norm=0.5473, duration=0.38s
Step 1007: loss=4.4958, lr=0.000600, tokens/sec=1381641.43, grad_norm=0.6143, duration=0.38s
Step 1008: loss=4.4331, lr=0.000600, tokens/sec=1376334.47, grad_norm=0.5907, duration=0.38s
Step 1009: loss=4.4479, lr=0.000600, tokens/sec=1374652.44, grad_norm=0.5183, duration=0.38s
Step 1010: loss=4.4962, lr=0.000600, tokens/sec=1374779.63, grad_norm=0.4917, duration=0.38s
Step 1011: loss=4.4000, lr=0.000600, tokens/sec=1381226.61, grad_norm=0.4739, duration=0.38s
Step 1012: loss=4.4260, lr=0.000600, tokens/sec=1380921.29, grad_norm=0.4236, duration=0.38s
Step 1013: loss=4.4433, lr=0.000600, tokens/sec=1373421.27, grad_norm=0.4397, duration=0.38s
Step 1014: loss=4.4118, lr=0.000600, tokens/sec=1378497.46, grad_norm=0.5715, duration=0.38s
Step 1015: loss=4.4880, lr=0.000600, tokens/sec=1375606.09, grad_norm=0.6262, duration=0.38s
Step 1016: loss=4.4159, lr=0.000600, tokens/sec=1372842.51, grad_norm=0.5653, duration=0.38s
Step 1017: loss=4.4958, lr=0.000600, tokens/sec=1374279.59, grad_norm=0.5240, duration=0.38s
Step 1018: loss=4.4586, lr=0.000600, tokens/sec=1379589.73, grad_norm=0.6112, duration=0.38s
Step 1019: loss=4.3906, lr=0.000600, tokens/sec=1373320.92, grad_norm=0.6356, duration=0.38s
Step 1020: loss=4.3972, lr=0.000600, tokens/sec=1371953.46, grad_norm=0.5359, duration=0.38s
Step 1021: loss=4.4171, lr=0.000600, tokens/sec=1374859.56, grad_norm=0.5294, duration=0.38s
Step 1022: loss=4.4172, lr=0.000600, tokens/sec=1372750.81, grad_norm=0.5582, duration=0.38s
Step 1023: loss=4.3876, lr=0.000600, tokens/sec=1375331.64, grad_norm=0.6198, duration=0.38s
Step 1024: loss=4.3425, lr=0.000600, tokens/sec=1377199.02, grad_norm=0.4768, duration=0.38s
Step 1025: loss=4.3890, lr=0.000600, tokens/sec=1379047.27, grad_norm=0.4967, duration=0.38s
Step 1026: loss=4.3748, lr=0.000600, tokens/sec=1377174.01, grad_norm=0.5084, duration=0.38s
Step 1027: loss=4.3490, lr=0.000600, tokens/sec=1377307.71, grad_norm=0.5186, duration=0.38s
Step 1028: loss=4.3446, lr=0.000600, tokens/sec=1378476.72, grad_norm=0.5384, duration=0.38s
Step 1029: loss=4.3590, lr=0.000600, tokens/sec=1378443.02, grad_norm=0.4870, duration=0.38s
Step 1030: loss=4.3484, lr=0.000600, tokens/sec=1380097.96, grad_norm=0.4614, duration=0.38s
Step 1031: loss=4.2674, lr=0.000600, tokens/sec=1375173.38, grad_norm=0.4725, duration=0.38s
Step 1032: loss=4.3435, lr=0.000600, tokens/sec=1375655.14, grad_norm=0.5072, duration=0.38s
Step 1033: loss=4.2632, lr=0.000600, tokens/sec=1375089.97, grad_norm=0.4003, duration=0.38s
Step 1034: loss=4.2540, lr=0.000600, tokens/sec=1376648.10, grad_norm=0.4570, duration=0.38s
Step 1035: loss=4.2281, lr=0.000600, tokens/sec=1381646.63, grad_norm=0.5597, duration=0.38s
Step 1036: loss=4.2673, lr=0.000600, tokens/sec=1378890.75, grad_norm=0.6321, duration=0.38s
Step 1037: loss=4.3513, lr=0.000600, tokens/sec=1378689.32, grad_norm=0.5698, duration=0.38s
Step 1038: loss=4.3201, lr=0.000600, tokens/sec=1380574.51, grad_norm=0.4978, duration=0.38s
Step 1039: loss=4.3298, lr=0.000600, tokens/sec=1377552.74, grad_norm=0.4640, duration=0.38s
Step 1040: loss=4.2962, lr=0.000600, tokens/sec=1378285.78, grad_norm=0.4551, duration=0.38s
Step 1041: loss=4.3462, lr=0.000600, tokens/sec=1376183.73, grad_norm=0.5746, duration=0.38s
Step 1042: loss=4.3319, lr=0.000600, tokens/sec=1377372.41, grad_norm=0.6880, duration=0.38s
Step 1043: loss=4.3821, lr=0.000600, tokens/sec=1378241.72, grad_norm=0.6318, duration=0.38s
Step 1044: loss=4.4671, lr=0.000600, tokens/sec=1376605.01, grad_norm=0.5486, duration=0.38s
Step 1045: loss=4.4681, lr=0.000600, tokens/sec=1376568.82, grad_norm=0.5217, duration=0.38s
Step 1046: loss=4.4369, lr=0.000600, tokens/sec=1376089.87, grad_norm=0.4973, duration=0.38s
Step 1047: loss=4.4521, lr=0.000600, tokens/sec=1375364.32, grad_norm=0.5349, duration=0.38s
Step 1048: loss=4.4133, lr=0.000600, tokens/sec=1375732.59, grad_norm=0.7871, duration=0.38s
Step 1049: loss=4.4804, lr=0.000600, tokens/sec=1376038.20, grad_norm=0.9609, duration=0.38s
Step 1050: loss=4.4276, lr=0.000600, tokens/sec=1376011.51, grad_norm=0.7523, duration=0.38s
Step 1051: loss=4.4706, lr=0.000600, tokens/sec=1377181.77, grad_norm=0.7742, duration=0.38s
Step 1052: loss=4.4544, lr=0.000600, tokens/sec=1379049.86, grad_norm=0.6943, duration=0.38s
Step 1053: loss=4.3779, lr=0.000600, tokens/sec=1376589.50, grad_norm=0.6153, duration=0.38s
Step 1054: loss=4.4245, lr=0.000600, tokens/sec=1377954.13, grad_norm=0.4633, duration=0.38s
Step 1055: loss=4.3870, lr=0.000600, tokens/sec=1376099.34, grad_norm=0.4230, duration=0.38s
Step 1056: loss=4.4003, lr=0.000600, tokens/sec=1379438.28, grad_norm=0.4848, duration=0.38s
Step 1057: loss=4.3545, lr=0.000600, tokens/sec=1379852.89, grad_norm=0.6130, duration=0.38s
Step 1058: loss=4.3985, lr=0.000600, tokens/sec=1381188.43, grad_norm=0.5802, duration=0.38s
Step 1059: loss=4.3355, lr=0.000600, tokens/sec=1379174.41, grad_norm=0.4572, duration=0.38s
Step 1060: loss=4.3640, lr=0.000600, tokens/sec=1376992.05, grad_norm=0.5271, duration=0.38s
Step 1061: loss=4.3436, lr=0.000600, tokens/sec=1377788.37, grad_norm=0.5341, duration=0.38s
Step 1062: loss=4.3412, lr=0.000600, tokens/sec=1377601.07, grad_norm=0.5571, duration=0.38s
Step 1063: loss=4.3176, lr=0.000600, tokens/sec=1376374.96, grad_norm=0.5715, duration=0.38s
Step 1064: loss=4.3815, lr=0.000600, tokens/sec=1375674.07, grad_norm=0.4524, duration=0.38s
Step 1065: loss=4.3726, lr=0.000600, tokens/sec=1380515.57, grad_norm=0.4519, duration=0.38s
Step 1066: loss=4.3338, lr=0.000600, tokens/sec=1376856.69, grad_norm=0.4408, duration=0.38s
Step 1067: loss=4.3116, lr=0.000600, tokens/sec=1377604.52, grad_norm=0.4269, duration=0.38s
Step 1068: loss=4.2990, lr=0.000600, tokens/sec=1379183.06, grad_norm=0.4242, duration=0.38s
Step 1069: loss=4.3013, lr=0.000600, tokens/sec=1377347.39, grad_norm=0.4171, duration=0.38s
Step 1070: loss=4.2749, lr=0.000600, tokens/sec=1379574.15, grad_norm=0.4693, duration=0.38s
Step 1071: loss=4.2717, lr=0.000600, tokens/sec=1382696.08, grad_norm=0.4978, duration=0.38s
Step 1072: loss=4.3169, lr=0.000600, tokens/sec=1378556.22, grad_norm=0.5643, duration=0.38s
Step 1073: loss=4.2637, lr=0.000600, tokens/sec=1379569.82, grad_norm=0.6119, duration=0.38s
Step 1074: loss=4.2882, lr=0.000600, tokens/sec=1375769.60, grad_norm=0.6958, duration=0.38s
Step 1075: loss=4.3147, lr=0.000600, tokens/sec=1380692.40, grad_norm=0.6726, duration=0.38s
Step 1076: loss=4.2855, lr=0.000600, tokens/sec=1380282.48, grad_norm=0.5766, duration=0.38s
Step 1077: loss=4.3318, lr=0.000600, tokens/sec=1379052.46, grad_norm=0.4962, duration=0.38s
Step 1078: loss=4.2787, lr=0.000600, tokens/sec=1374880.20, grad_norm=0.4605, duration=0.38s
Step 1079: loss=4.1838, lr=0.000600, tokens/sec=1374557.92, grad_norm=0.3943, duration=0.38s
Step 1080: loss=4.2507, lr=0.000600, tokens/sec=1375138.13, grad_norm=0.4565, duration=0.38s
Step 1081: loss=4.1640, lr=0.000600, tokens/sec=1377021.37, grad_norm=0.4832, duration=0.38s
Step 1082: loss=4.2248, lr=0.000600, tokens/sec=1376587.77, grad_norm=0.5004, duration=0.38s
Step 1083: loss=4.1909, lr=0.000600, tokens/sec=1372901.65, grad_norm=0.4370, duration=0.38s
Step 1084: loss=4.2803, lr=0.000600, tokens/sec=1374515.82, grad_norm=0.3948, duration=0.38s
Step 1085: loss=4.2879, lr=0.000600, tokens/sec=1374536.44, grad_norm=0.5050, duration=0.38s
Step 1086: loss=4.2450, lr=0.000600, tokens/sec=1377378.45, grad_norm=0.5139, duration=0.38s
Step 1087: loss=4.2042, lr=0.000600, tokens/sec=1377802.18, grad_norm=0.5017, duration=0.38s
Step 1088: loss=4.2573, lr=0.000600, tokens/sec=1377124.85, grad_norm=0.4838, duration=0.38s
Step 1089: loss=4.2841, lr=0.000600, tokens/sec=1375060.74, grad_norm=0.5962, duration=0.38s
Step 1090: loss=4.3685, lr=0.000600, tokens/sec=1375421.10, grad_norm=0.7478, duration=0.38s
Step 1091: loss=4.3854, lr=0.000600, tokens/sec=1374939.51, grad_norm=0.7963, duration=0.38s
Step 1092: loss=4.3957, lr=0.000600, tokens/sec=1374378.37, grad_norm=0.5744, duration=0.38s
Step 1093: loss=4.3565, lr=0.000600, tokens/sec=1380059.86, grad_norm=0.8090, duration=0.38s
Step 1094: loss=4.3858, lr=0.000600, tokens/sec=1382643.05, grad_norm=0.6839, duration=0.38s
Step 1095: loss=4.3350, lr=0.000600, tokens/sec=1377173.15, grad_norm=0.6449, duration=0.38s
Step 1096: loss=4.3617, lr=0.000600, tokens/sec=1378780.95, grad_norm=0.6737, duration=0.38s
Step 1097: loss=4.4075, lr=0.000600, tokens/sec=1377006.71, grad_norm=0.6073, duration=0.38s
Step 1098: loss=4.4831, lr=0.000600, tokens/sec=1380235.70, grad_norm=0.5996, duration=0.38s
Step 1099: loss=4.3919, lr=0.000600, tokens/sec=1381143.33, grad_norm=0.6382, duration=0.38s
Step 1100/19073 (5.8%), Elapsed time: 455.70s, Steps per hour: 8689.86, Estimated hours remaining: 2.07
Step 1100: loss=4.3576, lr=0.000600, tokens/sec=1377483.71, grad_norm=0.6325, duration=0.38s
Step 1101: loss=4.2828, lr=0.000600, tokens/sec=1378587.33, grad_norm=0.5187, duration=0.38s
Step 1102: loss=4.3427, lr=0.000600, tokens/sec=1377511.32, grad_norm=0.5306, duration=0.38s
Step 1103: loss=4.3182, lr=0.000600, tokens/sec=1375572.53, grad_norm=0.4945, duration=0.38s
Step 1104: loss=4.2982, lr=0.000600, tokens/sec=1374392.97, grad_norm=0.5263, duration=0.38s
Step 1105: loss=4.3552, lr=0.000600, tokens/sec=1378908.91, grad_norm=0.4708, duration=0.38s
Step 1106: loss=4.2878, lr=0.000600, tokens/sec=1374722.05, grad_norm=0.5036, duration=0.38s
Step 1107: loss=4.3403, lr=0.000600, tokens/sec=1376418.89, grad_norm=0.4889, duration=0.38s
Step 1108: loss=4.3907, lr=0.000600, tokens/sec=1376623.97, grad_norm=0.4369, duration=0.38s
Step 1109: loss=4.3058, lr=0.000600, tokens/sec=1377128.30, grad_norm=0.3852, duration=0.38s
Step 1110: loss=4.2067, lr=0.000600, tokens/sec=1377471.63, grad_norm=0.3565, duration=0.38s
Step 1111: loss=4.2840, lr=0.000600, tokens/sec=1376675.68, grad_norm=0.3721, duration=0.38s
Step 1112: loss=4.2725, lr=0.000600, tokens/sec=1373791.08, grad_norm=0.4547, duration=0.38s
Step 1113: loss=4.2392, lr=0.000600, tokens/sec=1377055.86, grad_norm=0.6084, duration=0.38s
Step 1114: loss=4.1796, lr=0.000600, tokens/sec=1373650.34, grad_norm=0.5989, duration=0.38s
Step 1115: loss=4.2395, lr=0.000600, tokens/sec=1376394.77, grad_norm=0.5321, duration=0.38s
Step 1116: loss=4.2462, lr=0.000600, tokens/sec=1371751.49, grad_norm=0.5279, duration=0.38s
Step 1117: loss=4.2649, lr=0.000600, tokens/sec=1378888.16, grad_norm=0.4508, duration=0.38s
Step 1118: loss=4.2715, lr=0.000600, tokens/sec=1378145.84, grad_norm=0.4969, duration=0.38s
Step 1119: loss=4.1960, lr=0.000600, tokens/sec=1375074.49, grad_norm=0.5726, duration=0.38s
Step 1120: loss=4.2106, lr=0.000600, tokens/sec=1378729.95, grad_norm=0.4879, duration=0.38s
Step 1121: loss=4.1921, lr=0.000600, tokens/sec=1377361.19, grad_norm=0.4149, duration=0.38s
Step 1122: loss=4.1504, lr=0.000600, tokens/sec=1381014.95, grad_norm=0.4130, duration=0.38s
Step 1123: loss=4.1832, lr=0.000600, tokens/sec=1377259.40, grad_norm=0.4323, duration=0.38s
Step 1124: loss=4.1186, lr=0.000600, tokens/sec=1379729.95, grad_norm=0.3953, duration=0.38s
Step 1125: loss=4.1280, lr=0.000600, tokens/sec=1379849.43, grad_norm=0.4255, duration=0.38s
Step 1126: loss=4.1166, lr=0.000600, tokens/sec=1376712.74, grad_norm=0.4353, duration=0.38s
Step 1127: loss=4.1922, lr=0.000600, tokens/sec=1381510.36, grad_norm=0.3918, duration=0.38s
Step 1128: loss=4.1316, lr=0.000600, tokens/sec=1378806.89, grad_norm=0.4148, duration=0.38s
Step 1129: loss=4.0952, lr=0.000600, tokens/sec=1376879.97, grad_norm=0.4629, duration=0.38s
Step 1130: loss=4.0882, lr=0.000600, tokens/sec=1380135.21, grad_norm=0.4625, duration=0.38s
Step 1131: loss=4.1654, lr=0.000600, tokens/sec=1377478.53, grad_norm=0.4073, duration=0.38s
Step 1132: loss=4.1821, lr=0.000600, tokens/sec=1374263.27, grad_norm=0.3719, duration=0.38s
Step 1133: loss=4.1815, lr=0.000600, tokens/sec=1377834.98, grad_norm=0.4020, duration=0.38s
Step 1134: loss=4.1702, lr=0.000600, tokens/sec=1379194.30, grad_norm=0.4075, duration=0.38s
Step 1135: loss=4.2604, lr=0.000600, tokens/sec=1378498.32, grad_norm=0.4742, duration=0.38s
Step 1136: loss=4.2658, lr=0.000600, tokens/sec=1377156.76, grad_norm=0.5350, duration=0.38s
Step 1137: loss=4.3067, lr=0.000600, tokens/sec=1375993.43, grad_norm=0.5578, duration=0.38s
Step 1138: loss=4.2160, lr=0.000600, tokens/sec=1377261.12, grad_norm=0.6378, duration=0.38s
Step 1139: loss=4.3144, lr=0.000600, tokens/sec=1375567.36, grad_norm=0.6090, duration=0.38s
Step 1140: loss=4.2772, lr=0.000600, tokens/sec=1376306.90, grad_norm=0.5362, duration=0.38s
Step 1141: loss=4.3284, lr=0.000600, tokens/sec=1376757.56, grad_norm=0.5291, duration=0.38s
Step 1142: loss=4.3431, lr=0.000600, tokens/sec=1378366.98, grad_norm=0.5250, duration=0.38s
Step 1143: loss=4.3549, lr=0.000600, tokens/sec=1375176.82, grad_norm=0.4945, duration=0.38s
Step 1144: loss=4.2922, lr=0.000600, tokens/sec=1377727.08, grad_norm=0.4325, duration=0.38s
Step 1145: loss=4.3327, lr=0.000600, tokens/sec=1377836.71, grad_norm=0.4390, duration=0.38s
Step 1146: loss=4.3520, lr=0.000600, tokens/sec=1377312.02, grad_norm=0.4579, duration=0.38s
Step 1147: loss=4.3268, lr=0.000600, tokens/sec=1376347.39, grad_norm=0.4821, duration=0.38s
Step 1148: loss=4.3163, lr=0.000600, tokens/sec=1375170.80, grad_norm=0.5263, duration=0.38s
Step 1149: loss=4.3043, lr=0.000600, tokens/sec=1374740.09, grad_norm=0.5623, duration=0.38s
Step 1150: loss=4.3021, lr=0.000600, tokens/sec=1376299.15, grad_norm=0.4964, duration=0.38s
Step 1151: loss=4.2303, lr=0.000600, tokens/sec=1374300.21, grad_norm=0.4084, duration=0.38s
Step 1152: loss=4.3396, lr=0.000600, tokens/sec=1378831.09, grad_norm=0.4475, duration=0.38s
Step 1153: loss=4.2072, lr=0.000600, tokens/sec=1377058.45, grad_norm=0.4479, duration=0.38s
Step 1154: loss=4.2610, lr=0.000600, tokens/sec=1375401.31, grad_norm=0.4935, duration=0.38s
Step 1155: loss=4.2558, lr=0.000600, tokens/sec=1379529.14, grad_norm=0.5411, duration=0.38s
Step 1156: loss=4.2475, lr=0.000600, tokens/sec=1378984.14, grad_norm=0.5592, duration=0.38s
Step 1157: loss=4.2700, lr=0.000600, tokens/sec=1375449.49, grad_norm=0.7085, duration=0.38s
Step 1158: loss=4.2622, lr=0.000600, tokens/sec=1379377.71, grad_norm=0.8009, duration=0.38s
Step 1159: loss=4.1231, lr=0.000600, tokens/sec=1378941.77, grad_norm=0.8078, duration=0.38s
Step 1160: loss=4.2308, lr=0.000600, tokens/sec=1379907.44, grad_norm=0.4687, duration=0.38s
Step 1161: loss=4.2377, lr=0.000600, tokens/sec=1376791.17, grad_norm=0.4683, duration=0.38s
Step 1162: loss=4.2792, lr=0.000600, tokens/sec=1375835.02, grad_norm=0.4713, duration=0.38s
Step 1163: loss=4.1469, lr=0.000600, tokens/sec=1377498.38, grad_norm=0.4681, duration=0.38s
Step 1164: loss=4.2272, lr=0.000600, tokens/sec=1378537.21, grad_norm=0.4953, duration=0.38s
Step 1165: loss=4.2091, lr=0.000600, tokens/sec=1380612.65, grad_norm=0.6279, duration=0.38s
Step 1166: loss=4.2120, lr=0.000600, tokens/sec=1379902.24, grad_norm=0.6649, duration=0.38s
Step 1167: loss=4.1716, lr=0.000600, tokens/sec=1377728.80, grad_norm=0.5524, duration=0.38s
Step 1168: loss=4.2110, lr=0.000600, tokens/sec=1377269.75, grad_norm=0.5046, duration=0.38s
Step 1169: loss=4.1323, lr=0.000600, tokens/sec=1378324.65, grad_norm=0.4170, duration=0.38s
Step 1170: loss=4.1818, lr=0.000600, tokens/sec=1381158.94, grad_norm=0.4123, duration=0.38s
Step 1171: loss=4.1362, lr=0.000600, tokens/sec=1381332.46, grad_norm=0.3922, duration=0.38s
Step 1172: loss=4.2119, lr=0.000600, tokens/sec=1379343.10, grad_norm=0.6309, duration=0.38s
Step 1173: loss=4.2318, lr=0.000600, tokens/sec=1373319.20, grad_norm=0.5109, duration=0.38s
Step 1174: loss=4.1133, lr=0.000600, tokens/sec=1378835.42, grad_norm=0.5326, duration=0.38s
Step 1175: loss=4.1011, lr=0.000600, tokens/sec=1377755.56, grad_norm=0.5187, duration=0.38s
Step 1176: loss=4.1463, lr=0.000600, tokens/sec=1376931.69, grad_norm=0.4841, duration=0.38s
Step 1177: loss=4.0231, lr=0.000600, tokens/sec=1376112.26, grad_norm=0.5065, duration=0.38s
Step 1178: loss=4.0565, lr=0.000600, tokens/sec=1374091.53, grad_norm=0.4611, duration=0.38s
Step 1179: loss=4.1161, lr=0.000600, tokens/sec=1373527.65, grad_norm=0.5490, duration=0.38s
Step 1180: loss=4.1636, lr=0.000600, tokens/sec=1376985.15, grad_norm=0.5819, duration=0.38s
Step 1181: loss=4.1574, lr=0.000600, tokens/sec=1375304.97, grad_norm=0.5421, duration=0.38s
Step 1182: loss=4.1389, lr=0.000600, tokens/sec=1372038.20, grad_norm=0.4828, duration=0.38s
Step 1183: loss=4.1397, lr=0.000600, tokens/sec=1375565.64, grad_norm=0.4521, duration=0.38s
Step 1184: loss=4.1230, lr=0.000600, tokens/sec=1376922.21, grad_norm=0.4651, duration=0.38s
Step 1185: loss=4.1339, lr=0.000600, tokens/sec=1378281.46, grad_norm=0.4410, duration=0.38s
Step 1186: loss=4.1376, lr=0.000600, tokens/sec=1373106.54, grad_norm=0.4359, duration=0.38s
Step 1187: loss=4.2339, lr=0.000600, tokens/sec=1376059.73, grad_norm=0.4218, duration=0.38s
Step 1188: loss=4.3028, lr=0.000600, tokens/sec=1374792.52, grad_norm=0.4840, duration=0.38s
Step 1189: loss=4.2373, lr=0.000600, tokens/sec=1372353.31, grad_norm=0.5497, duration=0.38s
Step 1190: loss=4.2466, lr=0.000600, tokens/sec=1374399.84, grad_norm=0.5032, duration=0.38s
Step 1191: loss=4.2622, lr=0.000600, tokens/sec=1373296.05, grad_norm=0.5669, duration=0.38s
Step 1192: loss=4.2495, lr=0.000600, tokens/sec=1378058.62, grad_norm=0.5160, duration=0.38s
Step 1193: loss=4.2263, lr=0.000600, tokens/sec=1376294.84, grad_norm=0.4797, duration=0.38s
Step 1194: loss=4.1674, lr=0.000600, tokens/sec=1376285.37, grad_norm=0.4387, duration=0.38s
Step 1195: loss=4.1992, lr=0.000600, tokens/sec=1374204.88, grad_norm=0.4111, duration=0.38s
Step 1196: loss=4.2220, lr=0.000600, tokens/sec=1373521.64, grad_norm=0.3863, duration=0.38s
Step 1197: loss=4.1899, lr=0.000600, tokens/sec=1375231.86, grad_norm=0.3812, duration=0.38s
Step 1198: loss=4.1728, lr=0.000600, tokens/sec=1375159.62, grad_norm=0.3565, duration=0.38s
Step 1199: loss=4.1975, lr=0.000600, tokens/sec=1379574.15, grad_norm=0.3479, duration=0.38s
Step 1200/19073 (6.3%), Elapsed time: 493.87s, Steps per hour: 8747.17, Estimated hours remaining: 2.04
Step 1200: loss=4.1775, lr=0.000600, tokens/sec=1378570.91, grad_norm=0.3841, duration=0.38s
Step 1201: loss=4.1320, lr=0.000600, tokens/sec=1376573.99, grad_norm=0.3674, duration=0.38s
Step 1202: loss=4.1407, lr=0.000600, tokens/sec=1375354.86, grad_norm=0.3486, duration=0.38s
Step 1203: loss=4.1508, lr=0.000600, tokens/sec=1375787.68, grad_norm=0.3705, duration=0.38s
Step 1204: loss=4.1504, lr=0.000600, tokens/sec=1375623.30, grad_norm=0.3769, duration=0.38s
Step 1205: loss=4.2197, lr=0.000600, tokens/sec=1378963.38, grad_norm=0.3641, duration=0.38s
Step 1206: loss=4.1391, lr=0.000600, tokens/sec=1375153.60, grad_norm=0.4404, duration=0.38s
Step 1207: loss=4.2491, lr=0.000600, tokens/sec=1377769.38, grad_norm=0.4459, duration=0.38s
Step 1208: loss=4.1702, lr=0.000600, tokens/sec=1376812.73, grad_norm=0.4431, duration=0.38s
Step 1209: loss=4.1235, lr=0.000600, tokens/sec=1381643.16, grad_norm=0.4526, duration=0.38s
Step 1210: loss=4.1484, lr=0.000600, tokens/sec=1378441.29, grad_norm=0.4353, duration=0.38s
Step 1211: loss=4.1697, lr=0.000600, tokens/sec=1377814.26, grad_norm=0.3811, duration=0.38s
Step 1212: loss=4.1037, lr=0.000600, tokens/sec=1379681.48, grad_norm=0.4189, duration=0.38s
Step 1213: loss=4.1140, lr=0.000600, tokens/sec=1375826.41, grad_norm=0.4568, duration=0.38s
Step 1214: loss=4.1089, lr=0.000600, tokens/sec=1375661.16, grad_norm=0.4802, duration=0.38s
Step 1215: loss=4.1431, lr=0.000600, tokens/sec=1377450.92, grad_norm=0.4616, duration=0.38s
Step 1216: loss=4.1153, lr=0.000600, tokens/sec=1377102.43, grad_norm=0.4281, duration=0.38s
Step 1217: loss=4.0636, lr=0.000600, tokens/sec=1380744.41, grad_norm=0.4658, duration=0.38s
Step 1218: loss=4.1195, lr=0.000600, tokens/sec=1378700.56, grad_norm=0.4788, duration=0.38s
Step 1219: loss=4.1164, lr=0.000600, tokens/sec=1379104.35, grad_norm=0.5826, duration=0.38s
Step 1220: loss=4.1177, lr=0.000600, tokens/sec=1376854.97, grad_norm=0.5508, duration=0.38s
Step 1221: loss=4.0565, lr=0.000600, tokens/sec=1375581.13, grad_norm=0.5267, duration=0.38s
Step 1222: loss=4.1102, lr=0.000600, tokens/sec=1378999.70, grad_norm=0.5555, duration=0.38s
Step 1223: loss=4.0090, lr=0.000600, tokens/sec=1379647.72, grad_norm=0.4876, duration=0.38s
Step 1224: loss=4.0558, lr=0.000600, tokens/sec=1378106.11, grad_norm=0.4733, duration=0.38s
Step 1225: loss=3.9820, lr=0.000600, tokens/sec=1378330.70, grad_norm=0.4243, duration=0.38s
Step 1226: loss=4.0752, lr=0.000600, tokens/sec=1378995.38, grad_norm=0.4110, duration=0.38s
Step 1227: loss=4.1133, lr=0.000600, tokens/sec=1377238.70, grad_norm=0.4348, duration=0.38s
Step 1228: loss=4.0646, lr=0.000600, tokens/sec=1380376.92, grad_norm=0.4362, duration=0.38s
Step 1229: loss=4.0744, lr=0.000600, tokens/sec=1379235.82, grad_norm=0.4424, duration=0.38s
Step 1230: loss=4.0491, lr=0.000600, tokens/sec=1378817.26, grad_norm=0.4120, duration=0.38s
Step 1231: loss=4.0867, lr=0.000600, tokens/sec=1378829.37, grad_norm=0.4873, duration=0.38s
Step 1232: loss=4.0689, lr=0.000600, tokens/sec=1377268.89, grad_norm=0.5655, duration=0.38s
Step 1233: loss=4.1621, lr=0.000600, tokens/sec=1377044.65, grad_norm=0.4514, duration=0.38s
Step 1234: loss=4.2089, lr=0.000600, tokens/sec=1377276.65, grad_norm=0.4188, duration=0.38s
Step 1235: loss=4.1953, lr=0.000600, tokens/sec=1379189.98, grad_norm=0.3844, duration=0.38s
Step 1236: loss=4.2159, lr=0.000600, tokens/sec=1379408.86, grad_norm=0.3441, duration=0.38s
Step 1237: loss=4.1657, lr=0.000600, tokens/sec=1375458.95, grad_norm=0.4032, duration=0.38s
Step 1238: loss=4.1541, lr=0.000600, tokens/sec=1379228.90, grad_norm=0.4600, duration=0.38s
Step 1239: loss=4.1745, lr=0.000600, tokens/sec=1379139.81, grad_norm=0.4565, duration=0.38s
Step 1240: loss=4.1483, lr=0.000600, tokens/sec=1376546.41, grad_norm=0.4663, duration=0.38s
Step 1241: loss=4.2088, lr=0.000600, tokens/sec=1377553.60, grad_norm=0.5355, duration=0.38s
Step 1242: loss=4.1423, lr=0.000600, tokens/sec=1379221.12, grad_norm=0.5838, duration=0.38s
Step 1243: loss=4.1642, lr=0.000600, tokens/sec=1380384.72, grad_norm=0.7705, duration=0.38s
Step 1244: loss=4.1991, lr=0.000600, tokens/sec=1381721.29, grad_norm=0.6596, duration=0.38s
Step 1245: loss=4.1753, lr=0.000600, tokens/sec=1378717.85, grad_norm=0.5308, duration=0.38s
Step 1246: loss=4.1741, lr=0.000600, tokens/sec=1373405.83, grad_norm=0.5324, duration=0.38s
Step 1247: loss=4.1282, lr=0.000600, tokens/sec=1376704.98, grad_norm=0.5822, duration=0.38s
Step 1248: loss=4.1760, lr=0.000600, tokens/sec=1377710.68, grad_norm=0.5345, duration=0.38s
Step 1249: loss=4.1136, lr=0.000600, tokens/sec=1381395.80, grad_norm=0.4918, duration=0.38s
Validation loss at step 1250: 4.165469646453857
Step 1250: loss=4.1393, lr=0.000600, tokens/sec=156870.85, grad_norm=0.4241, duration=3.34s
Step 1251: loss=4.1256, lr=0.000600, tokens/sec=1375678.37, grad_norm=0.3635, duration=0.38s
Step 1252: loss=4.1236, lr=0.000600, tokens/sec=1376125.17, grad_norm=0.3747, duration=0.38s
Step 1253: loss=4.0792, lr=0.000600, tokens/sec=1378107.84, grad_norm=0.3350, duration=0.38s
Step 1254: loss=4.1818, lr=0.000600, tokens/sec=1378920.15, grad_norm=0.3413, duration=0.38s
Step 1255: loss=4.0936, lr=0.000600, tokens/sec=1373997.09, grad_norm=0.3519, duration=0.38s
Step 1256: loss=4.1289, lr=0.000600, tokens/sec=1374286.46, grad_norm=0.3924, duration=0.38s
Step 1257: loss=4.0756, lr=0.000600, tokens/sec=1375690.42, grad_norm=0.3787, duration=0.38s
Step 1258: loss=4.0988, lr=0.000600, tokens/sec=1382933.47, grad_norm=0.4259, duration=0.38s
Step 1259: loss=4.0773, lr=0.000600, tokens/sec=1374716.03, grad_norm=0.5564, duration=0.38s
Step 1260: loss=4.0786, lr=0.000600, tokens/sec=1378146.71, grad_norm=0.4891, duration=0.38s
Step 1261: loss=4.0776, lr=0.000600, tokens/sec=1379478.95, grad_norm=0.4220, duration=0.38s
Step 1262: loss=4.0616, lr=0.000600, tokens/sec=1378700.56, grad_norm=0.4749, duration=0.38s
Step 1263: loss=4.0468, lr=0.000600, tokens/sec=1379690.13, grad_norm=0.4979, duration=0.38s
Step 1264: loss=4.0771, lr=0.000600, tokens/sec=1374192.85, grad_norm=0.4800, duration=0.38s
Step 1265: loss=4.0904, lr=0.000600, tokens/sec=1373079.10, grad_norm=0.5562, duration=0.38s
Step 1266: loss=4.1472, lr=0.000600, tokens/sec=1377903.19, grad_norm=0.5233, duration=0.38s
Step 1267: loss=4.0905, lr=0.000600, tokens/sec=1376152.73, grad_norm=0.4829, duration=0.38s
Step 1268: loss=4.0318, lr=0.000600, tokens/sec=1378013.71, grad_norm=0.3818, duration=0.38s
Step 1269: loss=3.9826, lr=0.000600, tokens/sec=1376505.91, grad_norm=0.3007, duration=0.38s
Step 1270: loss=4.0133, lr=0.000600, tokens/sec=1378303.05, grad_norm=0.3028, duration=0.38s
Step 1271: loss=3.9595, lr=0.000600, tokens/sec=1379836.44, grad_norm=0.3193, duration=0.38s
Step 1272: loss=4.0085, lr=0.000600, tokens/sec=1377726.22, grad_norm=0.3261, duration=0.38s
Step 1273: loss=4.0021, lr=0.000600, tokens/sec=1375661.16, grad_norm=0.3216, duration=0.38s
Step 1274: loss=4.0829, lr=0.000600, tokens/sec=1374252.97, grad_norm=0.3519, duration=0.38s
Step 1275: loss=4.0569, lr=0.000600, tokens/sec=1375897.00, grad_norm=0.3936, duration=0.38s
Step 1276: loss=3.9905, lr=0.000600, tokens/sec=1375074.49, grad_norm=0.3954, duration=0.38s
Step 1277: loss=4.0118, lr=0.000600, tokens/sec=1376051.98, grad_norm=0.4185, duration=0.38s
Step 1278: loss=4.0217, lr=0.000600, tokens/sec=1374840.65, grad_norm=0.4950, duration=0.38s
Step 1279: loss=4.0684, lr=0.000600, tokens/sec=1378940.04, grad_norm=0.6276, duration=0.38s
Step 1280: loss=4.1801, lr=0.000600, tokens/sec=1377087.77, grad_norm=0.6264, duration=0.38s
Step 1281: loss=4.1533, lr=0.000600, tokens/sec=1376538.66, grad_norm=0.5645, duration=0.38s
Step 1282: loss=4.1700, lr=0.000600, tokens/sec=1375313.57, grad_norm=0.5144, duration=0.38s
Step 1283: loss=4.1224, lr=0.000600, tokens/sec=1375261.97, grad_norm=0.4038, duration=0.38s
Step 1284: loss=4.1373, lr=0.000600, tokens/sec=1376920.49, grad_norm=0.3818, duration=0.38s
Step 1285: loss=4.0925, lr=0.000600, tokens/sec=1375235.30, grad_norm=0.3900, duration=0.38s
Step 1286: loss=4.1183, lr=0.000600, tokens/sec=1377375.00, grad_norm=0.4274, duration=0.38s
Step 1287: loss=4.1942, lr=0.000600, tokens/sec=1377003.26, grad_norm=0.4633, duration=0.38s
Step 1288: loss=4.2289, lr=0.000600, tokens/sec=1375635.34, grad_norm=0.5167, duration=0.38s
Step 1289: loss=4.1428, lr=0.000600, tokens/sec=1374969.60, grad_norm=0.4821, duration=0.38s
Step 1290: loss=4.1339, lr=0.000600, tokens/sec=1375576.83, grad_norm=0.4410, duration=0.38s
Step 1291: loss=4.0833, lr=0.000600, tokens/sec=1374908.56, grad_norm=0.4129, duration=0.38s
Step 1292: loss=4.1181, lr=0.000600, tokens/sec=1375191.44, grad_norm=0.4309, duration=0.38s
Step 1293: loss=4.0971, lr=0.000600, tokens/sec=1374692.83, grad_norm=0.3774, duration=0.38s
Step 1294: loss=4.0850, lr=0.000600, tokens/sec=1378562.27, grad_norm=0.3200, duration=0.38s
Step 1295: loss=4.1344, lr=0.000600, tokens/sec=1378160.53, grad_norm=0.3737, duration=0.38s
Step 1296: loss=4.0475, lr=0.000600, tokens/sec=1372523.76, grad_norm=0.3540, duration=0.38s
Step 1297: loss=4.1286, lr=0.000600, tokens/sec=1373343.22, grad_norm=0.4001, duration=0.38s
Step 1298: loss=4.1851, lr=0.000600, tokens/sec=1374210.89, grad_norm=0.4618, duration=0.38s
Step 1299: loss=4.0942, lr=0.000600, tokens/sec=1377809.95, grad_norm=0.4511, duration=0.38s
Step 1300/19073 (6.8%), Elapsed time: 534.99s, Steps per hour: 8747.89, Estimated hours remaining: 2.03
Step 1300: loss=3.9979, lr=0.000600, tokens/sec=1376097.62, grad_norm=0.4167, duration=0.38s
Step 1301: loss=4.1104, lr=0.000600, tokens/sec=1373139.98, grad_norm=0.4446, duration=0.38s
Step 1302: loss=4.0859, lr=0.000600, tokens/sec=1375211.22, grad_norm=0.4601, duration=0.38s
Step 1303: loss=4.0357, lr=0.000600, tokens/sec=1375070.19, grad_norm=0.5078, duration=0.38s
Step 1304: loss=3.9682, lr=0.000600, tokens/sec=1368432.58, grad_norm=0.4707, duration=0.38s
Step 1305: loss=4.0770, lr=0.000600, tokens/sec=1376542.96, grad_norm=0.4441, duration=0.38s
Step 1306: loss=4.0481, lr=0.000600, tokens/sec=1378786.14, grad_norm=0.4547, duration=0.38s
Step 1307: loss=4.0473, lr=0.000600, tokens/sec=1376160.48, grad_norm=0.4551, duration=0.38s
Step 1308: loss=4.0761, lr=0.000600, tokens/sec=1376474.03, grad_norm=0.4120, duration=0.38s
Step 1309: loss=3.9998, lr=0.000600, tokens/sec=1376033.90, grad_norm=0.3805, duration=0.38s
Step 1310: loss=3.9966, lr=0.000600, tokens/sec=1376608.46, grad_norm=0.3710, duration=0.38s
Step 1311: loss=4.0145, lr=0.000600, tokens/sec=1374771.89, grad_norm=0.3747, duration=0.38s
Step 1312: loss=3.9342, lr=0.000600, tokens/sec=1379404.53, grad_norm=0.3350, duration=0.38s
Step 1313: loss=3.9743, lr=0.000600, tokens/sec=1377069.66, grad_norm=0.3310, duration=0.38s
Step 1314: loss=3.9359, lr=0.000600, tokens/sec=1377276.65, grad_norm=0.3342, duration=0.38s
Step 1315: loss=3.9225, lr=0.000600, tokens/sec=1375490.79, grad_norm=0.3473, duration=0.38s
Step 1316: loss=3.9304, lr=0.000600, tokens/sec=1375926.27, grad_norm=0.3652, duration=0.38s
Step 1317: loss=4.0085, lr=0.000600, tokens/sec=1378872.59, grad_norm=0.4256, duration=0.38s
Step 1318: loss=3.9226, lr=0.000600, tokens/sec=1373322.63, grad_norm=0.4681, duration=0.38s
Step 1319: loss=3.9409, lr=0.000600, tokens/sec=1372096.42, grad_norm=0.4535, duration=0.38s
Step 1320: loss=3.9322, lr=0.000600, tokens/sec=1376900.66, grad_norm=0.4686, duration=0.38s
Step 1321: loss=3.9958, lr=0.000600, tokens/sec=1380115.29, grad_norm=0.5129, duration=0.38s
Step 1322: loss=4.0105, lr=0.000600, tokens/sec=1378841.47, grad_norm=0.4141, duration=0.38s
Step 1323: loss=3.9677, lr=0.000600, tokens/sec=1374977.34, grad_norm=0.3710, duration=0.38s
Step 1324: loss=4.0137, lr=0.000600, tokens/sec=1377331.86, grad_norm=0.4048, duration=0.38s
Step 1325: loss=4.0595, lr=0.000600, tokens/sec=1371229.71, grad_norm=0.4270, duration=0.38s
Step 1326: loss=4.0930, lr=0.000600, tokens/sec=1374331.13, grad_norm=0.4533, duration=0.38s
Step 1327: loss=4.0687, lr=0.000600, tokens/sec=1378082.80, grad_norm=0.5550, duration=0.38s
Step 1328: loss=4.0829, lr=0.000600, tokens/sec=1375634.48, grad_norm=0.5843, duration=0.38s
Step 1329: loss=4.0934, lr=0.000600, tokens/sec=1375580.27, grad_norm=0.4768, duration=0.38s
Step 1330: loss=4.0971, lr=0.000600, tokens/sec=1376569.68, grad_norm=0.3945, duration=0.38s
Step 1331: loss=4.1400, lr=0.000600, tokens/sec=1373081.67, grad_norm=0.3955, duration=0.38s
Step 1332: loss=4.1156, lr=0.000600, tokens/sec=1377386.21, grad_norm=0.3509, duration=0.38s
Step 1333: loss=4.1587, lr=0.000600, tokens/sec=1373116.83, grad_norm=0.3801, duration=0.38s
Step 1334: loss=4.1238, lr=0.000600, tokens/sec=1378229.63, grad_norm=0.4227, duration=0.38s
Step 1335: loss=4.0987, lr=0.000600, tokens/sec=1376724.80, grad_norm=0.4128, duration=0.38s
Step 1336: loss=4.2097, lr=0.000600, tokens/sec=1374016.83, grad_norm=0.4144, duration=0.38s
Step 1337: loss=4.1063, lr=0.000600, tokens/sec=1377293.04, grad_norm=0.4958, duration=0.38s
Step 1338: loss=4.1671, lr=0.000600, tokens/sec=1376695.50, grad_norm=0.5525, duration=0.38s
Step 1339: loss=4.0972, lr=0.000600, tokens/sec=1376072.64, grad_norm=0.4529, duration=0.38s
Step 1340: loss=4.1098, lr=0.000600, tokens/sec=1373703.54, grad_norm=0.4478, duration=0.38s
Step 1341: loss=4.1140, lr=0.000600, tokens/sec=1372923.94, grad_norm=0.4287, duration=0.38s
Step 1342: loss=4.0908, lr=0.000600, tokens/sec=1373985.07, grad_norm=0.4746, duration=0.38s
Step 1343: loss=4.0482, lr=0.000600, tokens/sec=1372272.80, grad_norm=0.5268, duration=0.38s
Step 1344: loss=4.0667, lr=0.000600, tokens/sec=1378852.71, grad_norm=0.4696, duration=0.38s
Step 1345: loss=4.0897, lr=0.000600, tokens/sec=1377559.64, grad_norm=0.4400, duration=0.38s
Step 1346: loss=4.0338, lr=0.000600, tokens/sec=1376671.37, grad_norm=0.3968, duration=0.38s
Step 1347: loss=4.0698, lr=0.000600, tokens/sec=1371351.99, grad_norm=0.3632, duration=0.38s
Step 1348: loss=4.0171, lr=0.000600, tokens/sec=1375098.57, grad_norm=0.3798, duration=0.38s
Step 1349: loss=3.9349, lr=0.000600, tokens/sec=1376089.00, grad_norm=0.5423, duration=0.38s
Step 1350: loss=4.0586, lr=0.000600, tokens/sec=1376138.09, grad_norm=0.3537, duration=0.38s
Step 1351: loss=4.0027, lr=0.000600, tokens/sec=1379106.08, grad_norm=0.3125, duration=0.38s
Step 1352: loss=4.0772, lr=0.000600, tokens/sec=1377324.96, grad_norm=0.2921, duration=0.38s
Step 1353: loss=3.9523, lr=0.000600, tokens/sec=1370350.43, grad_norm=0.3296, duration=0.38s
Step 1354: loss=4.0509, lr=0.000600, tokens/sec=1372379.86, grad_norm=0.3667, duration=0.38s
Step 1355: loss=4.0347, lr=0.000600, tokens/sec=1374600.88, grad_norm=0.4361, duration=0.38s
Step 1356: loss=3.9692, lr=0.000600, tokens/sec=1378005.08, grad_norm=0.4302, duration=0.38s
Step 1357: loss=3.9863, lr=0.000600, tokens/sec=1374070.92, grad_norm=0.3837, duration=0.38s
Step 1358: loss=3.9916, lr=0.000600, tokens/sec=1373797.94, grad_norm=0.4215, duration=0.38s
Step 1359: loss=3.9748, lr=0.000600, tokens/sec=1376543.83, grad_norm=0.4242, duration=0.38s
Step 1360: loss=3.9587, lr=0.000600, tokens/sec=1372768.81, grad_norm=0.4082, duration=0.38s
Step 1361: loss=4.0071, lr=0.000600, tokens/sec=1377404.33, grad_norm=0.4174, duration=0.38s
Step 1362: loss=4.0347, lr=0.000600, tokens/sec=1377888.51, grad_norm=0.7827, duration=0.38s
Step 1363: loss=4.0452, lr=0.000600, tokens/sec=1374508.09, grad_norm=0.5242, duration=0.38s
Step 1364: loss=3.9962, lr=0.000600, tokens/sec=1378504.37, grad_norm=0.5393, duration=0.38s
Step 1365: loss=3.9202, lr=0.000600, tokens/sec=1376829.10, grad_norm=0.6273, duration=0.38s
Step 1366: loss=3.9649, lr=0.000600, tokens/sec=1376357.73, grad_norm=0.6847, duration=0.38s
Step 1367: loss=3.9198, lr=0.000600, tokens/sec=1375847.07, grad_norm=0.5952, duration=0.38s
Step 1368: loss=3.9407, lr=0.000600, tokens/sec=1374734.94, grad_norm=0.7427, duration=0.38s
Step 1369: loss=4.0231, lr=0.000600, tokens/sec=1368225.68, grad_norm=0.6269, duration=0.38s
Step 1370: loss=4.0074, lr=0.000600, tokens/sec=1375421.96, grad_norm=0.5654, duration=0.38s
Step 1371: loss=4.0076, lr=0.000600, tokens/sec=1372511.77, grad_norm=0.4379, duration=0.38s
Step 1372: loss=3.9589, lr=0.000600, tokens/sec=1379025.65, grad_norm=0.3523, duration=0.38s
Step 1373: loss=3.9840, lr=0.000600, tokens/sec=1376020.98, grad_norm=0.3348, duration=0.38s
Step 1374: loss=3.9911, lr=0.000600, tokens/sec=1376921.35, grad_norm=0.3464, duration=0.38s
Step 1375: loss=3.9651, lr=0.000600, tokens/sec=1374234.93, grad_norm=0.3462, duration=0.38s
Step 1376: loss=3.9550, lr=0.000600, tokens/sec=1375162.20, grad_norm=0.3310, duration=0.38s
Step 1377: loss=4.0958, lr=0.000600, tokens/sec=1377233.52, grad_norm=0.3684, duration=0.38s
Step 1378: loss=4.1137, lr=0.000600, tokens/sec=1378783.55, grad_norm=0.4087, duration=0.38s
Step 1379: loss=4.0757, lr=0.000600, tokens/sec=1375182.84, grad_norm=0.4609, duration=0.38s
Step 1380: loss=4.0847, lr=0.000600, tokens/sec=1377252.50, grad_norm=0.4752, duration=0.38s
Step 1381: loss=4.0687, lr=0.000600, tokens/sec=1377984.35, grad_norm=0.4178, duration=0.38s
Step 1382: loss=4.0786, lr=0.000599, tokens/sec=1375281.75, grad_norm=0.4113, duration=0.38s
Step 1383: loss=4.0505, lr=0.000599, tokens/sec=1377115.36, grad_norm=0.3756, duration=0.38s
Step 1384: loss=4.0015, lr=0.000599, tokens/sec=1376923.94, grad_norm=0.3269, duration=0.38s
Step 1385: loss=4.0448, lr=0.000599, tokens/sec=1377591.57, grad_norm=0.3135, duration=0.38s
Step 1386: loss=4.0201, lr=0.000599, tokens/sec=1378102.66, grad_norm=0.2926, duration=0.38s
Step 1387: loss=4.0255, lr=0.000599, tokens/sec=1377236.11, grad_norm=0.2694, duration=0.38s
Step 1388: loss=4.0357, lr=0.000599, tokens/sec=1377494.06, grad_norm=0.2886, duration=0.38s
Step 1389: loss=3.9863, lr=0.000599, tokens/sec=1375196.60, grad_norm=0.2812, duration=0.38s
Step 1390: loss=4.0224, lr=0.000599, tokens/sec=1372986.51, grad_norm=0.2873, duration=0.38s
Step 1391: loss=3.9551, lr=0.000599, tokens/sec=1376650.68, grad_norm=0.3589, duration=0.38s
Step 1392: loss=3.9564, lr=0.000599, tokens/sec=1379654.64, grad_norm=0.3720, duration=0.38s
Step 1393: loss=4.0105, lr=0.000599, tokens/sec=1378782.68, grad_norm=0.4248, duration=0.38s
Step 1394: loss=4.0112, lr=0.000599, tokens/sec=1373689.81, grad_norm=0.5287, duration=0.38s
Step 1395: loss=4.0829, lr=0.000599, tokens/sec=1374356.03, grad_norm=0.5650, duration=0.38s
Step 1396: loss=4.0190, lr=0.000599, tokens/sec=1376398.22, grad_norm=0.5301, duration=0.38s
Step 1397: loss=4.0939, lr=0.000599, tokens/sec=1377719.31, grad_norm=0.5241, duration=0.38s
Step 1398: loss=4.0278, lr=0.000599, tokens/sec=1375194.02, grad_norm=0.4757, duration=0.38s
Step 1399: loss=3.9953, lr=0.000599, tokens/sec=1378643.51, grad_norm=0.4136, duration=0.38s
Step 1400/19073 (7.3%), Elapsed time: 573.18s, Steps per hour: 8793.12, Estimated hours remaining: 2.01
Step 1400: loss=4.0341, lr=0.000599, tokens/sec=1373209.43, grad_norm=0.4452, duration=0.38s
Step 1401: loss=3.9798, lr=0.000599, tokens/sec=1375464.98, grad_norm=0.3729, duration=0.38s
Step 1402: loss=3.9490, lr=0.000599, tokens/sec=1378671.17, grad_norm=0.3291, duration=0.38s
Step 1403: loss=3.9726, lr=0.000599, tokens/sec=1377155.03, grad_norm=0.3126, duration=0.38s
Step 1404: loss=3.9631, lr=0.000599, tokens/sec=1376065.75, grad_norm=0.3015, duration=0.38s
Step 1405: loss=3.9698, lr=0.000599, tokens/sec=1378447.34, grad_norm=0.3057, duration=0.38s
Step 1406: loss=3.9243, lr=0.000599, tokens/sec=1377175.73, grad_norm=0.3336, duration=0.38s
Step 1407: loss=3.9249, lr=0.000599, tokens/sec=1377128.30, grad_norm=0.3413, duration=0.38s
Step 1408: loss=3.9490, lr=0.000599, tokens/sec=1380636.05, grad_norm=0.3525, duration=0.38s
Step 1409: loss=3.9327, lr=0.000599, tokens/sec=1379148.46, grad_norm=0.3298, duration=0.38s
Step 1410: loss=3.9581, lr=0.000599, tokens/sec=1377707.23, grad_norm=0.3336, duration=0.38s
Step 1411: loss=3.8749, lr=0.000599, tokens/sec=1379034.29, grad_norm=0.3599, duration=0.38s
Step 1412: loss=3.8975, lr=0.000599, tokens/sec=1373054.24, grad_norm=0.3868, duration=0.38s
Step 1413: loss=3.8770, lr=0.000599, tokens/sec=1377224.03, grad_norm=0.3905, duration=0.38s
Step 1414: loss=3.8745, lr=0.000599, tokens/sec=1379100.02, grad_norm=0.3963, duration=0.38s
Step 1415: loss=3.8690, lr=0.000599, tokens/sec=1378905.45, grad_norm=0.4043, duration=0.38s
Step 1416: loss=3.9226, lr=0.000599, tokens/sec=1380758.28, grad_norm=0.3868, duration=0.38s
Step 1417: loss=3.9494, lr=0.000599, tokens/sec=1376142.40, grad_norm=0.3711, duration=0.38s
Step 1418: loss=3.8920, lr=0.000599, tokens/sec=1380068.52, grad_norm=0.3862, duration=0.38s
Step 1419: loss=3.9181, lr=0.000599, tokens/sec=1375987.40, grad_norm=0.4019, duration=0.38s
Step 1420: loss=3.8810, lr=0.000599, tokens/sec=1377983.49, grad_norm=0.3725, duration=0.38s
Step 1421: loss=3.9176, lr=0.000599, tokens/sec=1375899.58, grad_norm=0.4009, duration=0.38s
Step 1422: loss=3.9399, lr=0.000599, tokens/sec=1377149.00, grad_norm=0.3823, duration=0.38s
Step 1423: loss=3.9922, lr=0.000599, tokens/sec=1379934.28, grad_norm=0.3305, duration=0.38s
Step 1424: loss=4.0281, lr=0.000599, tokens/sec=1376427.51, grad_norm=0.3524, duration=0.38s
Step 1425: loss=4.0749, lr=0.000599, tokens/sec=1376765.32, grad_norm=0.3893, duration=0.38s
Step 1426: loss=4.0271, lr=0.000599, tokens/sec=1375494.23, grad_norm=0.3770, duration=0.38s
Step 1427: loss=4.0333, lr=0.000599, tokens/sec=1376674.81, grad_norm=0.4257, duration=0.38s
Step 1428: loss=3.9963, lr=0.000599, tokens/sec=1378484.50, grad_norm=0.5084, duration=0.38s
Step 1429: loss=4.0369, lr=0.000599, tokens/sec=1377641.63, grad_norm=0.5242, duration=0.38s
Step 1430: loss=4.0172, lr=0.000599, tokens/sec=1379880.60, grad_norm=0.4869, duration=0.38s
Step 1431: loss=4.0178, lr=0.000599, tokens/sec=1375729.15, grad_norm=0.5511, duration=0.38s
Step 1432: loss=4.0185, lr=0.000599, tokens/sec=1378350.57, grad_norm=0.5353, duration=0.38s
Step 1433: loss=3.9944, lr=0.000599, tokens/sec=1378698.83, grad_norm=0.4866, duration=0.38s
Step 1434: loss=4.0412, lr=0.000599, tokens/sec=1376234.55, grad_norm=0.4493, duration=0.38s
Step 1435: loss=3.9952, lr=0.000599, tokens/sec=1378893.35, grad_norm=0.4355, duration=0.38s
Step 1436: loss=4.0055, lr=0.000599, tokens/sec=1376493.85, grad_norm=0.4274, duration=0.38s
Step 1437: loss=3.9673, lr=0.000599, tokens/sec=1377330.13, grad_norm=0.3798, duration=0.38s
Step 1438: loss=4.0119, lr=0.000599, tokens/sec=1374620.64, grad_norm=0.3975, duration=0.38s
Step 1439: loss=3.9444, lr=0.000599, tokens/sec=1375329.92, grad_norm=0.4279, duration=0.38s
Step 1440: loss=3.9903, lr=0.000599, tokens/sec=1374063.19, grad_norm=0.4160, duration=0.38s
Step 1441: loss=3.9736, lr=0.000599, tokens/sec=1379183.92, grad_norm=0.3508, duration=0.38s
Step 1442: loss=3.9711, lr=0.000599, tokens/sec=1376511.08, grad_norm=0.3926, duration=0.38s
Step 1443: loss=3.9605, lr=0.000599, tokens/sec=1375881.51, grad_norm=0.4317, duration=0.38s
Step 1444: loss=3.9841, lr=0.000599, tokens/sec=1373786.79, grad_norm=0.4136, duration=0.38s
Step 1445: loss=3.9546, lr=0.000599, tokens/sec=1377522.54, grad_norm=0.3249, duration=0.38s
Step 1446: loss=3.9686, lr=0.000599, tokens/sec=1374980.78, grad_norm=0.3115, duration=0.38s
Step 1447: loss=3.9377, lr=0.000599, tokens/sec=1377870.38, grad_norm=0.2853, duration=0.38s
Step 1448: loss=3.9275, lr=0.000599, tokens/sec=1378625.36, grad_norm=0.3071, duration=0.38s
Step 1449: loss=3.9250, lr=0.000599, tokens/sec=1375162.20, grad_norm=0.3194, duration=0.38s
Step 1450: loss=3.9334, lr=0.000599, tokens/sec=1370992.90, grad_norm=0.3300, duration=0.38s
Step 1451: loss=3.8856, lr=0.000599, tokens/sec=1373717.27, grad_norm=0.3378, duration=0.38s
Step 1452: loss=3.9051, lr=0.000599, tokens/sec=1372839.08, grad_norm=0.3734, duration=0.38s
Step 1453: loss=3.9069, lr=0.000599, tokens/sec=1374502.93, grad_norm=0.4350, duration=0.38s
Step 1454: loss=3.9168, lr=0.000599, tokens/sec=1374194.57, grad_norm=0.4303, duration=0.38s
Step 1455: loss=4.0049, lr=0.000599, tokens/sec=1366771.49, grad_norm=0.3990, duration=0.38s
Step 1456: loss=3.9512, lr=0.000599, tokens/sec=1367481.19, grad_norm=0.4185, duration=0.38s
Step 1457: loss=3.9011, lr=0.000599, tokens/sec=1371827.65, grad_norm=0.4332, duration=0.38s
Step 1458: loss=3.8948, lr=0.000599, tokens/sec=1371326.33, grad_norm=0.3948, duration=0.38s
Step 1459: loss=3.8251, lr=0.000599, tokens/sec=1371493.97, grad_norm=0.4161, duration=0.38s
Step 1460: loss=3.9012, lr=0.000599, tokens/sec=1373606.58, grad_norm=0.3891, duration=0.38s
Step 1461: loss=3.8266, lr=0.000599, tokens/sec=1374304.50, grad_norm=0.3656, duration=0.38s
Step 1462: loss=3.8870, lr=0.000599, tokens/sec=1374936.93, grad_norm=0.3275, duration=0.38s
Step 1463: loss=3.8827, lr=0.000599, tokens/sec=1374089.81, grad_norm=0.3241, duration=0.38s
Step 1464: loss=3.9301, lr=0.000599, tokens/sec=1371322.91, grad_norm=0.3075, duration=0.38s
Step 1465: loss=3.8751, lr=0.000599, tokens/sec=1375527.78, grad_norm=0.3085, duration=0.38s
Step 1466: loss=3.8780, lr=0.000599, tokens/sec=1372374.72, grad_norm=0.3166, duration=0.38s
Step 1467: loss=3.8397, lr=0.000599, tokens/sec=1376538.66, grad_norm=0.3520, duration=0.38s
Step 1468: loss=3.8635, lr=0.000599, tokens/sec=1371377.65, grad_norm=0.3578, duration=0.38s
Step 1469: loss=3.9346, lr=0.000599, tokens/sec=1376318.10, grad_norm=0.3358, duration=0.38s
Step 1470: loss=3.9915, lr=0.000599, tokens/sec=1371232.27, grad_norm=0.3856, duration=0.38s
Step 1471: loss=3.9843, lr=0.000599, tokens/sec=1374198.86, grad_norm=0.4780, duration=0.38s
Step 1472: loss=4.0121, lr=0.000599, tokens/sec=1374835.50, grad_norm=0.5176, duration=0.38s
Step 1473: loss=3.9721, lr=0.000599, tokens/sec=1377762.47, grad_norm=0.4579, duration=0.38s
Step 1474: loss=4.0010, lr=0.000599, tokens/sec=1378854.44, grad_norm=0.4952, duration=0.38s
Step 1475: loss=3.9586, lr=0.000599, tokens/sec=1376499.88, grad_norm=0.4588, duration=0.38s
Step 1476: loss=4.0033, lr=0.000599, tokens/sec=1377075.69, grad_norm=0.4957, duration=0.38s
Step 1477: loss=4.0257, lr=0.000599, tokens/sec=1376869.62, grad_norm=0.4299, duration=0.38s
Step 1478: loss=4.0693, lr=0.000599, tokens/sec=1376779.97, grad_norm=0.3742, duration=0.38s
Step 1479: loss=4.0005, lr=0.000599, tokens/sec=1375837.60, grad_norm=0.3471, duration=0.38s
Step 1480: loss=4.0058, lr=0.000599, tokens/sec=1373219.72, grad_norm=0.3885, duration=0.38s
Step 1481: loss=3.9398, lr=0.000599, tokens/sec=1373515.64, grad_norm=0.4319, duration=0.38s
Step 1482: loss=3.9777, lr=0.000599, tokens/sec=1375274.87, grad_norm=0.4110, duration=0.38s
Step 1483: loss=3.9740, lr=0.000599, tokens/sec=1379975.85, grad_norm=0.3579, duration=0.38s
Step 1484: loss=3.9376, lr=0.000599, tokens/sec=1376694.64, grad_norm=0.3640, duration=0.38s
Step 1485: loss=3.9888, lr=0.000599, tokens/sec=1378872.59, grad_norm=0.3741, duration=0.38s
Step 1486: loss=3.9158, lr=0.000599, tokens/sec=1378346.25, grad_norm=0.3658, duration=0.38s
Step 1487: loss=3.9925, lr=0.000599, tokens/sec=1376466.28, grad_norm=0.3250, duration=0.38s
Step 1488: loss=4.0219, lr=0.000599, tokens/sec=1370942.47, grad_norm=0.3167, duration=0.38s
Step 1489: loss=3.9379, lr=0.000599, tokens/sec=1374453.96, grad_norm=0.2931, duration=0.38s
Step 1490: loss=3.8696, lr=0.000599, tokens/sec=1379860.68, grad_norm=0.2755, duration=0.38s
Step 1491: loss=3.9608, lr=0.000599, tokens/sec=1377821.17, grad_norm=0.3103, duration=0.38s
Step 1492: loss=3.9286, lr=0.000599, tokens/sec=1376404.25, grad_norm=0.3216, duration=0.38s
Step 1493: loss=3.8849, lr=0.000599, tokens/sec=1375431.42, grad_norm=0.3754, duration=0.38s
Step 1494: loss=3.8625, lr=0.000599, tokens/sec=1375091.69, grad_norm=0.5194, duration=0.38s
Step 1495: loss=3.9529, lr=0.000599, tokens/sec=1379429.63, grad_norm=0.5646, duration=0.38s
Step 1496: loss=3.8939, lr=0.000599, tokens/sec=1378685.86, grad_norm=0.4900, duration=0.38s
Step 1497: loss=3.9144, lr=0.000599, tokens/sec=1374850.97, grad_norm=0.4389, duration=0.38s
Step 1498: loss=3.9509, lr=0.000599, tokens/sec=1374879.34, grad_norm=0.3576, duration=0.38s
Step 1499: loss=3.8627, lr=0.000599, tokens/sec=1376269.00, grad_norm=0.3535, duration=0.38s
Step 1500/19073 (7.9%), Elapsed time: 611.37s, Steps per hour: 8832.63, Estimated hours remaining: 1.99
Validation loss at step 1500: 3.9915690422058105
Step 1500: loss=3.8816, lr=0.000599, tokens/sec=156407.79, grad_norm=0.3204, duration=3.35s
Step 1501: loss=3.8643, lr=0.000599, tokens/sec=1381902.77, grad_norm=0.3396, duration=0.38s
Step 1502: loss=3.7944, lr=0.000599, tokens/sec=1377161.07, grad_norm=0.3280, duration=0.38s
Step 1503: loss=3.8620, lr=0.000599, tokens/sec=1374675.64, grad_norm=0.3294, duration=0.38s
Step 1504: loss=3.8040, lr=0.000599, tokens/sec=1381417.50, grad_norm=0.3518, duration=0.38s
Step 1505: loss=3.8023, lr=0.000599, tokens/sec=1380563.24, grad_norm=0.3209, duration=0.38s
Step 1506: loss=3.8018, lr=0.000599, tokens/sec=1380621.31, grad_norm=0.3201, duration=0.38s
Step 1507: loss=3.8534, lr=0.000599, tokens/sec=1375704.19, grad_norm=0.3186, duration=0.38s
Step 1508: loss=3.8176, lr=0.000599, tokens/sec=1370623.75, grad_norm=0.3211, duration=0.38s
Step 1509: loss=3.8354, lr=0.000599, tokens/sec=1378610.67, grad_norm=0.3021, duration=0.38s
Step 1510: loss=3.7956, lr=0.000599, tokens/sec=1379803.54, grad_norm=0.3301, duration=0.38s
Step 1511: loss=3.8502, lr=0.000599, tokens/sec=1375121.79, grad_norm=0.3693, duration=0.38s
Step 1512: loss=3.8457, lr=0.000599, tokens/sec=1382750.86, grad_norm=0.3518, duration=0.38s
Step 1513: loss=3.8566, lr=0.000599, tokens/sec=1378052.57, grad_norm=0.3641, duration=0.38s
Step 1514: loss=3.8631, lr=0.000599, tokens/sec=1377279.24, grad_norm=0.3600, duration=0.38s
Step 1515: loss=3.9359, lr=0.000599, tokens/sec=1380427.18, grad_norm=0.3476, duration=0.38s
Step 1516: loss=3.9030, lr=0.000599, tokens/sec=1378853.57, grad_norm=0.4405, duration=0.38s
Step 1517: loss=3.9850, lr=0.000599, tokens/sec=1367553.48, grad_norm=0.5130, duration=0.38s
Step 1518: loss=3.9171, lr=0.000599, tokens/sec=1379897.91, grad_norm=0.4112, duration=0.38s
Step 1519: loss=3.9687, lr=0.000599, tokens/sec=1378721.30, grad_norm=0.3512, duration=0.38s
Step 1520: loss=3.9702, lr=0.000599, tokens/sec=1375212.94, grad_norm=0.4328, duration=0.38s
Step 1521: loss=3.9874, lr=0.000599, tokens/sec=1380191.51, grad_norm=0.4994, duration=0.38s
Step 1522: loss=3.9901, lr=0.000599, tokens/sec=1376799.79, grad_norm=0.4259, duration=0.38s
Step 1523: loss=4.0546, lr=0.000599, tokens/sec=1376196.65, grad_norm=0.3623, duration=0.38s
Step 1524: loss=3.9447, lr=0.000599, tokens/sec=1376307.76, grad_norm=0.4190, duration=0.38s
Step 1525: loss=4.0223, lr=0.000599, tokens/sec=1379711.77, grad_norm=0.4293, duration=0.38s
Step 1526: loss=4.0515, lr=0.000599, tokens/sec=1377598.48, grad_norm=0.4280, duration=0.38s
Step 1527: loss=4.0057, lr=0.000599, tokens/sec=1375034.94, grad_norm=0.4535, duration=0.38s
Step 1528: loss=4.0069, lr=0.000599, tokens/sec=1375049.56, grad_norm=0.4592, duration=0.38s
Step 1529: loss=3.9582, lr=0.000599, tokens/sec=1375323.90, grad_norm=0.3777, duration=0.38s
Step 1530: loss=4.0208, lr=0.000599, tokens/sec=1377704.64, grad_norm=0.3382, duration=0.38s
Step 1531: loss=3.9281, lr=0.000599, tokens/sec=1378468.94, grad_norm=0.3982, duration=0.38s
Step 1532: loss=3.9655, lr=0.000599, tokens/sec=1377797.00, grad_norm=0.3573, duration=0.38s
Step 1533: loss=3.8819, lr=0.000599, tokens/sec=1375073.63, grad_norm=0.3359, duration=0.38s
Step 1534: loss=3.9339, lr=0.000599, tokens/sec=1377012.74, grad_norm=0.3054, duration=0.38s
Step 1535: loss=3.9227, lr=0.000599, tokens/sec=1375918.52, grad_norm=0.3558, duration=0.38s
Step 1536: loss=3.9022, lr=0.000599, tokens/sec=1376300.87, grad_norm=0.3922, duration=0.38s
Step 1537: loss=3.9321, lr=0.000599, tokens/sec=1378909.77, grad_norm=0.4281, duration=0.38s
Step 1538: loss=3.8994, lr=0.000599, tokens/sec=1377151.58, grad_norm=0.4413, duration=0.38s
Step 1539: loss=3.8473, lr=0.000599, tokens/sec=1377252.50, grad_norm=0.6172, duration=0.38s
Step 1540: loss=3.8934, lr=0.000599, tokens/sec=1373864.03, grad_norm=0.3848, duration=0.38s
Step 1541: loss=3.8791, lr=0.000599, tokens/sec=1378557.09, grad_norm=0.3644, duration=0.38s
Step 1542: loss=3.9760, lr=0.000599, tokens/sec=1376504.19, grad_norm=0.3971, duration=0.38s
Step 1543: loss=3.8558, lr=0.000599, tokens/sec=1380182.85, grad_norm=0.4506, duration=0.38s
Step 1544: loss=3.9548, lr=0.000599, tokens/sec=1376554.17, grad_norm=0.3971, duration=0.38s
Step 1545: loss=3.8847, lr=0.000599, tokens/sec=1376203.54, grad_norm=0.3927, duration=0.38s
Step 1546: loss=3.8542, lr=0.000599, tokens/sec=1376420.62, grad_norm=0.3462, duration=0.38s
Step 1547: loss=3.8380, lr=0.000599, tokens/sec=1375788.54, grad_norm=0.3760, duration=0.38s
Step 1548: loss=3.8917, lr=0.000599, tokens/sec=1378822.45, grad_norm=0.4030, duration=0.38s
Step 1549: loss=3.8201, lr=0.000599, tokens/sec=1378975.49, grad_norm=0.4232, duration=0.38s
Step 1550: loss=3.8823, lr=0.000599, tokens/sec=1377731.39, grad_norm=0.3793, duration=0.38s
Step 1551: loss=3.8687, lr=0.000599, tokens/sec=1375847.07, grad_norm=0.3471, duration=0.38s
Step 1552: loss=3.8772, lr=0.000599, tokens/sec=1377503.55, grad_norm=0.4402, duration=0.38s
Step 1553: loss=3.9546, lr=0.000599, tokens/sec=1375643.95, grad_norm=0.3692, duration=0.38s
Step 1554: loss=3.8415, lr=0.000599, tokens/sec=1377145.55, grad_norm=0.4524, duration=0.38s
Step 1555: loss=3.7403, lr=0.000599, tokens/sec=1377999.03, grad_norm=0.6006, duration=0.38s
Step 1556: loss=3.8518, lr=0.000599, tokens/sec=1378751.56, grad_norm=0.4268, duration=0.38s
Step 1557: loss=3.7990, lr=0.000599, tokens/sec=1377667.52, grad_norm=0.4389, duration=0.38s
Step 1558: loss=3.8396, lr=0.000599, tokens/sec=1378037.89, grad_norm=0.4045, duration=0.38s
Step 1559: loss=3.8630, lr=0.000599, tokens/sec=1377538.07, grad_norm=0.4222, duration=0.38s
Step 1560: loss=3.8583, lr=0.000599, tokens/sec=1377822.03, grad_norm=0.3520, duration=0.38s
Step 1561: loss=3.8453, lr=0.000599, tokens/sec=1380325.80, grad_norm=0.3182, duration=0.38s
Step 1562: loss=3.8296, lr=0.000599, tokens/sec=1378095.75, grad_norm=0.2948, duration=0.38s
Step 1563: loss=3.8820, lr=0.000599, tokens/sec=1378438.70, grad_norm=0.2786, duration=0.38s
Step 1564: loss=3.8466, lr=0.000599, tokens/sec=1375307.55, grad_norm=0.2887, duration=0.38s
Step 1565: loss=3.8158, lr=0.000599, tokens/sec=1376449.05, grad_norm=0.2843, duration=0.38s
Step 1566: loss=3.8482, lr=0.000599, tokens/sec=1377237.83, grad_norm=0.2712, duration=0.38s
Step 1567: loss=3.9448, lr=0.000599, tokens/sec=1380010.49, grad_norm=0.3166, duration=0.38s
Step 1568: loss=3.9938, lr=0.000599, tokens/sec=1374291.62, grad_norm=0.3586, duration=0.38s
Step 1569: loss=3.9546, lr=0.000599, tokens/sec=1379183.06, grad_norm=0.4231, duration=0.38s
Step 1570: loss=3.9374, lr=0.000599, tokens/sec=1379228.90, grad_norm=0.4604, duration=0.38s
Step 1571: loss=3.9641, lr=0.000599, tokens/sec=1375836.74, grad_norm=0.5171, duration=0.38s
Step 1572: loss=3.9556, lr=0.000599, tokens/sec=1375788.54, grad_norm=0.4489, duration=0.38s
Step 1573: loss=3.9417, lr=0.000599, tokens/sec=1375768.74, grad_norm=0.4060, duration=0.38s
Step 1574: loss=3.9068, lr=0.000599, tokens/sec=1375637.93, grad_norm=0.3413, duration=0.38s
Step 1575: loss=3.8996, lr=0.000599, tokens/sec=1377070.52, grad_norm=0.3256, duration=0.38s
Step 1576: loss=3.9186, lr=0.000599, tokens/sec=1375322.18, grad_norm=0.3062, duration=0.38s
Step 1577: loss=3.9452, lr=0.000599, tokens/sec=1374194.57, grad_norm=0.2981, duration=0.38s
Step 1578: loss=3.8816, lr=0.000599, tokens/sec=1378144.98, grad_norm=0.3182, duration=0.38s
Step 1579: loss=3.8919, lr=0.000599, tokens/sec=1380424.58, grad_norm=0.3145, duration=0.38s
Step 1580: loss=3.8988, lr=0.000599, tokens/sec=1378215.81, grad_norm=0.3383, duration=0.38s
Step 1581: loss=3.8158, lr=0.000599, tokens/sec=1374959.28, grad_norm=0.3605, duration=0.38s
Step 1582: loss=3.8573, lr=0.000599, tokens/sec=1376967.91, grad_norm=0.3722, duration=0.38s
Step 1583: loss=3.8920, lr=0.000599, tokens/sec=1373947.30, grad_norm=0.3483, duration=0.38s
Step 1584: loss=3.8894, lr=0.000599, tokens/sec=1378395.50, grad_norm=0.4123, duration=0.38s
Step 1585: loss=3.9649, lr=0.000599, tokens/sec=1375958.99, grad_norm=0.4144, duration=0.38s
Step 1586: loss=3.8661, lr=0.000599, tokens/sec=1379473.76, grad_norm=0.3752, duration=0.38s
Step 1587: loss=3.9512, lr=0.000599, tokens/sec=1382482.24, grad_norm=0.3427, duration=0.38s
Step 1588: loss=3.8970, lr=0.000599, tokens/sec=1376446.46, grad_norm=0.3269, duration=0.38s
Step 1589: loss=3.8874, lr=0.000599, tokens/sec=1371305.81, grad_norm=0.2919, duration=0.38s
Step 1590: loss=3.8483, lr=0.000599, tokens/sec=1380101.43, grad_norm=0.2626, duration=0.38s
Step 1591: loss=3.8481, lr=0.000599, tokens/sec=1375675.79, grad_norm=0.3084, duration=0.38s
Step 1592: loss=3.8353, lr=0.000599, tokens/sec=1378402.41, grad_norm=0.3454, duration=0.38s
Step 1593: loss=3.8680, lr=0.000599, tokens/sec=1377355.15, grad_norm=0.3876, duration=0.38s
Step 1594: loss=3.8450, lr=0.000599, tokens/sec=1376761.01, grad_norm=0.4149, duration=0.38s
Step 1595: loss=3.8303, lr=0.000599, tokens/sec=1377733.12, grad_norm=0.4027, duration=0.38s
Step 1596: loss=3.8408, lr=0.000599, tokens/sec=1379503.18, grad_norm=0.3912, duration=0.38s
Step 1597: loss=3.8027, lr=0.000599, tokens/sec=1377639.90, grad_norm=0.3755, duration=0.38s
Step 1598: loss=3.8310, lr=0.000599, tokens/sec=1375579.41, grad_norm=0.3566, duration=0.38s
Step 1599: loss=3.8435, lr=0.000599, tokens/sec=1379032.56, grad_norm=0.3940, duration=0.38s
Step 1600/19073 (8.4%), Elapsed time: 652.49s, Steps per hour: 8827.66, Estimated hours remaining: 1.98
Step 1600: loss=3.8500, lr=0.000599, tokens/sec=1380746.14, grad_norm=0.3892, duration=0.38s
Step 1601: loss=3.7217, lr=0.000599, tokens/sec=1377724.49, grad_norm=0.3436, duration=0.38s
Step 1602: loss=3.8276, lr=0.000599, tokens/sec=1377025.68, grad_norm=0.3543, duration=0.38s
Step 1603: loss=3.7586, lr=0.000599, tokens/sec=1376739.46, grad_norm=0.3951, duration=0.38s
Step 1604: loss=3.8194, lr=0.000599, tokens/sec=1371104.03, grad_norm=0.4300, duration=0.38s
Step 1605: loss=3.7632, lr=0.000599, tokens/sec=1377316.33, grad_norm=0.3632, duration=0.38s
Step 1606: loss=3.8060, lr=0.000599, tokens/sec=1375467.56, grad_norm=0.3128, duration=0.38s
Step 1607: loss=3.8231, lr=0.000599, tokens/sec=1376620.52, grad_norm=0.3178, duration=0.38s
Step 1608: loss=3.7747, lr=0.000599, tokens/sec=1377351.70, grad_norm=0.3007, duration=0.38s
Step 1609: loss=3.7847, lr=0.000599, tokens/sec=1378484.50, grad_norm=0.3083, duration=0.38s
Step 1610: loss=3.7474, lr=0.000599, tokens/sec=1374668.76, grad_norm=0.4078, duration=0.38s
Step 1611: loss=3.8486, lr=0.000599, tokens/sec=1375069.33, grad_norm=0.4340, duration=0.38s
Step 1612: loss=3.8273, lr=0.000599, tokens/sec=1373070.53, grad_norm=0.3594, duration=0.38s
Step 1613: loss=3.8630, lr=0.000599, tokens/sec=1378246.04, grad_norm=0.3406, duration=0.38s
Step 1614: loss=3.9570, lr=0.000599, tokens/sec=1375984.82, grad_norm=0.3819, duration=0.38s
Step 1615: loss=3.9224, lr=0.000599, tokens/sec=1378254.68, grad_norm=0.4349, duration=0.38s
Step 1616: loss=3.9373, lr=0.000599, tokens/sec=1379334.45, grad_norm=0.4724, duration=0.38s
Step 1617: loss=3.9139, lr=0.000599, tokens/sec=1378570.91, grad_norm=0.4082, duration=0.38s
Step 1618: loss=3.8768, lr=0.000599, tokens/sec=1378470.67, grad_norm=0.4032, duration=0.38s
Step 1619: loss=3.9305, lr=0.000599, tokens/sec=1374985.08, grad_norm=0.4143, duration=0.38s
Step 1620: loss=3.8479, lr=0.000599, tokens/sec=1375969.32, grad_norm=0.4293, duration=0.38s
Step 1621: loss=3.9149, lr=0.000599, tokens/sec=1377230.07, grad_norm=0.4475, duration=0.38s
Step 1622: loss=3.8969, lr=0.000599, tokens/sec=1377900.60, grad_norm=0.4557, duration=0.38s
Step 1623: loss=3.8978, lr=0.000599, tokens/sec=1377171.42, grad_norm=0.4373, duration=0.38s
Step 1624: loss=3.9164, lr=0.000599, tokens/sec=1379001.43, grad_norm=0.4914, duration=0.38s
Step 1625: loss=3.8718, lr=0.000599, tokens/sec=1377731.39, grad_norm=0.4349, duration=0.38s
Step 1626: loss=3.8874, lr=0.000599, tokens/sec=1374578.54, grad_norm=0.3683, duration=0.38s
Step 1627: loss=3.8500, lr=0.000599, tokens/sec=1377136.92, grad_norm=0.3179, duration=0.38s
Step 1628: loss=3.8837, lr=0.000599, tokens/sec=1378256.41, grad_norm=0.2821, duration=0.38s
Step 1629: loss=3.8296, lr=0.000599, tokens/sec=1377661.48, grad_norm=0.3120, duration=0.38s
Step 1630: loss=3.8708, lr=0.000599, tokens/sec=1378825.04, grad_norm=0.3005, duration=0.38s
Step 1631: loss=3.8547, lr=0.000599, tokens/sec=1377683.06, grad_norm=0.2889, duration=0.38s
Step 1632: loss=3.8745, lr=0.000599, tokens/sec=1378442.15, grad_norm=0.2873, duration=0.38s
Step 1633: loss=3.7839, lr=0.000599, tokens/sec=1374764.16, grad_norm=0.2989, duration=0.38s
Step 1634: loss=3.8741, lr=0.000599, tokens/sec=1373998.80, grad_norm=0.2954, duration=0.38s
Step 1635: loss=3.8288, lr=0.000599, tokens/sec=1376287.95, grad_norm=0.2892, duration=0.38s
Step 1636: loss=3.8626, lr=0.000599, tokens/sec=1377809.08, grad_norm=0.3004, duration=0.38s
Step 1637: loss=3.8077, lr=0.000599, tokens/sec=1378259.86, grad_norm=0.2969, duration=0.38s
Step 1638: loss=3.8307, lr=0.000599, tokens/sec=1378396.36, grad_norm=0.3202, duration=0.38s
Step 1639: loss=3.8306, lr=0.000599, tokens/sec=1373949.01, grad_norm=0.3628, duration=0.38s
Step 1640: loss=3.7888, lr=0.000599, tokens/sec=1375799.73, grad_norm=0.3475, duration=0.38s
Step 1641: loss=3.7748, lr=0.000599, tokens/sec=1376974.80, grad_norm=0.2940, duration=0.38s
Step 1642: loss=3.8175, lr=0.000599, tokens/sec=1379363.00, grad_norm=0.3297, duration=0.38s
Step 1643: loss=3.7882, lr=0.000599, tokens/sec=1376305.18, grad_norm=0.3689, duration=0.38s
Step 1644: loss=3.8862, lr=0.000599, tokens/sec=1377053.27, grad_norm=0.3877, duration=0.38s
Step 1645: loss=3.8640, lr=0.000599, tokens/sec=1379357.81, grad_norm=0.3801, duration=0.38s
Step 1646: loss=3.8035, lr=0.000599, tokens/sec=1377248.19, grad_norm=0.3531, duration=0.38s
Step 1647: loss=3.7986, lr=0.000599, tokens/sec=1377484.57, grad_norm=0.3857, duration=0.38s
Step 1648: loss=3.7636, lr=0.000599, tokens/sec=1377250.77, grad_norm=0.3483, duration=0.38s
Step 1649: loss=3.7428, lr=0.000599, tokens/sec=1376274.17, grad_norm=0.3330, duration=0.38s
Step 1650: loss=3.7982, lr=0.000599, tokens/sec=1373537.08, grad_norm=0.3257, duration=0.38s
Step 1651: loss=3.7372, lr=0.000599, tokens/sec=1380861.46, grad_norm=0.3530, duration=0.38s
Step 1652: loss=3.8040, lr=0.000599, tokens/sec=1379744.67, grad_norm=0.3795, duration=0.38s
Step 1653: loss=3.7671, lr=0.000599, tokens/sec=1378355.75, grad_norm=0.3789, duration=0.38s
Step 1654: loss=3.7937, lr=0.000599, tokens/sec=1377289.59, grad_norm=0.3447, duration=0.38s
Step 1655: loss=3.8059, lr=0.000599, tokens/sec=1372294.21, grad_norm=0.3175, duration=0.38s
Step 1656: loss=3.7445, lr=0.000599, tokens/sec=1376823.93, grad_norm=0.3009, duration=0.38s
Step 1657: loss=3.7256, lr=0.000599, tokens/sec=1376283.65, grad_norm=0.3377, duration=0.38s
Step 1658: loss=3.7905, lr=0.000599, tokens/sec=1376817.04, grad_norm=0.3835, duration=0.38s
Step 1659: loss=3.8270, lr=0.000599, tokens/sec=1378411.91, grad_norm=0.4259, duration=0.38s
Step 1660: loss=3.8830, lr=0.000599, tokens/sec=1380350.06, grad_norm=0.4473, duration=0.38s
Step 1661: loss=3.8800, lr=0.000599, tokens/sec=1375509.72, grad_norm=0.4581, duration=0.38s
Step 1662: loss=3.8955, lr=0.000599, tokens/sec=1378857.90, grad_norm=0.4489, duration=0.38s
Step 1663: loss=3.8749, lr=0.000599, tokens/sec=1375381.53, grad_norm=0.4380, duration=0.38s
Step 1664: loss=3.8959, lr=0.000599, tokens/sec=1380697.60, grad_norm=0.4024, duration=0.38s
Step 1665: loss=3.8637, lr=0.000599, tokens/sec=1377978.31, grad_norm=0.3314, duration=0.38s
Step 1666: loss=3.8582, lr=0.000599, tokens/sec=1377400.01, grad_norm=0.3319, duration=0.38s
Step 1667: loss=3.8956, lr=0.000599, tokens/sec=1377770.24, grad_norm=0.3059, duration=0.38s
Step 1668: loss=3.9628, lr=0.000599, tokens/sec=1379941.21, grad_norm=0.3005, duration=0.38s
Step 1669: loss=3.9038, lr=0.000599, tokens/sec=1374722.91, grad_norm=0.3085, duration=0.38s
Step 1670: loss=3.8915, lr=0.000599, tokens/sec=1376056.28, grad_norm=0.3261, duration=0.38s
Step 1671: loss=3.8256, lr=0.000599, tokens/sec=1377508.73, grad_norm=0.3311, duration=0.38s
Step 1672: loss=3.8780, lr=0.000599, tokens/sec=1378027.53, grad_norm=0.3204, duration=0.38s
Step 1673: loss=3.8430, lr=0.000599, tokens/sec=1379528.28, grad_norm=0.2795, duration=0.38s
Step 1674: loss=3.8092, lr=0.000599, tokens/sec=1377261.12, grad_norm=0.2842, duration=0.38s
Step 1675: loss=3.8739, lr=0.000599, tokens/sec=1377098.98, grad_norm=0.2708, duration=0.38s
Step 1676: loss=3.8036, lr=0.000599, tokens/sec=1378849.25, grad_norm=0.2802, duration=0.38s
Step 1677: loss=3.8692, lr=0.000599, tokens/sec=1376446.46, grad_norm=0.3257, duration=0.38s
Step 1678: loss=3.9119, lr=0.000599, tokens/sec=1374920.60, grad_norm=0.3548, duration=0.38s
Step 1679: loss=3.8503, lr=0.000599, tokens/sec=1376729.98, grad_norm=0.3334, duration=0.38s
Step 1680: loss=3.7693, lr=0.000599, tokens/sec=1376466.28, grad_norm=0.3553, duration=0.38s
Step 1681: loss=3.8477, lr=0.000599, tokens/sec=1369920.17, grad_norm=0.3643, duration=0.38s
Step 1682: loss=3.8281, lr=0.000599, tokens/sec=1373421.27, grad_norm=0.3571, duration=0.38s
Step 1683: loss=3.8197, lr=0.000599, tokens/sec=1378976.35, grad_norm=0.3630, duration=0.38s
Step 1684: loss=3.7526, lr=0.000599, tokens/sec=1378639.19, grad_norm=0.3950, duration=0.38s
Step 1685: loss=3.7982, lr=0.000599, tokens/sec=1375921.11, grad_norm=0.3362, duration=0.38s
Step 1686: loss=3.7718, lr=0.000599, tokens/sec=1377978.31, grad_norm=0.3647, duration=0.38s
Step 1687: loss=3.8154, lr=0.000599, tokens/sec=1380098.83, grad_norm=0.4336, duration=0.38s
Step 1688: loss=3.8412, lr=0.000599, tokens/sec=1379871.94, grad_norm=0.3793, duration=0.38s
Step 1689: loss=3.7765, lr=0.000599, tokens/sec=1371854.18, grad_norm=0.3593, duration=0.38s
Step 1690: loss=3.7646, lr=0.000599, tokens/sec=1378506.10, grad_norm=0.3534, duration=0.38s
Step 1691: loss=3.7527, lr=0.000599, tokens/sec=1374175.68, grad_norm=0.3835, duration=0.38s
Step 1692: loss=3.7210, lr=0.000599, tokens/sec=1374954.13, grad_norm=0.4221, duration=0.38s
Step 1693: loss=3.7617, lr=0.000599, tokens/sec=1378161.39, grad_norm=0.4033, duration=0.38s
Step 1694: loss=3.7159, lr=0.000599, tokens/sec=1376892.04, grad_norm=0.3340, duration=0.38s
Step 1695: loss=3.7040, lr=0.000599, tokens/sec=1371312.65, grad_norm=0.2814, duration=0.38s
Step 1696: loss=3.6778, lr=0.000599, tokens/sec=1374699.70, grad_norm=0.2542, duration=0.38s
Step 1697: loss=3.7806, lr=0.000598, tokens/sec=1375815.22, grad_norm=0.2797, duration=0.38s
Step 1698: loss=3.7516, lr=0.000598, tokens/sec=1377436.25, grad_norm=0.2947, duration=0.38s
Step 1699: loss=3.7391, lr=0.000598, tokens/sec=1377022.23, grad_norm=0.2999, duration=0.38s
Step 1700/19073 (8.9%), Elapsed time: 690.65s, Steps per hour: 8861.16, Estimated hours remaining: 1.96
Step 1700: loss=3.6986, lr=0.000598, tokens/sec=1376946.35, grad_norm=0.3056, duration=0.38s
Step 1701: loss=3.7236, lr=0.000598, tokens/sec=1375163.92, grad_norm=0.3505, duration=0.38s
Step 1702: loss=3.7704, lr=0.000598, tokens/sec=1378521.65, grad_norm=0.3454, duration=0.38s
Step 1703: loss=3.7438, lr=0.000598, tokens/sec=1377721.90, grad_norm=0.3525, duration=0.38s
Step 1704: loss=3.7844, lr=0.000598, tokens/sec=1375952.96, grad_norm=0.3645, duration=0.38s
Step 1705: loss=3.7867, lr=0.000598, tokens/sec=1380914.36, grad_norm=0.3374, duration=0.38s
Step 1706: loss=3.8518, lr=0.000598, tokens/sec=1375107.17, grad_norm=0.3480, duration=0.38s
Step 1707: loss=3.8548, lr=0.000598, tokens/sec=1377967.95, grad_norm=0.3541, duration=0.38s
Step 1708: loss=3.8335, lr=0.000598, tokens/sec=1378516.47, grad_norm=0.3773, duration=0.38s
Step 1709: loss=3.8811, lr=0.000598, tokens/sec=1378606.35, grad_norm=0.4133, duration=0.38s
Step 1710: loss=3.8460, lr=0.000598, tokens/sec=1376524.01, grad_norm=0.3997, duration=0.38s
Step 1711: loss=3.8782, lr=0.000598, tokens/sec=1375022.04, grad_norm=0.3930, duration=0.38s
Step 1712: loss=3.9084, lr=0.000598, tokens/sec=1373894.93, grad_norm=0.3847, duration=0.38s
Step 1713: loss=3.9066, lr=0.000598, tokens/sec=1370315.42, grad_norm=0.3940, duration=0.38s
Step 1714: loss=3.8990, lr=0.000598, tokens/sec=1375591.46, grad_norm=0.4509, duration=0.38s
Step 1715: loss=3.8942, lr=0.000598, tokens/sec=1376897.21, grad_norm=0.4199, duration=0.38s
Step 1716: loss=3.9801, lr=0.000598, tokens/sec=1374325.11, grad_norm=0.3845, duration=0.38s
Step 1717: loss=3.8702, lr=0.000598, tokens/sec=1372664.27, grad_norm=0.3377, duration=0.38s
Step 1718: loss=3.8893, lr=0.000598, tokens/sec=1376284.51, grad_norm=0.2845, duration=0.38s
Step 1719: loss=3.8929, lr=0.000598, tokens/sec=1381901.90, grad_norm=0.3317, duration=0.38s
Step 1720: loss=3.8786, lr=0.000598, tokens/sec=1375226.70, grad_norm=0.3957, duration=0.38s
Step 1721: loss=3.8437, lr=0.000598, tokens/sec=1372992.51, grad_norm=0.4303, duration=0.38s
Step 1722: loss=3.8447, lr=0.000598, tokens/sec=1372589.73, grad_norm=0.3623, duration=0.38s
Step 1723: loss=3.7956, lr=0.000598, tokens/sec=1374870.74, grad_norm=0.3500, duration=0.38s
Step 1724: loss=3.8162, lr=0.000598, tokens/sec=1374639.55, grad_norm=0.3356, duration=0.38s
Step 1725: loss=3.8299, lr=0.000598, tokens/sec=1375422.82, grad_norm=0.3394, duration=0.38s
Step 1726: loss=3.7845, lr=0.000598, tokens/sec=1376965.32, grad_norm=0.2812, duration=0.38s
Step 1727: loss=3.8357, lr=0.000598, tokens/sec=1379887.52, grad_norm=0.2848, duration=0.38s
Step 1728: loss=3.8209, lr=0.000598, tokens/sec=1375957.27, grad_norm=0.3271, duration=0.38s
Step 1729: loss=3.6977, lr=0.000598, tokens/sec=1376895.48, grad_norm=0.5072, duration=0.38s
Step 1730: loss=3.7851, lr=0.000598, tokens/sec=1376551.58, grad_norm=0.3443, duration=0.38s
Step 1731: loss=3.7937, lr=0.000598, tokens/sec=1377753.84, grad_norm=0.3772, duration=0.38s
Step 1732: loss=3.8912, lr=0.000598, tokens/sec=1374286.46, grad_norm=0.3629, duration=0.38s
Step 1733: loss=3.7739, lr=0.000598, tokens/sec=1377237.83, grad_norm=0.4424, duration=0.38s
Step 1734: loss=3.8274, lr=0.000598, tokens/sec=1377249.05, grad_norm=0.4248, duration=0.38s
Step 1735: loss=3.8022, lr=0.000598, tokens/sec=1378862.22, grad_norm=0.3351, duration=0.38s
Step 1736: loss=3.7237, lr=0.000598, tokens/sec=1376499.02, grad_norm=0.2948, duration=0.38s
Step 1737: loss=3.7621, lr=0.000598, tokens/sec=1373036.24, grad_norm=0.3169, duration=0.38s
Step 1738: loss=3.7552, lr=0.000598, tokens/sec=1371763.47, grad_norm=0.2718, duration=0.38s
Step 1739: loss=3.7659, lr=0.000598, tokens/sec=1374339.71, grad_norm=0.3121, duration=0.38s
Step 1740: loss=3.7669, lr=0.000598, tokens/sec=1375785.10, grad_norm=0.3386, duration=0.38s
Step 1741: loss=3.7505, lr=0.000598, tokens/sec=1373168.27, grad_norm=0.2972, duration=0.38s
Step 1742: loss=3.8051, lr=0.000598, tokens/sec=1373375.81, grad_norm=0.3346, duration=0.38s
Step 1743: loss=3.8322, lr=0.000598, tokens/sec=1375892.70, grad_norm=0.3694, duration=0.38s
Step 1744: loss=3.7144, lr=0.000598, tokens/sec=1374518.40, grad_norm=0.6741, duration=0.38s
Step 1745: loss=3.6882, lr=0.000598, tokens/sec=1373272.89, grad_norm=0.3963, duration=0.38s
Step 1746: loss=3.7699, lr=0.000598, tokens/sec=1376254.36, grad_norm=0.3916, duration=0.38s
Step 1747: loss=3.7562, lr=0.000598, tokens/sec=1377124.85, grad_norm=0.3931, duration=0.38s
Step 1748: loss=3.7327, lr=0.000598, tokens/sec=1376942.90, grad_norm=0.3542, duration=0.38s
Step 1749: loss=3.7711, lr=0.000598, tokens/sec=1375194.88, grad_norm=0.3778, duration=0.38s
Validation loss at step 1750: 3.915242910385132
Step 1750: loss=3.7460, lr=0.000598, tokens/sec=153667.20, grad_norm=0.3870, duration=3.41s
Step 1751: loss=3.7682, lr=0.000598, tokens/sec=1378508.69, grad_norm=0.4203, duration=0.38s
Step 1752: loss=3.7799, lr=0.000598, tokens/sec=1377828.94, grad_norm=0.3685, duration=0.38s
Step 1753: loss=3.7828, lr=0.000598, tokens/sec=1382263.25, grad_norm=0.3191, duration=0.38s
Step 1754: loss=3.7426, lr=0.000598, tokens/sec=1376885.14, grad_norm=0.3307, duration=0.38s
Step 1755: loss=3.7495, lr=0.000598, tokens/sec=1376603.28, grad_norm=0.3292, duration=0.38s
Step 1756: loss=3.7425, lr=0.000598, tokens/sec=1379081.86, grad_norm=0.3307, duration=0.38s
Step 1757: loss=3.8653, lr=0.000598, tokens/sec=1378850.11, grad_norm=0.3600, duration=0.38s
Step 1758: loss=3.9073, lr=0.000598, tokens/sec=1380766.95, grad_norm=0.3854, duration=0.38s
Step 1759: loss=3.8379, lr=0.000598, tokens/sec=1378869.14, grad_norm=0.3841, duration=0.38s
Step 1760: loss=3.8434, lr=0.000598, tokens/sec=1383019.58, grad_norm=0.3726, duration=0.38s
Step 1761: loss=3.8522, lr=0.000598, tokens/sec=1377220.58, grad_norm=0.3366, duration=0.38s
Step 1762: loss=3.8563, lr=0.000598, tokens/sec=1379006.62, grad_norm=0.3016, duration=0.38s
Step 1763: loss=3.8584, lr=0.000598, tokens/sec=1378193.35, grad_norm=0.3249, duration=0.38s
Step 1764: loss=3.7822, lr=0.000598, tokens/sec=1376115.70, grad_norm=0.2925, duration=0.38s
Step 1765: loss=3.8131, lr=0.000598, tokens/sec=1376296.57, grad_norm=0.2911, duration=0.38s
Step 1766: loss=3.8587, lr=0.000598, tokens/sec=1372551.17, grad_norm=0.2943, duration=0.38s
Step 1767: loss=3.8035, lr=0.000598, tokens/sec=1373043.95, grad_norm=0.2966, duration=0.38s
Step 1768: loss=3.8075, lr=0.000598, tokens/sec=1371279.30, grad_norm=0.3293, duration=0.38s
Step 1769: loss=3.7889, lr=0.000598, tokens/sec=1374839.79, grad_norm=0.3324, duration=0.38s
Step 1770: loss=3.7807, lr=0.000598, tokens/sec=1373060.24, grad_norm=0.2840, duration=0.38s
Step 1771: loss=3.7296, lr=0.000598, tokens/sec=1379606.17, grad_norm=0.2783, duration=0.38s
Step 1772: loss=3.7548, lr=0.000598, tokens/sec=1373359.52, grad_norm=0.2741, duration=0.38s
Step 1773: loss=3.8002, lr=0.000598, tokens/sec=1376987.74, grad_norm=0.2923, duration=0.38s
Step 1774: loss=3.8036, lr=0.000598, tokens/sec=1376336.19, grad_norm=0.3389, duration=0.38s
Step 1775: loss=3.8479, lr=0.000598, tokens/sec=1378871.73, grad_norm=0.3454, duration=0.38s
Step 1776: loss=3.7681, lr=0.000598, tokens/sec=1375032.36, grad_norm=0.3758, duration=0.38s
Step 1777: loss=3.8683, lr=0.000598, tokens/sec=1372560.60, grad_norm=0.3796, duration=0.38s
Step 1778: loss=3.8409, lr=0.000598, tokens/sec=1376463.69, grad_norm=0.3715, duration=0.38s
Step 1779: loss=3.7631, lr=0.000598, tokens/sec=1375701.61, grad_norm=0.4041, duration=0.38s
Step 1780: loss=3.7715, lr=0.000598, tokens/sec=1374942.95, grad_norm=0.4118, duration=0.38s
Step 1781: loss=3.7791, lr=0.000598, tokens/sec=1375772.19, grad_norm=0.4010, duration=0.38s
Step 1782: loss=3.7643, lr=0.000598, tokens/sec=1377547.56, grad_norm=0.3478, duration=0.38s
Step 1783: loss=3.7727, lr=0.000598, tokens/sec=1374315.67, grad_norm=0.3397, duration=0.38s
Step 1784: loss=3.7221, lr=0.000598, tokens/sec=1375689.56, grad_norm=0.3079, duration=0.38s
Step 1785: loss=3.7606, lr=0.000598, tokens/sec=1376354.28, grad_norm=0.2959, duration=0.38s
Step 1786: loss=3.7425, lr=0.000598, tokens/sec=1373451.30, grad_norm=0.2965, duration=0.38s
Step 1787: loss=3.7038, lr=0.000598, tokens/sec=1373456.44, grad_norm=0.2898, duration=0.38s
Step 1788: loss=3.7569, lr=0.000598, tokens/sec=1375856.54, grad_norm=0.2894, duration=0.38s
Step 1789: loss=3.7457, lr=0.000598, tokens/sec=1374182.55, grad_norm=0.2929, duration=0.38s
Step 1790: loss=3.7066, lr=0.000598, tokens/sec=1376399.94, grad_norm=0.2999, duration=0.38s
Step 1791: loss=3.6729, lr=0.000598, tokens/sec=1376185.46, grad_norm=0.3092, duration=0.38s
Step 1792: loss=3.7250, lr=0.000598, tokens/sec=1376879.10, grad_norm=0.3128, duration=0.38s
Step 1793: loss=3.7144, lr=0.000598, tokens/sec=1378342.79, grad_norm=0.3472, duration=0.38s
Step 1794: loss=3.7328, lr=0.000598, tokens/sec=1378201.99, grad_norm=0.4103, duration=0.38s
Step 1795: loss=3.6809, lr=0.000598, tokens/sec=1377010.16, grad_norm=0.4075, duration=0.38s
Step 1796: loss=3.7140, lr=0.000598, tokens/sec=1376966.18, grad_norm=0.3683, duration=0.38s
Step 1797: loss=3.7438, lr=0.000598, tokens/sec=1378610.67, grad_norm=0.3824, duration=0.38s
Step 1798: loss=3.6791, lr=0.000598, tokens/sec=1374560.50, grad_norm=0.3354, duration=0.38s
Step 1799: loss=3.6777, lr=0.000598, tokens/sec=1376031.31, grad_norm=0.3294, duration=0.38s
Step 1800/19073 (9.4%), Elapsed time: 731.88s, Steps per hour: 8853.94, Estimated hours remaining: 1.95
Step 1800: loss=3.6980, lr=0.000598, tokens/sec=1379234.96, grad_norm=0.3171, duration=0.38s
Step 1801: loss=3.7513, lr=0.000598, tokens/sec=1377314.61, grad_norm=0.3115, duration=0.38s
Step 1802: loss=3.7223, lr=0.000598, tokens/sec=1376961.87, grad_norm=0.3562, duration=0.38s
Step 1803: loss=3.8167, lr=0.000598, tokens/sec=1379982.78, grad_norm=0.4181, duration=0.38s
Step 1804: loss=3.8346, lr=0.000598, tokens/sec=1374646.42, grad_norm=0.4507, duration=0.38s
Step 1805: loss=3.8481, lr=0.000598, tokens/sec=1380253.89, grad_norm=0.3698, duration=0.38s
Step 1806: loss=3.8329, lr=0.000598, tokens/sec=1378153.62, grad_norm=0.3734, duration=0.38s
Step 1807: loss=3.8217, lr=0.000598, tokens/sec=1378169.16, grad_norm=0.3485, duration=0.38s
Step 1808: loss=3.7915, lr=0.000598, tokens/sec=1377948.95, grad_norm=0.3453, duration=0.38s
Step 1809: loss=3.7869, lr=0.000598, tokens/sec=1379183.92, grad_norm=0.3781, duration=0.38s
Step 1810: loss=3.7777, lr=0.000598, tokens/sec=1377127.44, grad_norm=0.4191, duration=0.38s
Step 1811: loss=3.8162, lr=0.000598, tokens/sec=1378406.73, grad_norm=0.3887, duration=0.38s
Step 1812: loss=3.8132, lr=0.000598, tokens/sec=1379692.73, grad_norm=0.3607, duration=0.38s
Step 1813: loss=3.7874, lr=0.000598, tokens/sec=1380787.76, grad_norm=0.3226, duration=0.38s
Step 1814: loss=3.8051, lr=0.000598, tokens/sec=1378222.72, grad_norm=0.3315, duration=0.38s
Step 1815: loss=3.7746, lr=0.000598, tokens/sec=1378327.24, grad_norm=0.3113, duration=0.38s
Step 1816: loss=3.7892, lr=0.000598, tokens/sec=1375705.91, grad_norm=0.3311, duration=0.38s
Step 1817: loss=3.7548, lr=0.000598, tokens/sec=1374137.04, grad_norm=0.3290, duration=0.38s
Step 1818: loss=3.7991, lr=0.000598, tokens/sec=1379674.55, grad_norm=0.3121, duration=0.38s
Step 1819: loss=3.7389, lr=0.000598, tokens/sec=1379960.26, grad_norm=0.3010, duration=0.38s
Step 1820: loss=3.7805, lr=0.000598, tokens/sec=1377915.28, grad_norm=0.3374, duration=0.38s
Step 1821: loss=3.7929, lr=0.000598, tokens/sec=1377765.92, grad_norm=0.3182, duration=0.38s
Step 1822: loss=3.7419, lr=0.000598, tokens/sec=1375860.85, grad_norm=0.3386, duration=0.38s
Step 1823: loss=3.7115, lr=0.000598, tokens/sec=1373670.08, grad_norm=0.3076, duration=0.38s
Step 1824: loss=3.7805, lr=0.000598, tokens/sec=1378488.82, grad_norm=0.2705, duration=0.38s
Step 1825: loss=3.7483, lr=0.000598, tokens/sec=1378520.79, grad_norm=0.2804, duration=0.38s
Step 1826: loss=3.7558, lr=0.000598, tokens/sec=1377274.06, grad_norm=0.2822, duration=0.38s
Step 1827: loss=3.7331, lr=0.000598, tokens/sec=1377482.84, grad_norm=0.2650, duration=0.38s
Step 1828: loss=3.7597, lr=0.000598, tokens/sec=1381025.36, grad_norm=0.2889, duration=0.38s
Step 1829: loss=3.7008, lr=0.000598, tokens/sec=1376469.72, grad_norm=0.2806, duration=0.38s
Step 1830: loss=3.7041, lr=0.000598, tokens/sec=1376119.14, grad_norm=0.3247, duration=0.38s
Step 1831: loss=3.7177, lr=0.000598, tokens/sec=1377509.59, grad_norm=0.3753, duration=0.38s
Step 1832: loss=3.7292, lr=0.000598, tokens/sec=1375829.86, grad_norm=0.3819, duration=0.38s
Step 1833: loss=3.7810, lr=0.000598, tokens/sec=1379887.52, grad_norm=0.3274, duration=0.38s
Step 1834: loss=3.7677, lr=0.000598, tokens/sec=1379999.23, grad_norm=0.3415, duration=0.38s
Step 1835: loss=3.7369, lr=0.000598, tokens/sec=1375886.67, grad_norm=0.2950, duration=0.38s
Step 1836: loss=3.7290, lr=0.000598, tokens/sec=1377564.82, grad_norm=0.3198, duration=0.38s
Step 1837: loss=3.6883, lr=0.000598, tokens/sec=1376530.04, grad_norm=0.3328, duration=0.38s
Step 1838: loss=3.7103, lr=0.000598, tokens/sec=1376346.53, grad_norm=0.2935, duration=0.38s
Step 1839: loss=3.6603, lr=0.000598, tokens/sec=1375104.59, grad_norm=0.2643, duration=0.38s
Step 1840: loss=3.7254, lr=0.000598, tokens/sec=1376640.34, grad_norm=0.2796, duration=0.38s
Step 1841: loss=3.6673, lr=0.000598, tokens/sec=1379301.57, grad_norm=0.2700, duration=0.38s
Step 1842: loss=3.6954, lr=0.000598, tokens/sec=1377027.40, grad_norm=0.2803, duration=0.38s
Step 1843: loss=3.6397, lr=0.000598, tokens/sec=1374166.23, grad_norm=0.3071, duration=0.38s
Step 1844: loss=3.7444, lr=0.000598, tokens/sec=1378966.84, grad_norm=0.3325, duration=0.38s
Step 1845: loss=3.6950, lr=0.000598, tokens/sec=1378275.41, grad_norm=0.3728, duration=0.38s
Step 1846: loss=3.6589, lr=0.000598, tokens/sec=1376388.74, grad_norm=0.3460, duration=0.38s
Step 1847: loss=3.6753, lr=0.000598, tokens/sec=1379613.09, grad_norm=0.3358, duration=0.38s
Step 1848: loss=3.6875, lr=0.000598, tokens/sec=1377285.28, grad_norm=0.3002, duration=0.38s
Step 1849: loss=3.7237, lr=0.000598, tokens/sec=1376483.51, grad_norm=0.2882, duration=0.38s
Step 1850: loss=3.7762, lr=0.000598, tokens/sec=1375335.94, grad_norm=0.2964, duration=0.38s
Step 1851: loss=3.7758, lr=0.000598, tokens/sec=1379430.49, grad_norm=0.3148, duration=0.38s
Step 1852: loss=3.8087, lr=0.000598, tokens/sec=1377102.43, grad_norm=0.3590, duration=0.38s
Step 1853: loss=3.7854, lr=0.000598, tokens/sec=1376925.66, grad_norm=0.4409, duration=0.38s
Step 1854: loss=3.8299, lr=0.000598, tokens/sec=1373900.94, grad_norm=0.3980, duration=0.38s
Step 1855: loss=3.7461, lr=0.000598, tokens/sec=1375884.09, grad_norm=0.3300, duration=0.38s
Step 1856: loss=3.7580, lr=0.000598, tokens/sec=1375252.51, grad_norm=0.3235, duration=0.38s
Step 1857: loss=3.8201, lr=0.000598, tokens/sec=1376840.31, grad_norm=0.3447, duration=0.38s
Step 1858: loss=3.8943, lr=0.000598, tokens/sec=1373428.14, grad_norm=0.3772, duration=0.38s
Step 1859: loss=3.8280, lr=0.000598, tokens/sec=1376480.93, grad_norm=0.4208, duration=0.38s
Step 1860: loss=3.8164, lr=0.000598, tokens/sec=1375904.75, grad_norm=0.3674, duration=0.38s
Step 1861: loss=3.7638, lr=0.000598, tokens/sec=1376736.01, grad_norm=0.3348, duration=0.38s
Step 1862: loss=3.7829, lr=0.000598, tokens/sec=1381615.38, grad_norm=0.3127, duration=0.38s
Step 1863: loss=3.7496, lr=0.000598, tokens/sec=1374797.68, grad_norm=0.2765, duration=0.38s
Step 1864: loss=3.7264, lr=0.000598, tokens/sec=1373762.76, grad_norm=0.3059, duration=0.38s
Step 1865: loss=3.8015, lr=0.000598, tokens/sec=1371996.26, grad_norm=0.3105, duration=0.38s
Step 1866: loss=3.7077, lr=0.000598, tokens/sec=1379649.45, grad_norm=0.3172, duration=0.38s
Step 1867: loss=3.7859, lr=0.000598, tokens/sec=1379843.37, grad_norm=0.3298, duration=0.38s
Step 1868: loss=3.8420, lr=0.000598, tokens/sec=1371990.27, grad_norm=0.3595, duration=0.38s
Step 1869: loss=3.7715, lr=0.000598, tokens/sec=1376660.16, grad_norm=0.3952, duration=0.38s
Step 1870: loss=3.6841, lr=0.000598, tokens/sec=1378084.52, grad_norm=0.3963, duration=0.38s
Step 1871: loss=3.7659, lr=0.000598, tokens/sec=1377580.36, grad_norm=0.3486, duration=0.38s
Step 1872: loss=3.7796, lr=0.000598, tokens/sec=1380312.80, grad_norm=0.2846, duration=0.38s
Step 1873: loss=3.7368, lr=0.000598, tokens/sec=1373235.16, grad_norm=0.3047, duration=0.38s
Step 1874: loss=3.6383, lr=0.000598, tokens/sec=1378483.63, grad_norm=0.3097, duration=0.38s
Step 1875: loss=3.7096, lr=0.000598, tokens/sec=1380032.14, grad_norm=0.3047, duration=0.38s
Step 1876: loss=3.6962, lr=0.000598, tokens/sec=1378726.49, grad_norm=0.2717, duration=0.38s
Step 1877: loss=3.7172, lr=0.000598, tokens/sec=1378002.48, grad_norm=0.2839, duration=0.38s
Step 1878: loss=3.7703, lr=0.000598, tokens/sec=1377525.13, grad_norm=0.2742, duration=0.38s
Step 1879: loss=3.6679, lr=0.000598, tokens/sec=1376819.62, grad_norm=0.2784, duration=0.38s
Step 1880: loss=3.6634, lr=0.000598, tokens/sec=1375356.58, grad_norm=0.3252, duration=0.38s
Step 1881: loss=3.6874, lr=0.000598, tokens/sec=1378721.30, grad_norm=0.3661, duration=0.38s
Step 1882: loss=3.6309, lr=0.000598, tokens/sec=1378615.85, grad_norm=0.3956, duration=0.38s
Step 1883: loss=3.6882, lr=0.000598, tokens/sec=1378055.16, grad_norm=0.3742, duration=0.38s
Step 1884: loss=3.6397, lr=0.000598, tokens/sec=1379199.49, grad_norm=0.3782, duration=0.38s
Step 1885: loss=3.6072, lr=0.000598, tokens/sec=1378204.58, grad_norm=0.3473, duration=0.38s
Step 1886: loss=3.6289, lr=0.000598, tokens/sec=1378643.51, grad_norm=0.2906, duration=0.38s
Step 1887: loss=3.7416, lr=0.000598, tokens/sec=1378461.16, grad_norm=0.3003, duration=0.38s
Step 1888: loss=3.6784, lr=0.000598, tokens/sec=1371445.21, grad_norm=0.3126, duration=0.38s
Step 1889: loss=3.6676, lr=0.000598, tokens/sec=1377184.36, grad_norm=0.3449, duration=0.38s
Step 1890: loss=3.5966, lr=0.000598, tokens/sec=1373563.68, grad_norm=0.3307, duration=0.38s
Step 1891: loss=3.6664, lr=0.000598, tokens/sec=1373226.58, grad_norm=0.3485, duration=0.38s
Step 1892: loss=3.6838, lr=0.000598, tokens/sec=1373344.08, grad_norm=0.3749, duration=0.38s
Step 1893: loss=3.6877, lr=0.000598, tokens/sec=1372743.10, grad_norm=0.3819, duration=0.38s
Step 1894: loss=3.6559, lr=0.000598, tokens/sec=1376560.20, grad_norm=0.3605, duration=0.38s
Step 1895: loss=3.7589, lr=0.000598, tokens/sec=1372345.60, grad_norm=0.3466, duration=0.38s
Step 1896: loss=3.7546, lr=0.000598, tokens/sec=1377640.77, grad_norm=0.3712, duration=0.38s
Step 1897: loss=3.7927, lr=0.000598, tokens/sec=1372672.84, grad_norm=0.3669, duration=0.38s
Step 1898: loss=3.7603, lr=0.000598, tokens/sec=1377489.75, grad_norm=0.3900, duration=0.38s
Step 1899: loss=3.7791, lr=0.000598, tokens/sec=1374221.19, grad_norm=0.4124, duration=0.38s
Step 1900/19073 (10.0%), Elapsed time: 770.03s, Steps per hour: 8882.74, Estimated hours remaining: 1.93
Step 1900: loss=3.7586, lr=0.000598, tokens/sec=1376562.78, grad_norm=0.3998, duration=0.38s
Step 1901: loss=3.8162, lr=0.000598, tokens/sec=1375871.18, grad_norm=0.3603, duration=0.38s
Step 1902: loss=3.7690, lr=0.000598, tokens/sec=1375503.69, grad_norm=0.3186, duration=0.38s
Step 1903: loss=3.8737, lr=0.000598, tokens/sec=1376537.79, grad_norm=0.3338, duration=0.38s
Step 1904: loss=3.7804, lr=0.000598, tokens/sec=1376829.97, grad_norm=0.3095, duration=0.38s
Step 1905: loss=3.8334, lr=0.000598, tokens/sec=1372665.12, grad_norm=0.3313, duration=0.38s
Step 1906: loss=3.8656, lr=0.000598, tokens/sec=1375457.23, grad_norm=0.3110, duration=0.38s
Step 1907: loss=3.7818, lr=0.000598, tokens/sec=1377249.05, grad_norm=0.2858, duration=0.38s
Step 1908: loss=3.8452, lr=0.000598, tokens/sec=1378157.94, grad_norm=0.4060, duration=0.38s
Step 1909: loss=3.7814, lr=0.000598, tokens/sec=1375373.79, grad_norm=0.3902, duration=0.38s
Step 1910: loss=3.8095, lr=0.000598, tokens/sec=1374845.81, grad_norm=0.3416, duration=0.38s
Step 1911: loss=3.7342, lr=0.000598, tokens/sec=1381264.78, grad_norm=0.2922, duration=0.38s
Step 1912: loss=3.7759, lr=0.000598, tokens/sec=1375538.11, grad_norm=0.3013, duration=0.38s
Step 1913: loss=3.6840, lr=0.000598, tokens/sec=1377017.06, grad_norm=0.2581, duration=0.38s
Step 1914: loss=3.7394, lr=0.000598, tokens/sec=1377449.19, grad_norm=0.2806, duration=0.38s
Step 1915: loss=3.7286, lr=0.000597, tokens/sec=1374844.09, grad_norm=0.2597, duration=0.38s
Step 1916: loss=3.7121, lr=0.000597, tokens/sec=1375824.69, grad_norm=0.2698, duration=0.38s
Step 1917: loss=3.7874, lr=0.000597, tokens/sec=1374084.66, grad_norm=0.2766, duration=0.38s
Step 1918: loss=3.6997, lr=0.000597, tokens/sec=1376317.24, grad_norm=0.3007, duration=0.38s
Step 1919: loss=3.6227, lr=0.000597, tokens/sec=1376566.23, grad_norm=0.5592, duration=0.38s
Step 1920: loss=3.7113, lr=0.000597, tokens/sec=1376293.98, grad_norm=0.3008, duration=0.38s
Step 1921: loss=3.7254, lr=0.000597, tokens/sec=1376418.89, grad_norm=0.2821, duration=0.38s
Step 1922: loss=3.8344, lr=0.000597, tokens/sec=1371919.22, grad_norm=0.2941, duration=0.38s
Step 1923: loss=3.6502, lr=0.000597, tokens/sec=1375020.33, grad_norm=0.3029, duration=0.38s
Step 1924: loss=3.7516, lr=0.000597, tokens/sec=1376159.62, grad_norm=0.3076, duration=0.38s
Step 1925: loss=3.6884, lr=0.000597, tokens/sec=1377778.01, grad_norm=0.3221, duration=0.38s
Step 1926: loss=3.6692, lr=0.000597, tokens/sec=1377917.00, grad_norm=0.2923, duration=0.38s
Step 1927: loss=3.6500, lr=0.000597, tokens/sec=1377498.38, grad_norm=0.2914, duration=0.38s
Step 1928: loss=3.7230, lr=0.000597, tokens/sec=1379201.22, grad_norm=0.3033, duration=0.38s
Step 1929: loss=3.6816, lr=0.000597, tokens/sec=1376177.71, grad_norm=0.3715, duration=0.38s
Step 1930: loss=3.6747, lr=0.000597, tokens/sec=1377376.72, grad_norm=0.4207, duration=0.38s
Step 1931: loss=3.7292, lr=0.000597, tokens/sec=1373230.87, grad_norm=0.4020, duration=0.38s
Step 1932: loss=3.7571, lr=0.000597, tokens/sec=1374478.88, grad_norm=0.9323, duration=0.38s
Step 1933: loss=3.7466, lr=0.000597, tokens/sec=1378571.78, grad_norm=0.7530, duration=0.38s
Step 1934: loss=3.7156, lr=0.000597, tokens/sec=1377929.09, grad_norm=0.6658, duration=0.38s
Step 1935: loss=3.6597, lr=0.000597, tokens/sec=1378202.85, grad_norm=0.6417, duration=0.38s
Step 1936: loss=3.7858, lr=0.000597, tokens/sec=1379032.56, grad_norm=0.6817, duration=0.38s
Step 1937: loss=3.6895, lr=0.000597, tokens/sec=1376731.70, grad_norm=0.4595, duration=0.38s
Step 1938: loss=3.6840, lr=0.000597, tokens/sec=1374227.20, grad_norm=0.3881, duration=0.38s
Step 1939: loss=3.6905, lr=0.000597, tokens/sec=1373648.62, grad_norm=0.4102, duration=0.38s
Step 1940: loss=3.6980, lr=0.000597, tokens/sec=1378658.21, grad_norm=0.4223, duration=0.38s
Step 1941: loss=3.7365, lr=0.000597, tokens/sec=1376857.55, grad_norm=0.3927, duration=0.38s
Step 1942: loss=3.7027, lr=0.000597, tokens/sec=1373876.05, grad_norm=0.3705, duration=0.38s
Step 1943: loss=3.7016, lr=0.000597, tokens/sec=1373991.94, grad_norm=0.3220, duration=0.38s
Step 1944: loss=3.6921, lr=0.000597, tokens/sec=1374121.58, grad_norm=0.2801, duration=0.38s
Step 1945: loss=3.6599, lr=0.000597, tokens/sec=1375165.64, grad_norm=0.2732, duration=0.38s
Step 1946: loss=3.6775, lr=0.000597, tokens/sec=1377512.18, grad_norm=0.2431, duration=0.38s
Step 1947: loss=3.7879, lr=0.000597, tokens/sec=1375341.10, grad_norm=0.2552, duration=0.38s
Step 1948: loss=3.8061, lr=0.000597, tokens/sec=1377755.56, grad_norm=0.2824, duration=0.38s
Step 1949: loss=3.7629, lr=0.000597, tokens/sec=1375023.76, grad_norm=0.2768, duration=0.38s
Step 1950: loss=3.7557, lr=0.000597, tokens/sec=1376403.38, grad_norm=0.2569, duration=0.38s
Step 1951: loss=3.7773, lr=0.000597, tokens/sec=1376858.41, grad_norm=0.2891, duration=0.38s
Step 1952: loss=3.7967, lr=0.000597, tokens/sec=1376451.63, grad_norm=0.2812, duration=0.38s
Step 1953: loss=3.7543, lr=0.000597, tokens/sec=1374855.27, grad_norm=0.2794, duration=0.38s
Step 1954: loss=3.7210, lr=0.000597, tokens/sec=1378506.10, grad_norm=0.2988, duration=0.38s
Step 1955: loss=3.7672, lr=0.000597, tokens/sec=1377565.68, grad_norm=0.2651, duration=0.38s
Step 1956: loss=3.7372, lr=0.000597, tokens/sec=1376606.73, grad_norm=0.2850, duration=0.38s
Step 1957: loss=3.7506, lr=0.000597, tokens/sec=1373302.05, grad_norm=0.3007, duration=0.38s
Step 1958: loss=3.7143, lr=0.000597, tokens/sec=1378892.48, grad_norm=0.2684, duration=0.38s
Step 1959: loss=3.6889, lr=0.000597, tokens/sec=1375610.39, grad_norm=0.2665, duration=0.38s
Step 1960: loss=3.7166, lr=0.000597, tokens/sec=1373276.32, grad_norm=0.2989, duration=0.38s
Step 1961: loss=3.6532, lr=0.000597, tokens/sec=1375926.27, grad_norm=0.3064, duration=0.38s
Step 1962: loss=3.6832, lr=0.000597, tokens/sec=1379913.50, grad_norm=0.2763, duration=0.38s
Step 1963: loss=3.7355, lr=0.000597, tokens/sec=1379489.33, grad_norm=0.2699, duration=0.38s
Step 1964: loss=3.7079, lr=0.000597, tokens/sec=1373484.75, grad_norm=0.2854, duration=0.38s
Step 1965: loss=3.7634, lr=0.000597, tokens/sec=1373463.31, grad_norm=0.2866, duration=0.38s
Step 1966: loss=3.6942, lr=0.000597, tokens/sec=1372879.37, grad_norm=0.2851, duration=0.38s
Step 1967: loss=3.8120, lr=0.000597, tokens/sec=1369100.53, grad_norm=0.2785, duration=0.38s
Step 1968: loss=3.7108, lr=0.000597, tokens/sec=1378190.76, grad_norm=0.2590, duration=0.38s
Step 1969: loss=3.6754, lr=0.000597, tokens/sec=1375721.41, grad_norm=0.2703, duration=0.38s
Step 1970: loss=3.6963, lr=0.000597, tokens/sec=1377161.93, grad_norm=0.3026, duration=0.38s
Step 1971: loss=3.7080, lr=0.000597, tokens/sec=1381017.56, grad_norm=0.3240, duration=0.38s
Step 1972: loss=3.6815, lr=0.000597, tokens/sec=1378747.24, grad_norm=0.3322, duration=0.38s
Step 1973: loss=3.6640, lr=0.000597, tokens/sec=1374903.40, grad_norm=0.3203, duration=0.38s
Step 1974: loss=3.6721, lr=0.000597, tokens/sec=1376448.19, grad_norm=0.3270, duration=0.38s
Step 1975: loss=3.6802, lr=0.000597, tokens/sec=1377812.54, grad_norm=0.3361, duration=0.38s
Step 1976: loss=3.6650, lr=0.000597, tokens/sec=1373578.27, grad_norm=0.3375, duration=0.38s
Step 1977: loss=3.6498, lr=0.000597, tokens/sec=1380198.44, grad_norm=0.3316, duration=0.38s
Step 1978: loss=3.6840, lr=0.000597, tokens/sec=1376762.73, grad_norm=0.3402, duration=0.38s
Step 1979: loss=3.6262, lr=0.000597, tokens/sec=1379688.40, grad_norm=0.3430, duration=0.38s
Step 1980: loss=3.6832, lr=0.000597, tokens/sec=1377778.01, grad_norm=0.3267, duration=0.38s
Step 1981: loss=3.5939, lr=0.000597, tokens/sec=1374029.71, grad_norm=0.3180, duration=0.38s
Step 1982: loss=3.7071, lr=0.000597, tokens/sec=1375831.58, grad_norm=0.3460, duration=0.38s
Step 1983: loss=3.6452, lr=0.000597, tokens/sec=1378945.23, grad_norm=0.3261, duration=0.38s
Step 1984: loss=3.6538, lr=0.000597, tokens/sec=1374194.57, grad_norm=0.2910, duration=0.38s
Step 1985: loss=3.5883, lr=0.000597, tokens/sec=1377546.70, grad_norm=0.3125, duration=0.38s
Step 1986: loss=3.6363, lr=0.000597, tokens/sec=1378486.22, grad_norm=0.3154, duration=0.38s
Step 1987: loss=3.6485, lr=0.000597, tokens/sec=1377265.44, grad_norm=0.2944, duration=0.38s
Step 1988: loss=3.5786, lr=0.000597, tokens/sec=1376094.17, grad_norm=0.2916, duration=0.38s
Step 1989: loss=3.6479, lr=0.000597, tokens/sec=1375851.38, grad_norm=0.2783, duration=0.38s
Step 1990: loss=3.6252, lr=0.000597, tokens/sec=1380013.09, grad_norm=0.2946, duration=0.38s
Step 1991: loss=3.6623, lr=0.000597, tokens/sec=1373130.55, grad_norm=0.3012, duration=0.38s
Step 1992: loss=3.6805, lr=0.000597, tokens/sec=1375218.96, grad_norm=0.2898, duration=0.38s
Step 1993: loss=3.6933, lr=0.000597, tokens/sec=1380874.47, grad_norm=0.2903, duration=0.38s
Step 1994: loss=3.7616, lr=0.000597, tokens/sec=1379239.28, grad_norm=0.3305, duration=0.38s
Step 1995: loss=3.7622, lr=0.000597, tokens/sec=1377313.74, grad_norm=0.3972, duration=0.38s
Step 1996: loss=3.7578, lr=0.000597, tokens/sec=1374924.04, grad_norm=0.3854, duration=0.38s
Step 1997: loss=3.7582, lr=0.000597, tokens/sec=1375551.88, grad_norm=0.3323, duration=0.38s
Step 1998: loss=3.6641, lr=0.000597, tokens/sec=1376324.99, grad_norm=0.3146, duration=0.38s
Step 1999: loss=3.7289, lr=0.000597, tokens/sec=1376415.45, grad_norm=0.3366, duration=0.38s
Step 2000/19073 (10.5%), Elapsed time: 808.21s, Steps per hour: 8908.57, Estimated hours remaining: 1.92
Validation loss at step 2000: 3.8324365615844727
Step 2000: loss=3.6868, lr=0.000597, tokens/sec=153943.56, grad_norm=0.3246, duration=3.41s
Step 2001: loss=3.7525, lr=0.000597, tokens/sec=1377413.82, grad_norm=0.3273, duration=0.38s
Step 2002: loss=3.7300, lr=0.000597, tokens/sec=1372693.40, grad_norm=0.3642, duration=0.38s
Step 2003: loss=3.7038, lr=0.000597, tokens/sec=1379196.90, grad_norm=0.3372, duration=0.38s
Step 2004: loss=3.7387, lr=0.000597, tokens/sec=1379614.83, grad_norm=0.3433, duration=0.38s
Step 2005: loss=3.7012, lr=0.000597, tokens/sec=1377549.29, grad_norm=0.3261, duration=0.38s
Step 2006: loss=3.7129, lr=0.000597, tokens/sec=1378117.34, grad_norm=0.2959, duration=0.38s
Step 2007: loss=3.6910, lr=0.000597, tokens/sec=1379466.83, grad_norm=0.2971, duration=0.38s
Step 2008: loss=3.7258, lr=0.000597, tokens/sec=1372886.22, grad_norm=0.2991, duration=0.38s
Step 2009: loss=3.6620, lr=0.000597, tokens/sec=1377576.04, grad_norm=0.2927, duration=0.38s
Step 2010: loss=3.7313, lr=0.000597, tokens/sec=1375063.32, grad_norm=0.2797, duration=0.38s
Step 2011: loss=3.6710, lr=0.000597, tokens/sec=1379170.95, grad_norm=0.3018, duration=0.38s
Step 2012: loss=3.6859, lr=0.000597, tokens/sec=1374905.12, grad_norm=0.3268, duration=0.38s
Step 2013: loss=3.6359, lr=0.000597, tokens/sec=1376100.20, grad_norm=0.3178, duration=0.38s
Step 2014: loss=3.7205, lr=0.000597, tokens/sec=1371618.01, grad_norm=0.3436, duration=0.38s
Step 2015: loss=3.6600, lr=0.000597, tokens/sec=1372558.88, grad_norm=0.3277, duration=0.38s
Step 2016: loss=3.7027, lr=0.000597, tokens/sec=1378087.98, grad_norm=0.2860, duration=0.38s
Step 2017: loss=3.6773, lr=0.000597, tokens/sec=1375194.02, grad_norm=0.2735, duration=0.38s
Step 2018: loss=3.6522, lr=0.000597, tokens/sec=1376395.63, grad_norm=0.2550, duration=0.38s
Step 2019: loss=3.6315, lr=0.000597, tokens/sec=1379471.16, grad_norm=0.2584, duration=0.38s
Step 2020: loss=3.6513, lr=0.000597, tokens/sec=1377540.66, grad_norm=0.2871, duration=0.38s
Step 2021: loss=3.6426, lr=0.000597, tokens/sec=1372872.51, grad_norm=0.3769, duration=0.38s
Step 2022: loss=3.7488, lr=0.000597, tokens/sec=1375109.75, grad_norm=0.4309, duration=0.38s
Step 2023: loss=3.6880, lr=0.000597, tokens/sec=1374502.07, grad_norm=0.4088, duration=0.38s
Step 2024: loss=3.6669, lr=0.000597, tokens/sec=1377734.85, grad_norm=0.3467, duration=0.38s
Step 2025: loss=3.6787, lr=0.000597, tokens/sec=1377395.70, grad_norm=0.2746, duration=0.38s
Step 2026: loss=3.6344, lr=0.000597, tokens/sec=1374832.06, grad_norm=0.2728, duration=0.38s
Step 2027: loss=3.6524, lr=0.000597, tokens/sec=1378943.50, grad_norm=0.2956, duration=0.38s
Step 2028: loss=3.6526, lr=0.000597, tokens/sec=1375522.62, grad_norm=0.3034, duration=0.38s
Step 2029: loss=3.6137, lr=0.000597, tokens/sec=1374003.95, grad_norm=0.3148, duration=0.38s
Step 2030: loss=3.6811, lr=0.000597, tokens/sec=1378704.88, grad_norm=0.2964, duration=0.38s
Step 2031: loss=3.5881, lr=0.000597, tokens/sec=1378565.73, grad_norm=0.3276, duration=0.38s
Step 2032: loss=3.5947, lr=0.000597, tokens/sec=1377081.73, grad_norm=0.3014, duration=0.38s
Step 2033: loss=3.6096, lr=0.000597, tokens/sec=1379267.83, grad_norm=0.2928, duration=0.38s
Step 2034: loss=3.6502, lr=0.000597, tokens/sec=1374415.31, grad_norm=0.3612, duration=0.38s
Step 2035: loss=3.6245, lr=0.000597, tokens/sec=1373414.41, grad_norm=0.3852, duration=0.38s
Step 2036: loss=3.6190, lr=0.000597, tokens/sec=1378498.32, grad_norm=0.3346, duration=0.38s
Step 2037: loss=3.5945, lr=0.000597, tokens/sec=1378985.87, grad_norm=0.3541, duration=0.38s
Step 2038: loss=3.6144, lr=0.000597, tokens/sec=1378715.25, grad_norm=0.3381, duration=0.38s
Step 2039: loss=3.6549, lr=0.000597, tokens/sec=1375611.25, grad_norm=0.3188, duration=0.38s
Step 2040: loss=3.7032, lr=0.000597, tokens/sec=1374345.73, grad_norm=0.3240, duration=0.38s
Step 2041: loss=3.7204, lr=0.000597, tokens/sec=1379435.68, grad_norm=0.3511, duration=0.38s
Step 2042: loss=3.7406, lr=0.000597, tokens/sec=1378525.97, grad_norm=0.3905, duration=0.38s
Step 2043: loss=3.7346, lr=0.000597, tokens/sec=1373404.12, grad_norm=0.3703, duration=0.38s
Step 2044: loss=3.7289, lr=0.000597, tokens/sec=1372653.13, grad_norm=0.3571, duration=0.38s
Step 2045: loss=3.6671, lr=0.000597, tokens/sec=1375639.65, grad_norm=0.3519, duration=0.38s
Step 2046: loss=3.7020, lr=0.000597, tokens/sec=1373988.50, grad_norm=0.3181, duration=0.38s
Step 2047: loss=3.7560, lr=0.000597, tokens/sec=1375921.11, grad_norm=0.3046, duration=0.38s
Step 2048: loss=3.8236, lr=0.000597, tokens/sec=1379867.61, grad_norm=0.3279, duration=0.38s
Step 2049: loss=3.7594, lr=0.000597, tokens/sec=1374600.88, grad_norm=0.3893, duration=0.38s
Step 2050: loss=3.7600, lr=0.000597, tokens/sec=1375583.71, grad_norm=0.3451, duration=0.38s
Step 2051: loss=3.6752, lr=0.000597, tokens/sec=1373034.52, grad_norm=0.3065, duration=0.38s
Step 2052: loss=3.7007, lr=0.000597, tokens/sec=1376129.48, grad_norm=0.3345, duration=0.38s
Step 2053: loss=3.6825, lr=0.000597, tokens/sec=1379912.64, grad_norm=0.3158, duration=0.38s
Step 2054: loss=3.6615, lr=0.000597, tokens/sec=1374704.00, grad_norm=0.2869, duration=0.38s
Step 2055: loss=3.7132, lr=0.000597, tokens/sec=1376767.04, grad_norm=0.2720, duration=0.38s
Step 2056: loss=3.6317, lr=0.000597, tokens/sec=1374075.22, grad_norm=0.2713, duration=0.38s
Step 2057: loss=3.7270, lr=0.000597, tokens/sec=1374478.02, grad_norm=0.2755, duration=0.38s
Step 2058: loss=3.7627, lr=0.000597, tokens/sec=1372855.37, grad_norm=0.2938, duration=0.38s
Step 2059: loss=3.6826, lr=0.000597, tokens/sec=1374649.86, grad_norm=0.2643, duration=0.38s
Step 2060: loss=3.6045, lr=0.000597, tokens/sec=1376099.34, grad_norm=0.2772, duration=0.38s
Step 2061: loss=3.7205, lr=0.000597, tokens/sec=1374893.95, grad_norm=0.2935, duration=0.38s
Step 2062: loss=3.7125, lr=0.000597, tokens/sec=1374440.22, grad_norm=0.3281, duration=0.38s
Step 2063: loss=3.6439, lr=0.000597, tokens/sec=1376802.38, grad_norm=0.3480, duration=0.38s
Step 2064: loss=3.5670, lr=0.000597, tokens/sec=1379639.06, grad_norm=0.3117, duration=0.38s
Step 2065: loss=3.6479, lr=0.000597, tokens/sec=1374823.46, grad_norm=0.2683, duration=0.38s
Step 2066: loss=3.6255, lr=0.000597, tokens/sec=1374892.23, grad_norm=0.2811, duration=0.38s
Step 2067: loss=3.6670, lr=0.000597, tokens/sec=1373736.15, grad_norm=0.2754, duration=0.38s
Step 2068: loss=3.6844, lr=0.000597, tokens/sec=1374982.50, grad_norm=0.2723, duration=0.38s
Step 2069: loss=3.5843, lr=0.000597, tokens/sec=1374353.46, grad_norm=0.2728, duration=0.38s
Step 2070: loss=3.6122, lr=0.000597, tokens/sec=1378005.08, grad_norm=0.2701, duration=0.38s
Step 2071: loss=3.6067, lr=0.000597, tokens/sec=1380263.42, grad_norm=0.2668, duration=0.38s
Step 2072: loss=3.5646, lr=0.000597, tokens/sec=1375366.04, grad_norm=0.3124, duration=0.38s
Step 2073: loss=3.6228, lr=0.000597, tokens/sec=1378076.75, grad_norm=0.3814, duration=0.38s
Step 2074: loss=3.5511, lr=0.000597, tokens/sec=1374545.03, grad_norm=0.3860, duration=0.38s
Step 2075: loss=3.5729, lr=0.000597, tokens/sec=1372530.61, grad_norm=0.4230, duration=0.38s
Step 2076: loss=3.6118, lr=0.000597, tokens/sec=1372667.69, grad_norm=0.4077, duration=0.38s
Step 2077: loss=3.6931, lr=0.000597, tokens/sec=1374151.63, grad_norm=0.4296, duration=0.38s
Step 2078: loss=3.6235, lr=0.000597, tokens/sec=1370501.59, grad_norm=0.3539, duration=0.38s
Step 2079: loss=3.5735, lr=0.000597, tokens/sec=1372117.82, grad_norm=0.3256, duration=0.38s
Step 2080: loss=3.5600, lr=0.000597, tokens/sec=1378399.82, grad_norm=0.3442, duration=0.38s
Step 2081: loss=3.5879, lr=0.000597, tokens/sec=1378385.99, grad_norm=0.3155, duration=0.38s
Step 2082: loss=3.6303, lr=0.000597, tokens/sec=1377579.49, grad_norm=0.2843, duration=0.38s
Step 2083: loss=3.5664, lr=0.000597, tokens/sec=1375080.51, grad_norm=0.2875, duration=0.38s
Step 2084: loss=3.6405, lr=0.000597, tokens/sec=1372513.48, grad_norm=0.3642, duration=0.38s
Step 2085: loss=3.6831, lr=0.000597, tokens/sec=1376870.48, grad_norm=0.3899, duration=0.38s
Step 2086: loss=3.7117, lr=0.000597, tokens/sec=1379232.36, grad_norm=0.3268, duration=0.38s
Step 2087: loss=3.7349, lr=0.000597, tokens/sec=1379762.85, grad_norm=0.3374, duration=0.38s
Step 2088: loss=3.6696, lr=0.000597, tokens/sec=1374478.02, grad_norm=0.3428, duration=0.38s
Step 2089: loss=3.6998, lr=0.000597, tokens/sec=1375652.56, grad_norm=0.3047, duration=0.38s
Step 2090: loss=3.7087, lr=0.000597, tokens/sec=1374118.15, grad_norm=0.2991, duration=0.38s
Step 2091: loss=3.6899, lr=0.000596, tokens/sec=1374438.50, grad_norm=0.3104, duration=0.38s
Step 2092: loss=3.7487, lr=0.000596, tokens/sec=1370924.52, grad_norm=0.2903, duration=0.38s
Step 2093: loss=3.7756, lr=0.000596, tokens/sec=1368684.69, grad_norm=0.2942, duration=0.38s
Step 2094: loss=3.7410, lr=0.000596, tokens/sec=1372514.34, grad_norm=0.3099, duration=0.38s
Step 2095: loss=3.7367, lr=0.000596, tokens/sec=1377175.73, grad_norm=0.3133, duration=0.38s
Step 2096: loss=3.7940, lr=0.000596, tokens/sec=1376747.21, grad_norm=0.3143, duration=0.38s
Step 2097: loss=3.7475, lr=0.000596, tokens/sec=1373548.24, grad_norm=0.4111, duration=0.38s
Step 2098: loss=3.7522, lr=0.000596, tokens/sec=1369080.07, grad_norm=0.4273, duration=0.38s
Step 2099: loss=3.7203, lr=0.000596, tokens/sec=1379149.32, grad_norm=0.3462, duration=0.38s
Step 2100/19073 (11.0%), Elapsed time: 849.43s, Steps per hour: 8900.05, Estimated hours remaining: 1.91
Step 2100: loss=3.7191, lr=0.000596, tokens/sec=1377834.98, grad_norm=0.3411, duration=0.38s
Step 2101: loss=3.6802, lr=0.000596, tokens/sec=1370029.42, grad_norm=0.2998, duration=0.38s
Step 2102: loss=3.6797, lr=0.000596, tokens/sec=1375505.41, grad_norm=0.2833, duration=0.38s
Step 2103: loss=3.6222, lr=0.000596, tokens/sec=1371612.02, grad_norm=0.2903, duration=0.38s
Step 2104: loss=3.6590, lr=0.000596, tokens/sec=1375275.73, grad_norm=0.2954, duration=0.38s
Step 2105: loss=3.6786, lr=0.000596, tokens/sec=1375292.07, grad_norm=0.3354, duration=0.38s
Step 2106: loss=3.6807, lr=0.000596, tokens/sec=1377011.88, grad_norm=0.3200, duration=0.38s
Step 2107: loss=3.6919, lr=0.000596, tokens/sec=1377660.62, grad_norm=0.3249, duration=0.38s
Step 2108: loss=3.6249, lr=0.000596, tokens/sec=1376550.72, grad_norm=0.2780, duration=0.38s
Step 2109: loss=3.5785, lr=0.000596, tokens/sec=1378828.50, grad_norm=0.6027, duration=0.38s
Step 2110: loss=3.6633, lr=0.000596, tokens/sec=1374763.30, grad_norm=0.2991, duration=0.38s
Step 2111: loss=3.6880, lr=0.000596, tokens/sec=1374894.81, grad_norm=0.2988, duration=0.38s
Step 2112: loss=3.7378, lr=0.000596, tokens/sec=1377304.25, grad_norm=0.3123, duration=0.38s
Step 2113: loss=3.6023, lr=0.000596, tokens/sec=1376990.33, grad_norm=0.3368, duration=0.38s
Step 2114: loss=3.6603, lr=0.000596, tokens/sec=1366879.39, grad_norm=0.3143, duration=0.38s
Step 2115: loss=3.6497, lr=0.000596, tokens/sec=1380255.62, grad_norm=0.2884, duration=0.38s
Step 2116: loss=3.5748, lr=0.000596, tokens/sec=1378934.85, grad_norm=0.3258, duration=0.38s
Step 2117: loss=3.6363, lr=0.000596, tokens/sec=1375199.18, grad_norm=0.3455, duration=0.38s
Step 2118: loss=3.6606, lr=0.000596, tokens/sec=1376761.01, grad_norm=0.3758, duration=0.38s
Step 2119: loss=3.6071, lr=0.000596, tokens/sec=1374636.97, grad_norm=0.3478, duration=0.38s
Step 2120: loss=3.6531, lr=0.000596, tokens/sec=1370454.62, grad_norm=0.3158, duration=0.38s
Step 2121: loss=3.6370, lr=0.000596, tokens/sec=1372624.00, grad_norm=0.2852, duration=0.38s
Step 2122: loss=3.6039, lr=0.000596, tokens/sec=1374765.88, grad_norm=0.4122, duration=0.38s
Step 2123: loss=3.6860, lr=0.000596, tokens/sec=1375464.98, grad_norm=0.2976, duration=0.38s
Step 2124: loss=3.6121, lr=0.000596, tokens/sec=1371988.55, grad_norm=0.3259, duration=0.38s
Step 2125: loss=3.5951, lr=0.000596, tokens/sec=1376392.19, grad_norm=0.2880, duration=0.38s
Step 2126: loss=3.6511, lr=0.000596, tokens/sec=1376575.71, grad_norm=0.2722, duration=0.38s
Step 2127: loss=3.5947, lr=0.000596, tokens/sec=1380002.70, grad_norm=0.2890, duration=0.38s
Step 2128: loss=3.5658, lr=0.000596, tokens/sec=1378060.34, grad_norm=0.2987, duration=0.38s
Step 2129: loss=3.6038, lr=0.000596, tokens/sec=1375191.44, grad_norm=0.2848, duration=0.38s
Step 2130: loss=3.6364, lr=0.000596, tokens/sec=1375185.42, grad_norm=0.2923, duration=0.38s
Step 2131: loss=3.6363, lr=0.000596, tokens/sec=1375333.36, grad_norm=0.3110, duration=0.38s
Step 2132: loss=3.5986, lr=0.000596, tokens/sec=1378705.75, grad_norm=0.2768, duration=0.38s
Step 2133: loss=3.6390, lr=0.000596, tokens/sec=1377578.63, grad_norm=0.2879, duration=0.38s
Step 2134: loss=3.5965, lr=0.000596, tokens/sec=1377298.22, grad_norm=0.2803, duration=0.38s
Step 2135: loss=3.5925, lr=0.000596, tokens/sec=1374969.60, grad_norm=0.2719, duration=0.38s
Step 2136: loss=3.6084, lr=0.000596, tokens/sec=1378156.21, grad_norm=0.2609, duration=0.38s
Step 2137: loss=3.6924, lr=0.000596, tokens/sec=1375810.06, grad_norm=0.2821, duration=0.38s
Step 2138: loss=3.7403, lr=0.000596, tokens/sec=1371983.42, grad_norm=0.3001, duration=0.38s
Step 2139: loss=3.6893, lr=0.000596, tokens/sec=1378409.32, grad_norm=0.3344, duration=0.38s
Step 2140: loss=3.6987, lr=0.000596, tokens/sec=1376950.66, grad_norm=0.3175, duration=0.38s
Step 2141: loss=3.7261, lr=0.000596, tokens/sec=1378100.07, grad_norm=0.2646, duration=0.38s
Step 2142: loss=3.7000, lr=0.000596, tokens/sec=1375830.72, grad_norm=0.2752, duration=0.38s
Step 2143: loss=3.6995, lr=0.000596, tokens/sec=1377454.37, grad_norm=0.2850, duration=0.38s
Step 2144: loss=3.6858, lr=0.000596, tokens/sec=1371928.64, grad_norm=0.2900, duration=0.38s
Step 2145: loss=3.6563, lr=0.000596, tokens/sec=1376468.86, grad_norm=0.3131, duration=0.38s
Step 2146: loss=3.6952, lr=0.000596, tokens/sec=1373519.07, grad_norm=0.3354, duration=0.38s
Step 2147: loss=3.6723, lr=0.000596, tokens/sec=1371079.24, grad_norm=0.3225, duration=0.38s
Step 2148: loss=3.6297, lr=0.000596, tokens/sec=1373554.24, grad_norm=0.2824, duration=0.38s
Step 2149: loss=3.6378, lr=0.000596, tokens/sec=1373243.73, grad_norm=0.2834, duration=0.38s
Step 2150: loss=3.6520, lr=0.000596, tokens/sec=1378313.42, grad_norm=0.3339, duration=0.38s
Step 2151: loss=3.5925, lr=0.000596, tokens/sec=1376554.17, grad_norm=0.3476, duration=0.38s
Step 2152: loss=3.6351, lr=0.000596, tokens/sec=1376034.76, grad_norm=0.3024, duration=0.38s
Step 2153: loss=3.6570, lr=0.000596, tokens/sec=1376177.71, grad_norm=0.2634, duration=0.38s
Step 2154: loss=3.6385, lr=0.000596, tokens/sec=1374244.38, grad_norm=0.2714, duration=0.38s
Step 2155: loss=3.7070, lr=0.000596, tokens/sec=1376382.71, grad_norm=0.2886, duration=0.38s
Step 2156: loss=3.6607, lr=0.000596, tokens/sec=1374225.49, grad_norm=0.3108, duration=0.38s
Step 2157: loss=3.7045, lr=0.000596, tokens/sec=1371169.00, grad_norm=0.3106, duration=0.38s
Step 2158: loss=3.6524, lr=0.000596, tokens/sec=1374652.44, grad_norm=0.3874, duration=0.38s
Step 2159: loss=3.6329, lr=0.000596, tokens/sec=1377182.63, grad_norm=0.3906, duration=0.38s
Step 2160: loss=3.6535, lr=0.000596, tokens/sec=1372787.66, grad_norm=0.3422, duration=0.38s
Step 2161: loss=3.6394, lr=0.000596, tokens/sec=1376723.94, grad_norm=0.2797, duration=0.38s
Step 2162: loss=3.5879, lr=0.000596, tokens/sec=1376070.92, grad_norm=0.3070, duration=0.38s
Step 2163: loss=3.6251, lr=0.000596, tokens/sec=1376053.70, grad_norm=0.2839, duration=0.38s
Step 2164: loss=3.6063, lr=0.000596, tokens/sec=1378534.62, grad_norm=0.3199, duration=0.38s
Step 2165: loss=3.6169, lr=0.000596, tokens/sec=1379685.80, grad_norm=0.3331, duration=0.38s
Step 2166: loss=3.6162, lr=0.000596, tokens/sec=1371399.03, grad_norm=0.2491, duration=0.38s
Step 2167: loss=3.5829, lr=0.000596, tokens/sec=1374948.11, grad_norm=0.2779, duration=0.38s
Step 2168: loss=3.5733, lr=0.000596, tokens/sec=1378647.83, grad_norm=0.3322, duration=0.38s
Step 2169: loss=3.6138, lr=0.000596, tokens/sec=1377105.01, grad_norm=0.3441, duration=0.38s
Step 2170: loss=3.6153, lr=0.000596, tokens/sec=1375946.93, grad_norm=0.3032, duration=0.38s
Step 2171: loss=3.5830, lr=0.000596, tokens/sec=1379653.78, grad_norm=0.2844, duration=0.38s
Step 2172: loss=3.6493, lr=0.000596, tokens/sec=1378350.57, grad_norm=0.3266, duration=0.38s
Step 2173: loss=3.5853, lr=0.000596, tokens/sec=1378302.19, grad_norm=0.3387, duration=0.38s
Step 2174: loss=3.5830, lr=0.000596, tokens/sec=1376727.39, grad_norm=0.3245, duration=0.38s
Step 2175: loss=3.5272, lr=0.000596, tokens/sec=1378972.03, grad_norm=0.2861, duration=0.38s
Step 2176: loss=3.5622, lr=0.000596, tokens/sec=1378331.56, grad_norm=0.2999, duration=0.38s
Step 2177: loss=3.5637, lr=0.000596, tokens/sec=1376924.80, grad_norm=0.3038, duration=0.38s
Step 2178: loss=3.5606, lr=0.000596, tokens/sec=1381210.12, grad_norm=0.2906, duration=0.38s
Step 2179: loss=3.5830, lr=0.000596, tokens/sec=1376648.10, grad_norm=0.2764, duration=0.38s
Step 2180: loss=3.5474, lr=0.000596, tokens/sec=1374032.29, grad_norm=0.2694, duration=0.38s
Step 2181: loss=3.6335, lr=0.000596, tokens/sec=1373403.26, grad_norm=0.2717, duration=0.38s
Step 2182: loss=3.5788, lr=0.000596, tokens/sec=1374417.02, grad_norm=0.2812, duration=0.38s
Step 2183: loss=3.6456, lr=0.000596, tokens/sec=1375545.85, grad_norm=0.2834, duration=0.38s
Step 2184: loss=3.6844, lr=0.000596, tokens/sec=1381142.46, grad_norm=0.2825, duration=0.38s
Step 2185: loss=3.6911, lr=0.000596, tokens/sec=1378699.69, grad_norm=0.3498, duration=0.38s
Step 2186: loss=3.7038, lr=0.000596, tokens/sec=1375443.47, grad_norm=0.3485, duration=0.38s
Step 2187: loss=3.6432, lr=0.000596, tokens/sec=1375246.49, grad_norm=0.3126, duration=0.38s
Step 2188: loss=3.6191, lr=0.000596, tokens/sec=1379473.76, grad_norm=0.3319, duration=0.38s
Step 2189: loss=3.6551, lr=0.000596, tokens/sec=1375921.11, grad_norm=0.3105, duration=0.38s
Step 2190: loss=3.6366, lr=0.000596, tokens/sec=1376061.45, grad_norm=0.2812, duration=0.38s
Step 2191: loss=3.6772, lr=0.000596, tokens/sec=1377754.70, grad_norm=0.2719, duration=0.38s
Step 2192: loss=3.6512, lr=0.000596, tokens/sec=1379319.74, grad_norm=0.2746, duration=0.38s
Step 2193: loss=3.6501, lr=0.000596, tokens/sec=1377729.67, grad_norm=0.3182, duration=0.38s
Step 2194: loss=3.6689, lr=0.000596, tokens/sec=1373544.80, grad_norm=0.3502, duration=0.38s
Step 2195: loss=3.6384, lr=0.000596, tokens/sec=1377155.90, grad_norm=0.3638, duration=0.38s
Step 2196: loss=3.6604, lr=0.000596, tokens/sec=1378826.77, grad_norm=0.3575, duration=0.38s
Step 2197: loss=3.6329, lr=0.000596, tokens/sec=1373993.65, grad_norm=0.3381, duration=0.38s
Step 2198: loss=3.6646, lr=0.000596, tokens/sec=1379580.21, grad_norm=0.3462, duration=0.38s
Step 2199: loss=3.6253, lr=0.000596, tokens/sec=1370810.01, grad_norm=0.2820, duration=0.38s
Step 2200/19073 (11.5%), Elapsed time: 887.62s, Steps per hour: 8922.77, Estimated hours remaining: 1.89
Step 2200: loss=3.6219, lr=0.000596, tokens/sec=1377465.59, grad_norm=0.2725, duration=0.38s
Step 2201: loss=3.6233, lr=0.000596, tokens/sec=1378821.58, grad_norm=0.2885, duration=0.38s
Step 2202: loss=3.6171, lr=0.000596, tokens/sec=1378572.64, grad_norm=0.2853, duration=0.38s
Step 2203: loss=3.5805, lr=0.000596, tokens/sec=1379570.68, grad_norm=0.3364, duration=0.38s
Step 2204: loss=3.6427, lr=0.000596, tokens/sec=1375201.76, grad_norm=0.3536, duration=0.38s
Step 2205: loss=3.6157, lr=0.000596, tokens/sec=1379411.45, grad_norm=0.3554, duration=0.38s
Step 2206: loss=3.6600, lr=0.000596, tokens/sec=1380656.85, grad_norm=0.3415, duration=0.38s
Step 2207: loss=3.5908, lr=0.000596, tokens/sec=1377050.69, grad_norm=0.3748, duration=0.38s
Step 2208: loss=3.5990, lr=0.000596, tokens/sec=1378344.52, grad_norm=0.3369, duration=0.38s
Step 2209: loss=3.5961, lr=0.000596, tokens/sec=1378405.86, grad_norm=0.3138, duration=0.38s
Step 2210: loss=3.5883, lr=0.000596, tokens/sec=1376935.14, grad_norm=0.2847, duration=0.38s
Step 2211: loss=3.6651, lr=0.000596, tokens/sec=1377792.68, grad_norm=0.3593, duration=0.38s
Step 2212: loss=3.6536, lr=0.000596, tokens/sec=1373702.68, grad_norm=0.3456, duration=0.38s
Step 2213: loss=3.5899, lr=0.000596, tokens/sec=1376156.18, grad_norm=0.3166, duration=0.38s
Step 2214: loss=3.6153, lr=0.000596, tokens/sec=1377348.25, grad_norm=0.3017, duration=0.38s
Step 2215: loss=3.5968, lr=0.000596, tokens/sec=1379257.45, grad_norm=0.2910, duration=0.38s
Step 2216: loss=3.6107, lr=0.000596, tokens/sec=1378056.89, grad_norm=0.3017, duration=0.38s
Step 2217: loss=3.6050, lr=0.000596, tokens/sec=1377199.02, grad_norm=0.3219, duration=0.38s
Step 2218: loss=3.6062, lr=0.000596, tokens/sec=1379644.25, grad_norm=0.3081, duration=0.38s
Step 2219: loss=3.5773, lr=0.000596, tokens/sec=1378817.26, grad_norm=0.2943, duration=0.38s
Step 2220: loss=3.6058, lr=0.000596, tokens/sec=1374503.79, grad_norm=0.2788, duration=0.38s
Step 2221: loss=3.4905, lr=0.000596, tokens/sec=1380444.51, grad_norm=0.2794, duration=0.38s
Step 2222: loss=3.5703, lr=0.000596, tokens/sec=1374738.37, grad_norm=0.2823, duration=0.38s
Step 2223: loss=3.5210, lr=0.000596, tokens/sec=1375630.18, grad_norm=0.3068, duration=0.38s
Step 2224: loss=3.5826, lr=0.000596, tokens/sec=1378169.16, grad_norm=0.3129, duration=0.38s
Step 2225: loss=3.5830, lr=0.000596, tokens/sec=1381305.56, grad_norm=0.2644, duration=0.38s
Step 2226: loss=3.5419, lr=0.000596, tokens/sec=1380212.31, grad_norm=0.2723, duration=0.38s
Step 2227: loss=3.5210, lr=0.000596, tokens/sec=1377898.87, grad_norm=0.2833, duration=0.38s
Step 2228: loss=3.5459, lr=0.000596, tokens/sec=1375417.66, grad_norm=0.2744, duration=0.38s
Step 2229: loss=3.5829, lr=0.000596, tokens/sec=1379112.13, grad_norm=0.2705, duration=0.38s
Step 2230: loss=3.6511, lr=0.000596, tokens/sec=1377751.25, grad_norm=0.3104, duration=0.38s
Step 2231: loss=3.6570, lr=0.000596, tokens/sec=1379491.93, grad_norm=0.3181, duration=0.38s
Step 2232: loss=3.6989, lr=0.000596, tokens/sec=1379232.36, grad_norm=0.3311, duration=0.38s
Step 2233: loss=3.6391, lr=0.000596, tokens/sec=1375732.59, grad_norm=0.3237, duration=0.38s
Step 2234: loss=3.6521, lr=0.000596, tokens/sec=1374949.83, grad_norm=0.3214, duration=0.38s
Step 2235: loss=3.6148, lr=0.000596, tokens/sec=1376721.36, grad_norm=0.3390, duration=0.38s
Step 2236: loss=3.6424, lr=0.000596, tokens/sec=1377723.63, grad_norm=0.3167, duration=0.38s
Step 2237: loss=3.6983, lr=0.000596, tokens/sec=1380104.89, grad_norm=0.3276, duration=0.38s
Step 2238: loss=3.7649, lr=0.000596, tokens/sec=1379656.37, grad_norm=0.3581, duration=0.38s
Step 2239: loss=3.7109, lr=0.000596, tokens/sec=1376544.69, grad_norm=0.3423, duration=0.38s
Step 2240: loss=3.6843, lr=0.000596, tokens/sec=1377942.90, grad_norm=0.3389, duration=0.38s
Step 2241: loss=3.6030, lr=0.000596, tokens/sec=1378229.63, grad_norm=0.3108, duration=0.38s
Step 2242: loss=3.6380, lr=0.000596, tokens/sec=1377180.91, grad_norm=0.2653, duration=0.38s
Step 2243: loss=3.6205, lr=0.000596, tokens/sec=1381907.11, grad_norm=0.2604, duration=0.38s
Step 2244: loss=3.5830, lr=0.000596, tokens/sec=1378921.88, grad_norm=0.2870, duration=0.38s
Step 2245: loss=3.6499, lr=0.000595, tokens/sec=1375366.90, grad_norm=0.2678, duration=0.38s
Step 2246: loss=3.5824, lr=0.000595, tokens/sec=1370939.91, grad_norm=0.2531, duration=0.38s
Step 2247: loss=3.6631, lr=0.000595, tokens/sec=1374399.84, grad_norm=0.2614, duration=0.38s
Step 2248: loss=3.6890, lr=0.000595, tokens/sec=1373679.51, grad_norm=0.2841, duration=0.38s
Step 2249: loss=3.6177, lr=0.000595, tokens/sec=1375286.05, grad_norm=0.2870, duration=0.38s
Validation loss at step 2250: 3.7840356826782227
Step 2250: loss=3.5786, lr=0.000595, tokens/sec=154002.29, grad_norm=0.3202, duration=3.40s
Step 2251: loss=3.6652, lr=0.000595, tokens/sec=1380010.49, grad_norm=0.3621, duration=0.38s
Step 2252: loss=3.6263, lr=0.000595, tokens/sec=1380611.78, grad_norm=0.3298, duration=0.38s
Step 2253: loss=3.5769, lr=0.000595, tokens/sec=1381313.37, grad_norm=0.2889, duration=0.38s
Step 2254: loss=3.5121, lr=0.000595, tokens/sec=1375516.60, grad_norm=0.2453, duration=0.38s
Step 2255: loss=3.5833, lr=0.000595, tokens/sec=1374070.06, grad_norm=0.2696, duration=0.38s
Step 2256: loss=3.5825, lr=0.000595, tokens/sec=1374605.18, grad_norm=0.2566, duration=0.38s
Step 2257: loss=3.5891, lr=0.000595, tokens/sec=1379469.43, grad_norm=0.2860, duration=0.38s
Step 2258: loss=3.6116, lr=0.000595, tokens/sec=1376093.31, grad_norm=0.2920, duration=0.38s
Step 2259: loss=3.5495, lr=0.000595, tokens/sec=1374949.83, grad_norm=0.2970, duration=0.38s
Step 2260: loss=3.5525, lr=0.000595, tokens/sec=1378296.14, grad_norm=0.3197, duration=0.38s
Step 2261: loss=3.5601, lr=0.000595, tokens/sec=1377674.43, grad_norm=0.2980, duration=0.38s
Step 2262: loss=3.5064, lr=0.000595, tokens/sec=1376856.69, grad_norm=0.3106, duration=0.38s
Step 2263: loss=3.5387, lr=0.000595, tokens/sec=1373718.13, grad_norm=0.3546, duration=0.38s
Step 2264: loss=3.5170, lr=0.000595, tokens/sec=1375293.79, grad_norm=0.3418, duration=0.38s
Step 2265: loss=3.5440, lr=0.000595, tokens/sec=1377188.67, grad_norm=0.2943, duration=0.38s
Step 2266: loss=3.5478, lr=0.000595, tokens/sec=1372542.61, grad_norm=0.2916, duration=0.38s
Step 2267: loss=3.6212, lr=0.000595, tokens/sec=1374627.52, grad_norm=0.3066, duration=0.38s
Step 2268: loss=3.5278, lr=0.000595, tokens/sec=1376539.52, grad_norm=0.2868, duration=0.38s
Step 2269: loss=3.5345, lr=0.000595, tokens/sec=1375444.33, grad_norm=0.2868, duration=0.38s
Step 2270: loss=3.4792, lr=0.000595, tokens/sec=1376550.72, grad_norm=0.2816, duration=0.38s
Step 2271: loss=3.5434, lr=0.000595, tokens/sec=1379652.04, grad_norm=0.2901, duration=0.38s
Step 2272: loss=3.5222, lr=0.000595, tokens/sec=1373532.79, grad_norm=0.2783, duration=0.38s
Step 2273: loss=3.5529, lr=0.000595, tokens/sec=1376558.47, grad_norm=0.2719, duration=0.38s
Step 2274: loss=3.5620, lr=0.000595, tokens/sec=1374271.86, grad_norm=0.2860, duration=0.38s
Step 2275: loss=3.6367, lr=0.000595, tokens/sec=1375986.54, grad_norm=0.2720, duration=0.38s
Step 2276: loss=3.6585, lr=0.000595, tokens/sec=1374853.55, grad_norm=0.3178, duration=0.38s
Step 2277: loss=3.6564, lr=0.000595, tokens/sec=1371717.26, grad_norm=0.3945, duration=0.38s
Step 2278: loss=3.6070, lr=0.000595, tokens/sec=1374655.01, grad_norm=0.3576, duration=0.38s
Step 2279: loss=3.6698, lr=0.000595, tokens/sec=1380402.91, grad_norm=0.3670, duration=0.38s
Step 2280: loss=3.5965, lr=0.000595, tokens/sec=1377595.89, grad_norm=0.3359, duration=0.38s
Step 2281: loss=3.6805, lr=0.000595, tokens/sec=1375795.43, grad_norm=0.2967, duration=0.38s
Step 2282: loss=3.6669, lr=0.000595, tokens/sec=1374241.80, grad_norm=0.2911, duration=0.38s
Step 2283: loss=3.7505, lr=0.000595, tokens/sec=1376710.15, grad_norm=0.3107, duration=0.38s
Step 2284: loss=3.6530, lr=0.000595, tokens/sec=1374675.64, grad_norm=0.3065, duration=0.38s
Step 2285: loss=3.6749, lr=0.000595, tokens/sec=1374729.78, grad_norm=0.2734, duration=0.38s
Step 2286: loss=3.7609, lr=0.000595, tokens/sec=1378058.62, grad_norm=0.3391, duration=0.38s
Step 2287: loss=3.6715, lr=0.000595, tokens/sec=1375693.00, grad_norm=0.4680, duration=0.38s
Step 2288: loss=3.7118, lr=0.000595, tokens/sec=1379094.83, grad_norm=0.4924, duration=0.38s
Step 2289: loss=3.6499, lr=0.000595, tokens/sec=1381252.63, grad_norm=0.3702, duration=0.38s
Step 2290: loss=3.6771, lr=0.000595, tokens/sec=1372989.09, grad_norm=0.3401, duration=0.38s
Step 2291: loss=3.6051, lr=0.000595, tokens/sec=1376267.28, grad_norm=0.3401, duration=0.38s
Step 2292: loss=3.6334, lr=0.000595, tokens/sec=1376940.32, grad_norm=0.3180, duration=0.38s
Step 2293: loss=3.5565, lr=0.000595, tokens/sec=1379953.33, grad_norm=0.2976, duration=0.38s
Step 2294: loss=3.6208, lr=0.000595, tokens/sec=1376768.76, grad_norm=0.2823, duration=0.38s
Step 2295: loss=3.6522, lr=0.000595, tokens/sec=1378132.02, grad_norm=0.2586, duration=0.38s
Step 2296: loss=3.5812, lr=0.000595, tokens/sec=1379560.30, grad_norm=0.2465, duration=0.38s
Step 2297: loss=3.6272, lr=0.000595, tokens/sec=1378400.68, grad_norm=0.2682, duration=0.38s
Step 2298: loss=3.5777, lr=0.000595, tokens/sec=1375314.43, grad_norm=0.2790, duration=0.38s
Step 2299: loss=3.5222, lr=0.000595, tokens/sec=1373671.79, grad_norm=0.3938, duration=0.38s
Step 2300/19073 (12.1%), Elapsed time: 928.80s, Steps per hour: 8914.73, Estimated hours remaining: 1.88
Step 2300: loss=3.6221, lr=0.000595, tokens/sec=1375006.57, grad_norm=0.2838, duration=0.38s
Step 2301: loss=3.5913, lr=0.000595, tokens/sec=1374750.41, grad_norm=0.2734, duration=0.38s
Step 2302: loss=3.6889, lr=0.000595, tokens/sec=1377889.37, grad_norm=0.2913, duration=0.38s
Step 2303: loss=3.5100, lr=0.000595, tokens/sec=1373699.25, grad_norm=0.3170, duration=0.38s
Step 2304: loss=3.6230, lr=0.000595, tokens/sec=1371832.78, grad_norm=0.2894, duration=0.38s
Step 2305: loss=3.5596, lr=0.000595, tokens/sec=1377017.92, grad_norm=0.2931, duration=0.38s
Step 2306: loss=3.5570, lr=0.000595, tokens/sec=1378770.58, grad_norm=0.2750, duration=0.38s
Step 2307: loss=3.5633, lr=0.000595, tokens/sec=1378246.04, grad_norm=0.2578, duration=0.38s
Step 2308: loss=3.5710, lr=0.000595, tokens/sec=1374980.78, grad_norm=0.2462, duration=0.38s
Step 2309: loss=3.5852, lr=0.000595, tokens/sec=1375144.15, grad_norm=0.2608, duration=0.38s
Step 2310: loss=3.5666, lr=0.000595, tokens/sec=1375293.79, grad_norm=0.2807, duration=0.38s
Step 2311: loss=3.5343, lr=0.000595, tokens/sec=1380125.68, grad_norm=0.5656, duration=0.38s
Step 2312: loss=3.5968, lr=0.000595, tokens/sec=1379312.82, grad_norm=0.4567, duration=0.38s
Step 2313: loss=3.6592, lr=0.000595, tokens/sec=1380056.39, grad_norm=0.4269, duration=0.38s
Step 2314: loss=3.6190, lr=0.000595, tokens/sec=1375415.08, grad_norm=0.3899, duration=0.38s
Step 2315: loss=3.5412, lr=0.000595, tokens/sec=1376516.25, grad_norm=0.3417, duration=0.38s
Step 2316: loss=3.6094, lr=0.000595, tokens/sec=1377602.79, grad_norm=0.3173, duration=0.38s
Step 2317: loss=3.5243, lr=0.000595, tokens/sec=1380234.83, grad_norm=0.3118, duration=0.38s
Step 2318: loss=3.5268, lr=0.000595, tokens/sec=1376440.43, grad_norm=0.3085, duration=0.38s
Step 2319: loss=3.5873, lr=0.000595, tokens/sec=1381167.61, grad_norm=0.3187, duration=0.38s
Step 2320: loss=3.5743, lr=0.000595, tokens/sec=1376805.83, grad_norm=0.2898, duration=0.38s
Step 2321: loss=3.5648, lr=0.000595, tokens/sec=1374857.85, grad_norm=0.2948, duration=0.38s
Step 2322: loss=3.5642, lr=0.000595, tokens/sec=1377541.52, grad_norm=0.2716, duration=0.38s
Step 2323: loss=3.5685, lr=0.000595, tokens/sec=1374522.69, grad_norm=0.2759, duration=0.38s
Step 2324: loss=3.5500, lr=0.000595, tokens/sec=1378885.56, grad_norm=0.2683, duration=0.38s
Step 2325: loss=3.5436, lr=0.000595, tokens/sec=1377513.05, grad_norm=0.2443, duration=0.38s
Step 2326: loss=3.5334, lr=0.000595, tokens/sec=1379955.93, grad_norm=0.2750, duration=0.38s
Step 2327: loss=3.6456, lr=0.000595, tokens/sec=1376522.28, grad_norm=0.2899, duration=0.38s
Step 2328: loss=3.6802, lr=0.000595, tokens/sec=1376644.65, grad_norm=0.2768, duration=0.38s
Step 2329: loss=3.6357, lr=0.000595, tokens/sec=1375540.69, grad_norm=0.2670, duration=0.38s
Step 2330: loss=3.6599, lr=0.000595, tokens/sec=1371956.88, grad_norm=0.2608, duration=0.38s
Step 2331: loss=3.6472, lr=0.000595, tokens/sec=1377270.61, grad_norm=0.2911, duration=0.38s
Step 2332: loss=3.6595, lr=0.000595, tokens/sec=1375471.86, grad_norm=0.2785, duration=0.38s
Step 2333: loss=3.6770, lr=0.000595, tokens/sec=1376837.72, grad_norm=0.2967, duration=0.38s
Step 2334: loss=3.5897, lr=0.000595, tokens/sec=1371492.26, grad_norm=0.3156, duration=0.38s
Step 2335: loss=3.6259, lr=0.000595, tokens/sec=1378856.17, grad_norm=0.3146, duration=0.38s
Step 2336: loss=3.6287, lr=0.000595, tokens/sec=1376213.88, grad_norm=0.3025, duration=0.38s
Step 2337: loss=3.5975, lr=0.000595, tokens/sec=1376918.76, grad_norm=0.3288, duration=0.38s
Step 2338: loss=3.5916, lr=0.000595, tokens/sec=1373821.97, grad_norm=0.3310, duration=0.38s
Step 2339: loss=3.5870, lr=0.000595, tokens/sec=1371252.79, grad_norm=0.3099, duration=0.38s
Step 2340: loss=3.6056, lr=0.000595, tokens/sec=1373416.98, grad_norm=0.3079, duration=0.38s
Step 2341: loss=3.5453, lr=0.000595, tokens/sec=1375804.89, grad_norm=0.3017, duration=0.38s
Step 2342: loss=3.5642, lr=0.000595, tokens/sec=1378118.21, grad_norm=0.3009, duration=0.38s
Step 2343: loss=3.5982, lr=0.000595, tokens/sec=1376157.90, grad_norm=0.3030, duration=0.38s
Step 2344: loss=3.5953, lr=0.000595, tokens/sec=1376478.34, grad_norm=0.2885, duration=0.38s
Step 2345: loss=3.6806, lr=0.000595, tokens/sec=1381390.59, grad_norm=0.2741, duration=0.38s
Step 2346: loss=3.5573, lr=0.000595, tokens/sec=1376192.35, grad_norm=0.2704, duration=0.38s
Step 2347: loss=3.6478, lr=0.000595, tokens/sec=1374750.41, grad_norm=0.2660, duration=0.38s
Step 2348: loss=3.6017, lr=0.000595, tokens/sec=1377949.81, grad_norm=0.2795, duration=0.38s
Step 2349: loss=3.5810, lr=0.000595, tokens/sec=1377232.66, grad_norm=0.2668, duration=0.38s
Step 2350: loss=3.5876, lr=0.000595, tokens/sec=1375433.14, grad_norm=0.2811, duration=0.38s
Step 2351: loss=3.5517, lr=0.000595, tokens/sec=1380952.51, grad_norm=0.2380, duration=0.38s
Step 2352: loss=3.5530, lr=0.000595, tokens/sec=1379600.11, grad_norm=0.2781, duration=0.38s
Step 2353: loss=3.5618, lr=0.000595, tokens/sec=1372271.09, grad_norm=0.2845, duration=0.38s
Step 2354: loss=3.5442, lr=0.000595, tokens/sec=1374820.88, grad_norm=0.2821, duration=0.38s
Step 2355: loss=3.5739, lr=0.000595, tokens/sec=1378574.37, grad_norm=0.2743, duration=0.38s
Step 2356: loss=3.5611, lr=0.000595, tokens/sec=1379025.65, grad_norm=0.2540, duration=0.38s
Step 2357: loss=3.4741, lr=0.000595, tokens/sec=1380871.00, grad_norm=0.2634, duration=0.38s
Step 2358: loss=3.5622, lr=0.000595, tokens/sec=1378866.54, grad_norm=0.2685, duration=0.38s
Step 2359: loss=3.5485, lr=0.000595, tokens/sec=1375835.88, grad_norm=0.2677, duration=0.38s
Step 2360: loss=3.6063, lr=0.000595, tokens/sec=1374985.08, grad_norm=0.2875, duration=0.38s
Step 2361: loss=3.5293, lr=0.000595, tokens/sec=1371874.72, grad_norm=0.3020, duration=0.38s
Step 2362: loss=3.5927, lr=0.000595, tokens/sec=1378879.51, grad_norm=0.3116, duration=0.38s
Step 2363: loss=3.5213, lr=0.000595, tokens/sec=1377220.58, grad_norm=0.3245, duration=0.38s
Step 2364: loss=3.5342, lr=0.000595, tokens/sec=1375582.85, grad_norm=0.3148, duration=0.38s
Step 2365: loss=3.4663, lr=0.000595, tokens/sec=1379565.49, grad_norm=0.3065, duration=0.38s
Step 2366: loss=3.4879, lr=0.000595, tokens/sec=1379350.02, grad_norm=0.3265, duration=0.38s
Step 2367: loss=3.5642, lr=0.000595, tokens/sec=1378597.70, grad_norm=0.3819, duration=0.38s
Step 2368: loss=3.5161, lr=0.000595, tokens/sec=1378793.92, grad_norm=0.3617, duration=0.38s
Step 2369: loss=3.5203, lr=0.000595, tokens/sec=1381575.45, grad_norm=0.3546, duration=0.38s
Step 2370: loss=3.5325, lr=0.000595, tokens/sec=1376150.15, grad_norm=0.3121, duration=0.38s
Step 2371: loss=3.5494, lr=0.000595, tokens/sec=1378277.14, grad_norm=0.3484, duration=0.38s
Step 2372: loss=3.5503, lr=0.000595, tokens/sec=1372507.48, grad_norm=0.3201, duration=0.38s
Step 2373: loss=3.5826, lr=0.000595, tokens/sec=1374013.40, grad_norm=0.3048, duration=0.38s
Step 2374: loss=3.6341, lr=0.000595, tokens/sec=1380492.17, grad_norm=0.3356, duration=0.38s
Step 2375: loss=3.6538, lr=0.000595, tokens/sec=1380489.57, grad_norm=0.3241, duration=0.38s
Step 2376: loss=3.5978, lr=0.000595, tokens/sec=1374992.81, grad_norm=0.2997, duration=0.38s
Step 2377: loss=3.6143, lr=0.000595, tokens/sec=1376844.62, grad_norm=0.3301, duration=0.38s
Step 2378: loss=3.5714, lr=0.000595, tokens/sec=1376832.55, grad_norm=0.4860, duration=0.38s
Step 2379: loss=3.6388, lr=0.000595, tokens/sec=1379877.13, grad_norm=0.5060, duration=0.38s
Step 2380: loss=3.5865, lr=0.000595, tokens/sec=1373894.93, grad_norm=0.3532, duration=0.38s
Step 2381: loss=3.6223, lr=0.000595, tokens/sec=1377520.81, grad_norm=0.3041, duration=0.38s
Step 2382: loss=3.6135, lr=0.000594, tokens/sec=1377900.60, grad_norm=0.2979, duration=0.38s
Step 2383: loss=3.5998, lr=0.000594, tokens/sec=1379655.51, grad_norm=0.2928, duration=0.38s
Step 2384: loss=3.6207, lr=0.000594, tokens/sec=1372886.22, grad_norm=0.2856, duration=0.38s
Step 2385: loss=3.5916, lr=0.000594, tokens/sec=1380443.64, grad_norm=0.2664, duration=0.38s
Step 2386: loss=3.6088, lr=0.000594, tokens/sec=1377945.50, grad_norm=0.2922, duration=0.38s
Step 2387: loss=3.5765, lr=0.000594, tokens/sec=1377194.71, grad_norm=0.3276, duration=0.38s
Step 2388: loss=3.6337, lr=0.000594, tokens/sec=1376511.08, grad_norm=0.3049, duration=0.38s
Step 2389: loss=3.5246, lr=0.000594, tokens/sec=1376316.38, grad_norm=0.2510, duration=0.38s
Step 2390: loss=3.5850, lr=0.000594, tokens/sec=1375390.13, grad_norm=0.2742, duration=0.38s
Step 2391: loss=3.5667, lr=0.000594, tokens/sec=1376413.72, grad_norm=0.2745, duration=0.38s
Step 2392: loss=3.5714, lr=0.000594, tokens/sec=1374630.09, grad_norm=0.2662, duration=0.38s
Step 2393: loss=3.5072, lr=0.000594, tokens/sec=1377214.55, grad_norm=0.2746, duration=0.38s
Step 2394: loss=3.6013, lr=0.000594, tokens/sec=1379683.21, grad_norm=0.2768, duration=0.38s
Step 2395: loss=3.5711, lr=0.000594, tokens/sec=1374269.29, grad_norm=0.2760, duration=0.38s
Step 2396: loss=3.5680, lr=0.000594, tokens/sec=1377010.16, grad_norm=0.2883, duration=0.38s
Step 2397: loss=3.5286, lr=0.000594, tokens/sec=1370249.67, grad_norm=0.2850, duration=0.38s
Step 2398: loss=3.5627, lr=0.000594, tokens/sec=1378335.02, grad_norm=0.3001, duration=0.38s
Step 2399: loss=3.5393, lr=0.000594, tokens/sec=1375698.17, grad_norm=0.3341, duration=0.38s
Step 2400/19073 (12.6%), Elapsed time: 966.97s, Steps per hour: 8935.16, Estimated hours remaining: 1.87
Step 2400: loss=3.6182, lr=0.000594, tokens/sec=1379202.95, grad_norm=0.3245, duration=0.38s
Step 2401: loss=3.5788, lr=0.000594, tokens/sec=1378580.42, grad_norm=0.2872, duration=0.38s
Step 2402: loss=3.5560, lr=0.000594, tokens/sec=1377050.69, grad_norm=0.2773, duration=0.38s
Step 2403: loss=3.5386, lr=0.000594, tokens/sec=1373755.03, grad_norm=0.2776, duration=0.38s
Step 2404: loss=3.5314, lr=0.000594, tokens/sec=1377800.45, grad_norm=0.2648, duration=0.38s
Step 2405: loss=3.5753, lr=0.000594, tokens/sec=1376633.45, grad_norm=0.2475, duration=0.38s
Step 2406: loss=3.5659, lr=0.000594, tokens/sec=1376046.81, grad_norm=0.2608, duration=0.38s
Step 2407: loss=3.5577, lr=0.000594, tokens/sec=1374776.19, grad_norm=0.2664, duration=0.38s
Step 2408: loss=3.5742, lr=0.000594, tokens/sec=1374140.47, grad_norm=0.2497, duration=0.38s
Step 2409: loss=3.5028, lr=0.000594, tokens/sec=1377384.49, grad_norm=0.2401, duration=0.38s
Step 2410: loss=3.5173, lr=0.000594, tokens/sec=1376255.22, grad_norm=0.2836, duration=0.38s
Step 2411: loss=3.4747, lr=0.000594, tokens/sec=1376650.68, grad_norm=0.2922, duration=0.38s
Step 2412: loss=3.4804, lr=0.000594, tokens/sec=1365489.11, grad_norm=0.2531, duration=0.38s
Step 2413: loss=3.4602, lr=0.000594, tokens/sec=1376044.23, grad_norm=0.2706, duration=0.38s
Step 2414: loss=3.5587, lr=0.000594, tokens/sec=1373469.31, grad_norm=0.2782, duration=0.38s
Step 2415: loss=3.5199, lr=0.000594, tokens/sec=1375667.19, grad_norm=0.2907, duration=0.38s
Step 2416: loss=3.4833, lr=0.000594, tokens/sec=1377492.34, grad_norm=0.2808, duration=0.38s
Step 2417: loss=3.4609, lr=0.000594, tokens/sec=1379523.09, grad_norm=0.3020, duration=0.38s
Step 2418: loss=3.4904, lr=0.000594, tokens/sec=1375062.46, grad_norm=0.2994, duration=0.38s
Step 2419: loss=3.5436, lr=0.000594, tokens/sec=1374666.19, grad_norm=0.2755, duration=0.38s
Step 2420: loss=3.5980, lr=0.000594, tokens/sec=1377387.07, grad_norm=0.2694, duration=0.38s
Step 2421: loss=3.6272, lr=0.000594, tokens/sec=1375399.59, grad_norm=0.2860, duration=0.38s
Step 2422: loss=3.6160, lr=0.000594, tokens/sec=1376757.56, grad_norm=0.3001, duration=0.38s
Step 2423: loss=3.5760, lr=0.000594, tokens/sec=1378502.64, grad_norm=0.3162, duration=0.38s
Step 2424: loss=3.6152, lr=0.000594, tokens/sec=1374691.97, grad_norm=0.3255, duration=0.38s
Step 2425: loss=3.5712, lr=0.000594, tokens/sec=1378541.53, grad_norm=0.3689, duration=0.38s
Step 2426: loss=3.6015, lr=0.000594, tokens/sec=1379439.14, grad_norm=0.3771, duration=0.38s
Step 2427: loss=3.6493, lr=0.000594, tokens/sec=1373991.08, grad_norm=0.3397, duration=0.38s
Step 2428: loss=3.7333, lr=0.000594, tokens/sec=1372716.54, grad_norm=0.3456, duration=0.38s
Step 2429: loss=3.6435, lr=0.000594, tokens/sec=1373544.80, grad_norm=0.3497, duration=0.38s
Step 2430: loss=3.6273, lr=0.000594, tokens/sec=1373489.04, grad_norm=0.3745, duration=0.38s
Step 2431: loss=3.5569, lr=0.000594, tokens/sec=1377108.46, grad_norm=0.3051, duration=0.38s
Step 2432: loss=3.5955, lr=0.000594, tokens/sec=1379820.85, grad_norm=0.2804, duration=0.38s
Step 2433: loss=3.5589, lr=0.000594, tokens/sec=1379182.19, grad_norm=0.3001, duration=0.38s
Step 2434: loss=3.5312, lr=0.000594, tokens/sec=1379475.49, grad_norm=0.3153, duration=0.38s
Step 2435: loss=3.6154, lr=0.000594, tokens/sec=1379382.90, grad_norm=0.2945, duration=0.38s
Step 2436: loss=3.5283, lr=0.000594, tokens/sec=1377710.68, grad_norm=0.2617, duration=0.38s
Step 2437: loss=3.6031, lr=0.000594, tokens/sec=1376443.02, grad_norm=0.2591, duration=0.38s
Step 2438: loss=3.6308, lr=0.000594, tokens/sec=1376183.73, grad_norm=0.2577, duration=0.38s
Step 2439: loss=3.5971, lr=0.000594, tokens/sec=1375701.61, grad_norm=0.2718, duration=0.38s
Step 2440: loss=3.5247, lr=0.000594, tokens/sec=1375444.33, grad_norm=0.2498, duration=0.38s
Step 2441: loss=3.5771, lr=0.000594, tokens/sec=1376960.15, grad_norm=0.2508, duration=0.38s
Step 2442: loss=3.5671, lr=0.000594, tokens/sec=1375024.62, grad_norm=0.2767, duration=0.38s
Step 2443: loss=3.5354, lr=0.000594, tokens/sec=1378923.61, grad_norm=0.2671, duration=0.38s
Step 2444: loss=3.4553, lr=0.000594, tokens/sec=1374439.36, grad_norm=0.2697, duration=0.38s
Step 2445: loss=3.5500, lr=0.000594, tokens/sec=1376354.28, grad_norm=0.2888, duration=0.38s
Step 2446: loss=3.5178, lr=0.000594, tokens/sec=1377889.37, grad_norm=0.2928, duration=0.38s
Step 2447: loss=3.5254, lr=0.000594, tokens/sec=1379487.60, grad_norm=0.2956, duration=0.38s
Step 2448: loss=3.5802, lr=0.000594, tokens/sec=1374319.10, grad_norm=0.2770, duration=0.38s
Step 2449: loss=3.4898, lr=0.000594, tokens/sec=1378134.62, grad_norm=0.2737, duration=0.38s
Step 2450: loss=3.5109, lr=0.000594, tokens/sec=1379440.87, grad_norm=0.3140, duration=0.38s
Step 2451: loss=3.5094, lr=0.000594, tokens/sec=1376290.54, grad_norm=0.2864, duration=0.38s
Step 2452: loss=3.4335, lr=0.000594, tokens/sec=1375511.44, grad_norm=0.2928, duration=0.38s
Step 2453: loss=3.5102, lr=0.000594, tokens/sec=1377106.74, grad_norm=0.2925, duration=0.38s
Step 2454: loss=3.5042, lr=0.000594, tokens/sec=1379152.78, grad_norm=0.3069, duration=0.38s
Step 2455: loss=3.5026, lr=0.000594, tokens/sec=1377916.14, grad_norm=0.3200, duration=0.38s
Step 2456: loss=3.4954, lr=0.000594, tokens/sec=1375428.84, grad_norm=0.3095, duration=0.38s
Step 2457: loss=3.5446, lr=0.000594, tokens/sec=1379582.80, grad_norm=0.3073, duration=0.38s
Step 2458: loss=3.5021, lr=0.000594, tokens/sec=1377711.54, grad_norm=0.3041, duration=0.38s
Step 2459: loss=3.4706, lr=0.000594, tokens/sec=1374619.78, grad_norm=0.2918, duration=0.38s
Step 2460: loss=3.4517, lr=0.000594, tokens/sec=1375682.68, grad_norm=0.2890, duration=0.38s
Step 2461: loss=3.4455, lr=0.000594, tokens/sec=1379645.99, grad_norm=0.2755, duration=0.38s
Step 2462: loss=3.5178, lr=0.000594, tokens/sec=1376492.99, grad_norm=0.2927, duration=0.38s
Step 2463: loss=3.4918, lr=0.000594, tokens/sec=1379282.54, grad_norm=0.3084, duration=0.38s
Step 2464: loss=3.5378, lr=0.000594, tokens/sec=1377552.74, grad_norm=0.3086, duration=0.38s
Step 2465: loss=3.5962, lr=0.000594, tokens/sec=1378174.35, grad_norm=0.2978, duration=0.38s
Step 2466: loss=3.5858, lr=0.000594, tokens/sec=1376916.18, grad_norm=0.3112, duration=0.38s
Step 2467: loss=3.5975, lr=0.000594, tokens/sec=1376964.46, grad_norm=0.3174, duration=0.38s
Step 2468: loss=3.5799, lr=0.000594, tokens/sec=1378057.75, grad_norm=0.3173, duration=0.38s
Step 2469: loss=3.5599, lr=0.000594, tokens/sec=1375768.74, grad_norm=0.3202, duration=0.38s
Step 2470: loss=3.5948, lr=0.000594, tokens/sec=1378463.76, grad_norm=0.3317, duration=0.38s
Step 2471: loss=3.6063, lr=0.000594, tokens/sec=1381412.29, grad_norm=0.3156, duration=0.38s
Step 2472: loss=3.6465, lr=0.000594, tokens/sec=1379692.73, grad_norm=0.3149, duration=0.38s
Step 2473: loss=3.6721, lr=0.000594, tokens/sec=1375333.36, grad_norm=0.3311, duration=0.38s
Step 2474: loss=3.6046, lr=0.000594, tokens/sec=1377536.34, grad_norm=0.3088, duration=0.38s
Step 2475: loss=3.6486, lr=0.000594, tokens/sec=1377097.25, grad_norm=0.3277, duration=0.38s
Step 2476: loss=3.6935, lr=0.000594, tokens/sec=1374706.58, grad_norm=0.3506, duration=0.38s
Step 2477: loss=3.6277, lr=0.000594, tokens/sec=1377331.00, grad_norm=0.3436, duration=0.38s
Step 2478: loss=3.6276, lr=0.000594, tokens/sec=1377841.03, grad_norm=0.3051, duration=0.38s
Step 2479: loss=3.6078, lr=0.000594, tokens/sec=1380153.40, grad_norm=0.2941, duration=0.38s
Step 2480: loss=3.5986, lr=0.000594, tokens/sec=1376432.68, grad_norm=0.3001, duration=0.38s
Step 2481: loss=3.5549, lr=0.000594, tokens/sec=1377941.18, grad_norm=0.2719, duration=0.38s
Step 2482: loss=3.5657, lr=0.000594, tokens/sec=1376077.81, grad_norm=0.2630, duration=0.38s
Step 2483: loss=3.5110, lr=0.000594, tokens/sec=1377548.43, grad_norm=0.2422, duration=0.38s
Step 2484: loss=3.5982, lr=0.000594, tokens/sec=1377696.87, grad_norm=0.2524, duration=0.38s
Step 2485: loss=3.5645, lr=0.000594, tokens/sec=1374856.99, grad_norm=0.2684, duration=0.38s
Step 2486: loss=3.5250, lr=0.000594, tokens/sec=1376836.86, grad_norm=0.2609, duration=0.38s
Step 2487: loss=3.5890, lr=0.000594, tokens/sec=1379774.97, grad_norm=0.2812, duration=0.38s
Step 2488: loss=3.5416, lr=0.000594, tokens/sec=1372244.54, grad_norm=0.2962, duration=0.38s
Step 2489: loss=3.5081, lr=0.000594, tokens/sec=1375416.80, grad_norm=0.5829, duration=0.38s
Step 2490: loss=3.5411, lr=0.000594, tokens/sec=1376978.25, grad_norm=0.3038, duration=0.38s
Step 2491: loss=3.5615, lr=0.000594, tokens/sec=1373914.68, grad_norm=0.3048, duration=0.38s
Step 2492: loss=3.6128, lr=0.000594, tokens/sec=1374390.39, grad_norm=0.2998, duration=0.38s
Step 2493: loss=3.4851, lr=0.000594, tokens/sec=1374153.35, grad_norm=0.2868, duration=0.38s
Step 2494: loss=3.5442, lr=0.000594, tokens/sec=1376299.15, grad_norm=0.2816, duration=0.38s
Step 2495: loss=3.5563, lr=0.000594, tokens/sec=1377380.17, grad_norm=0.2649, duration=0.38s
Step 2496: loss=3.5029, lr=0.000594, tokens/sec=1380242.63, grad_norm=0.2746, duration=0.38s
Step 2497: loss=3.4987, lr=0.000594, tokens/sec=1377636.45, grad_norm=0.2606, duration=0.38s
Step 2498: loss=3.5721, lr=0.000594, tokens/sec=1375858.26, grad_norm=0.2830, duration=0.38s
Step 2499: loss=3.5236, lr=0.000594, tokens/sec=1374153.35, grad_norm=0.3312, duration=0.38s
Step 2500/19073 (13.1%), Elapsed time: 1005.14s, Steps per hour: 8954.01, Estimated hours remaining: 1.85
Validation loss at step 2500: 3.76556658744812
Step 2500: loss=3.4893, lr=0.000594, tokens/sec=156722.41, grad_norm=0.4891, duration=3.35s
Step 2501: loss=3.5325, lr=0.000594, tokens/sec=1376469.72, grad_norm=0.3854, duration=0.38s
Step 2502: loss=3.5709, lr=0.000594, tokens/sec=1376567.09, grad_norm=0.4614, duration=0.38s
Step 2503: loss=3.6500, lr=0.000594, tokens/sec=1376816.17, grad_norm=0.4091, duration=0.38s
Step 2504: loss=3.5525, lr=0.000594, tokens/sec=1371866.16, grad_norm=0.3328, duration=0.38s
Step 2505: loss=3.4918, lr=0.000594, tokens/sec=1376308.63, grad_norm=0.2936, duration=0.38s
Step 2506: loss=3.5356, lr=0.000594, tokens/sec=1376985.15, grad_norm=0.2866, duration=0.38s
Step 2507: loss=3.4811, lr=0.000593, tokens/sec=1377338.76, grad_norm=0.2755, duration=0.38s
Step 2508: loss=3.5078, lr=0.000593, tokens/sec=1378252.95, grad_norm=0.2851, duration=0.38s
Step 2509: loss=3.5241, lr=0.000593, tokens/sec=1364236.21, grad_norm=0.3051, duration=0.38s
Step 2510: loss=3.5119, lr=0.000593, tokens/sec=1374884.49, grad_norm=0.3272, duration=0.38s
Step 2511: loss=3.5349, lr=0.000593, tokens/sec=1377956.72, grad_norm=0.3025, duration=0.38s
Step 2512: loss=3.4986, lr=0.000593, tokens/sec=1366033.68, grad_norm=0.2740, duration=0.38s
Step 2513: loss=3.5294, lr=0.000593, tokens/sec=1373335.50, grad_norm=0.2982, duration=0.38s
Step 2514: loss=3.5084, lr=0.000593, tokens/sec=1375903.89, grad_norm=0.2829, duration=0.38s
Step 2515: loss=3.4738, lr=0.000593, tokens/sec=1371285.29, grad_norm=0.2589, duration=0.38s
Step 2516: loss=3.4956, lr=0.000593, tokens/sec=1372769.67, grad_norm=0.2764, duration=0.38s
Step 2517: loss=3.5899, lr=0.000593, tokens/sec=1373800.52, grad_norm=0.2567, duration=0.38s
Step 2518: loss=3.6372, lr=0.000593, tokens/sec=1372101.56, grad_norm=0.2661, duration=0.38s
Step 2519: loss=3.6124, lr=0.000593, tokens/sec=1375692.14, grad_norm=0.3111, duration=0.38s
Step 2520: loss=3.5881, lr=0.000593, tokens/sec=1371952.60, grad_norm=0.3268, duration=0.38s
Step 2521: loss=3.6141, lr=0.000593, tokens/sec=1371379.36, grad_norm=0.3390, duration=0.38s
Step 2522: loss=3.6445, lr=0.000593, tokens/sec=1372271.95, grad_norm=0.3279, duration=0.38s
Step 2523: loss=3.5848, lr=0.000593, tokens/sec=1372844.23, grad_norm=0.3172, duration=0.38s
Step 2524: loss=3.5652, lr=0.000593, tokens/sec=1372456.09, grad_norm=0.2975, duration=0.38s
Step 2525: loss=3.5607, lr=0.000593, tokens/sec=1371974.00, grad_norm=0.2729, duration=0.38s
Step 2526: loss=3.5571, lr=0.000593, tokens/sec=1366753.65, grad_norm=0.2736, duration=0.38s
Step 2527: loss=3.5613, lr=0.000593, tokens/sec=1372594.01, grad_norm=0.3043, duration=0.38s
Step 2528: loss=3.5404, lr=0.000593, tokens/sec=1374000.52, grad_norm=0.2675, duration=0.38s
Step 2529: loss=3.5400, lr=0.000593, tokens/sec=1370256.50, grad_norm=0.2613, duration=0.38s
Step 2530: loss=3.5626, lr=0.000593, tokens/sec=1375030.64, grad_norm=0.2555, duration=0.38s
Step 2531: loss=3.4779, lr=0.000593, tokens/sec=1371811.39, grad_norm=0.2728, duration=0.38s
Step 2532: loss=3.5115, lr=0.000593, tokens/sec=1374949.83, grad_norm=0.3267, duration=0.38s
Step 2533: loss=3.5613, lr=0.000593, tokens/sec=1373694.10, grad_norm=0.3213, duration=0.38s
Step 2534: loss=3.5718, lr=0.000593, tokens/sec=1371347.71, grad_norm=0.2436, duration=0.38s
Step 2535: loss=3.5867, lr=0.000593, tokens/sec=1370639.12, grad_norm=0.2518, duration=0.38s
Step 2536: loss=3.5082, lr=0.000593, tokens/sec=1372754.24, grad_norm=0.2400, duration=0.38s
Step 2537: loss=3.6046, lr=0.000593, tokens/sec=1372137.51, grad_norm=0.2653, duration=0.38s
Step 2538: loss=3.5642, lr=0.000593, tokens/sec=1377069.66, grad_norm=0.2753, duration=0.38s
Step 2539: loss=3.5310, lr=0.000593, tokens/sec=1377316.33, grad_norm=0.3000, duration=0.38s
Step 2540: loss=3.5083, lr=0.000593, tokens/sec=1375536.39, grad_norm=0.3159, duration=0.38s
Step 2541: loss=3.5315, lr=0.000593, tokens/sec=1373365.52, grad_norm=0.3095, duration=0.38s
Step 2542: loss=3.4997, lr=0.000593, tokens/sec=1369799.00, grad_norm=0.3071, duration=0.38s
Step 2543: loss=3.5086, lr=0.000593, tokens/sec=1371619.72, grad_norm=0.2607, duration=0.38s
Step 2544: loss=3.5141, lr=0.000593, tokens/sec=1371695.01, grad_norm=0.2711, duration=0.38s
Step 2545: loss=3.5257, lr=0.000593, tokens/sec=1373350.94, grad_norm=0.2778, duration=0.38s
Step 2546: loss=3.4669, lr=0.000593, tokens/sec=1370745.07, grad_norm=0.2907, duration=0.38s
Step 2547: loss=3.4795, lr=0.000593, tokens/sec=1374508.09, grad_norm=0.2860, duration=0.38s
Step 2548: loss=3.5119, lr=0.000593, tokens/sec=1375526.06, grad_norm=0.2850, duration=0.38s
Step 2549: loss=3.5487, lr=0.000593, tokens/sec=1369822.03, grad_norm=0.2801, duration=0.38s
Step 2550: loss=3.5589, lr=0.000593, tokens/sec=1375003.13, grad_norm=0.2612, duration=0.38s
Step 2551: loss=3.4827, lr=0.000593, tokens/sec=1376199.24, grad_norm=0.2940, duration=0.38s
Step 2552: loss=3.5336, lr=0.000593, tokens/sec=1374837.22, grad_norm=0.2967, duration=0.38s
Step 2553: loss=3.4756, lr=0.000593, tokens/sec=1376742.90, grad_norm=0.3182, duration=0.38s
Step 2554: loss=3.4721, lr=0.000593, tokens/sec=1373062.81, grad_norm=0.2989, duration=0.38s
Step 2555: loss=3.3931, lr=0.000593, tokens/sec=1375999.45, grad_norm=0.2689, duration=0.38s
Step 2556: loss=3.4833, lr=0.000593, tokens/sec=1375676.65, grad_norm=0.3096, duration=0.38s
Step 2557: loss=3.5115, lr=0.000593, tokens/sec=1376279.34, grad_norm=0.3032, duration=0.38s
Step 2558: loss=3.4485, lr=0.000593, tokens/sec=1376210.43, grad_norm=0.2900, duration=0.38s
Step 2559: loss=3.5124, lr=0.000593, tokens/sec=1378907.18, grad_norm=0.2900, duration=0.38s
Step 2560: loss=3.4452, lr=0.000593, tokens/sec=1378857.03, grad_norm=0.2737, duration=0.38s
Step 2561: loss=3.5176, lr=0.000593, tokens/sec=1377355.15, grad_norm=0.2945, duration=0.38s
Step 2562: loss=3.4884, lr=0.000593, tokens/sec=1373313.20, grad_norm=0.2947, duration=0.38s
Step 2563: loss=3.5347, lr=0.000593, tokens/sec=1374838.08, grad_norm=0.3224, duration=0.38s
Step 2564: loss=3.6026, lr=0.000593, tokens/sec=1376934.28, grad_norm=0.3508, duration=0.38s
Step 2565: loss=3.5553, lr=0.000593, tokens/sec=1377484.57, grad_norm=0.3228, duration=0.38s
Step 2566: loss=3.5772, lr=0.000593, tokens/sec=1373151.12, grad_norm=0.3315, duration=0.38s
Step 2567: loss=3.5629, lr=0.000593, tokens/sec=1375816.08, grad_norm=0.4082, duration=0.38s
Step 2568: loss=3.5406, lr=0.000593, tokens/sec=1370364.94, grad_norm=0.4116, duration=0.38s
Step 2569: loss=3.5720, lr=0.000593, tokens/sec=1377664.07, grad_norm=0.3534, duration=0.38s
Step 2570: loss=3.5283, lr=0.000593, tokens/sec=1377384.49, grad_norm=0.3865, duration=0.38s
Step 2571: loss=3.5855, lr=0.000593, tokens/sec=1372417.54, grad_norm=0.3426, duration=0.38s
Step 2572: loss=3.5682, lr=0.000593, tokens/sec=1375428.84, grad_norm=0.3441, duration=0.38s
Step 2573: loss=3.5569, lr=0.000593, tokens/sec=1374227.20, grad_norm=0.3066, duration=0.38s
Step 2574: loss=3.5833, lr=0.000593, tokens/sec=1374067.49, grad_norm=0.2853, duration=0.38s
Step 2575: loss=3.5461, lr=0.000593, tokens/sec=1378630.55, grad_norm=0.2918, duration=0.38s
Step 2576: loss=3.5541, lr=0.000593, tokens/sec=1375623.30, grad_norm=0.2881, duration=0.38s
Step 2577: loss=3.5572, lr=0.000593, tokens/sec=1372464.65, grad_norm=0.2879, duration=0.38s
Step 2578: loss=3.5373, lr=0.000593, tokens/sec=1374796.82, grad_norm=0.2707, duration=0.38s
Step 2579: loss=3.4914, lr=0.000593, tokens/sec=1375159.62, grad_norm=0.2672, duration=0.38s
Step 2580: loss=3.5322, lr=0.000593, tokens/sec=1373815.11, grad_norm=0.2580, duration=0.38s
Step 2581: loss=3.5243, lr=0.000593, tokens/sec=1374131.03, grad_norm=0.2663, duration=0.38s
Step 2582: loss=3.5096, lr=0.000593, tokens/sec=1375001.41, grad_norm=0.2614, duration=0.38s
Step 2583: loss=3.4748, lr=0.000593, tokens/sec=1373519.92, grad_norm=0.2659, duration=0.38s
Step 2584: loss=3.5673, lr=0.000593, tokens/sec=1374942.95, grad_norm=0.2706, duration=0.38s
Step 2585: loss=3.4886, lr=0.000593, tokens/sec=1372104.98, grad_norm=0.2754, duration=0.38s
Step 2586: loss=3.5183, lr=0.000593, tokens/sec=1374961.86, grad_norm=0.2841, duration=0.38s
Step 2587: loss=3.5045, lr=0.000593, tokens/sec=1376631.72, grad_norm=0.2835, duration=0.38s
Step 2588: loss=3.5094, lr=0.000593, tokens/sec=1377530.30, grad_norm=0.2959, duration=0.38s
Step 2589: loss=3.5653, lr=0.000593, tokens/sec=1375683.54, grad_norm=0.2651, duration=0.38s
Step 2590: loss=3.5401, lr=0.000593, tokens/sec=1375910.78, grad_norm=0.2758, duration=0.38s
Step 2591: loss=3.4958, lr=0.000593, tokens/sec=1372948.80, grad_norm=0.2684, duration=0.38s
Step 2592: loss=3.5268, lr=0.000593, tokens/sec=1375578.55, grad_norm=0.3198, duration=0.38s
Step 2593: loss=3.4720, lr=0.000593, tokens/sec=1374546.75, grad_norm=0.3212, duration=0.38s
Step 2594: loss=3.5261, lr=0.000593, tokens/sec=1373140.83, grad_norm=0.2748, duration=0.38s
Step 2595: loss=3.5418, lr=0.000593, tokens/sec=1375417.66, grad_norm=0.2957, duration=0.38s
Step 2596: loss=3.5342, lr=0.000593, tokens/sec=1369614.72, grad_norm=0.3000, duration=0.38s
Step 2597: loss=3.5427, lr=0.000593, tokens/sec=1376184.60, grad_norm=0.2958, duration=0.38s
Step 2598: loss=3.5198, lr=0.000593, tokens/sec=1376107.09, grad_norm=0.2949, duration=0.38s
Step 2599: loss=3.4290, lr=0.000593, tokens/sec=1372265.10, grad_norm=0.3184, duration=0.38s
Step 2600/19073 (13.6%), Elapsed time: 1046.34s, Steps per hour: 8945.48, Estimated hours remaining: 1.84
Step 2600: loss=3.5119, lr=0.000593, tokens/sec=1380430.64, grad_norm=0.3163, duration=0.38s
Step 2601: loss=3.3959, lr=0.000593, tokens/sec=1370254.79, grad_norm=0.2777, duration=0.38s
Step 2602: loss=3.4318, lr=0.000593, tokens/sec=1376211.29, grad_norm=0.2657, duration=0.38s
Step 2603: loss=3.4461, lr=0.000593, tokens/sec=1376794.62, grad_norm=0.2808, duration=0.38s
Step 2604: loss=3.4992, lr=0.000593, tokens/sec=1374998.83, grad_norm=0.2803, duration=0.38s
Step 2605: loss=3.4684, lr=0.000593, tokens/sec=1373127.12, grad_norm=0.2792, duration=0.38s
Step 2606: loss=3.4338, lr=0.000593, tokens/sec=1370484.51, grad_norm=0.3065, duration=0.38s
Step 2607: loss=3.4156, lr=0.000593, tokens/sec=1374431.63, grad_norm=0.2842, duration=0.38s
Step 2608: loss=3.4596, lr=0.000593, tokens/sec=1375374.65, grad_norm=0.2917, duration=0.38s
Step 2609: loss=3.5036, lr=0.000593, tokens/sec=1373795.37, grad_norm=0.2968, duration=0.38s
Step 2610: loss=3.5734, lr=0.000593, tokens/sec=1374995.39, grad_norm=0.2981, duration=0.38s
Step 2611: loss=3.5535, lr=0.000593, tokens/sec=1375674.93, grad_norm=0.2886, duration=0.38s
Step 2612: loss=3.5580, lr=0.000593, tokens/sec=1375921.97, grad_norm=0.2667, duration=0.38s
Step 2613: loss=3.5437, lr=0.000593, tokens/sec=1373519.92, grad_norm=0.2877, duration=0.38s
Step 2614: loss=3.5680, lr=0.000593, tokens/sec=1375646.53, grad_norm=0.2698, duration=0.38s
Step 2615: loss=3.5219, lr=0.000593, tokens/sec=1375094.27, grad_norm=0.2590, duration=0.38s
Step 2616: loss=3.5491, lr=0.000593, tokens/sec=1375711.94, grad_norm=0.2621, duration=0.38s
Step 2617: loss=3.6164, lr=0.000593, tokens/sec=1378982.41, grad_norm=0.2610, duration=0.38s
Step 2618: loss=3.6638, lr=0.000593, tokens/sec=1377142.96, grad_norm=0.2874, duration=0.38s
Step 2619: loss=3.5836, lr=0.000593, tokens/sec=1375967.60, grad_norm=0.3359, duration=0.38s
Step 2620: loss=3.5741, lr=0.000593, tokens/sec=1378786.14, grad_norm=0.2927, duration=0.38s
Step 2621: loss=3.5094, lr=0.000593, tokens/sec=1376895.48, grad_norm=0.2527, duration=0.38s
Step 2622: loss=3.5320, lr=0.000593, tokens/sec=1377898.01, grad_norm=0.3034, duration=0.38s
Step 2623: loss=3.5068, lr=0.000592, tokens/sec=1375381.53, grad_norm=0.2747, duration=0.38s
Step 2624: loss=3.4948, lr=0.000592, tokens/sec=1379516.16, grad_norm=0.3259, duration=0.38s
Step 2625: loss=3.5675, lr=0.000592, tokens/sec=1374572.52, grad_norm=0.3076, duration=0.38s
Step 2626: loss=3.4774, lr=0.000592, tokens/sec=1377494.06, grad_norm=0.3311, duration=0.38s
Step 2627: loss=3.5542, lr=0.000592, tokens/sec=1379345.70, grad_norm=0.2970, duration=0.38s
Step 2628: loss=3.6212, lr=0.000592, tokens/sec=1377523.40, grad_norm=0.3058, duration=0.38s
Step 2629: loss=3.5561, lr=0.000592, tokens/sec=1374520.97, grad_norm=0.3302, duration=0.38s
Step 2630: loss=3.4538, lr=0.000592, tokens/sec=1376199.24, grad_norm=0.2817, duration=0.38s
Step 2631: loss=3.5254, lr=0.000592, tokens/sec=1374356.89, grad_norm=0.2741, duration=0.38s
Step 2632: loss=3.5337, lr=0.000592, tokens/sec=1375576.83, grad_norm=0.2734, duration=0.38s
Step 2633: loss=3.4838, lr=0.000592, tokens/sec=1376293.12, grad_norm=0.2658, duration=0.38s
Step 2634: loss=3.4319, lr=0.000592, tokens/sec=1374434.20, grad_norm=0.2696, duration=0.38s
Step 2635: loss=3.4843, lr=0.000592, tokens/sec=1376473.17, grad_norm=0.2805, duration=0.38s
Step 2636: loss=3.4602, lr=0.000592, tokens/sec=1379021.32, grad_norm=0.3019, duration=0.38s
Step 2637: loss=3.5018, lr=0.000592, tokens/sec=1375866.87, grad_norm=0.2803, duration=0.38s
Step 2638: loss=3.5295, lr=0.000592, tokens/sec=1378227.90, grad_norm=0.2782, duration=0.38s
Step 2639: loss=3.4569, lr=0.000592, tokens/sec=1380011.36, grad_norm=0.2907, duration=0.38s
Step 2640: loss=3.4658, lr=0.000592, tokens/sec=1376180.29, grad_norm=0.2852, duration=0.38s
Step 2641: loss=3.4429, lr=0.000592, tokens/sec=1373027.66, grad_norm=0.3194, duration=0.38s
Step 2642: loss=3.4161, lr=0.000592, tokens/sec=1379938.61, grad_norm=0.3288, duration=0.38s
Step 2643: loss=3.5069, lr=0.000592, tokens/sec=1378335.02, grad_norm=0.3157, duration=0.38s
Step 2644: loss=3.4693, lr=0.000592, tokens/sec=1375996.01, grad_norm=0.3308, duration=0.38s
Step 2645: loss=3.4526, lr=0.000592, tokens/sec=1376262.11, grad_norm=0.2958, duration=0.38s
Step 2646: loss=3.4261, lr=0.000592, tokens/sec=1375346.26, grad_norm=0.2780, duration=0.38s
Step 2647: loss=3.5198, lr=0.000592, tokens/sec=1376847.21, grad_norm=0.2723, duration=0.38s
Step 2648: loss=3.4413, lr=0.000592, tokens/sec=1377043.79, grad_norm=0.2581, duration=0.38s
Step 2649: loss=3.4433, lr=0.000592, tokens/sec=1376187.18, grad_norm=0.2464, duration=0.38s
Step 2650: loss=3.3561, lr=0.000592, tokens/sec=1377795.27, grad_norm=0.2301, duration=0.38s
Step 2651: loss=3.4428, lr=0.000592, tokens/sec=1374648.14, grad_norm=0.2633, duration=0.38s
Step 2652: loss=3.4602, lr=0.000592, tokens/sec=1375857.40, grad_norm=0.2729, duration=0.38s
Step 2653: loss=3.4708, lr=0.000592, tokens/sec=1375176.82, grad_norm=0.2820, duration=0.38s
Step 2654: loss=3.5042, lr=0.000592, tokens/sec=1373648.62, grad_norm=0.3222, duration=0.38s
Step 2655: loss=3.5325, lr=0.000592, tokens/sec=1373050.81, grad_norm=0.3456, duration=0.38s
Step 2656: loss=3.5387, lr=0.000592, tokens/sec=1374500.35, grad_norm=0.3415, duration=0.38s
Step 2657: loss=3.5736, lr=0.000592, tokens/sec=1377585.53, grad_norm=0.2956, duration=0.38s
Step 2658: loss=3.4804, lr=0.000592, tokens/sec=1376098.48, grad_norm=0.3106, duration=0.38s
Step 2659: loss=3.5623, lr=0.000592, tokens/sec=1373705.26, grad_norm=0.2829, duration=0.38s
Step 2660: loss=3.5303, lr=0.000592, tokens/sec=1374979.06, grad_norm=0.3184, duration=0.38s
Step 2661: loss=3.5940, lr=0.000592, tokens/sec=1374974.76, grad_norm=0.3164, duration=0.38s
Step 2662: loss=3.5678, lr=0.000592, tokens/sec=1375911.64, grad_norm=0.2975, duration=0.38s
Step 2663: loss=3.6258, lr=0.000592, tokens/sec=1375348.84, grad_norm=0.3031, duration=0.38s
Step 2664: loss=3.5751, lr=0.000592, tokens/sec=1380061.59, grad_norm=0.3158, duration=0.38s
Step 2665: loss=3.5872, lr=0.000592, tokens/sec=1377280.96, grad_norm=0.3122, duration=0.38s
Step 2666: loss=3.6526, lr=0.000592, tokens/sec=1375771.33, grad_norm=0.2879, duration=0.38s
Step 2667: loss=3.5559, lr=0.000592, tokens/sec=1376207.85, grad_norm=0.2919, duration=0.38s
Step 2668: loss=3.5901, lr=0.000592, tokens/sec=1380215.77, grad_norm=0.2621, duration=0.38s
Step 2669: loss=3.5344, lr=0.000592, tokens/sec=1373645.19, grad_norm=0.2721, duration=0.38s
Step 2670: loss=3.5565, lr=0.000592, tokens/sec=1377023.95, grad_norm=0.2951, duration=0.38s
Step 2671: loss=3.4985, lr=0.000592, tokens/sec=1376875.66, grad_norm=0.2815, duration=0.38s
Step 2672: loss=3.5336, lr=0.000592, tokens/sec=1372315.62, grad_norm=0.2778, duration=0.38s
Step 2673: loss=3.5000, lr=0.000592, tokens/sec=1378850.98, grad_norm=0.2731, duration=0.38s
Step 2674: loss=3.5167, lr=0.000592, tokens/sec=1367267.78, grad_norm=0.2622, duration=0.38s
Step 2675: loss=3.5128, lr=0.000592, tokens/sec=1373049.10, grad_norm=0.2745, duration=0.38s
Step 2676: loss=3.4923, lr=0.000592, tokens/sec=1378628.82, grad_norm=0.2933, duration=0.38s
Step 2677: loss=3.5601, lr=0.000592, tokens/sec=1373668.36, grad_norm=0.2975, duration=0.38s
Step 2678: loss=3.5199, lr=0.000592, tokens/sec=1374963.58, grad_norm=0.3215, duration=0.38s
Step 2679: loss=3.4270, lr=0.000592, tokens/sec=1380399.45, grad_norm=0.5463, duration=0.38s
Step 2680: loss=3.5055, lr=0.000592, tokens/sec=1377674.43, grad_norm=0.2949, duration=0.38s
Step 2681: loss=3.4861, lr=0.000592, tokens/sec=1370831.37, grad_norm=0.3058, duration=0.38s
Step 2682: loss=3.5901, lr=0.000592, tokens/sec=1371150.19, grad_norm=0.2637, duration=0.38s
Step 2683: loss=3.4090, lr=0.000592, tokens/sec=1371121.12, grad_norm=0.2737, duration=0.38s
Step 2684: loss=3.5444, lr=0.000592, tokens/sec=1372453.52, grad_norm=0.2890, duration=0.38s
Step 2685: loss=3.5076, lr=0.000592, tokens/sec=1378405.00, grad_norm=0.2868, duration=0.38s
Step 2686: loss=3.4412, lr=0.000592, tokens/sec=1375543.27, grad_norm=0.3056, duration=0.38s
Step 2687: loss=3.5033, lr=0.000592, tokens/sec=1373092.82, grad_norm=0.2918, duration=0.38s
Step 2688: loss=3.5104, lr=0.000592, tokens/sec=1375326.48, grad_norm=0.3257, duration=0.38s
Step 2689: loss=3.4443, lr=0.000592, tokens/sec=1374465.99, grad_norm=0.4997, duration=0.38s
Step 2690: loss=3.4773, lr=0.000592, tokens/sec=1378118.21, grad_norm=0.3283, duration=0.38s
Step 2691: loss=3.5042, lr=0.000592, tokens/sec=1375345.40, grad_norm=0.3560, duration=0.38s
Step 2692: loss=3.5591, lr=0.000592, tokens/sec=1377224.90, grad_norm=0.3641, duration=0.38s
Step 2693: loss=3.5835, lr=0.000592, tokens/sec=1372775.67, grad_norm=0.2709, duration=0.38s
Step 2694: loss=3.5084, lr=0.000592, tokens/sec=1371927.78, grad_norm=0.2968, duration=0.38s
Step 2695: loss=3.4238, lr=0.000592, tokens/sec=1369868.11, grad_norm=0.2850, duration=0.38s
Step 2696: loss=3.4992, lr=0.000592, tokens/sec=1374841.51, grad_norm=0.2694, duration=0.38s
Step 2697: loss=3.4695, lr=0.000592, tokens/sec=1366163.52, grad_norm=0.2833, duration=0.38s
Step 2698: loss=3.4509, lr=0.000592, tokens/sec=1374192.85, grad_norm=0.2792, duration=0.38s
Step 2699: loss=3.4589, lr=0.000592, tokens/sec=1373655.49, grad_norm=0.2656, duration=0.38s
Step 2700/19073 (14.2%), Elapsed time: 1084.54s, Steps per hour: 8962.33, Estimated hours remaining: 1.83
Step 2700: loss=3.4769, lr=0.000592, tokens/sec=1373876.91, grad_norm=0.2569, duration=0.38s
Step 2701: loss=3.4702, lr=0.000592, tokens/sec=1373679.51, grad_norm=0.2825, duration=0.38s
Step 2702: loss=3.4626, lr=0.000592, tokens/sec=1371966.30, grad_norm=0.2568, duration=0.38s
Step 2703: loss=3.4872, lr=0.000592, tokens/sec=1378698.83, grad_norm=0.2664, duration=0.38s
Step 2704: loss=3.4378, lr=0.000592, tokens/sec=1375427.98, grad_norm=0.2604, duration=0.38s
Step 2705: loss=3.4378, lr=0.000592, tokens/sec=1372476.65, grad_norm=0.2744, duration=0.38s
Step 2706: loss=3.4477, lr=0.000592, tokens/sec=1374777.91, grad_norm=0.2715, duration=0.38s
Step 2707: loss=3.5522, lr=0.000592, tokens/sec=1371541.02, grad_norm=0.2713, duration=0.38s
Step 2708: loss=3.6144, lr=0.000592, tokens/sec=1377841.89, grad_norm=0.2542, duration=0.38s
Step 2709: loss=3.5415, lr=0.000592, tokens/sec=1372454.37, grad_norm=0.2933, duration=0.38s
Step 2710: loss=3.5543, lr=0.000592, tokens/sec=1372574.31, grad_norm=0.3105, duration=0.38s
Step 2711: loss=3.6017, lr=0.000592, tokens/sec=1375148.44, grad_norm=0.3150, duration=0.38s
Step 2712: loss=3.5554, lr=0.000592, tokens/sec=1372652.27, grad_norm=0.3158, duration=0.38s
Step 2713: loss=3.5655, lr=0.000592, tokens/sec=1371940.62, grad_norm=0.2980, duration=0.38s
Step 2714: loss=3.5075, lr=0.000592, tokens/sec=1375353.14, grad_norm=0.2749, duration=0.38s
Step 2715: loss=3.4988, lr=0.000592, tokens/sec=1374181.69, grad_norm=0.2697, duration=0.38s
Step 2716: loss=3.5275, lr=0.000592, tokens/sec=1374777.91, grad_norm=0.2689, duration=0.38s
Step 2717: loss=3.5160, lr=0.000592, tokens/sec=1370573.34, grad_norm=0.2879, duration=0.38s
Step 2718: loss=3.5028, lr=0.000592, tokens/sec=1373855.45, grad_norm=0.2444, duration=0.38s
Step 2719: loss=3.5076, lr=0.000592, tokens/sec=1376047.67, grad_norm=0.2439, duration=0.38s
Step 2720: loss=3.5035, lr=0.000592, tokens/sec=1375612.11, grad_norm=0.2650, duration=0.38s
Step 2721: loss=3.4293, lr=0.000592, tokens/sec=1377629.55, grad_norm=0.2746, duration=0.38s
Step 2722: loss=3.4737, lr=0.000592, tokens/sec=1370099.41, grad_norm=0.2828, duration=0.38s
Step 2723: loss=3.5441, lr=0.000592, tokens/sec=1374402.42, grad_norm=0.2722, duration=0.38s
Step 2724: loss=3.4866, lr=0.000592, tokens/sec=1370808.30, grad_norm=0.2514, duration=0.38s
Step 2725: loss=3.5418, lr=0.000592, tokens/sec=1375477.02, grad_norm=0.2594, duration=0.38s
Step 2726: loss=3.4727, lr=0.000592, tokens/sec=1376879.97, grad_norm=0.2716, duration=0.38s
Step 2727: loss=3.5745, lr=0.000592, tokens/sec=1372598.29, grad_norm=0.2962, duration=0.38s
Step 2728: loss=3.5206, lr=0.000592, tokens/sec=1373782.49, grad_norm=0.2815, duration=0.38s
Step 2729: loss=3.4591, lr=0.000592, tokens/sec=1373280.61, grad_norm=0.2579, duration=0.38s
Step 2730: loss=3.4868, lr=0.000592, tokens/sec=1370277.84, grad_norm=0.2549, duration=0.38s
Step 2731: loss=3.4825, lr=0.000592, tokens/sec=1373986.78, grad_norm=0.2639, duration=0.38s
Step 2732: loss=3.4558, lr=0.000591, tokens/sec=1375876.34, grad_norm=0.2981, duration=0.38s
Step 2733: loss=3.4832, lr=0.000591, tokens/sec=1374620.64, grad_norm=0.2703, duration=0.38s
Step 2734: loss=3.4733, lr=0.000591, tokens/sec=1376898.93, grad_norm=0.2640, duration=0.38s
Step 2735: loss=3.4331, lr=0.000591, tokens/sec=1374989.37, grad_norm=0.2947, duration=0.38s
Step 2736: loss=3.4790, lr=0.000591, tokens/sec=1376609.32, grad_norm=0.2989, duration=0.38s
Step 2737: loss=3.4349, lr=0.000591, tokens/sec=1369156.79, grad_norm=0.2848, duration=0.38s
Step 2738: loss=3.5212, lr=0.000591, tokens/sec=1373524.21, grad_norm=0.3293, duration=0.38s
Step 2739: loss=3.5090, lr=0.000591, tokens/sec=1373896.65, grad_norm=0.3035, duration=0.38s
Step 2740: loss=3.5222, lr=0.000591, tokens/sec=1375046.98, grad_norm=0.2707, duration=0.38s
Step 2741: loss=3.4313, lr=0.000591, tokens/sec=1373495.05, grad_norm=0.2812, duration=0.38s
Step 2742: loss=3.4963, lr=0.000591, tokens/sec=1374711.73, grad_norm=0.2959, duration=0.38s
Step 2743: loss=3.4200, lr=0.000591, tokens/sec=1368545.84, grad_norm=0.2718, duration=0.38s
Step 2744: loss=3.4063, lr=0.000591, tokens/sec=1375573.39, grad_norm=0.2795, duration=0.38s
Step 2745: loss=3.3982, lr=0.000591, tokens/sec=1375569.95, grad_norm=0.2731, duration=0.38s
Step 2746: loss=3.4436, lr=0.000591, tokens/sec=1372143.51, grad_norm=0.2783, duration=0.38s
Step 2747: loss=3.4534, lr=0.000591, tokens/sec=1375445.19, grad_norm=0.2539, duration=0.38s
Step 2748: loss=3.4423, lr=0.000591, tokens/sec=1376684.30, grad_norm=0.2788, duration=0.38s
Step 2749: loss=3.4317, lr=0.000591, tokens/sec=1375243.04, grad_norm=0.3364, duration=0.38s
Validation loss at step 2750: 3.7456674575805664
Step 2750: loss=3.4276, lr=0.000591, tokens/sec=153173.37, grad_norm=0.3160, duration=3.42s
Step 2751: loss=3.4657, lr=0.000591, tokens/sec=1382329.29, grad_norm=0.2794, duration=0.38s
Step 2752: loss=3.4495, lr=0.000591, tokens/sec=1375026.34, grad_norm=0.2930, duration=0.38s
Step 2753: loss=3.5046, lr=0.000591, tokens/sec=1378606.35, grad_norm=0.2603, duration=0.38s
Step 2754: loss=3.5043, lr=0.000591, tokens/sec=1376631.72, grad_norm=0.2798, duration=0.38s
Step 2755: loss=3.5325, lr=0.000591, tokens/sec=1376246.61, grad_norm=0.3039, duration=0.38s
Step 2756: loss=3.5243, lr=0.000591, tokens/sec=1376536.07, grad_norm=0.3555, duration=0.38s
Step 2757: loss=3.5436, lr=0.000591, tokens/sec=1379965.46, grad_norm=0.3690, duration=0.38s
Step 2758: loss=3.4806, lr=0.000591, tokens/sec=1376880.83, grad_norm=0.3125, duration=0.38s
Step 2759: loss=3.5122, lr=0.000591, tokens/sec=1375312.71, grad_norm=0.3244, duration=0.38s
Step 2760: loss=3.4943, lr=0.000591, tokens/sec=1377254.22, grad_norm=0.3580, duration=0.38s
Step 2761: loss=3.5414, lr=0.000591, tokens/sec=1375169.94, grad_norm=0.3237, duration=0.38s
Step 2762: loss=3.5326, lr=0.000591, tokens/sec=1376436.98, grad_norm=0.3356, duration=0.38s
Step 2763: loss=3.5261, lr=0.000591, tokens/sec=1378383.40, grad_norm=0.2923, duration=0.38s
Step 2764: loss=3.5450, lr=0.000591, tokens/sec=1372785.95, grad_norm=0.3066, duration=0.38s
Step 2765: loss=3.5010, lr=0.000591, tokens/sec=1375871.18, grad_norm=0.3092, duration=0.38s
Step 2766: loss=3.5399, lr=0.000591, tokens/sec=1374900.83, grad_norm=0.3098, duration=0.38s
Step 2767: loss=3.4694, lr=0.000591, tokens/sec=1375369.49, grad_norm=0.2757, duration=0.38s
Step 2768: loss=3.5074, lr=0.000591, tokens/sec=1374945.53, grad_norm=0.2777, duration=0.38s
Step 2769: loss=3.4498, lr=0.000591, tokens/sec=1369851.05, grad_norm=0.2864, duration=0.38s
Step 2770: loss=3.4968, lr=0.000591, tokens/sec=1374394.69, grad_norm=0.2675, duration=0.38s
Step 2771: loss=3.4665, lr=0.000591, tokens/sec=1379201.22, grad_norm=0.2794, duration=0.38s
Step 2772: loss=3.4850, lr=0.000591, tokens/sec=1373158.84, grad_norm=0.2853, duration=0.38s
Step 2773: loss=3.4516, lr=0.000591, tokens/sec=1376681.71, grad_norm=0.2866, duration=0.38s
Step 2774: loss=3.4907, lr=0.000591, tokens/sec=1372287.36, grad_norm=0.2824, duration=0.38s
Step 2775: loss=3.4414, lr=0.000591, tokens/sec=1372387.56, grad_norm=0.2675, duration=0.38s
Step 2776: loss=3.4947, lr=0.000591, tokens/sec=1375886.67, grad_norm=0.2518, duration=0.38s
Step 2777: loss=3.4555, lr=0.000591, tokens/sec=1375041.82, grad_norm=0.2892, duration=0.38s
Step 2778: loss=3.5480, lr=0.000591, tokens/sec=1375823.83, grad_norm=0.3087, duration=0.38s
Step 2779: loss=3.5004, lr=0.000591, tokens/sec=1375312.71, grad_norm=0.3039, duration=0.38s
Step 2780: loss=3.4658, lr=0.000591, tokens/sec=1369603.63, grad_norm=0.2979, duration=0.38s
Step 2781: loss=3.4661, lr=0.000591, tokens/sec=1370440.95, grad_norm=0.2595, duration=0.38s
Step 2782: loss=3.4577, lr=0.000591, tokens/sec=1372983.09, grad_norm=0.2937, duration=0.38s
Step 2783: loss=3.4678, lr=0.000591, tokens/sec=1372111.83, grad_norm=0.2844, duration=0.38s
Step 2784: loss=3.4954, lr=0.000591, tokens/sec=1373518.21, grad_norm=0.2956, duration=0.38s
Step 2785: loss=3.5096, lr=0.000591, tokens/sec=1373381.82, grad_norm=0.2952, duration=0.38s
Step 2786: loss=3.5178, lr=0.000591, tokens/sec=1374636.97, grad_norm=0.2820, duration=0.38s
Step 2787: loss=3.4870, lr=0.000591, tokens/sec=1373621.17, grad_norm=0.2586, duration=0.38s
Step 2788: loss=3.4407, lr=0.000591, tokens/sec=1371835.35, grad_norm=0.2435, duration=0.38s
Step 2789: loss=3.4252, lr=0.000591, tokens/sec=1373253.17, grad_norm=0.2632, duration=0.38s
Step 2790: loss=3.4328, lr=0.000591, tokens/sec=1376830.83, grad_norm=0.2375, duration=0.38s
Step 2791: loss=3.3496, lr=0.000591, tokens/sec=1373379.24, grad_norm=0.2714, duration=0.38s
Step 2792: loss=3.4201, lr=0.000591, tokens/sec=1372097.27, grad_norm=0.3141, duration=0.38s
Step 2793: loss=3.3963, lr=0.000591, tokens/sec=1373776.49, grad_norm=0.3660, duration=0.38s
Step 2794: loss=3.4590, lr=0.000591, tokens/sec=1373902.66, grad_norm=0.3521, duration=0.38s
Step 2795: loss=3.4215, lr=0.000591, tokens/sec=1372809.95, grad_norm=0.3077, duration=0.38s
Step 2796: loss=3.3907, lr=0.000591, tokens/sec=1377180.05, grad_norm=0.3039, duration=0.38s
Step 2797: loss=3.3930, lr=0.000591, tokens/sec=1375866.87, grad_norm=0.2897, duration=0.38s
Step 2798: loss=3.4210, lr=0.000591, tokens/sec=1374989.37, grad_norm=0.2699, duration=0.38s
Step 2799: loss=3.4817, lr=0.000591, tokens/sec=1372092.14, grad_norm=0.2547, duration=0.38s
Step 2800/19073 (14.7%), Elapsed time: 1125.82s, Steps per hour: 8953.49, Estimated hours remaining: 1.82
Step 2800: loss=3.5025, lr=0.000591, tokens/sec=1372692.54, grad_norm=0.3092, duration=0.38s
Step 2801: loss=3.5036, lr=0.000591, tokens/sec=1372117.82, grad_norm=0.3131, duration=0.38s
Step 2802: loss=3.5304, lr=0.000591, tokens/sec=1374089.81, grad_norm=0.2845, duration=0.38s
Step 2803: loss=3.5029, lr=0.000591, tokens/sec=1377013.61, grad_norm=0.2932, duration=0.38s
Step 2804: loss=3.5377, lr=0.000591, tokens/sec=1376849.79, grad_norm=0.3320, duration=0.38s
Step 2805: loss=3.4840, lr=0.000591, tokens/sec=1372747.39, grad_norm=0.3223, duration=0.38s
Step 2806: loss=3.5313, lr=0.000591, tokens/sec=1375040.96, grad_norm=0.3295, duration=0.38s
Step 2807: loss=3.5605, lr=0.000591, tokens/sec=1374066.63, grad_norm=0.3244, duration=0.38s
Step 2808: loss=3.6159, lr=0.000591, tokens/sec=1374410.15, grad_norm=0.3357, duration=0.38s
Step 2809: loss=3.5460, lr=0.000591, tokens/sec=1378697.10, grad_norm=0.3284, duration=0.38s
Step 2810: loss=3.5396, lr=0.000591, tokens/sec=1375887.53, grad_norm=0.3119, duration=0.38s
Step 2811: loss=3.4553, lr=0.000591, tokens/sec=1375634.48, grad_norm=0.2703, duration=0.38s
Step 2812: loss=3.4914, lr=0.000591, tokens/sec=1376512.81, grad_norm=0.2624, duration=0.38s
Step 2813: loss=3.4756, lr=0.000591, tokens/sec=1376089.87, grad_norm=0.2783, duration=0.38s
Step 2814: loss=3.4489, lr=0.000591, tokens/sec=1376999.81, grad_norm=0.2871, duration=0.38s
Step 2815: loss=3.5170, lr=0.000591, tokens/sec=1372610.29, grad_norm=0.3133, duration=0.38s
Step 2816: loss=3.4272, lr=0.000591, tokens/sec=1375304.97, grad_norm=0.3170, duration=0.38s
Step 2817: loss=3.5430, lr=0.000591, tokens/sec=1374865.58, grad_norm=0.2830, duration=0.38s
Step 2818: loss=3.5763, lr=0.000591, tokens/sec=1373472.74, grad_norm=0.2742, duration=0.38s
Step 2819: loss=3.4800, lr=0.000591, tokens/sec=1373496.76, grad_norm=0.2577, duration=0.38s
Step 2820: loss=3.4091, lr=0.000591, tokens/sec=1373466.74, grad_norm=0.2792, duration=0.38s
Step 2821: loss=3.4930, lr=0.000591, tokens/sec=1369741.83, grad_norm=0.2609, duration=0.38s
Step 2822: loss=3.4867, lr=0.000591, tokens/sec=1371750.63, grad_norm=0.2446, duration=0.38s
Step 2823: loss=3.4620, lr=0.000591, tokens/sec=1375250.79, grad_norm=0.2411, duration=0.38s
Step 2824: loss=3.3726, lr=0.000591, tokens/sec=1374949.83, grad_norm=0.2582, duration=0.38s
Step 2825: loss=3.4302, lr=0.000591, tokens/sec=1374956.70, grad_norm=0.2620, duration=0.38s
Step 2826: loss=3.4366, lr=0.000591, tokens/sec=1375318.73, grad_norm=0.2608, duration=0.38s
Step 2827: loss=3.4538, lr=0.000591, tokens/sec=1371486.27, grad_norm=0.2585, duration=0.38s
Step 2828: loss=3.4977, lr=0.000591, tokens/sec=1370887.77, grad_norm=0.2778, duration=0.38s
Step 2829: loss=3.4167, lr=0.000591, tokens/sec=1373094.53, grad_norm=0.2746, duration=0.38s
Step 2830: loss=3.4002, lr=0.000591, tokens/sec=1373583.41, grad_norm=0.2587, duration=0.38s
Step 2831: loss=3.4217, lr=0.000591, tokens/sec=1367361.30, grad_norm=0.2499, duration=0.38s
Step 2832: loss=3.4123, lr=0.000591, tokens/sec=1373763.61, grad_norm=0.2516, duration=0.38s
Step 2833: loss=3.4687, lr=0.000591, tokens/sec=1372115.25, grad_norm=0.2525, duration=0.38s
Step 2834: loss=3.4179, lr=0.000590, tokens/sec=1371123.69, grad_norm=0.2521, duration=0.38s
Step 2835: loss=3.3830, lr=0.000590, tokens/sec=1370576.76, grad_norm=0.2767, duration=0.38s
Step 2836: loss=3.4068, lr=0.000590, tokens/sec=1375569.95, grad_norm=0.3096, duration=0.38s
Step 2837: loss=3.4686, lr=0.000590, tokens/sec=1372927.37, grad_norm=0.3350, duration=0.38s
Step 2838: loss=3.4252, lr=0.000590, tokens/sec=1374669.62, grad_norm=0.3045, duration=0.38s
Step 2839: loss=3.3586, lr=0.000590, tokens/sec=1374276.16, grad_norm=0.2849, duration=0.38s
Step 2840: loss=3.3638, lr=0.000590, tokens/sec=1374292.48, grad_norm=0.2803, duration=0.38s
Step 2841: loss=3.3928, lr=0.000590, tokens/sec=1374038.30, grad_norm=0.2843, duration=0.38s
Step 2842: loss=3.4464, lr=0.000590, tokens/sec=1375239.60, grad_norm=0.2841, duration=0.38s
Step 2843: loss=3.4419, lr=0.000590, tokens/sec=1377155.90, grad_norm=0.3027, duration=0.38s
Step 2844: loss=3.4442, lr=0.000590, tokens/sec=1376667.06, grad_norm=0.3255, duration=0.38s
Step 2845: loss=3.4881, lr=0.000590, tokens/sec=1374616.35, grad_norm=0.2897, duration=0.38s
Step 2846: loss=3.5218, lr=0.000590, tokens/sec=1371938.91, grad_norm=0.2915, duration=0.38s
Step 2847: loss=3.4797, lr=0.000590, tokens/sec=1376161.34, grad_norm=0.3263, duration=0.38s
Step 2848: loss=3.4929, lr=0.000590, tokens/sec=1376054.56, grad_norm=0.3413, duration=0.38s
Step 2849: loss=3.5088, lr=0.000590, tokens/sec=1375416.80, grad_norm=0.3387, duration=0.38s
Step 2850: loss=3.5191, lr=0.000590, tokens/sec=1376436.12, grad_norm=0.3516, duration=0.38s
Step 2851: loss=3.5204, lr=0.000590, tokens/sec=1377086.04, grad_norm=0.3153, duration=0.38s
Step 2852: loss=3.5272, lr=0.000590, tokens/sec=1377168.83, grad_norm=0.2949, duration=0.38s
Step 2853: loss=3.5994, lr=0.000590, tokens/sec=1378397.22, grad_norm=0.3463, duration=0.38s
Step 2854: loss=3.5291, lr=0.000590, tokens/sec=1374387.82, grad_norm=0.3799, duration=0.38s
Step 2855: loss=3.5648, lr=0.000590, tokens/sec=1374135.32, grad_norm=0.3762, duration=0.38s
Step 2856: loss=3.5940, lr=0.000590, tokens/sec=1376392.19, grad_norm=0.3013, duration=0.38s
Step 2857: loss=3.5335, lr=0.000590, tokens/sec=1377668.39, grad_norm=0.2987, duration=0.38s
Step 2858: loss=3.5310, lr=0.000590, tokens/sec=1376954.97, grad_norm=0.2945, duration=0.38s
Step 2859: loss=3.5010, lr=0.000590, tokens/sec=1376312.93, grad_norm=0.2690, duration=0.38s
Step 2860: loss=3.5067, lr=0.000590, tokens/sec=1376381.85, grad_norm=0.2720, duration=0.38s
Step 2861: loss=3.4720, lr=0.000590, tokens/sec=1376578.29, grad_norm=0.2674, duration=0.38s
Step 2862: loss=3.5260, lr=0.000590, tokens/sec=1375031.50, grad_norm=0.2537, duration=0.38s
Step 2863: loss=3.4210, lr=0.000590, tokens/sec=1370639.12, grad_norm=0.2305, duration=0.38s
Step 2864: loss=3.4759, lr=0.000590, tokens/sec=1373798.80, grad_norm=0.2817, duration=0.38s
Step 2865: loss=3.4862, lr=0.000590, tokens/sec=1377802.18, grad_norm=0.2851, duration=0.38s
Step 2866: loss=3.4658, lr=0.000590, tokens/sec=1373995.37, grad_norm=0.2738, duration=0.38s
Step 2867: loss=3.5417, lr=0.000590, tokens/sec=1375761.00, grad_norm=0.2771, duration=0.38s
Step 2868: loss=3.4407, lr=0.000590, tokens/sec=1374660.17, grad_norm=0.2597, duration=0.38s
Step 2869: loss=3.3962, lr=0.000590, tokens/sec=1373280.61, grad_norm=0.4054, duration=0.38s
Step 2870: loss=3.4329, lr=0.000590, tokens/sec=1377657.17, grad_norm=0.3105, duration=0.38s
Step 2871: loss=3.4671, lr=0.000590, tokens/sec=1374386.10, grad_norm=0.2978, duration=0.38s
Step 2872: loss=3.5209, lr=0.000590, tokens/sec=1372562.31, grad_norm=0.3030, duration=0.38s
Step 2873: loss=3.4088, lr=0.000590, tokens/sec=1379068.02, grad_norm=0.2846, duration=0.38s
Step 2874: loss=3.4945, lr=0.000590, tokens/sec=1376898.93, grad_norm=0.2597, duration=0.38s
Step 2875: loss=3.4445, lr=0.000590, tokens/sec=1375686.98, grad_norm=0.2646, duration=0.38s
Step 2876: loss=3.4432, lr=0.000590, tokens/sec=1374401.56, grad_norm=0.2509, duration=0.38s
Step 2877: loss=3.4425, lr=0.000590, tokens/sec=1377693.42, grad_norm=0.2583, duration=0.38s
Step 2878: loss=3.4220, lr=0.000590, tokens/sec=1374919.74, grad_norm=0.3443, duration=0.38s
Step 2879: loss=3.4314, lr=0.000590, tokens/sec=1376137.23, grad_norm=0.2969, duration=0.38s
Step 2880: loss=3.4394, lr=0.000590, tokens/sec=1372789.38, grad_norm=0.2603, duration=0.38s
Step 2881: loss=3.4995, lr=0.000590, tokens/sec=1376683.43, grad_norm=0.3041, duration=0.38s
Step 2882: loss=3.5015, lr=0.000590, tokens/sec=1375023.76, grad_norm=0.4319, duration=0.38s
Step 2883: loss=3.5474, lr=0.000590, tokens/sec=1376695.50, grad_norm=0.2943, duration=0.38s
Step 2884: loss=3.4445, lr=0.000590, tokens/sec=1376139.81, grad_norm=0.3002, duration=0.38s
Step 2885: loss=3.3948, lr=0.000590, tokens/sec=1375974.49, grad_norm=0.2654, duration=0.38s
Step 2886: loss=3.4921, lr=0.000590, tokens/sec=1375132.97, grad_norm=0.2618, duration=0.38s
Step 2887: loss=3.4168, lr=0.000590, tokens/sec=1376436.12, grad_norm=0.2640, duration=0.38s
Step 2888: loss=3.3944, lr=0.000590, tokens/sec=1377567.41, grad_norm=0.2835, duration=0.38s
Step 2889: loss=3.4311, lr=0.000590, tokens/sec=1379768.04, grad_norm=0.2724, duration=0.38s
Step 2890: loss=3.4205, lr=0.000590, tokens/sec=1371824.22, grad_norm=0.2586, duration=0.38s
Step 2891: loss=3.4354, lr=0.000590, tokens/sec=1376571.40, grad_norm=0.2488, duration=0.38s
Step 2892: loss=3.4279, lr=0.000590, tokens/sec=1374250.39, grad_norm=0.2610, duration=0.38s
Step 2893: loss=3.4262, lr=0.000590, tokens/sec=1378093.16, grad_norm=0.2715, duration=0.38s
Step 2894: loss=3.4091, lr=0.000590, tokens/sec=1376377.54, grad_norm=0.2694, duration=0.38s
Step 2895: loss=3.3993, lr=0.000590, tokens/sec=1376586.91, grad_norm=0.2906, duration=0.38s
Step 2896: loss=3.4168, lr=0.000590, tokens/sec=1376376.68, grad_norm=0.2931, duration=0.38s
Step 2897: loss=3.5385, lr=0.000590, tokens/sec=1377806.49, grad_norm=0.2729, duration=0.38s
Step 2898: loss=3.5495, lr=0.000590, tokens/sec=1380148.20, grad_norm=0.2649, duration=0.38s
Step 2899: loss=3.5116, lr=0.000590, tokens/sec=1374714.31, grad_norm=0.2653, duration=0.38s
Step 2900/19073 (15.2%), Elapsed time: 1164.03s, Steps per hour: 8968.85, Estimated hours remaining: 1.80
Step 2900: loss=3.5426, lr=0.000590, tokens/sec=1379171.81, grad_norm=0.2769, duration=0.38s
Step 2901: loss=3.5162, lr=0.000590, tokens/sec=1375889.25, grad_norm=0.2909, duration=0.38s
Step 2902: loss=3.5389, lr=0.000590, tokens/sec=1372725.96, grad_norm=0.2804, duration=0.38s
Step 2903: loss=3.5119, lr=0.000590, tokens/sec=1369993.57, grad_norm=0.2838, duration=0.38s
Step 2904: loss=3.4523, lr=0.000590, tokens/sec=1379729.09, grad_norm=0.2968, duration=0.38s
Step 2905: loss=3.4717, lr=0.000590, tokens/sec=1377873.83, grad_norm=0.2699, duration=0.38s
Step 2906: loss=3.4874, lr=0.000590, tokens/sec=1376027.01, grad_norm=0.2900, duration=0.38s
Step 2907: loss=3.4801, lr=0.000590, tokens/sec=1375071.91, grad_norm=0.2700, duration=0.38s
Step 2908: loss=3.4736, lr=0.000590, tokens/sec=1375510.58, grad_norm=0.2770, duration=0.38s
Step 2909: loss=3.4564, lr=0.000590, tokens/sec=1379396.74, grad_norm=0.2800, duration=0.38s
Step 2910: loss=3.4620, lr=0.000590, tokens/sec=1378125.12, grad_norm=0.2887, duration=0.38s
Step 2911: loss=3.3977, lr=0.000590, tokens/sec=1372283.08, grad_norm=0.2789, duration=0.38s
Step 2912: loss=3.4619, lr=0.000590, tokens/sec=1376977.39, grad_norm=0.2646, duration=0.38s
Step 2913: loss=3.4619, lr=0.000590, tokens/sec=1376209.57, grad_norm=0.2461, duration=0.38s
Step 2914: loss=3.4512, lr=0.000590, tokens/sec=1377020.50, grad_norm=0.2650, duration=0.38s
Step 2915: loss=3.5085, lr=0.000590, tokens/sec=1375462.40, grad_norm=0.2796, duration=0.38s
Step 2916: loss=3.4470, lr=0.000590, tokens/sec=1376224.21, grad_norm=0.3013, duration=0.38s
Step 2917: loss=3.5351, lr=0.000590, tokens/sec=1376691.19, grad_norm=0.3188, duration=0.38s
Step 2918: loss=3.4522, lr=0.000590, tokens/sec=1376572.26, grad_norm=0.3110, duration=0.38s
Step 2919: loss=3.4456, lr=0.000590, tokens/sec=1375692.14, grad_norm=0.3151, duration=0.38s
Step 2920: loss=3.4441, lr=0.000590, tokens/sec=1375846.21, grad_norm=0.2879, duration=0.38s
Step 2921: loss=3.4405, lr=0.000590, tokens/sec=1377444.88, grad_norm=0.2717, duration=0.38s
Step 2922: loss=3.4296, lr=0.000590, tokens/sec=1376774.80, grad_norm=0.2630, duration=0.38s
Step 2923: loss=3.4479, lr=0.000590, tokens/sec=1375966.74, grad_norm=0.2867, duration=0.38s
Step 2924: loss=3.3869, lr=0.000590, tokens/sec=1374317.38, grad_norm=0.3237, duration=0.38s
Step 2925: loss=3.4520, lr=0.000590, tokens/sec=1378336.75, grad_norm=0.3150, duration=0.38s
Step 2926: loss=3.4353, lr=0.000590, tokens/sec=1377147.27, grad_norm=0.2792, duration=0.38s
Step 2927: loss=3.4504, lr=0.000590, tokens/sec=1376719.63, grad_norm=0.3579, duration=0.38s
Step 2928: loss=3.4800, lr=0.000590, tokens/sec=1374492.62, grad_norm=0.2977, duration=0.38s
Step 2929: loss=3.4729, lr=0.000590, tokens/sec=1376859.28, grad_norm=0.3033, duration=0.38s
Step 2930: loss=3.4749, lr=0.000590, tokens/sec=1372372.15, grad_norm=0.3077, duration=0.38s
Step 2931: loss=3.3972, lr=0.000590, tokens/sec=1376454.22, grad_norm=0.3113, duration=0.38s
Step 2932: loss=3.4460, lr=0.000589, tokens/sec=1378450.80, grad_norm=0.2793, duration=0.38s
Step 2933: loss=3.3617, lr=0.000589, tokens/sec=1370049.90, grad_norm=0.2952, duration=0.38s
Step 2934: loss=3.4212, lr=0.000589, tokens/sec=1376085.56, grad_norm=0.3290, duration=0.38s
Step 2935: loss=3.3704, lr=0.000589, tokens/sec=1375589.74, grad_norm=0.3299, duration=0.38s
Step 2936: loss=3.3946, lr=0.000589, tokens/sec=1375962.43, grad_norm=0.2672, duration=0.38s
Step 2937: loss=3.4546, lr=0.000589, tokens/sec=1376261.25, grad_norm=0.2737, duration=0.38s
Step 2938: loss=3.3637, lr=0.000589, tokens/sec=1374851.83, grad_norm=0.2606, duration=0.38s
Step 2939: loss=3.4139, lr=0.000589, tokens/sec=1369712.82, grad_norm=0.2756, duration=0.38s
Step 2940: loss=3.3780, lr=0.000589, tokens/sec=1373364.66, grad_norm=0.2763, duration=0.38s
Step 2941: loss=3.4293, lr=0.000589, tokens/sec=1374213.46, grad_norm=0.3041, duration=0.38s
Step 2942: loss=3.4289, lr=0.000589, tokens/sec=1375208.64, grad_norm=0.3252, duration=0.38s
Step 2943: loss=3.4175, lr=0.000589, tokens/sec=1374673.92, grad_norm=0.3098, duration=0.38s
Step 2944: loss=3.4897, lr=0.000589, tokens/sec=1370721.14, grad_norm=0.2901, duration=0.38s
Step 2945: loss=3.4826, lr=0.000589, tokens/sec=1373165.70, grad_norm=0.3276, duration=0.38s
Step 2946: loss=3.5040, lr=0.000589, tokens/sec=1373896.65, grad_norm=0.3230, duration=0.38s
Step 2947: loss=3.4933, lr=0.000589, tokens/sec=1373406.69, grad_norm=0.2804, duration=0.38s
Step 2948: loss=3.4305, lr=0.000589, tokens/sec=1380821.57, grad_norm=0.3328, duration=0.38s
Step 2949: loss=3.4884, lr=0.000589, tokens/sec=1377156.76, grad_norm=0.3673, duration=0.38s
Step 2950: loss=3.4498, lr=0.000589, tokens/sec=1374905.98, grad_norm=0.3453, duration=0.38s
Step 2951: loss=3.5069, lr=0.000589, tokens/sec=1375318.73, grad_norm=0.3111, duration=0.38s
Step 2952: loss=3.5013, lr=0.000589, tokens/sec=1374721.19, grad_norm=0.2971, duration=0.38s
Step 2953: loss=3.4912, lr=0.000589, tokens/sec=1377395.70, grad_norm=0.3530, duration=0.38s
Step 2954: loss=3.5015, lr=0.000589, tokens/sec=1378867.41, grad_norm=0.3043, duration=0.38s
Step 2955: loss=3.4894, lr=0.000589, tokens/sec=1374737.52, grad_norm=0.3006, duration=0.38s
Step 2956: loss=3.4539, lr=0.000589, tokens/sec=1378750.70, grad_norm=0.2614, duration=0.38s
Step 2957: loss=3.4437, lr=0.000589, tokens/sec=1374043.45, grad_norm=0.2687, duration=0.38s
Step 2958: loss=3.4715, lr=0.000589, tokens/sec=1374546.75, grad_norm=0.2464, duration=0.38s
Step 2959: loss=3.4138, lr=0.000589, tokens/sec=1375672.35, grad_norm=0.2580, duration=0.38s
Step 2960: loss=3.4430, lr=0.000589, tokens/sec=1376257.80, grad_norm=0.2456, duration=0.38s
Step 2961: loss=3.4456, lr=0.000589, tokens/sec=1372507.48, grad_norm=0.2559, duration=0.38s
Step 2962: loss=3.4659, lr=0.000589, tokens/sec=1370894.61, grad_norm=0.2836, duration=0.38s
Step 2963: loss=3.3775, lr=0.000589, tokens/sec=1375272.29, grad_norm=0.2749, duration=0.38s
Step 2964: loss=3.4497, lr=0.000589, tokens/sec=1378291.82, grad_norm=0.2801, duration=0.38s
Step 2965: loss=3.4201, lr=0.000589, tokens/sec=1375844.49, grad_norm=0.2656, duration=0.38s
Step 2966: loss=3.4540, lr=0.000589, tokens/sec=1379674.55, grad_norm=0.2778, duration=0.38s
Step 2967: loss=3.4973, lr=0.000589, tokens/sec=1377508.73, grad_norm=0.3063, duration=0.38s
Step 2968: loss=3.4816, lr=0.000589, tokens/sec=1375219.82, grad_norm=0.2793, duration=0.38s
Step 2969: loss=3.4263, lr=0.000589, tokens/sec=1376309.49, grad_norm=0.2812, duration=0.38s
Step 2970: loss=3.4385, lr=0.000589, tokens/sec=1378946.95, grad_norm=0.2739, duration=0.38s
Step 2971: loss=3.4035, lr=0.000589, tokens/sec=1377532.89, grad_norm=0.2649, duration=0.38s
Step 2972: loss=3.4587, lr=0.000589, tokens/sec=1374173.96, grad_norm=0.2729, duration=0.38s
Step 2973: loss=3.4458, lr=0.000589, tokens/sec=1374831.20, grad_norm=0.3119, duration=0.38s
Step 2974: loss=3.4664, lr=0.000589, tokens/sec=1378148.43, grad_norm=0.2789, duration=0.38s
Step 2975: loss=3.4989, lr=0.000589, tokens/sec=1377303.39, grad_norm=0.2621, duration=0.38s
Step 2976: loss=3.4690, lr=0.000589, tokens/sec=1370769.85, grad_norm=0.2955, duration=0.38s
Step 2977: loss=3.4211, lr=0.000589, tokens/sec=1378182.98, grad_norm=0.3022, duration=0.38s
Step 2978: loss=3.4477, lr=0.000589, tokens/sec=1374176.54, grad_norm=0.2814, duration=0.38s
Step 2979: loss=3.3590, lr=0.000589, tokens/sec=1378852.71, grad_norm=0.2804, duration=0.38s
Step 2980: loss=3.3964, lr=0.000589, tokens/sec=1379601.84, grad_norm=0.2768, duration=0.38s
Step 2981: loss=3.3459, lr=0.000589, tokens/sec=1374917.16, grad_norm=0.2818, duration=0.38s
Step 2982: loss=3.3725, lr=0.000589, tokens/sec=1378059.48, grad_norm=0.2653, duration=0.38s
Step 2983: loss=3.3530, lr=0.000589, tokens/sec=1379011.81, grad_norm=0.2959, duration=0.38s
Step 2984: loss=3.4110, lr=0.000589, tokens/sec=1377472.49, grad_norm=0.2771, duration=0.38s
Step 2985: loss=3.3813, lr=0.000589, tokens/sec=1375987.40, grad_norm=0.2775, duration=0.38s
Step 2986: loss=3.3689, lr=0.000589, tokens/sec=1374056.33, grad_norm=0.2486, duration=0.38s
Step 2987: loss=3.3581, lr=0.000589, tokens/sec=1370096.00, grad_norm=0.2598, duration=0.38s
Step 2988: loss=3.4100, lr=0.000589, tokens/sec=1376203.54, grad_norm=0.2633, duration=0.38s
Step 2989: loss=3.4216, lr=0.000589, tokens/sec=1375641.37, grad_norm=0.2628, duration=0.38s
Step 2990: loss=3.4555, lr=0.000589, tokens/sec=1373139.12, grad_norm=0.2649, duration=0.38s
Step 2991: loss=3.4804, lr=0.000589, tokens/sec=1375600.92, grad_norm=0.2745, duration=0.38s
Step 2992: loss=3.4988, lr=0.000589, tokens/sec=1375635.34, grad_norm=0.3175, duration=0.38s
Step 2993: loss=3.4769, lr=0.000589, tokens/sec=1374639.55, grad_norm=0.3373, duration=0.38s
Step 2994: loss=3.5017, lr=0.000589, tokens/sec=1375413.36, grad_norm=0.3005, duration=0.38s
Step 2995: loss=3.4638, lr=0.000589, tokens/sec=1375881.51, grad_norm=0.2674, duration=0.38s
Step 2996: loss=3.4766, lr=0.000589, tokens/sec=1374796.82, grad_norm=0.2992, duration=0.38s
Step 2997: loss=3.5142, lr=0.000589, tokens/sec=1377632.14, grad_norm=0.3040, duration=0.38s
Step 2998: loss=3.5771, lr=0.000589, tokens/sec=1375170.80, grad_norm=0.2818, duration=0.38s
Step 2999: loss=3.5172, lr=0.000589, tokens/sec=1375894.42, grad_norm=0.3293, duration=0.38s
Step 3000/19073 (15.7%), Elapsed time: 1202.22s, Steps per hour: 8983.36, Estimated hours remaining: 1.79
Validation loss at step 3000: 3.722931385040283
Step 3000: loss=3.4885, lr=0.000589, tokens/sec=156736.52, grad_norm=0.3241, duration=3.35s
Step 3001: loss=3.4229, lr=0.000589, tokens/sec=1378614.13, grad_norm=0.2859, duration=0.38s
Step 3002: loss=3.4671, lr=0.000589, tokens/sec=1379238.42, grad_norm=0.2699, duration=0.38s
Step 3003: loss=3.4406, lr=0.000589, tokens/sec=1376843.76, grad_norm=0.2771, duration=0.38s
Step 3004: loss=3.3989, lr=0.000589, tokens/sec=1378519.93, grad_norm=0.2874, duration=0.38s
Step 3005: loss=3.4747, lr=0.000589, tokens/sec=1374569.09, grad_norm=0.3051, duration=0.38s
Step 3006: loss=3.4224, lr=0.000589, tokens/sec=1378570.91, grad_norm=0.2717, duration=0.38s
Step 3007: loss=3.5070, lr=0.000589, tokens/sec=1376481.79, grad_norm=0.2691, duration=0.38s
Step 3008: loss=3.5139, lr=0.000589, tokens/sec=1379486.74, grad_norm=0.3095, duration=0.38s
Step 3009: loss=3.4408, lr=0.000589, tokens/sec=1376413.72, grad_norm=0.2815, duration=0.38s
Step 3010: loss=3.3843, lr=0.000589, tokens/sec=1372438.96, grad_norm=0.2619, duration=0.38s
Step 3011: loss=3.4532, lr=0.000589, tokens/sec=1378987.60, grad_norm=0.2873, duration=0.38s
Step 3012: loss=3.4720, lr=0.000589, tokens/sec=1377433.66, grad_norm=0.2834, duration=0.38s
Step 3013: loss=3.4121, lr=0.000589, tokens/sec=1379093.97, grad_norm=0.2870, duration=0.38s
Step 3014: loss=3.3257, lr=0.000589, tokens/sec=1378517.33, grad_norm=0.3032, duration=0.38s
Step 3015: loss=3.4169, lr=0.000589, tokens/sec=1376227.66, grad_norm=0.2808, duration=0.38s
Step 3016: loss=3.3964, lr=0.000589, tokens/sec=1377299.94, grad_norm=0.2764, duration=0.38s
Step 3017: loss=3.4308, lr=0.000589, tokens/sec=1377278.38, grad_norm=0.2694, duration=0.38s
Step 3018: loss=3.4653, lr=0.000589, tokens/sec=1377235.25, grad_norm=0.2670, duration=0.38s
Step 3019: loss=3.3561, lr=0.000589, tokens/sec=1371627.42, grad_norm=0.2771, duration=0.38s
Step 3020: loss=3.3907, lr=0.000589, tokens/sec=1376502.47, grad_norm=0.2690, duration=0.38s
Step 3021: loss=3.4312, lr=0.000589, tokens/sec=1373881.20, grad_norm=0.2838, duration=0.38s
Step 3022: loss=3.3862, lr=0.000589, tokens/sec=1373320.92, grad_norm=0.2954, duration=0.38s
Step 3023: loss=3.4277, lr=0.000589, tokens/sec=1376086.42, grad_norm=0.2706, duration=0.38s
Step 3024: loss=3.3611, lr=0.000589, tokens/sec=1373284.04, grad_norm=0.2573, duration=0.38s
Step 3025: loss=3.3699, lr=0.000588, tokens/sec=1377299.08, grad_norm=0.2685, duration=0.38s
Step 3026: loss=3.3560, lr=0.000588, tokens/sec=1376233.69, grad_norm=0.2751, duration=0.38s
Step 3027: loss=3.4510, lr=0.000588, tokens/sec=1371916.65, grad_norm=0.2794, duration=0.38s
Step 3028: loss=3.3397, lr=0.000588, tokens/sec=1373781.64, grad_norm=0.2602, duration=0.38s
Step 3029: loss=3.3645, lr=0.000588, tokens/sec=1373688.10, grad_norm=0.2560, duration=0.38s
Step 3030: loss=3.3179, lr=0.000588, tokens/sec=1369398.08, grad_norm=0.2660, duration=0.38s
Step 3031: loss=3.3821, lr=0.000588, tokens/sec=1374588.85, grad_norm=0.2724, duration=0.38s
Step 3032: loss=3.4218, lr=0.000588, tokens/sec=1372594.01, grad_norm=0.3073, duration=0.38s
Step 3033: loss=3.3853, lr=0.000588, tokens/sec=1371803.68, grad_norm=0.2942, duration=0.38s
Step 3034: loss=3.4066, lr=0.000588, tokens/sec=1372626.57, grad_norm=0.3063, duration=0.38s
Step 3035: loss=3.4788, lr=0.000588, tokens/sec=1374143.91, grad_norm=0.2731, duration=0.38s
Step 3036: loss=3.4255, lr=0.000588, tokens/sec=1377777.14, grad_norm=0.2679, duration=0.38s
Step 3037: loss=3.4937, lr=0.000588, tokens/sec=1377847.93, grad_norm=0.2990, duration=0.38s
Step 3038: loss=3.4365, lr=0.000588, tokens/sec=1374978.20, grad_norm=0.2842, duration=0.38s
Step 3039: loss=3.5039, lr=0.000588, tokens/sec=1371141.64, grad_norm=0.3229, duration=0.38s
Step 3040: loss=3.4480, lr=0.000588, tokens/sec=1372455.23, grad_norm=0.3001, duration=0.38s
Step 3041: loss=3.4871, lr=0.000588, tokens/sec=1369622.39, grad_norm=0.3019, duration=0.38s
Step 3042: loss=3.5034, lr=0.000588, tokens/sec=1371857.60, grad_norm=0.3575, duration=0.38s
Step 3043: loss=3.5613, lr=0.000588, tokens/sec=1377045.51, grad_norm=0.3509, duration=0.38s
Step 3044: loss=3.5005, lr=0.000588, tokens/sec=1372891.37, grad_norm=0.3057, duration=0.38s
Step 3045: loss=3.5082, lr=0.000588, tokens/sec=1373494.19, grad_norm=0.3160, duration=0.38s
Step 3046: loss=3.5745, lr=0.000588, tokens/sec=1373727.57, grad_norm=0.3253, duration=0.38s
Step 3047: loss=3.4736, lr=0.000588, tokens/sec=1371247.66, grad_norm=0.3280, duration=0.38s
Step 3048: loss=3.5003, lr=0.000588, tokens/sec=1377423.31, grad_norm=0.2723, duration=0.38s
Step 3049: loss=3.4548, lr=0.000588, tokens/sec=1375937.46, grad_norm=0.2782, duration=0.38s
Step 3050: loss=3.4843, lr=0.000588, tokens/sec=1373138.26, grad_norm=0.2668, duration=0.38s
Step 3051: loss=3.4685, lr=0.000588, tokens/sec=1373475.32, grad_norm=0.2528, duration=0.38s
Step 3052: loss=3.4518, lr=0.000588, tokens/sec=1375686.12, grad_norm=0.2339, duration=0.38s
Step 3053: loss=3.3832, lr=0.000588, tokens/sec=1377112.78, grad_norm=0.2546, duration=0.38s
Step 3054: loss=3.4509, lr=0.000588, tokens/sec=1374395.55, grad_norm=0.2603, duration=0.38s
Step 3055: loss=3.4660, lr=0.000588, tokens/sec=1377679.61, grad_norm=0.2727, duration=0.38s
Step 3056: loss=3.4500, lr=0.000588, tokens/sec=1374387.82, grad_norm=0.2618, duration=0.38s
Step 3057: loss=3.4724, lr=0.000588, tokens/sec=1371469.16, grad_norm=0.2808, duration=0.38s
Step 3058: loss=3.4180, lr=0.000588, tokens/sec=1373467.59, grad_norm=0.2926, duration=0.38s
Step 3059: loss=3.3336, lr=0.000588, tokens/sec=1376299.15, grad_norm=0.4668, duration=0.38s
Step 3060: loss=3.4197, lr=0.000588, tokens/sec=1378143.25, grad_norm=0.2710, duration=0.38s
Step 3061: loss=3.3992, lr=0.000588, tokens/sec=1379356.08, grad_norm=0.2455, duration=0.38s
Step 3062: loss=3.5273, lr=0.000588, tokens/sec=1374024.56, grad_norm=0.2654, duration=0.38s
Step 3063: loss=3.3633, lr=0.000588, tokens/sec=1370388.00, grad_norm=0.2825, duration=0.38s
Step 3064: loss=3.4376, lr=0.000588, tokens/sec=1375261.11, grad_norm=0.2685, duration=0.38s
Step 3065: loss=3.4586, lr=0.000588, tokens/sec=1372897.37, grad_norm=0.2901, duration=0.38s
Step 3066: loss=3.3923, lr=0.000588, tokens/sec=1376555.03, grad_norm=0.2750, duration=0.38s
Step 3067: loss=3.3648, lr=0.000588, tokens/sec=1376735.15, grad_norm=0.4473, duration=0.38s
Step 3068: loss=3.4330, lr=0.000588, tokens/sec=1375801.45, grad_norm=0.3183, duration=0.38s
Step 3069: loss=3.4167, lr=0.000588, tokens/sec=1373541.37, grad_norm=0.3485, duration=0.38s
Step 3070: loss=3.4602, lr=0.000588, tokens/sec=1374846.67, grad_norm=0.3403, duration=0.38s
Step 3071: loss=3.4629, lr=0.000588, tokens/sec=1373061.96, grad_norm=0.3199, duration=0.38s
Step 3072: loss=3.4895, lr=0.000588, tokens/sec=1376570.54, grad_norm=0.3219, duration=0.38s
Step 3073: loss=3.4944, lr=0.000588, tokens/sec=1375772.19, grad_norm=0.2935, duration=0.38s
Step 3074: loss=3.4255, lr=0.000588, tokens/sec=1375812.64, grad_norm=0.2896, duration=0.38s
Step 3075: loss=3.3972, lr=0.000588, tokens/sec=1374006.53, grad_norm=0.2878, duration=0.38s
Step 3076: loss=3.4522, lr=0.000588, tokens/sec=1376457.66, grad_norm=0.3128, duration=0.38s
Step 3077: loss=3.3705, lr=0.000588, tokens/sec=1376153.59, grad_norm=0.3004, duration=0.38s
Step 3078: loss=3.3723, lr=0.000588, tokens/sec=1371837.06, grad_norm=0.2392, duration=0.38s
Step 3079: loss=3.3832, lr=0.000588, tokens/sec=1378181.26, grad_norm=0.2585, duration=0.38s
Step 3080: loss=3.3977, lr=0.000588, tokens/sec=1377906.64, grad_norm=0.2581, duration=0.38s
Step 3081: loss=3.4087, lr=0.000588, tokens/sec=1374058.90, grad_norm=0.2759, duration=0.38s
Step 3082: loss=3.3734, lr=0.000588, tokens/sec=1373533.65, grad_norm=0.2619, duration=0.38s
Step 3083: loss=3.4016, lr=0.000588, tokens/sec=1374992.81, grad_norm=0.2659, duration=0.38s
Step 3084: loss=3.3738, lr=0.000588, tokens/sec=1375450.35, grad_norm=0.2752, duration=0.38s
Step 3085: loss=3.3690, lr=0.000588, tokens/sec=1372965.08, grad_norm=0.2688, duration=0.38s
Step 3086: loss=3.4054, lr=0.000588, tokens/sec=1374582.84, grad_norm=0.2626, duration=0.38s
Step 3087: loss=3.4773, lr=0.000588, tokens/sec=1374264.13, grad_norm=0.2861, duration=0.38s
Step 3088: loss=3.5269, lr=0.000588, tokens/sec=1373809.10, grad_norm=0.2815, duration=0.38s
Step 3089: loss=3.5066, lr=0.000588, tokens/sec=1374045.16, grad_norm=0.2641, duration=0.38s
Step 3090: loss=3.4639, lr=0.000588, tokens/sec=1375864.29, grad_norm=0.2827, duration=0.38s
Step 3091: loss=3.5071, lr=0.000588, tokens/sec=1373961.89, grad_norm=0.2746, duration=0.38s
Step 3092: loss=3.4893, lr=0.000588, tokens/sec=1375910.78, grad_norm=0.2710, duration=0.38s
Step 3093: loss=3.4574, lr=0.000588, tokens/sec=1374569.95, grad_norm=0.2609, duration=0.38s
Step 3094: loss=3.4297, lr=0.000588, tokens/sec=1376466.28, grad_norm=0.2675, duration=0.38s
Step 3095: loss=3.4350, lr=0.000588, tokens/sec=1375191.44, grad_norm=0.2820, duration=0.38s
Step 3096: loss=3.4594, lr=0.000588, tokens/sec=1372803.95, grad_norm=0.2991, duration=0.38s
Step 3097: loss=3.4542, lr=0.000588, tokens/sec=1374874.18, grad_norm=0.2509, duration=0.38s
Step 3098: loss=3.4272, lr=0.000588, tokens/sec=1371470.02, grad_norm=0.2823, duration=0.38s
Step 3099: loss=3.4182, lr=0.000588, tokens/sec=1371902.96, grad_norm=0.2767, duration=0.38s
Step 3100/19073 (16.3%), Elapsed time: 1243.41s, Steps per hour: 8975.31, Estimated hours remaining: 1.78
Step 3100: loss=3.4337, lr=0.000588, tokens/sec=1375464.98, grad_norm=0.2697, duration=0.38s
Step 3101: loss=3.3939, lr=0.000588, tokens/sec=1377304.25, grad_norm=0.2731, duration=0.38s
Step 3102: loss=3.3870, lr=0.000588, tokens/sec=1372507.48, grad_norm=0.2575, duration=0.38s
Step 3103: loss=3.4306, lr=0.000588, tokens/sec=1374601.74, grad_norm=0.2737, duration=0.38s
Step 3104: loss=3.4231, lr=0.000588, tokens/sec=1375728.29, grad_norm=0.2659, duration=0.38s
Step 3105: loss=3.4836, lr=0.000588, tokens/sec=1374060.62, grad_norm=0.2554, duration=0.38s
Step 3106: loss=3.4091, lr=0.000588, tokens/sec=1378442.15, grad_norm=0.2624, duration=0.38s
Step 3107: loss=3.4665, lr=0.000588, tokens/sec=1370454.62, grad_norm=0.2460, duration=0.38s
Step 3108: loss=3.4371, lr=0.000588, tokens/sec=1372079.30, grad_norm=0.2771, duration=0.38s
Step 3109: loss=3.4095, lr=0.000588, tokens/sec=1376008.93, grad_norm=0.3520, duration=0.38s
Step 3110: loss=3.4099, lr=0.000588, tokens/sec=1375372.07, grad_norm=0.3373, duration=0.38s
Step 3111: loss=3.4233, lr=0.000588, tokens/sec=1375869.45, grad_norm=0.3041, duration=0.38s
Step 3112: loss=3.3991, lr=0.000588, tokens/sec=1379672.82, grad_norm=0.2679, duration=0.38s
Step 3113: loss=3.3617, lr=0.000588, tokens/sec=1375184.56, grad_norm=0.3074, duration=0.38s
Step 3114: loss=3.4096, lr=0.000587, tokens/sec=1374106.98, grad_norm=0.3277, duration=0.38s
Step 3115: loss=3.4133, lr=0.000587, tokens/sec=1376449.91, grad_norm=0.2936, duration=0.38s
Step 3116: loss=3.4483, lr=0.000587, tokens/sec=1371082.65, grad_norm=0.2653, duration=0.38s
Step 3117: loss=3.4165, lr=0.000587, tokens/sec=1376372.37, grad_norm=0.3119, duration=0.38s
Step 3118: loss=3.4491, lr=0.000587, tokens/sec=1375941.77, grad_norm=0.2673, duration=0.38s
Step 3119: loss=3.4291, lr=0.000587, tokens/sec=1375962.43, grad_norm=0.3067, duration=0.38s
Step 3120: loss=3.4433, lr=0.000587, tokens/sec=1376307.76, grad_norm=0.3164, duration=0.38s
Step 3121: loss=3.3525, lr=0.000587, tokens/sec=1370895.46, grad_norm=0.2994, duration=0.38s
Step 3122: loss=3.3915, lr=0.000587, tokens/sec=1377694.28, grad_norm=0.2785, duration=0.38s
Step 3123: loss=3.3755, lr=0.000587, tokens/sec=1376291.40, grad_norm=0.2922, duration=0.38s
Step 3124: loss=3.3871, lr=0.000587, tokens/sec=1376691.19, grad_norm=0.2946, duration=0.38s
Step 3125: loss=3.3209, lr=0.000587, tokens/sec=1374693.69, grad_norm=0.2789, duration=0.38s
Step 3126: loss=3.3956, lr=0.000587, tokens/sec=1376104.50, grad_norm=0.2646, duration=0.38s
Step 3127: loss=3.3805, lr=0.000587, tokens/sec=1377406.92, grad_norm=0.2650, duration=0.38s
Step 3128: loss=3.3524, lr=0.000587, tokens/sec=1377451.78, grad_norm=0.2863, duration=0.38s
Step 3129: loss=3.3670, lr=0.000587, tokens/sec=1378790.46, grad_norm=0.2639, duration=0.38s
Step 3130: loss=3.3406, lr=0.000587, tokens/sec=1375536.39, grad_norm=0.2757, duration=0.38s
Step 3131: loss=3.4095, lr=0.000587, tokens/sec=1379215.93, grad_norm=0.2726, duration=0.38s
Step 3132: loss=3.3413, lr=0.000587, tokens/sec=1376786.00, grad_norm=0.2954, duration=0.38s
Step 3133: loss=3.4061, lr=0.000587, tokens/sec=1377261.12, grad_norm=0.2885, duration=0.38s
Step 3134: loss=3.4387, lr=0.000587, tokens/sec=1376856.69, grad_norm=0.2585, duration=0.38s
Step 3135: loss=3.4704, lr=0.000587, tokens/sec=1376351.70, grad_norm=0.2876, duration=0.38s
Step 3136: loss=3.4616, lr=0.000587, tokens/sec=1376400.80, grad_norm=0.2930, duration=0.38s
Step 3137: loss=3.4433, lr=0.000587, tokens/sec=1376562.78, grad_norm=0.2977, duration=0.38s
Step 3138: loss=3.4064, lr=0.000587, tokens/sec=1376283.65, grad_norm=0.3179, duration=0.38s
Step 3139: loss=3.4468, lr=0.000587, tokens/sec=1374026.28, grad_norm=0.3292, duration=0.38s
Step 3140: loss=3.4181, lr=0.000587, tokens/sec=1377064.48, grad_norm=0.3041, duration=0.38s
Step 3141: loss=3.4826, lr=0.000587, tokens/sec=1378442.15, grad_norm=0.2940, duration=0.38s
Step 3142: loss=3.4700, lr=0.000587, tokens/sec=1375506.27, grad_norm=0.3022, duration=0.38s
Step 3143: loss=3.4526, lr=0.000587, tokens/sec=1378057.75, grad_norm=0.3338, duration=0.38s
Step 3144: loss=3.4880, lr=0.000587, tokens/sec=1375919.39, grad_norm=0.2692, duration=0.38s
Step 3145: loss=3.4065, lr=0.000587, tokens/sec=1377188.67, grad_norm=0.2700, duration=0.38s
Step 3146: loss=3.4293, lr=0.000587, tokens/sec=1375195.74, grad_norm=0.2605, duration=0.38s
Step 3147: loss=3.4107, lr=0.000587, tokens/sec=1374277.02, grad_norm=0.2625, duration=0.38s
Step 3148: loss=3.4390, lr=0.000587, tokens/sec=1372183.75, grad_norm=0.2702, duration=0.38s
Step 3149: loss=3.3599, lr=0.000587, tokens/sec=1379876.27, grad_norm=0.2537, duration=0.38s
Step 3150: loss=3.4250, lr=0.000587, tokens/sec=1376456.80, grad_norm=0.2573, duration=0.38s
Step 3151: loss=3.4260, lr=0.000587, tokens/sec=1373171.70, grad_norm=0.2602, duration=0.38s
Step 3152: loss=3.3955, lr=0.000587, tokens/sec=1373425.56, grad_norm=0.2528, duration=0.38s
Step 3153: loss=3.3386, lr=0.000587, tokens/sec=1377543.25, grad_norm=0.2360, duration=0.38s
Step 3154: loss=3.4338, lr=0.000587, tokens/sec=1376711.01, grad_norm=0.2686, duration=0.38s
Step 3155: loss=3.3804, lr=0.000587, tokens/sec=1376360.31, grad_norm=0.2610, duration=0.38s
Step 3156: loss=3.4953, lr=0.000587, tokens/sec=1376344.80, grad_norm=0.2869, duration=0.38s
Step 3157: loss=3.4379, lr=0.000587, tokens/sec=1376259.53, grad_norm=0.3243, duration=0.38s
Step 3158: loss=3.4156, lr=0.000587, tokens/sec=1378348.84, grad_norm=0.2707, duration=0.38s
Step 3159: loss=3.4022, lr=0.000587, tokens/sec=1373091.11, grad_norm=0.2928, duration=0.38s
Step 3160: loss=3.3788, lr=0.000587, tokens/sec=1377106.74, grad_norm=0.2746, duration=0.38s
Step 3161: loss=3.4111, lr=0.000587, tokens/sec=1377301.67, grad_norm=0.2863, duration=0.38s
Step 3162: loss=3.4406, lr=0.000587, tokens/sec=1375584.57, grad_norm=0.3168, duration=0.38s
Step 3163: loss=3.4222, lr=0.000587, tokens/sec=1377670.97, grad_norm=0.3512, duration=0.38s
Step 3164: loss=3.4637, lr=0.000587, tokens/sec=1379326.66, grad_norm=0.3006, duration=0.38s
Step 3165: loss=3.4527, lr=0.000587, tokens/sec=1376140.67, grad_norm=0.2907, duration=0.38s
Step 3166: loss=3.4047, lr=0.000587, tokens/sec=1377427.62, grad_norm=0.3094, duration=0.38s
Step 3167: loss=3.4296, lr=0.000587, tokens/sec=1380090.17, grad_norm=0.3157, duration=0.38s
Step 3168: loss=3.3849, lr=0.000587, tokens/sec=1374374.93, grad_norm=0.3071, duration=0.38s
Step 3169: loss=3.3235, lr=0.000587, tokens/sec=1377667.52, grad_norm=0.2866, duration=0.38s
Step 3170: loss=3.3916, lr=0.000587, tokens/sec=1378534.62, grad_norm=0.2567, duration=0.38s
Step 3171: loss=3.3016, lr=0.000587, tokens/sec=1375766.16, grad_norm=0.3146, duration=0.38s
Step 3172: loss=3.3338, lr=0.000587, tokens/sec=1376421.48, grad_norm=0.2596, duration=0.38s
Step 3173: loss=3.3153, lr=0.000587, tokens/sec=1378617.58, grad_norm=0.2877, duration=0.38s
Step 3174: loss=3.3762, lr=0.000587, tokens/sec=1377371.54, grad_norm=0.2730, duration=0.38s
Step 3175: loss=3.3666, lr=0.000587, tokens/sec=1374858.71, grad_norm=0.3183, duration=0.38s
Step 3176: loss=3.3413, lr=0.000587, tokens/sec=1374883.63, grad_norm=0.3000, duration=0.38s
Step 3177: loss=3.3491, lr=0.000587, tokens/sec=1377303.39, grad_norm=0.2583, duration=0.38s
Step 3178: loss=3.3481, lr=0.000587, tokens/sec=1376176.84, grad_norm=0.2488, duration=0.38s
Step 3179: loss=3.3827, lr=0.000587, tokens/sec=1376641.20, grad_norm=0.2859, duration=0.38s
Step 3180: loss=3.4392, lr=0.000587, tokens/sec=1379315.41, grad_norm=0.2970, duration=0.38s
Step 3181: loss=3.4465, lr=0.000587, tokens/sec=1378692.78, grad_norm=0.2643, duration=0.38s
Step 3182: loss=3.4728, lr=0.000587, tokens/sec=1376353.42, grad_norm=0.3069, duration=0.38s
Step 3183: loss=3.4410, lr=0.000587, tokens/sec=1375182.84, grad_norm=0.3007, duration=0.38s
Step 3184: loss=3.4871, lr=0.000587, tokens/sec=1373352.65, grad_norm=0.2866, duration=0.38s
Step 3185: loss=3.4220, lr=0.000587, tokens/sec=1376027.01, grad_norm=0.3541, duration=0.38s
Step 3186: loss=3.4365, lr=0.000587, tokens/sec=1378710.93, grad_norm=0.3332, duration=0.38s
Step 3187: loss=3.4820, lr=0.000587, tokens/sec=1375366.04, grad_norm=0.2891, duration=0.38s
Step 3188: loss=3.5545, lr=0.000587, tokens/sec=1376583.46, grad_norm=0.3345, duration=0.38s
Step 3189: loss=3.4678, lr=0.000587, tokens/sec=1378598.57, grad_norm=0.3352, duration=0.38s
Step 3190: loss=3.4558, lr=0.000587, tokens/sec=1376877.38, grad_norm=0.3014, duration=0.38s
Step 3191: loss=3.4005, lr=0.000587, tokens/sec=1377907.50, grad_norm=0.2976, duration=0.38s
Step 3192: loss=3.4346, lr=0.000587, tokens/sec=1374339.71, grad_norm=0.2721, duration=0.38s
Step 3193: loss=3.3974, lr=0.000587, tokens/sec=1373633.18, grad_norm=0.2889, duration=0.38s
Step 3194: loss=3.3597, lr=0.000587, tokens/sec=1376586.91, grad_norm=0.2675, duration=0.38s
Step 3195: loss=3.4725, lr=0.000587, tokens/sec=1374537.30, grad_norm=0.2690, duration=0.38s
Step 3196: loss=3.3902, lr=0.000587, tokens/sec=1375660.30, grad_norm=0.2582, duration=0.38s
Step 3197: loss=3.4475, lr=0.000587, tokens/sec=1378312.56, grad_norm=0.2700, duration=0.38s
Step 3198: loss=3.4709, lr=0.000587, tokens/sec=1374048.60, grad_norm=0.2804, duration=0.38s
Step 3199: loss=3.4194, lr=0.000587, tokens/sec=1379393.28, grad_norm=0.2640, duration=0.38s
Step 3200/19073 (16.8%), Elapsed time: 1281.59s, Steps per hour: 8988.84, Estimated hours remaining: 1.77
Step 3200: loss=3.3435, lr=0.000586, tokens/sec=1373640.90, grad_norm=0.2477, duration=0.38s
Step 3201: loss=3.4349, lr=0.000586, tokens/sec=1377758.15, grad_norm=0.2558, duration=0.38s
Step 3202: loss=3.4199, lr=0.000586, tokens/sec=1378341.07, grad_norm=0.2407, duration=0.38s
Step 3203: loss=3.3632, lr=0.000586, tokens/sec=1372528.90, grad_norm=0.2436, duration=0.38s
Step 3204: loss=3.3135, lr=0.000586, tokens/sec=1374610.33, grad_norm=0.2741, duration=0.38s
Step 3205: loss=3.3777, lr=0.000586, tokens/sec=1373640.04, grad_norm=0.2664, duration=0.38s
Step 3206: loss=3.3772, lr=0.000586, tokens/sec=1377670.11, grad_norm=0.2946, duration=0.38s
Step 3207: loss=3.4035, lr=0.000586, tokens/sec=1375984.82, grad_norm=0.3062, duration=0.38s
Step 3208: loss=3.4103, lr=0.000586, tokens/sec=1375118.35, grad_norm=0.2953, duration=0.38s
Step 3209: loss=3.3490, lr=0.000586, tokens/sec=1376340.50, grad_norm=0.3061, duration=0.38s
Step 3210: loss=3.4004, lr=0.000586, tokens/sec=1376032.17, grad_norm=0.2769, duration=0.38s
Step 3211: loss=3.4085, lr=0.000586, tokens/sec=1375473.58, grad_norm=0.3125, duration=0.38s
Step 3212: loss=3.3464, lr=0.000586, tokens/sec=1375557.04, grad_norm=0.2747, duration=0.38s
Step 3213: loss=3.3716, lr=0.000586, tokens/sec=1376784.28, grad_norm=0.2704, duration=0.38s
Step 3214: loss=3.3516, lr=0.000586, tokens/sec=1377771.10, grad_norm=0.2641, duration=0.38s
Step 3215: loss=3.3274, lr=0.000586, tokens/sec=1376563.64, grad_norm=0.2700, duration=0.38s
Step 3216: loss=3.3462, lr=0.000586, tokens/sec=1373548.24, grad_norm=0.2825, duration=0.38s
Step 3217: loss=3.3720, lr=0.000586, tokens/sec=1374417.02, grad_norm=0.2748, duration=0.38s
Step 3218: loss=3.3513, lr=0.000586, tokens/sec=1375798.01, grad_norm=0.2703, duration=0.38s
Step 3219: loss=3.3197, lr=0.000586, tokens/sec=1379184.79, grad_norm=0.2395, duration=0.38s
Step 3220: loss=3.3115, lr=0.000586, tokens/sec=1374342.29, grad_norm=0.2601, duration=0.38s
Step 3221: loss=3.3592, lr=0.000586, tokens/sec=1380228.77, grad_norm=0.2515, duration=0.38s
Step 3222: loss=3.3672, lr=0.000586, tokens/sec=1374954.98, grad_norm=0.2584, duration=0.38s
Step 3223: loss=3.3506, lr=0.000586, tokens/sec=1377231.80, grad_norm=0.2783, duration=0.38s
Step 3224: loss=3.3999, lr=0.000586, tokens/sec=1376500.74, grad_norm=0.2716, duration=0.38s
Step 3225: loss=3.3896, lr=0.000586, tokens/sec=1379630.41, grad_norm=0.2861, duration=0.38s
Step 3226: loss=3.4442, lr=0.000586, tokens/sec=1374733.22, grad_norm=0.2649, duration=0.38s
Step 3227: loss=3.4415, lr=0.000586, tokens/sec=1376332.74, grad_norm=0.2771, duration=0.38s
Step 3228: loss=3.4344, lr=0.000586, tokens/sec=1378088.84, grad_norm=0.3113, duration=0.38s
Step 3229: loss=3.4319, lr=0.000586, tokens/sec=1374230.64, grad_norm=0.3253, duration=0.38s
Step 3230: loss=3.4170, lr=0.000586, tokens/sec=1379285.13, grad_norm=0.2801, duration=0.38s
Step 3231: loss=3.4592, lr=0.000586, tokens/sec=1376132.06, grad_norm=0.3656, duration=0.38s
Step 3232: loss=3.4671, lr=0.000586, tokens/sec=1376524.01, grad_norm=0.3131, duration=0.38s
Step 3233: loss=3.5336, lr=0.000586, tokens/sec=1372718.25, grad_norm=0.3168, duration=0.38s
Step 3234: loss=3.4468, lr=0.000586, tokens/sec=1375102.01, grad_norm=0.3075, duration=0.38s
Step 3235: loss=3.4871, lr=0.000586, tokens/sec=1379942.94, grad_norm=0.3295, duration=0.38s
Step 3236: loss=3.5192, lr=0.000586, tokens/sec=1377709.82, grad_norm=0.3575, duration=0.38s
Step 3237: loss=3.4448, lr=0.000586, tokens/sec=1378629.68, grad_norm=0.2985, duration=0.38s
Step 3238: loss=3.4566, lr=0.000586, tokens/sec=1373308.05, grad_norm=0.2897, duration=0.38s
Step 3239: loss=3.4342, lr=0.000586, tokens/sec=1372983.09, grad_norm=0.2731, duration=0.38s
Step 3240: loss=3.4858, lr=0.000586, tokens/sec=1376898.07, grad_norm=0.2706, duration=0.38s
Step 3241: loss=3.4009, lr=0.000586, tokens/sec=1379177.00, grad_norm=0.2620, duration=0.38s
Step 3242: loss=3.4188, lr=0.000586, tokens/sec=1378487.95, grad_norm=0.2716, duration=0.38s
Step 3243: loss=3.3623, lr=0.000586, tokens/sec=1377403.47, grad_norm=0.2876, duration=0.38s
Step 3244: loss=3.4346, lr=0.000586, tokens/sec=1371294.69, grad_norm=0.2712, duration=0.38s
Step 3245: loss=3.4568, lr=0.000586, tokens/sec=1378761.93, grad_norm=0.2641, duration=0.38s
Step 3246: loss=3.3867, lr=0.000586, tokens/sec=1376175.12, grad_norm=0.2827, duration=0.38s
Step 3247: loss=3.4554, lr=0.000586, tokens/sec=1377643.36, grad_norm=0.3212, duration=0.38s
Step 3248: loss=3.3556, lr=0.000586, tokens/sec=1373965.32, grad_norm=0.2834, duration=0.38s
Step 3249: loss=3.3258, lr=0.000586, tokens/sec=1376847.21, grad_norm=0.4420, duration=0.38s
Validation loss at step 3250: 3.71868634223938
Step 3250: loss=3.3590, lr=0.000586, tokens/sec=154841.12, grad_norm=0.3273, duration=3.39s
Step 3251: loss=3.4142, lr=0.000586, tokens/sec=1377262.85, grad_norm=0.3203, duration=0.38s
Step 3252: loss=3.4866, lr=0.000586, tokens/sec=1377871.24, grad_norm=0.3182, duration=0.38s
Step 3253: loss=3.3115, lr=0.000586, tokens/sec=1378712.66, grad_norm=0.2743, duration=0.38s
Step 3254: loss=3.4541, lr=0.000586, tokens/sec=1376884.28, grad_norm=0.3116, duration=0.38s
Step 3255: loss=3.4073, lr=0.000586, tokens/sec=1379293.79, grad_norm=0.2802, duration=0.38s
Step 3256: loss=3.3142, lr=0.000586, tokens/sec=1378138.07, grad_norm=0.3430, duration=0.38s
Step 3257: loss=3.3725, lr=0.000586, tokens/sec=1379720.43, grad_norm=0.2861, duration=0.38s
Step 3258: loss=3.4049, lr=0.000586, tokens/sec=1374104.41, grad_norm=0.2761, duration=0.38s
Step 3259: loss=3.4187, lr=0.000586, tokens/sec=1375404.76, grad_norm=0.2716, duration=0.38s
Step 3260: loss=3.4025, lr=0.000586, tokens/sec=1373853.73, grad_norm=0.2639, duration=0.38s
Step 3261: loss=3.4254, lr=0.000586, tokens/sec=1376039.92, grad_norm=0.2560, duration=0.38s
Step 3262: loss=3.4240, lr=0.000586, tokens/sec=1378202.85, grad_norm=0.3160, duration=0.38s
Step 3263: loss=3.4675, lr=0.000586, tokens/sec=1365033.93, grad_norm=0.2732, duration=0.38s
Step 3264: loss=3.4227, lr=0.000586, tokens/sec=1376138.95, grad_norm=0.2942, duration=0.38s
Step 3265: loss=3.3506, lr=0.000586, tokens/sec=1377094.67, grad_norm=0.2841, duration=0.38s
Step 3266: loss=3.3963, lr=0.000586, tokens/sec=1371655.65, grad_norm=0.2910, duration=0.38s
Step 3267: loss=3.3486, lr=0.000586, tokens/sec=1374383.52, grad_norm=0.3015, duration=0.38s
Step 3268: loss=3.3238, lr=0.000586, tokens/sec=1376824.79, grad_norm=0.2608, duration=0.38s
Step 3269: loss=3.3564, lr=0.000586, tokens/sec=1375687.84, grad_norm=0.2605, duration=0.38s
Step 3270: loss=3.3661, lr=0.000586, tokens/sec=1373358.66, grad_norm=0.2305, duration=0.38s
Step 3271: loss=3.3505, lr=0.000586, tokens/sec=1378236.54, grad_norm=0.2752, duration=0.38s
Step 3272: loss=3.3507, lr=0.000586, tokens/sec=1372836.51, grad_norm=0.2874, duration=0.38s
Step 3273: loss=3.3658, lr=0.000586, tokens/sec=1376970.49, grad_norm=0.2689, duration=0.38s
Step 3274: loss=3.3471, lr=0.000586, tokens/sec=1374169.67, grad_norm=0.2908, duration=0.38s
Step 3275: loss=3.3620, lr=0.000586, tokens/sec=1375203.48, grad_norm=0.2826, duration=0.38s
Step 3276: loss=3.3480, lr=0.000586, tokens/sec=1375970.18, grad_norm=0.2631, duration=0.38s
Step 3277: loss=3.4535, lr=0.000586, tokens/sec=1375014.31, grad_norm=0.2691, duration=0.38s
Step 3278: loss=3.5211, lr=0.000586, tokens/sec=1374733.22, grad_norm=0.2817, duration=0.38s
Step 3279: loss=3.4294, lr=0.000586, tokens/sec=1371855.03, grad_norm=0.2898, duration=0.38s
Step 3280: loss=3.4559, lr=0.000586, tokens/sec=1374148.20, grad_norm=0.2701, duration=0.38s
Step 3281: loss=3.4615, lr=0.000586, tokens/sec=1377140.37, grad_norm=0.2797, duration=0.38s
Step 3282: loss=3.4381, lr=0.000586, tokens/sec=1377412.09, grad_norm=0.2820, duration=0.38s
Step 3283: loss=3.4388, lr=0.000585, tokens/sec=1378792.19, grad_norm=0.2931, duration=0.38s
Step 3284: loss=3.3936, lr=0.000585, tokens/sec=1370683.55, grad_norm=0.2827, duration=0.38s
Step 3285: loss=3.4083, lr=0.000585, tokens/sec=1373675.22, grad_norm=0.2646, duration=0.38s
Step 3286: loss=3.4340, lr=0.000585, tokens/sec=1376449.05, grad_norm=0.2573, duration=0.38s
Step 3287: loss=3.4092, lr=0.000585, tokens/sec=1377652.85, grad_norm=0.2636, duration=0.38s
Step 3288: loss=3.3884, lr=0.000585, tokens/sec=1375800.59, grad_norm=0.2653, duration=0.38s
Step 3289: loss=3.3918, lr=0.000585, tokens/sec=1374865.58, grad_norm=0.2829, duration=0.38s
Step 3290: loss=3.4284, lr=0.000585, tokens/sec=1370925.38, grad_norm=0.2657, duration=0.38s
Step 3291: loss=3.3170, lr=0.000585, tokens/sec=1375779.93, grad_norm=0.2940, duration=0.38s
Step 3292: loss=3.3596, lr=0.000585, tokens/sec=1375038.38, grad_norm=0.3110, duration=0.38s
Step 3293: loss=3.4041, lr=0.000585, tokens/sec=1375544.99, grad_norm=0.2878, duration=0.38s
Step 3294: loss=3.4019, lr=0.000585, tokens/sec=1375612.97, grad_norm=0.2684, duration=0.38s
Step 3295: loss=3.4519, lr=0.000585, tokens/sec=1374647.28, grad_norm=0.2819, duration=0.38s
Step 3296: loss=3.3459, lr=0.000585, tokens/sec=1377537.21, grad_norm=0.2533, duration=0.38s
Step 3297: loss=3.4572, lr=0.000585, tokens/sec=1377232.66, grad_norm=0.2731, duration=0.38s
Step 3298: loss=3.4018, lr=0.000585, tokens/sec=1376177.71, grad_norm=0.2839, duration=0.38s
Step 3299: loss=3.3705, lr=0.000585, tokens/sec=1377653.71, grad_norm=0.2816, duration=0.38s
Step 3300/19073 (17.3%), Elapsed time: 1322.78s, Steps per hour: 8981.06, Estimated hours remaining: 1.76
Step 3300: loss=3.3935, lr=0.000585, tokens/sec=1378671.17, grad_norm=0.3292, duration=0.38s
Step 3301: loss=3.3993, lr=0.000585, tokens/sec=1377518.22, grad_norm=0.3551, duration=0.38s
Step 3302: loss=3.3140, lr=0.000585, tokens/sec=1370918.54, grad_norm=0.2932, duration=0.38s
Step 3303: loss=3.3867, lr=0.000585, tokens/sec=1377219.72, grad_norm=0.3438, duration=0.38s
Step 3304: loss=3.3714, lr=0.000585, tokens/sec=1376346.53, grad_norm=0.2893, duration=0.38s
Step 3305: loss=3.4314, lr=0.000585, tokens/sec=1375867.73, grad_norm=0.2983, duration=0.38s
Step 3306: loss=3.4202, lr=0.000585, tokens/sec=1374155.07, grad_norm=0.2944, duration=0.38s
Step 3307: loss=3.3849, lr=0.000585, tokens/sec=1376391.32, grad_norm=0.2835, duration=0.38s
Step 3308: loss=3.4100, lr=0.000585, tokens/sec=1376872.21, grad_norm=0.2892, duration=0.38s
Step 3309: loss=3.3982, lr=0.000585, tokens/sec=1373720.70, grad_norm=0.2876, duration=0.38s
Step 3310: loss=3.4005, lr=0.000585, tokens/sec=1376024.42, grad_norm=0.2832, duration=0.38s
Step 3311: loss=3.2954, lr=0.000585, tokens/sec=1376251.78, grad_norm=0.2402, duration=0.38s
Step 3312: loss=3.4072, lr=0.000585, tokens/sec=1374487.47, grad_norm=0.2928, duration=0.38s
Step 3313: loss=3.3506, lr=0.000585, tokens/sec=1377331.00, grad_norm=0.3272, duration=0.38s
Step 3314: loss=3.3451, lr=0.000585, tokens/sec=1377002.40, grad_norm=0.2793, duration=0.38s
Step 3315: loss=3.3241, lr=0.000585, tokens/sec=1375736.04, grad_norm=0.2778, duration=0.38s
Step 3316: loss=3.3263, lr=0.000585, tokens/sec=1375794.56, grad_norm=0.2702, duration=0.38s
Step 3317: loss=3.3683, lr=0.000585, tokens/sec=1379485.87, grad_norm=0.2704, duration=0.38s
Step 3318: loss=3.3110, lr=0.000585, tokens/sec=1375506.27, grad_norm=0.2828, duration=0.38s
Step 3319: loss=3.3303, lr=0.000585, tokens/sec=1378188.17, grad_norm=0.2594, duration=0.38s
Step 3320: loss=3.3253, lr=0.000585, tokens/sec=1375138.99, grad_norm=0.2644, duration=0.38s
Step 3321: loss=3.3256, lr=0.000585, tokens/sec=1373281.47, grad_norm=0.2824, duration=0.38s
Step 3322: loss=3.3377, lr=0.000585, tokens/sec=1374379.23, grad_norm=0.3053, duration=0.38s
Step 3323: loss=3.3634, lr=0.000585, tokens/sec=1374060.62, grad_norm=0.3549, duration=0.38s
Step 3324: loss=3.4349, lr=0.000585, tokens/sec=1372998.52, grad_norm=0.3216, duration=0.38s
Step 3325: loss=3.4353, lr=0.000585, tokens/sec=1373264.32, grad_norm=0.3144, duration=0.38s
Step 3326: loss=3.4162, lr=0.000585, tokens/sec=1375298.95, grad_norm=0.3098, duration=0.38s
Step 3327: loss=3.4266, lr=0.000585, tokens/sec=1374808.85, grad_norm=0.3034, duration=0.38s
Step 3328: loss=3.3706, lr=0.000585, tokens/sec=1375607.81, grad_norm=0.2999, duration=0.38s
Step 3329: loss=3.4221, lr=0.000585, tokens/sec=1376047.67, grad_norm=0.3306, duration=0.38s
Step 3330: loss=3.3996, lr=0.000585, tokens/sec=1374617.21, grad_norm=0.3068, duration=0.38s
Step 3331: loss=3.4534, lr=0.000585, tokens/sec=1371204.91, grad_norm=0.2865, duration=0.38s
Step 3332: loss=3.4310, lr=0.000585, tokens/sec=1374773.61, grad_norm=0.2933, duration=0.38s
Step 3333: loss=3.4453, lr=0.000585, tokens/sec=1373561.11, grad_norm=0.2812, duration=0.38s
Step 3334: loss=3.4148, lr=0.000585, tokens/sec=1374350.02, grad_norm=0.3167, duration=0.38s
Step 3335: loss=3.3871, lr=0.000585, tokens/sec=1373535.37, grad_norm=0.2921, duration=0.38s
Step 3336: loss=3.4005, lr=0.000585, tokens/sec=1374679.94, grad_norm=0.2720, duration=0.38s
Step 3337: loss=3.3827, lr=0.000585, tokens/sec=1369777.66, grad_norm=0.2743, duration=0.38s
Step 3338: loss=3.3871, lr=0.000585, tokens/sec=1374596.58, grad_norm=0.2563, duration=0.38s
Step 3339: loss=3.3485, lr=0.000585, tokens/sec=1371676.19, grad_norm=0.2803, duration=0.38s
Step 3340: loss=3.4077, lr=0.000585, tokens/sec=1374659.31, grad_norm=0.2564, duration=0.38s
Step 3341: loss=3.3601, lr=0.000585, tokens/sec=1380345.73, grad_norm=0.2438, duration=0.38s
Step 3342: loss=3.3580, lr=0.000585, tokens/sec=1373049.95, grad_norm=0.2424, duration=0.38s
Step 3343: loss=3.3286, lr=0.000585, tokens/sec=1374348.30, grad_norm=0.2626, duration=0.38s
Step 3344: loss=3.4010, lr=0.000585, tokens/sec=1376658.44, grad_norm=0.2844, duration=0.38s
Step 3345: loss=3.4235, lr=0.000585, tokens/sec=1375354.86, grad_norm=0.2503, duration=0.38s
Step 3346: loss=3.4405, lr=0.000585, tokens/sec=1374896.53, grad_norm=0.2696, duration=0.38s
Step 3347: loss=3.3679, lr=0.000585, tokens/sec=1374488.33, grad_norm=0.2576, duration=0.38s
Step 3348: loss=3.3949, lr=0.000585, tokens/sec=1373277.18, grad_norm=0.2802, duration=0.38s
Step 3349: loss=3.3435, lr=0.000585, tokens/sec=1371946.61, grad_norm=0.2890, duration=0.38s
Step 3350: loss=3.3897, lr=0.000585, tokens/sec=1374509.80, grad_norm=0.2885, duration=0.38s
Step 3351: loss=3.3945, lr=0.000585, tokens/sec=1373271.18, grad_norm=0.2874, duration=0.38s
Step 3352: loss=3.4133, lr=0.000585, tokens/sec=1373034.52, grad_norm=0.2709, duration=0.38s
Step 3353: loss=3.4170, lr=0.000585, tokens/sec=1377180.05, grad_norm=0.2870, duration=0.38s
Step 3354: loss=3.4169, lr=0.000585, tokens/sec=1374670.48, grad_norm=0.2797, duration=0.38s
Step 3355: loss=3.3886, lr=0.000585, tokens/sec=1376439.57, grad_norm=0.2922, duration=0.38s
Step 3356: loss=3.4076, lr=0.000585, tokens/sec=1373460.73, grad_norm=0.2661, duration=0.38s
Step 3357: loss=3.3634, lr=0.000585, tokens/sec=1373206.00, grad_norm=0.2517, duration=0.38s
Step 3358: loss=3.3491, lr=0.000585, tokens/sec=1373139.12, grad_norm=0.2868, duration=0.38s
Step 3359: loss=3.3232, lr=0.000585, tokens/sec=1377765.92, grad_norm=0.3135, duration=0.38s
Step 3360: loss=3.3506, lr=0.000585, tokens/sec=1371890.98, grad_norm=0.2892, duration=0.38s
Step 3361: loss=3.2613, lr=0.000585, tokens/sec=1370558.82, grad_norm=0.2620, duration=0.38s
Step 3362: loss=3.3020, lr=0.000585, tokens/sec=1376138.95, grad_norm=0.3184, duration=0.38s
Step 3363: loss=3.2805, lr=0.000584, tokens/sec=1374701.42, grad_norm=0.3012, duration=0.38s
Step 3364: loss=3.3664, lr=0.000584, tokens/sec=1376661.89, grad_norm=0.2851, duration=0.38s
Step 3365: loss=3.3345, lr=0.000584, tokens/sec=1376868.76, grad_norm=0.2518, duration=0.38s
Step 3366: loss=3.3333, lr=0.000584, tokens/sec=1375192.30, grad_norm=0.2797, duration=0.38s
Step 3367: loss=3.2880, lr=0.000584, tokens/sec=1377017.06, grad_norm=0.2355, duration=0.38s
Step 3368: loss=3.3144, lr=0.000584, tokens/sec=1380636.92, grad_norm=0.2820, duration=0.38s
Step 3369: loss=3.3675, lr=0.000584, tokens/sec=1373268.60, grad_norm=0.2561, duration=0.38s
Step 3370: loss=3.4080, lr=0.000584, tokens/sec=1377315.47, grad_norm=0.2787, duration=0.38s
Step 3371: loss=3.4269, lr=0.000584, tokens/sec=1375820.39, grad_norm=0.2783, duration=0.38s
Step 3372: loss=3.4444, lr=0.000584, tokens/sec=1377143.82, grad_norm=0.3411, duration=0.38s
Step 3373: loss=3.4318, lr=0.000584, tokens/sec=1375723.13, grad_norm=0.3112, duration=0.38s
Step 3374: loss=3.4393, lr=0.000584, tokens/sec=1374498.64, grad_norm=0.2800, duration=0.38s
Step 3375: loss=3.3804, lr=0.000584, tokens/sec=1374597.44, grad_norm=0.3077, duration=0.38s
Step 3376: loss=3.4052, lr=0.000584, tokens/sec=1370636.56, grad_norm=0.2880, duration=0.38s
Step 3377: loss=3.4545, lr=0.000584, tokens/sec=1374594.01, grad_norm=0.2731, duration=0.38s
Step 3378: loss=3.5051, lr=0.000584, tokens/sec=1374392.11, grad_norm=0.3149, duration=0.38s
Step 3379: loss=3.4415, lr=0.000584, tokens/sec=1376535.21, grad_norm=0.3616, duration=0.38s
Step 3380: loss=3.4336, lr=0.000584, tokens/sec=1372453.52, grad_norm=0.2902, duration=0.38s
Step 3381: loss=3.3701, lr=0.000584, tokens/sec=1377535.48, grad_norm=0.2843, duration=0.38s
Step 3382: loss=3.3953, lr=0.000584, tokens/sec=1379817.39, grad_norm=0.2938, duration=0.38s
Step 3383: loss=3.3593, lr=0.000584, tokens/sec=1379201.22, grad_norm=0.2765, duration=0.38s
Step 3384: loss=3.3617, lr=0.000584, tokens/sec=1373091.11, grad_norm=0.2718, duration=0.38s
Step 3385: loss=3.4444, lr=0.000584, tokens/sec=1374961.86, grad_norm=0.3025, duration=0.38s
Step 3386: loss=3.3303, lr=0.000584, tokens/sec=1379169.22, grad_norm=0.2694, duration=0.38s
Step 3387: loss=3.4139, lr=0.000584, tokens/sec=1378822.45, grad_norm=0.2882, duration=0.38s
Step 3388: loss=3.4534, lr=0.000584, tokens/sec=1377410.37, grad_norm=0.2805, duration=0.38s
Step 3389: loss=3.3825, lr=0.000584, tokens/sec=1377130.02, grad_norm=0.2691, duration=0.38s
Step 3390: loss=3.3360, lr=0.000584, tokens/sec=1376231.10, grad_norm=0.2581, duration=0.38s
Step 3391: loss=3.3927, lr=0.000584, tokens/sec=1377529.44, grad_norm=0.2546, duration=0.38s
Step 3392: loss=3.3793, lr=0.000584, tokens/sec=1375940.05, grad_norm=0.2733, duration=0.38s
Step 3393: loss=3.3559, lr=0.000584, tokens/sec=1375117.49, grad_norm=0.2630, duration=0.38s
Step 3394: loss=3.2810, lr=0.000584, tokens/sec=1380014.82, grad_norm=0.2615, duration=0.38s
Step 3395: loss=3.3596, lr=0.000584, tokens/sec=1374040.01, grad_norm=0.2893, duration=0.38s
Step 3396: loss=3.3521, lr=0.000584, tokens/sec=1375027.20, grad_norm=0.3121, duration=0.38s
Step 3397: loss=3.3495, lr=0.000584, tokens/sec=1376959.28, grad_norm=0.2955, duration=0.38s
Step 3398: loss=3.4048, lr=0.000584, tokens/sec=1377255.95, grad_norm=0.3114, duration=0.38s
Step 3399: loss=3.3587, lr=0.000584, tokens/sec=1377115.36, grad_norm=0.2626, duration=0.38s
Step 3400/19073 (17.8%), Elapsed time: 1361.00s, Steps per hour: 8993.37, Estimated hours remaining: 1.74
Step 3400: loss=3.3766, lr=0.000584, tokens/sec=1373402.40, grad_norm=0.2713, duration=0.38s
Step 3401: loss=3.3721, lr=0.000584, tokens/sec=1376890.31, grad_norm=0.2834, duration=0.38s
Step 3402: loss=3.2960, lr=0.000584, tokens/sec=1373822.83, grad_norm=0.2792, duration=0.38s
Step 3403: loss=3.3679, lr=0.000584, tokens/sec=1379337.04, grad_norm=0.3066, duration=0.38s
Step 3404: loss=3.3108, lr=0.000584, tokens/sec=1373447.86, grad_norm=0.2908, duration=0.38s
Step 3405: loss=3.3193, lr=0.000584, tokens/sec=1374923.18, grad_norm=0.2812, duration=0.38s
Step 3406: loss=3.2738, lr=0.000584, tokens/sec=1374637.83, grad_norm=0.2824, duration=0.38s
Step 3407: loss=3.3857, lr=0.000584, tokens/sec=1373370.67, grad_norm=0.3057, duration=0.38s
Step 3408: loss=3.3146, lr=0.000584, tokens/sec=1377011.02, grad_norm=0.2804, duration=0.38s
Step 3409: loss=3.3182, lr=0.000584, tokens/sec=1375341.10, grad_norm=0.2662, duration=0.38s
Step 3410: loss=3.2944, lr=0.000584, tokens/sec=1374094.96, grad_norm=0.2754, duration=0.38s
Step 3411: loss=3.3102, lr=0.000584, tokens/sec=1377012.74, grad_norm=0.2652, duration=0.38s
Step 3412: loss=3.3371, lr=0.000584, tokens/sec=1375509.72, grad_norm=0.2790, duration=0.38s
Step 3413: loss=3.3467, lr=0.000584, tokens/sec=1376770.49, grad_norm=0.2647, duration=0.38s
Step 3414: loss=3.3116, lr=0.000584, tokens/sec=1376356.00, grad_norm=0.2769, duration=0.38s
Step 3415: loss=3.4111, lr=0.000584, tokens/sec=1375106.31, grad_norm=0.2723, duration=0.38s
Step 3416: loss=3.4015, lr=0.000584, tokens/sec=1380339.66, grad_norm=0.3019, duration=0.38s
Step 3417: loss=3.4395, lr=0.000584, tokens/sec=1372935.08, grad_norm=0.3046, duration=0.38s
Step 3418: loss=3.3689, lr=0.000584, tokens/sec=1375904.75, grad_norm=0.3241, duration=0.38s
Step 3419: loss=3.4070, lr=0.000584, tokens/sec=1378625.36, grad_norm=0.3097, duration=0.38s
Step 3420: loss=3.3933, lr=0.000584, tokens/sec=1373934.42, grad_norm=0.3515, duration=0.38s
Step 3421: loss=3.4338, lr=0.000584, tokens/sec=1375277.45, grad_norm=0.3558, duration=0.38s
Step 3422: loss=3.4449, lr=0.000584, tokens/sec=1374026.28, grad_norm=0.2900, duration=0.38s
Step 3423: loss=3.4854, lr=0.000584, tokens/sec=1375703.33, grad_norm=0.3145, duration=0.38s
Step 3424: loss=3.4338, lr=0.000584, tokens/sec=1376044.23, grad_norm=0.3058, duration=0.38s
Step 3425: loss=3.4331, lr=0.000584, tokens/sec=1377278.38, grad_norm=0.3197, duration=0.38s
Step 3426: loss=3.4912, lr=0.000584, tokens/sec=1374863.00, grad_norm=0.3077, duration=0.38s
Step 3427: loss=3.4041, lr=0.000584, tokens/sec=1377594.16, grad_norm=0.2994, duration=0.38s
Step 3428: loss=3.4373, lr=0.000584, tokens/sec=1376202.68, grad_norm=0.3088, duration=0.38s
Step 3429: loss=3.4387, lr=0.000584, tokens/sec=1374741.81, grad_norm=0.3085, duration=0.38s
Step 3430: loss=3.4168, lr=0.000584, tokens/sec=1378124.25, grad_norm=0.2862, duration=0.38s
Step 3431: loss=3.3655, lr=0.000584, tokens/sec=1377180.91, grad_norm=0.2773, duration=0.38s
Step 3432: loss=3.4004, lr=0.000584, tokens/sec=1375083.95, grad_norm=0.2697, duration=0.38s
Step 3433: loss=3.3489, lr=0.000584, tokens/sec=1375956.41, grad_norm=0.2800, duration=0.38s
Step 3434: loss=3.4271, lr=0.000584, tokens/sec=1375455.51, grad_norm=0.2555, duration=0.38s
Step 3435: loss=3.3925, lr=0.000584, tokens/sec=1372802.23, grad_norm=0.2835, duration=0.38s
Step 3436: loss=3.3670, lr=0.000584, tokens/sec=1381322.91, grad_norm=0.2755, duration=0.38s
Step 3437: loss=3.3927, lr=0.000584, tokens/sec=1377421.58, grad_norm=0.2731, duration=0.38s
Step 3438: loss=3.3466, lr=0.000584, tokens/sec=1376704.12, grad_norm=0.2671, duration=0.38s
Step 3439: loss=3.2598, lr=0.000584, tokens/sec=1376077.81, grad_norm=0.3389, duration=0.38s
Step 3440: loss=3.3627, lr=0.000584, tokens/sec=1377478.53, grad_norm=0.2539, duration=0.38s
Step 3441: loss=3.3721, lr=0.000583, tokens/sec=1379078.40, grad_norm=0.2870, duration=0.38s
Step 3442: loss=3.4346, lr=0.000583, tokens/sec=1372301.06, grad_norm=0.2571, duration=0.38s
Step 3443: loss=3.3256, lr=0.000583, tokens/sec=1376952.39, grad_norm=0.2899, duration=0.38s
Step 3444: loss=3.4044, lr=0.000583, tokens/sec=1375843.63, grad_norm=0.2881, duration=0.38s
Step 3445: loss=3.3204, lr=0.000583, tokens/sec=1375238.74, grad_norm=0.3149, duration=0.38s
Step 3446: loss=3.3223, lr=0.000583, tokens/sec=1374628.38, grad_norm=0.2840, duration=0.38s
Step 3447: loss=3.3522, lr=0.000583, tokens/sec=1377599.34, grad_norm=0.3029, duration=0.38s
Step 3448: loss=3.4152, lr=0.000583, tokens/sec=1373917.25, grad_norm=0.2709, duration=0.38s
Step 3449: loss=3.3782, lr=0.000583, tokens/sec=1375551.88, grad_norm=0.2893, duration=0.38s
Step 3450: loss=3.3778, lr=0.000583, tokens/sec=1377247.32, grad_norm=0.3051, duration=0.38s
Step 3451: loss=3.3704, lr=0.000583, tokens/sec=1373487.33, grad_norm=0.2739, duration=0.38s
Step 3452: loss=3.3926, lr=0.000583, tokens/sec=1373296.91, grad_norm=0.3108, duration=0.38s
Step 3453: loss=3.4718, lr=0.000583, tokens/sec=1376957.56, grad_norm=0.2863, duration=0.38s
Step 3454: loss=3.3790, lr=0.000583, tokens/sec=1377092.94, grad_norm=0.2755, duration=0.38s
Step 3455: loss=3.3048, lr=0.000583, tokens/sec=1373821.12, grad_norm=0.2790, duration=0.38s
Step 3456: loss=3.3770, lr=0.000583, tokens/sec=1378474.13, grad_norm=0.2775, duration=0.38s
Step 3457: loss=3.3018, lr=0.000583, tokens/sec=1379684.07, grad_norm=0.2698, duration=0.38s
Step 3458: loss=3.3019, lr=0.000583, tokens/sec=1375668.91, grad_norm=0.2756, duration=0.38s
Step 3459: loss=3.3339, lr=0.000583, tokens/sec=1373250.60, grad_norm=0.2998, duration=0.38s
Step 3460: loss=3.3145, lr=0.000583, tokens/sec=1375703.33, grad_norm=0.2648, duration=0.38s
Step 3461: loss=3.3323, lr=0.000583, tokens/sec=1375807.48, grad_norm=0.2678, duration=0.38s
Step 3462: loss=3.3172, lr=0.000583, tokens/sec=1378406.73, grad_norm=0.2574, duration=0.38s
Step 3463: loss=3.3391, lr=0.000583, tokens/sec=1376906.69, grad_norm=0.2422, duration=0.38s
Step 3464: loss=3.3407, lr=0.000583, tokens/sec=1378042.21, grad_norm=0.2439, duration=0.38s
Step 3465: loss=3.3054, lr=0.000583, tokens/sec=1377343.94, grad_norm=0.2684, duration=0.38s
Step 3466: loss=3.3312, lr=0.000583, tokens/sec=1375287.77, grad_norm=0.2739, duration=0.38s
Step 3467: loss=3.4524, lr=0.000583, tokens/sec=1376341.36, grad_norm=0.2881, duration=0.38s
Step 3468: loss=3.4498, lr=0.000583, tokens/sec=1379160.57, grad_norm=0.3036, duration=0.38s
Step 3469: loss=3.4299, lr=0.000583, tokens/sec=1375843.63, grad_norm=0.3118, duration=0.38s
Step 3470: loss=3.4135, lr=0.000583, tokens/sec=1375450.35, grad_norm=0.2767, duration=0.38s
Step 3471: loss=3.4154, lr=0.000583, tokens/sec=1376864.45, grad_norm=0.2918, duration=0.38s
Step 3472: loss=3.4219, lr=0.000583, tokens/sec=1380436.71, grad_norm=0.2726, duration=0.38s
Step 3473: loss=3.4090, lr=0.000583, tokens/sec=1377036.89, grad_norm=0.3005, duration=0.38s
Step 3474: loss=3.3744, lr=0.000583, tokens/sec=1375426.26, grad_norm=0.2687, duration=0.38s
Step 3475: loss=3.3840, lr=0.000583, tokens/sec=1374469.43, grad_norm=0.2577, duration=0.38s
Step 3476: loss=3.3914, lr=0.000583, tokens/sec=1371501.67, grad_norm=0.2618, duration=0.38s
Step 3477: loss=3.3747, lr=0.000583, tokens/sec=1372498.92, grad_norm=0.2825, duration=0.38s
Step 3478: loss=3.3650, lr=0.000583, tokens/sec=1374364.62, grad_norm=0.2504, duration=0.38s
Step 3479: loss=3.3907, lr=0.000583, tokens/sec=1376076.95, grad_norm=0.2672, duration=0.38s
Step 3480: loss=3.3579, lr=0.000583, tokens/sec=1378107.84, grad_norm=0.2329, duration=0.38s
Step 3481: loss=3.2875, lr=0.000583, tokens/sec=1374683.37, grad_norm=0.2657, duration=0.38s
Step 3482: loss=3.3302, lr=0.000583, tokens/sec=1372262.53, grad_norm=0.2771, duration=0.38s
Step 3483: loss=3.3852, lr=0.000583, tokens/sec=1373181.14, grad_norm=0.2705, duration=0.38s
Step 3484: loss=3.3770, lr=0.000583, tokens/sec=1374319.96, grad_norm=0.3222, duration=0.38s
Step 3485: loss=3.3936, lr=0.000583, tokens/sec=1376960.15, grad_norm=0.2892, duration=0.38s
Step 3486: loss=3.3395, lr=0.000583, tokens/sec=1376068.34, grad_norm=0.2562, duration=0.38s
Step 3487: loss=3.4219, lr=0.000583, tokens/sec=1372899.08, grad_norm=0.2649, duration=0.38s
Step 3488: loss=3.3685, lr=0.000583, tokens/sec=1378490.54, grad_norm=0.2781, duration=0.38s
Step 3489: loss=3.3567, lr=0.000583, tokens/sec=1375086.53, grad_norm=0.2684, duration=0.38s
Step 3490: loss=3.3655, lr=0.000583, tokens/sec=1380888.34, grad_norm=0.3482, duration=0.38s
Step 3491: loss=3.3116, lr=0.000583, tokens/sec=1376279.34, grad_norm=0.3447, duration=0.38s
Step 3492: loss=3.3438, lr=0.000583, tokens/sec=1374877.62, grad_norm=0.3435, duration=0.38s
Step 3493: loss=3.3566, lr=0.000583, tokens/sec=1375513.16, grad_norm=0.3516, duration=0.38s
Step 3494: loss=3.3927, lr=0.000583, tokens/sec=1377063.62, grad_norm=0.3280, duration=0.38s
Step 3495: loss=3.4016, lr=0.000583, tokens/sec=1377651.99, grad_norm=0.3049, duration=0.38s
Step 3496: loss=3.3951, lr=0.000583, tokens/sec=1376500.74, grad_norm=0.3068, duration=0.38s
Step 3497: loss=3.3446, lr=0.000583, tokens/sec=1378532.89, grad_norm=0.2714, duration=0.38s
Step 3498: loss=3.3835, lr=0.000583, tokens/sec=1377666.66, grad_norm=0.2939, duration=0.38s
Step 3499: loss=3.3595, lr=0.000583, tokens/sec=1377428.49, grad_norm=0.2989, duration=0.38s
Step 3500/19073 (18.4%), Elapsed time: 1399.19s, Steps per hour: 9005.18, Estimated hours remaining: 1.73
Validation loss at step 3500: 3.706681251525879
Step 3500: loss=3.3465, lr=0.000583, tokens/sec=156681.82, grad_norm=0.2759, duration=3.35s
Step 3501: loss=3.3132, lr=0.000583, tokens/sec=1380256.49, grad_norm=0.2534, duration=0.38s
Step 3502: loss=3.3817, lr=0.000583, tokens/sec=1378565.73, grad_norm=0.2742, duration=0.38s
Step 3503: loss=3.3070, lr=0.000583, tokens/sec=1372852.80, grad_norm=0.2453, duration=0.38s
Step 3504: loss=3.3491, lr=0.000583, tokens/sec=1375169.08, grad_norm=0.2660, duration=0.38s
Step 3505: loss=3.2606, lr=0.000583, tokens/sec=1374667.90, grad_norm=0.2727, duration=0.38s
Step 3506: loss=3.3193, lr=0.000583, tokens/sec=1375834.16, grad_norm=0.2695, duration=0.38s
Step 3507: loss=3.3304, lr=0.000583, tokens/sec=1378176.07, grad_norm=0.2840, duration=0.38s
Step 3508: loss=3.2779, lr=0.000583, tokens/sec=1377777.14, grad_norm=0.2756, duration=0.38s
Step 3509: loss=3.3191, lr=0.000583, tokens/sec=1377425.04, grad_norm=0.2700, duration=0.38s
Step 3510: loss=3.2446, lr=0.000583, tokens/sec=1378027.53, grad_norm=0.2802, duration=0.38s
Step 3511: loss=3.3235, lr=0.000583, tokens/sec=1373025.09, grad_norm=0.2871, duration=0.38s
Step 3512: loss=3.2915, lr=0.000583, tokens/sec=1372148.64, grad_norm=0.2698, duration=0.38s
Step 3513: loss=3.3559, lr=0.000583, tokens/sec=1376562.78, grad_norm=0.2915, duration=0.38s
Step 3514: loss=3.3962, lr=0.000583, tokens/sec=1376201.82, grad_norm=0.2982, duration=0.38s
Step 3515: loss=3.3868, lr=0.000583, tokens/sec=1370184.78, grad_norm=0.3035, duration=0.38s
Step 3516: loss=3.4011, lr=0.000582, tokens/sec=1375200.04, grad_norm=0.3117, duration=0.38s
Step 3517: loss=3.3930, lr=0.000582, tokens/sec=1378203.71, grad_norm=0.3022, duration=0.38s
Step 3518: loss=3.3512, lr=0.000582, tokens/sec=1376085.56, grad_norm=0.3169, duration=0.38s
Step 3519: loss=3.4031, lr=0.000582, tokens/sec=1372628.28, grad_norm=0.2853, duration=0.38s
Step 3520: loss=3.3694, lr=0.000582, tokens/sec=1377958.45, grad_norm=0.3046, duration=0.38s
Step 3521: loss=3.4180, lr=0.000582, tokens/sec=1373144.26, grad_norm=0.3109, duration=0.38s
Step 3522: loss=3.4296, lr=0.000582, tokens/sec=1375761.86, grad_norm=0.3178, duration=0.38s
Step 3523: loss=3.3701, lr=0.000582, tokens/sec=1376497.30, grad_norm=0.2860, duration=0.38s
Step 3524: loss=3.3976, lr=0.000582, tokens/sec=1374109.56, grad_norm=0.3008, duration=0.38s
Step 3525: loss=3.3658, lr=0.000582, tokens/sec=1375893.56, grad_norm=0.3117, duration=0.38s
Step 3526: loss=3.3779, lr=0.000582, tokens/sec=1379481.55, grad_norm=0.3180, duration=0.38s
Step 3527: loss=3.3389, lr=0.000582, tokens/sec=1376315.52, grad_norm=0.2924, duration=0.38s
Step 3528: loss=3.3815, lr=0.000582, tokens/sec=1372220.57, grad_norm=0.2780, duration=0.38s
Step 3529: loss=3.3345, lr=0.000582, tokens/sec=1374058.04, grad_norm=0.2790, duration=0.38s
Step 3530: loss=3.3459, lr=0.000582, tokens/sec=1378704.88, grad_norm=0.2626, duration=0.38s
Step 3531: loss=3.3316, lr=0.000582, tokens/sec=1375707.63, grad_norm=0.2731, duration=0.38s
Step 3532: loss=3.3543, lr=0.000582, tokens/sec=1376771.35, grad_norm=0.2612, duration=0.38s
Step 3533: loss=3.2940, lr=0.000582, tokens/sec=1378721.30, grad_norm=0.2442, duration=0.38s
Step 3534: loss=3.4470, lr=0.000582, tokens/sec=1379764.58, grad_norm=0.2712, duration=0.38s
Step 3535: loss=3.3720, lr=0.000582, tokens/sec=1378222.72, grad_norm=0.2784, duration=0.38s
Step 3536: loss=3.3746, lr=0.000582, tokens/sec=1376509.36, grad_norm=0.2658, duration=0.38s
Step 3537: loss=3.3503, lr=0.000582, tokens/sec=1376157.04, grad_norm=0.2657, duration=0.38s
Step 3538: loss=3.3375, lr=0.000582, tokens/sec=1375519.18, grad_norm=0.2829, duration=0.38s
Step 3539: loss=3.3560, lr=0.000582, tokens/sec=1377801.32, grad_norm=0.3029, duration=0.38s
Step 3540: loss=3.3696, lr=0.000582, tokens/sec=1377289.59, grad_norm=0.2592, duration=0.38s
Step 3541: loss=3.3731, lr=0.000582, tokens/sec=1376692.91, grad_norm=0.2847, duration=0.38s
Step 3542: loss=3.4112, lr=0.000582, tokens/sec=1378051.71, grad_norm=0.2697, duration=0.38s
Step 3543: loss=3.3745, lr=0.000582, tokens/sec=1375147.58, grad_norm=0.2800, duration=0.38s
Step 3544: loss=3.3579, lr=0.000582, tokens/sec=1377261.99, grad_norm=0.2894, duration=0.38s
Step 3545: loss=3.3976, lr=0.000582, tokens/sec=1378084.52, grad_norm=0.2985, duration=0.38s
Step 3546: loss=3.3442, lr=0.000582, tokens/sec=1376189.76, grad_norm=0.2492, duration=0.38s
Step 3547: loss=3.3290, lr=0.000582, tokens/sec=1380046.86, grad_norm=0.2558, duration=0.38s
Step 3548: loss=3.3479, lr=0.000582, tokens/sec=1376717.05, grad_norm=0.2686, duration=0.38s
Step 3549: loss=3.2782, lr=0.000582, tokens/sec=1373268.60, grad_norm=0.2676, duration=0.38s
Step 3550: loss=3.3140, lr=0.000582, tokens/sec=1378908.91, grad_norm=0.2585, duration=0.38s
Step 3551: loss=3.2220, lr=0.000582, tokens/sec=1375572.53, grad_norm=0.2570, duration=0.38s
Step 3552: loss=3.2646, lr=0.000582, tokens/sec=1374270.15, grad_norm=0.3141, duration=0.38s
Step 3553: loss=3.2705, lr=0.000582, tokens/sec=1378576.10, grad_norm=0.2858, duration=0.38s
Step 3554: loss=3.3386, lr=0.000582, tokens/sec=1373952.45, grad_norm=0.2955, duration=0.38s
Step 3555: loss=3.3309, lr=0.000582, tokens/sec=1379092.24, grad_norm=0.2936, duration=0.38s
Step 3556: loss=3.2767, lr=0.000582, tokens/sec=1374630.09, grad_norm=0.2850, duration=0.38s
Step 3557: loss=3.2555, lr=0.000582, tokens/sec=1377045.51, grad_norm=0.2695, duration=0.38s
Step 3558: loss=3.2999, lr=0.000582, tokens/sec=1377169.70, grad_norm=0.2735, duration=0.38s
Step 3559: loss=3.3399, lr=0.000582, tokens/sec=1378866.54, grad_norm=0.2721, duration=0.38s
Step 3560: loss=3.3852, lr=0.000582, tokens/sec=1373496.76, grad_norm=0.2624, duration=0.38s
Step 3561: loss=3.3982, lr=0.000582, tokens/sec=1376984.29, grad_norm=0.2930, duration=0.38s
Step 3562: loss=3.4355, lr=0.000582, tokens/sec=1375508.86, grad_norm=0.3094, duration=0.38s
Step 3563: loss=3.3856, lr=0.000582, tokens/sec=1373258.31, grad_norm=0.3012, duration=0.38s
Step 3564: loss=3.4055, lr=0.000582, tokens/sec=1377682.20, grad_norm=0.3210, duration=0.38s
Step 3565: loss=3.3547, lr=0.000582, tokens/sec=1377550.15, grad_norm=0.3441, duration=0.38s
Step 3566: loss=3.3837, lr=0.000582, tokens/sec=1378010.26, grad_norm=0.3005, duration=0.38s
Step 3567: loss=3.4150, lr=0.000582, tokens/sec=1377774.55, grad_norm=0.3186, duration=0.38s
Step 3568: loss=3.4816, lr=0.000582, tokens/sec=1375132.97, grad_norm=0.3285, duration=0.38s
Step 3569: loss=3.4226, lr=0.000582, tokens/sec=1376683.43, grad_norm=0.3513, duration=0.38s
Step 3570: loss=3.4087, lr=0.000582, tokens/sec=1377218.00, grad_norm=0.2959, duration=0.38s
Step 3571: loss=3.3329, lr=0.000582, tokens/sec=1378257.27, grad_norm=0.2844, duration=0.38s
Step 3572: loss=3.3601, lr=0.000582, tokens/sec=1375121.79, grad_norm=0.2730, duration=0.38s
Step 3573: loss=3.3667, lr=0.000582, tokens/sec=1377149.86, grad_norm=0.2634, duration=0.38s
Step 3574: loss=3.3343, lr=0.000582, tokens/sec=1376202.68, grad_norm=0.2781, duration=0.38s
Step 3575: loss=3.3915, lr=0.000582, tokens/sec=1381370.64, grad_norm=0.2936, duration=0.38s
Step 3576: loss=3.2979, lr=0.000582, tokens/sec=1378309.97, grad_norm=0.2665, duration=0.38s
Step 3577: loss=3.3968, lr=0.000582, tokens/sec=1373120.26, grad_norm=0.2843, duration=0.38s
Step 3578: loss=3.4195, lr=0.000582, tokens/sec=1375868.59, grad_norm=0.2789, duration=0.38s
Step 3579: loss=3.3737, lr=0.000582, tokens/sec=1377221.45, grad_norm=0.2725, duration=0.38s
Step 3580: loss=3.2920, lr=0.000582, tokens/sec=1378219.26, grad_norm=0.2572, duration=0.38s
Step 3581: loss=3.3457, lr=0.000582, tokens/sec=1381918.40, grad_norm=0.2555, duration=0.38s
Step 3582: loss=3.3706, lr=0.000582, tokens/sec=1374956.70, grad_norm=0.2510, duration=0.38s
Step 3583: loss=3.3243, lr=0.000582, tokens/sec=1379026.51, grad_norm=0.2610, duration=0.38s
Step 3584: loss=3.2591, lr=0.000582, tokens/sec=1379721.30, grad_norm=0.2462, duration=0.38s
Step 3585: loss=3.3315, lr=0.000582, tokens/sec=1377824.62, grad_norm=0.2681, duration=0.38s
Step 3586: loss=3.2995, lr=0.000582, tokens/sec=1373577.41, grad_norm=0.2822, duration=0.38s
Step 3587: loss=3.3434, lr=0.000582, tokens/sec=1376985.15, grad_norm=0.2927, duration=0.38s
Step 3588: loss=3.4194, lr=0.000582, tokens/sec=1376732.56, grad_norm=0.2875, duration=0.38s
Step 3589: loss=3.3406, lr=0.000582, tokens/sec=1377086.04, grad_norm=0.2737, duration=0.38s
Step 3590: loss=3.3433, lr=0.000581, tokens/sec=1378230.49, grad_norm=0.2937, duration=0.38s
Step 3591: loss=3.3213, lr=0.000581, tokens/sec=1376244.02, grad_norm=0.3005, duration=0.38s
Step 3592: loss=3.2921, lr=0.000581, tokens/sec=1376586.05, grad_norm=0.2927, duration=0.38s
Step 3593: loss=3.3265, lr=0.000581, tokens/sec=1376655.85, grad_norm=0.2941, duration=0.38s
Step 3594: loss=3.3070, lr=0.000581, tokens/sec=1375230.14, grad_norm=0.2935, duration=0.38s
Step 3595: loss=3.2482, lr=0.000581, tokens/sec=1377444.02, grad_norm=0.2694, duration=0.38s
Step 3596: loss=3.2887, lr=0.000581, tokens/sec=1380411.58, grad_norm=0.2782, duration=0.38s
Step 3597: loss=3.3468, lr=0.000581, tokens/sec=1376906.69, grad_norm=0.2733, duration=0.38s
Step 3598: loss=3.3114, lr=0.000581, tokens/sec=1376458.52, grad_norm=0.2628, duration=0.38s
Step 3599: loss=3.3008, lr=0.000581, tokens/sec=1376405.97, grad_norm=0.2678, duration=0.38s
Step 3600/19073 (18.9%), Elapsed time: 1440.33s, Steps per hour: 8997.96, Estimated hours remaining: 1.72
Step 3600: loss=3.2458, lr=0.000581, tokens/sec=1377005.85, grad_norm=0.2647, duration=0.38s
Step 3601: loss=3.2817, lr=0.000581, tokens/sec=1377609.70, grad_norm=0.2751, duration=0.38s
Step 3602: loss=3.3366, lr=0.000581, tokens/sec=1377735.71, grad_norm=0.2613, duration=0.38s
Step 3603: loss=3.2641, lr=0.000581, tokens/sec=1379072.35, grad_norm=0.2464, duration=0.38s
Step 3604: loss=3.3341, lr=0.000581, tokens/sec=1377664.07, grad_norm=0.2564, duration=0.38s
Step 3605: loss=3.3694, lr=0.000581, tokens/sec=1374072.64, grad_norm=0.2637, duration=0.38s
Step 3606: loss=3.4016, lr=0.000581, tokens/sec=1378554.49, grad_norm=0.2740, duration=0.38s
Step 3607: loss=3.3789, lr=0.000581, tokens/sec=1377790.96, grad_norm=0.3066, duration=0.38s
Step 3608: loss=3.3484, lr=0.000581, tokens/sec=1374534.72, grad_norm=0.2967, duration=0.38s
Step 3609: loss=3.3775, lr=0.000581, tokens/sec=1378018.03, grad_norm=0.3118, duration=0.38s
Step 3610: loss=3.3701, lr=0.000581, tokens/sec=1377690.83, grad_norm=0.3364, duration=0.38s
Step 3611: loss=3.4133, lr=0.000581, tokens/sec=1375709.36, grad_norm=0.3196, duration=0.38s
Step 3612: loss=3.3984, lr=0.000581, tokens/sec=1375366.90, grad_norm=0.3189, duration=0.38s
Step 3613: loss=3.4728, lr=0.000581, tokens/sec=1382647.40, grad_norm=0.3222, duration=0.38s
Step 3614: loss=3.3807, lr=0.000581, tokens/sec=1376370.65, grad_norm=0.2960, duration=0.38s
Step 3615: loss=3.4111, lr=0.000581, tokens/sec=1373713.84, grad_norm=0.2958, duration=0.38s
Step 3616: loss=3.4541, lr=0.000581, tokens/sec=1379223.71, grad_norm=0.3080, duration=0.38s
Step 3617: loss=3.3895, lr=0.000581, tokens/sec=1378520.79, grad_norm=0.3085, duration=0.38s
Step 3618: loss=3.4406, lr=0.000581, tokens/sec=1375699.03, grad_norm=0.2828, duration=0.38s
Step 3619: loss=3.3703, lr=0.000581, tokens/sec=1375269.71, grad_norm=0.2756, duration=0.38s
Step 3620: loss=3.3839, lr=0.000581, tokens/sec=1378761.07, grad_norm=0.2787, duration=0.38s
Step 3621: loss=3.3532, lr=0.000581, tokens/sec=1375028.92, grad_norm=0.2805, duration=0.38s
Step 3622: loss=3.3876, lr=0.000581, tokens/sec=1372101.56, grad_norm=0.2551, duration=0.38s
Step 3623: loss=3.3444, lr=0.000581, tokens/sec=1378712.66, grad_norm=0.2697, duration=0.38s
Step 3624: loss=3.3691, lr=0.000581, tokens/sec=1377526.85, grad_norm=0.3094, duration=0.38s
Step 3625: loss=3.3784, lr=0.000581, tokens/sec=1377738.30, grad_norm=0.3385, duration=0.38s
Step 3626: loss=3.3108, lr=0.000581, tokens/sec=1377601.93, grad_norm=0.3184, duration=0.38s
Step 3627: loss=3.3949, lr=0.000581, tokens/sec=1377595.89, grad_norm=0.2987, duration=0.38s
Step 3628: loss=3.2922, lr=0.000581, tokens/sec=1376150.15, grad_norm=0.2988, duration=0.38s
Step 3629: loss=3.2718, lr=0.000581, tokens/sec=1378459.44, grad_norm=0.3827, duration=0.38s
Step 3630: loss=3.3273, lr=0.000581, tokens/sec=1379221.98, grad_norm=0.2905, duration=0.38s
Step 3631: loss=3.3249, lr=0.000581, tokens/sec=1376929.97, grad_norm=0.2743, duration=0.38s
Step 3632: loss=3.4520, lr=0.000581, tokens/sec=1375353.14, grad_norm=0.2824, duration=0.38s
Step 3633: loss=3.2831, lr=0.000581, tokens/sec=1378801.70, grad_norm=0.2963, duration=0.38s
Step 3634: loss=3.3181, lr=0.000581, tokens/sec=1374748.69, grad_norm=0.2868, duration=0.38s
Step 3635: loss=3.3396, lr=0.000581, tokens/sec=1379901.38, grad_norm=0.2629, duration=0.38s
Step 3636: loss=3.3027, lr=0.000581, tokens/sec=1379832.11, grad_norm=0.2808, duration=0.38s
Step 3637: loss=3.3629, lr=0.000581, tokens/sec=1376805.83, grad_norm=0.2586, duration=0.38s
Step 3638: loss=3.3775, lr=0.000581, tokens/sec=1375891.84, grad_norm=0.2531, duration=0.38s
Step 3639: loss=3.3536, lr=0.000581, tokens/sec=1375054.72, grad_norm=0.2474, duration=0.38s
Step 3640: loss=3.3237, lr=0.000581, tokens/sec=1376792.04, grad_norm=0.2609, duration=0.38s
Step 3641: loss=3.3566, lr=0.000581, tokens/sec=1376322.41, grad_norm=0.2601, duration=0.38s
Step 3642: loss=3.4070, lr=0.000581, tokens/sec=1375963.29, grad_norm=0.3055, duration=0.38s
Step 3643: loss=3.4280, lr=0.000581, tokens/sec=1374789.08, grad_norm=0.2808, duration=0.38s
Step 3644: loss=3.3350, lr=0.000581, tokens/sec=1379406.26, grad_norm=0.2791, duration=0.38s
Step 3645: loss=3.2891, lr=0.000581, tokens/sec=1380116.15, grad_norm=0.2879, duration=0.38s
Step 3646: loss=3.3352, lr=0.000581, tokens/sec=1373902.66, grad_norm=0.2996, duration=0.38s
Step 3647: loss=3.2826, lr=0.000581, tokens/sec=1376767.90, grad_norm=0.3013, duration=0.38s
Step 3648: loss=3.2813, lr=0.000581, tokens/sec=1377080.00, grad_norm=0.2665, duration=0.38s
Step 3649: loss=3.2827, lr=0.000581, tokens/sec=1376284.51, grad_norm=0.2809, duration=0.38s
Step 3650: loss=3.2982, lr=0.000581, tokens/sec=1376539.52, grad_norm=0.2865, duration=0.38s
Step 3651: loss=3.3033, lr=0.000581, tokens/sec=1377404.33, grad_norm=0.2980, duration=0.38s
Step 3652: loss=3.2937, lr=0.000581, tokens/sec=1375713.66, grad_norm=0.2638, duration=0.38s
Step 3653: loss=3.3395, lr=0.000581, tokens/sec=1373758.46, grad_norm=0.2605, duration=0.38s
Step 3654: loss=3.2878, lr=0.000581, tokens/sec=1378642.65, grad_norm=0.2751, duration=0.38s
Step 3655: loss=3.2922, lr=0.000581, tokens/sec=1375569.09, grad_norm=0.2865, duration=0.38s
Step 3656: loss=3.3320, lr=0.000581, tokens/sec=1378250.36, grad_norm=0.2526, duration=0.38s
Step 3657: loss=3.3835, lr=0.000581, tokens/sec=1380180.25, grad_norm=0.2930, duration=0.38s
Step 3658: loss=3.4466, lr=0.000581, tokens/sec=1374774.47, grad_norm=0.2584, duration=0.38s
Step 3659: loss=3.3869, lr=0.000581, tokens/sec=1374857.85, grad_norm=0.2669, duration=0.38s
Step 3660: loss=3.3714, lr=0.000581, tokens/sec=1376766.18, grad_norm=0.3084, duration=0.38s
Step 3661: loss=3.4060, lr=0.000581, tokens/sec=1375562.20, grad_norm=0.3389, duration=0.38s
Step 3662: loss=3.3932, lr=0.000580, tokens/sec=1377502.69, grad_norm=0.3172, duration=0.38s
Step 3663: loss=3.3914, lr=0.000580, tokens/sec=1376095.89, grad_norm=0.3173, duration=0.38s
Step 3664: loss=3.3557, lr=0.000580, tokens/sec=1376277.62, grad_norm=0.2822, duration=0.38s
Step 3665: loss=3.3511, lr=0.000580, tokens/sec=1375736.04, grad_norm=0.2848, duration=0.38s
Step 3666: loss=3.3593, lr=0.000580, tokens/sec=1375146.72, grad_norm=0.2815, duration=0.38s
Step 3667: loss=3.3547, lr=0.000580, tokens/sec=1376101.06, grad_norm=0.2869, duration=0.38s
Step 3668: loss=3.3709, lr=0.000580, tokens/sec=1373804.81, grad_norm=0.2843, duration=0.38s
Step 3669: loss=3.3249, lr=0.000580, tokens/sec=1372352.45, grad_norm=0.2664, duration=0.38s
Step 3670: loss=3.3345, lr=0.000580, tokens/sec=1379217.66, grad_norm=0.2627, duration=0.38s
Step 3671: loss=3.2630, lr=0.000580, tokens/sec=1378749.83, grad_norm=0.2561, duration=0.38s
Step 3672: loss=3.3139, lr=0.000580, tokens/sec=1375810.06, grad_norm=0.2831, duration=0.38s
Step 3673: loss=3.3574, lr=0.000580, tokens/sec=1376388.74, grad_norm=0.2844, duration=0.38s
Step 3674: loss=3.3180, lr=0.000580, tokens/sec=1379564.63, grad_norm=0.3117, duration=0.38s
Step 3675: loss=3.3866, lr=0.000580, tokens/sec=1380705.40, grad_norm=0.3054, duration=0.38s
Step 3676: loss=3.3080, lr=0.000580, tokens/sec=1375298.09, grad_norm=0.2738, duration=0.38s
Step 3677: loss=3.3932, lr=0.000580, tokens/sec=1377675.29, grad_norm=0.2748, duration=0.38s
Step 3678: loss=3.3574, lr=0.000580, tokens/sec=1378402.41, grad_norm=0.2767, duration=0.38s
Step 3679: loss=3.3334, lr=0.000580, tokens/sec=1375742.06, grad_norm=0.2517, duration=0.38s
Step 3680: loss=3.2817, lr=0.000580, tokens/sec=1373214.58, grad_norm=0.2902, duration=0.38s
Step 3681: loss=3.3390, lr=0.000580, tokens/sec=1377811.67, grad_norm=0.2709, duration=0.38s
Step 3682: loss=3.3128, lr=0.000580, tokens/sec=1376832.55, grad_norm=0.3114, duration=0.38s
Step 3683: loss=3.3773, lr=0.000580, tokens/sec=1377579.49, grad_norm=0.3346, duration=0.38s
Step 3684: loss=3.3651, lr=0.000580, tokens/sec=1375367.77, grad_norm=0.2839, duration=0.38s
Step 3685: loss=3.3779, lr=0.000580, tokens/sec=1377015.33, grad_norm=0.3274, duration=0.38s
Step 3686: loss=3.3599, lr=0.000580, tokens/sec=1379604.44, grad_norm=0.3136, duration=0.38s
Step 3687: loss=3.3190, lr=0.000580, tokens/sec=1375071.91, grad_norm=0.2531, duration=0.38s
Step 3688: loss=3.3461, lr=0.000580, tokens/sec=1376041.64, grad_norm=0.2832, duration=0.38s
Step 3689: loss=3.3116, lr=0.000580, tokens/sec=1378903.72, grad_norm=0.2935, duration=0.38s
Step 3690: loss=3.3642, lr=0.000580, tokens/sec=1378949.55, grad_norm=0.2623, duration=0.38s
Step 3691: loss=3.2936, lr=0.000580, tokens/sec=1374665.33, grad_norm=0.3075, duration=0.38s
Step 3692: loss=3.3447, lr=0.000580, tokens/sec=1376732.56, grad_norm=0.2764, duration=0.38s
Step 3693: loss=3.3161, lr=0.000580, tokens/sec=1374780.49, grad_norm=0.2668, duration=0.38s
Step 3694: loss=3.2866, lr=0.000580, tokens/sec=1372937.65, grad_norm=0.2832, duration=0.38s
Step 3695: loss=3.2543, lr=0.000580, tokens/sec=1367807.81, grad_norm=0.2906, duration=0.38s
Step 3696: loss=3.2858, lr=0.000580, tokens/sec=1377594.16, grad_norm=0.3040, duration=0.38s
Step 3697: loss=3.3024, lr=0.000580, tokens/sec=1377836.71, grad_norm=0.2812, duration=0.38s
Step 3698: loss=3.2716, lr=0.000580, tokens/sec=1376943.77, grad_norm=0.2563, duration=0.38s
Step 3699: loss=3.2419, lr=0.000580, tokens/sec=1376293.98, grad_norm=0.2946, duration=0.38s
Step 3700/19073 (19.4%), Elapsed time: 1478.49s, Steps per hour: 9009.17, Estimated hours remaining: 1.71
Step 3700: loss=3.2465, lr=0.000580, tokens/sec=1375770.46, grad_norm=0.2855, duration=0.38s
Step 3701: loss=3.2813, lr=0.000580, tokens/sec=1375921.97, grad_norm=0.3021, duration=0.38s
Step 3702: loss=3.2922, lr=0.000580, tokens/sec=1374846.67, grad_norm=0.2698, duration=0.38s
Step 3703: loss=3.3249, lr=0.000580, tokens/sec=1375159.62, grad_norm=0.2842, duration=0.38s
Step 3704: loss=3.3525, lr=0.000580, tokens/sec=1375668.91, grad_norm=0.2926, duration=0.38s
Step 3705: loss=3.3772, lr=0.000580, tokens/sec=1375699.89, grad_norm=0.3095, duration=0.38s
Step 3706: loss=3.3728, lr=0.000580, tokens/sec=1377803.91, grad_norm=0.3127, duration=0.38s
Step 3707: loss=3.3768, lr=0.000580, tokens/sec=1378028.39, grad_norm=0.3015, duration=0.38s
Step 3708: loss=3.3351, lr=0.000580, tokens/sec=1376582.60, grad_norm=0.3100, duration=0.38s
Step 3709: loss=3.3809, lr=0.000580, tokens/sec=1378785.27, grad_norm=0.3401, duration=0.38s
Step 3710: loss=3.3381, lr=0.000580, tokens/sec=1376898.93, grad_norm=0.3007, duration=0.38s
Step 3711: loss=3.4170, lr=0.000580, tokens/sec=1374450.53, grad_norm=0.3128, duration=0.38s
Step 3712: loss=3.3572, lr=0.000580, tokens/sec=1379728.22, grad_norm=0.2874, duration=0.38s
Step 3713: loss=3.3597, lr=0.000580, tokens/sec=1378991.06, grad_norm=0.3017, duration=0.38s
Step 3714: loss=3.3745, lr=0.000580, tokens/sec=1375366.04, grad_norm=0.3014, duration=0.38s
Step 3715: loss=3.3396, lr=0.000580, tokens/sec=1376052.84, grad_norm=0.2956, duration=0.38s
Step 3716: loss=3.3304, lr=0.000580, tokens/sec=1375957.27, grad_norm=0.2715, duration=0.38s
Step 3717: loss=3.3308, lr=0.000580, tokens/sec=1374802.83, grad_norm=0.2775, duration=0.38s
Step 3718: loss=3.3705, lr=0.000580, tokens/sec=1378321.20, grad_norm=0.3012, duration=0.38s
Step 3719: loss=3.2776, lr=0.000580, tokens/sec=1372936.80, grad_norm=0.2793, duration=0.38s
Step 3720: loss=3.3176, lr=0.000580, tokens/sec=1376504.19, grad_norm=0.2966, duration=0.38s
Step 3721: loss=3.3245, lr=0.000580, tokens/sec=1372815.94, grad_norm=0.2755, duration=0.38s
Step 3722: loss=3.3244, lr=0.000580, tokens/sec=1378225.31, grad_norm=0.2659, duration=0.38s
Step 3723: loss=3.3463, lr=0.000580, tokens/sec=1376059.73, grad_norm=0.2640, duration=0.38s
Step 3724: loss=3.3975, lr=0.000580, tokens/sec=1374405.00, grad_norm=0.2887, duration=0.38s
Step 3725: loss=3.3100, lr=0.000580, tokens/sec=1376578.29, grad_norm=0.2773, duration=0.38s
Step 3726: loss=3.3600, lr=0.000580, tokens/sec=1374679.08, grad_norm=0.2823, duration=0.38s
Step 3727: loss=3.2988, lr=0.000580, tokens/sec=1378490.54, grad_norm=0.2758, duration=0.38s
Step 3728: loss=3.3497, lr=0.000580, tokens/sec=1375298.09, grad_norm=0.2741, duration=0.38s
Step 3729: loss=3.3435, lr=0.000580, tokens/sec=1377884.19, grad_norm=0.2657, duration=0.38s
Step 3730: loss=3.3474, lr=0.000580, tokens/sec=1375571.67, grad_norm=0.2708, duration=0.38s
Step 3731: loss=3.3750, lr=0.000580, tokens/sec=1380129.15, grad_norm=0.2799, duration=0.38s
Step 3732: loss=3.3704, lr=0.000579, tokens/sec=1376592.94, grad_norm=0.2601, duration=0.38s
Step 3733: loss=3.3140, lr=0.000579, tokens/sec=1374513.24, grad_norm=0.2806, duration=0.38s
Step 3734: loss=3.3678, lr=0.000579, tokens/sec=1376279.34, grad_norm=0.2620, duration=0.38s
Step 3735: loss=3.3403, lr=0.000579, tokens/sec=1378639.19, grad_norm=0.2901, duration=0.38s
Step 3736: loss=3.3140, lr=0.000579, tokens/sec=1376213.88, grad_norm=0.2553, duration=0.38s
Step 3737: loss=3.3318, lr=0.000579, tokens/sec=1374820.88, grad_norm=0.2722, duration=0.38s
Step 3738: loss=3.3112, lr=0.000579, tokens/sec=1375637.93, grad_norm=0.2910, duration=0.38s
Step 3739: loss=3.2469, lr=0.000579, tokens/sec=1378809.48, grad_norm=0.2580, duration=0.38s
Step 3740: loss=3.2764, lr=0.000579, tokens/sec=1377601.93, grad_norm=0.2638, duration=0.38s
Step 3741: loss=3.1983, lr=0.000579, tokens/sec=1374611.19, grad_norm=0.2676, duration=0.38s
Step 3742: loss=3.2553, lr=0.000579, tokens/sec=1375273.15, grad_norm=0.2651, duration=0.38s
Step 3743: loss=3.2456, lr=0.000579, tokens/sec=1371830.21, grad_norm=0.2651, duration=0.38s
Step 3744: loss=3.3363, lr=0.000579, tokens/sec=1377307.71, grad_norm=0.2481, duration=0.38s
Step 3745: loss=3.2789, lr=0.000579, tokens/sec=1376088.14, grad_norm=0.2807, duration=0.38s
Step 3746: loss=3.2460, lr=0.000579, tokens/sec=1376261.25, grad_norm=0.2699, duration=0.38s
Step 3747: loss=3.2452, lr=0.000579, tokens/sec=1377264.57, grad_norm=0.2641, duration=0.38s
Step 3748: loss=3.2734, lr=0.000579, tokens/sec=1375516.60, grad_norm=0.2701, duration=0.38s
Step 3749: loss=3.3240, lr=0.000579, tokens/sec=1380697.60, grad_norm=0.2523, duration=0.38s
Validation loss at step 3750: 3.7004380226135254
Step 3750: loss=3.3584, lr=0.000579, tokens/sec=156316.90, grad_norm=0.2710, duration=3.35s
Step 3751: loss=3.3934, lr=0.000579, tokens/sec=1376342.22, grad_norm=0.2650, duration=0.38s
Step 3752: loss=3.3870, lr=0.000579, tokens/sec=1378657.34, grad_norm=0.2710, duration=0.38s
Step 3753: loss=3.3496, lr=0.000579, tokens/sec=1379685.80, grad_norm=0.2681, duration=0.38s
Step 3754: loss=3.3798, lr=0.000579, tokens/sec=1379259.18, grad_norm=0.3319, duration=0.38s
Step 3755: loss=3.3392, lr=0.000579, tokens/sec=1375079.65, grad_norm=0.3942, duration=0.38s
Step 3756: loss=3.3423, lr=0.000579, tokens/sec=1376629.14, grad_norm=0.3108, duration=0.38s
Step 3757: loss=3.3949, lr=0.000579, tokens/sec=1375526.92, grad_norm=0.3459, duration=0.38s
Step 3758: loss=3.4629, lr=0.000579, tokens/sec=1376201.82, grad_norm=0.3272, duration=0.38s
Step 3759: loss=3.3960, lr=0.000579, tokens/sec=1377115.36, grad_norm=0.2967, duration=0.38s
Step 3760: loss=3.3728, lr=0.000579, tokens/sec=1377458.68, grad_norm=0.2863, duration=0.38s
Step 3761: loss=3.3055, lr=0.000579, tokens/sec=1373050.81, grad_norm=0.3066, duration=0.38s
Step 3762: loss=3.3693, lr=0.000579, tokens/sec=1375817.81, grad_norm=0.2852, duration=0.38s
Step 3763: loss=3.3423, lr=0.000579, tokens/sec=1375142.43, grad_norm=0.2818, duration=0.38s
Step 3764: loss=3.2822, lr=0.000579, tokens/sec=1371714.69, grad_norm=0.2855, duration=0.38s
Step 3765: loss=3.3565, lr=0.000579, tokens/sec=1376334.47, grad_norm=0.2764, duration=0.38s
Step 3766: loss=3.2868, lr=0.000579, tokens/sec=1376951.53, grad_norm=0.3089, duration=0.38s
Step 3767: loss=3.3659, lr=0.000579, tokens/sec=1377062.76, grad_norm=0.3267, duration=0.38s
Step 3768: loss=3.4121, lr=0.000579, tokens/sec=1375977.93, grad_norm=0.3077, duration=0.38s
Step 3769: loss=3.3299, lr=0.000579, tokens/sec=1377437.11, grad_norm=0.2674, duration=0.38s
Step 3770: loss=3.2506, lr=0.000579, tokens/sec=1377337.04, grad_norm=0.2758, duration=0.38s
Step 3771: loss=3.3407, lr=0.000579, tokens/sec=1373433.28, grad_norm=0.2767, duration=0.38s
Step 3772: loss=3.3420, lr=0.000579, tokens/sec=1375699.03, grad_norm=0.2594, duration=0.38s
Step 3773: loss=3.3089, lr=0.000579, tokens/sec=1373139.12, grad_norm=0.2644, duration=0.38s
Step 3774: loss=3.2367, lr=0.000579, tokens/sec=1373505.34, grad_norm=0.2599, duration=0.38s
Step 3775: loss=3.2843, lr=0.000579, tokens/sec=1372132.38, grad_norm=0.2812, duration=0.38s
Step 3776: loss=3.2956, lr=0.000579, tokens/sec=1375029.78, grad_norm=0.2586, duration=0.38s
Step 3777: loss=3.3615, lr=0.000579, tokens/sec=1372104.12, grad_norm=0.2643, duration=0.38s
Step 3778: loss=3.4017, lr=0.000579, tokens/sec=1366950.76, grad_norm=0.2952, duration=0.38s
Step 3779: loss=3.3097, lr=0.000579, tokens/sec=1376159.62, grad_norm=0.2668, duration=0.38s
Step 3780: loss=3.2942, lr=0.000579, tokens/sec=1370159.17, grad_norm=0.2543, duration=0.38s
Step 3781: loss=3.3185, lr=0.000579, tokens/sec=1376150.15, grad_norm=0.2804, duration=0.38s
Step 3782: loss=3.2537, lr=0.000579, tokens/sec=1375881.51, grad_norm=0.2819, duration=0.38s
Step 3783: loss=3.3206, lr=0.000579, tokens/sec=1376685.16, grad_norm=0.2874, duration=0.38s
Step 3784: loss=3.2362, lr=0.000579, tokens/sec=1371421.26, grad_norm=0.2764, duration=0.38s
Step 3785: loss=3.2651, lr=0.000579, tokens/sec=1373691.53, grad_norm=0.2854, duration=0.38s
Step 3786: loss=3.2523, lr=0.000579, tokens/sec=1373371.52, grad_norm=0.2702, duration=0.38s
Step 3787: loss=3.3503, lr=0.000579, tokens/sec=1374581.12, grad_norm=0.2873, duration=0.38s
Step 3788: loss=3.3010, lr=0.000579, tokens/sec=1372606.00, grad_norm=0.2933, duration=0.38s
Step 3789: loss=3.2586, lr=0.000579, tokens/sec=1375358.30, grad_norm=0.2941, duration=0.38s
Step 3790: loss=3.2220, lr=0.000579, tokens/sec=1376293.12, grad_norm=0.2763, duration=0.38s
Step 3791: loss=3.2883, lr=0.000579, tokens/sec=1374010.82, grad_norm=0.2981, duration=0.38s
Step 3792: loss=3.2545, lr=0.000579, tokens/sec=1373311.49, grad_norm=0.2608, duration=0.38s
Step 3793: loss=3.2912, lr=0.000579, tokens/sec=1377888.51, grad_norm=0.2659, duration=0.38s
Step 3794: loss=3.2989, lr=0.000579, tokens/sec=1373149.41, grad_norm=0.2638, duration=0.38s
Step 3795: loss=3.3759, lr=0.000579, tokens/sec=1373260.89, grad_norm=0.2591, duration=0.38s
Step 3796: loss=3.3426, lr=0.000579, tokens/sec=1373882.92, grad_norm=0.2893, duration=0.38s
Step 3797: loss=3.3615, lr=0.000579, tokens/sec=1376830.83, grad_norm=0.2697, duration=0.38s
Step 3798: loss=3.3200, lr=0.000579, tokens/sec=1376090.73, grad_norm=0.3160, duration=0.38s
Step 3799: loss=3.3616, lr=0.000579, tokens/sec=1376897.21, grad_norm=0.3287, duration=0.38s
Step 3800/19073 (19.9%), Elapsed time: 1519.66s, Steps per hour: 9002.00, Estimated hours remaining: 1.70
Step 3800: loss=3.3520, lr=0.000578, tokens/sec=1379330.12, grad_norm=0.3309, duration=0.38s
Step 3801: loss=3.3687, lr=0.000578, tokens/sec=1376231.10, grad_norm=0.2923, duration=0.38s
Step 3802: loss=3.3904, lr=0.000578, tokens/sec=1371852.47, grad_norm=0.3423, duration=0.38s
Step 3803: loss=3.4266, lr=0.000578, tokens/sec=1375205.20, grad_norm=0.3298, duration=0.38s
Step 3804: loss=3.3627, lr=0.000578, tokens/sec=1371540.16, grad_norm=0.3004, duration=0.38s
Step 3805: loss=3.3763, lr=0.000578, tokens/sec=1374617.21, grad_norm=0.3094, duration=0.38s
Step 3806: loss=3.4416, lr=0.000578, tokens/sec=1378508.69, grad_norm=0.3280, duration=0.38s
Step 3807: loss=3.3987, lr=0.000578, tokens/sec=1380504.31, grad_norm=0.3251, duration=0.38s
Step 3808: loss=3.3795, lr=0.000578, tokens/sec=1373379.24, grad_norm=0.2952, duration=0.38s
Step 3809: loss=3.3481, lr=0.000578, tokens/sec=1376018.40, grad_norm=0.3086, duration=0.38s
Step 3810: loss=3.3759, lr=0.000578, tokens/sec=1377550.15, grad_norm=0.2990, duration=0.38s
Step 3811: loss=3.3460, lr=0.000578, tokens/sec=1376592.94, grad_norm=0.3026, duration=0.38s
Step 3812: loss=3.3854, lr=0.000578, tokens/sec=1375453.79, grad_norm=0.2750, duration=0.38s
Step 3813: loss=3.2886, lr=0.000578, tokens/sec=1375089.97, grad_norm=0.2770, duration=0.38s
Step 3814: loss=3.3592, lr=0.000578, tokens/sec=1376315.52, grad_norm=0.2941, duration=0.38s
Step 3815: loss=3.3247, lr=0.000578, tokens/sec=1379393.28, grad_norm=0.3287, duration=0.38s
Step 3816: loss=3.3107, lr=0.000578, tokens/sec=1376545.55, grad_norm=0.2791, duration=0.38s
Step 3817: loss=3.3420, lr=0.000578, tokens/sec=1372116.97, grad_norm=0.2730, duration=0.38s
Step 3818: loss=3.3089, lr=0.000578, tokens/sec=1370309.44, grad_norm=0.2882, duration=0.38s
Step 3819: loss=3.2557, lr=0.000578, tokens/sec=1372172.62, grad_norm=0.5189, duration=0.38s
Step 3820: loss=3.2868, lr=0.000578, tokens/sec=1371365.67, grad_norm=0.2948, duration=0.38s
Step 3821: loss=3.3493, lr=0.000578, tokens/sec=1373128.83, grad_norm=0.2779, duration=0.38s
Step 3822: loss=3.4172, lr=0.000578, tokens/sec=1373773.91, grad_norm=0.2822, duration=0.38s
Step 3823: loss=3.2069, lr=0.000578, tokens/sec=1373732.72, grad_norm=0.3234, duration=0.38s
Step 3824: loss=3.3461, lr=0.000578, tokens/sec=1374634.39, grad_norm=0.2771, duration=0.38s
Step 3825: loss=3.3299, lr=0.000578, tokens/sec=1375463.26, grad_norm=0.2945, duration=0.38s
Step 3826: loss=3.3248, lr=0.000578, tokens/sec=1376566.23, grad_norm=0.3070, duration=0.38s
Step 3827: loss=3.3374, lr=0.000578, tokens/sec=1373162.27, grad_norm=0.2806, duration=0.38s
Step 3828: loss=3.3661, lr=0.000578, tokens/sec=1374450.53, grad_norm=0.2872, duration=0.38s
Step 3829: loss=3.3162, lr=0.000578, tokens/sec=1378316.01, grad_norm=0.3045, duration=0.38s
Step 3830: loss=3.3161, lr=0.000578, tokens/sec=1372390.13, grad_norm=0.2599, duration=0.38s
Step 3831: loss=3.3711, lr=0.000578, tokens/sec=1370980.08, grad_norm=0.2723, duration=0.38s
Step 3832: loss=3.3810, lr=0.000578, tokens/sec=1373815.11, grad_norm=0.3429, duration=0.38s
Step 3833: loss=3.3920, lr=0.000578, tokens/sec=1371558.12, grad_norm=0.2852, duration=0.38s
Step 3834: loss=3.3254, lr=0.000578, tokens/sec=1380338.79, grad_norm=0.2585, duration=0.38s
Step 3835: loss=3.2548, lr=0.000578, tokens/sec=1378710.93, grad_norm=0.3027, duration=0.38s
Step 3836: loss=3.3234, lr=0.000578, tokens/sec=1371581.22, grad_norm=0.2902, duration=0.38s
Step 3837: loss=3.2668, lr=0.000578, tokens/sec=1378584.74, grad_norm=0.2620, duration=0.38s
Step 3838: loss=3.2338, lr=0.000578, tokens/sec=1372098.13, grad_norm=0.2596, duration=0.38s
Step 3839: loss=3.2679, lr=0.000578, tokens/sec=1373282.33, grad_norm=0.2698, duration=0.38s
Step 3840: loss=3.2719, lr=0.000578, tokens/sec=1377231.80, grad_norm=0.2542, duration=0.38s
Step 3841: loss=3.2835, lr=0.000578, tokens/sec=1377124.85, grad_norm=0.2810, duration=0.38s
Step 3842: loss=3.2978, lr=0.000578, tokens/sec=1374537.30, grad_norm=0.2612, duration=0.38s
Step 3843: loss=3.2903, lr=0.000578, tokens/sec=1377926.50, grad_norm=0.2994, duration=0.38s
Step 3844: loss=3.2771, lr=0.000578, tokens/sec=1374240.94, grad_norm=0.3097, duration=0.38s
Step 3845: loss=3.2982, lr=0.000578, tokens/sec=1374693.69, grad_norm=0.2748, duration=0.38s
Step 3846: loss=3.2670, lr=0.000578, tokens/sec=1374303.64, grad_norm=0.2596, duration=0.38s
Step 3847: loss=3.3858, lr=0.000578, tokens/sec=1379150.19, grad_norm=0.2763, duration=0.38s
Step 3848: loss=3.4128, lr=0.000578, tokens/sec=1376448.19, grad_norm=0.2837, duration=0.38s
Step 3849: loss=3.3475, lr=0.000578, tokens/sec=1375350.56, grad_norm=0.2724, duration=0.38s
Step 3850: loss=3.3595, lr=0.000578, tokens/sec=1375716.24, grad_norm=0.2801, duration=0.38s
Step 3851: loss=3.3741, lr=0.000578, tokens/sec=1376792.90, grad_norm=0.2704, duration=0.38s
Step 3852: loss=3.3778, lr=0.000578, tokens/sec=1375227.56, grad_norm=0.2886, duration=0.38s
Step 3853: loss=3.3717, lr=0.000578, tokens/sec=1376537.79, grad_norm=0.2798, duration=0.38s
Step 3854: loss=3.3206, lr=0.000578, tokens/sec=1374053.75, grad_norm=0.2770, duration=0.38s
Step 3855: loss=3.3196, lr=0.000578, tokens/sec=1377240.42, grad_norm=0.2860, duration=0.38s
Step 3856: loss=3.3389, lr=0.000578, tokens/sec=1377594.16, grad_norm=0.2636, duration=0.38s
Step 3857: loss=3.3605, lr=0.000578, tokens/sec=1376594.67, grad_norm=0.2647, duration=0.38s
Step 3858: loss=3.3068, lr=0.000578, tokens/sec=1370921.10, grad_norm=0.2737, duration=0.38s
Step 3859: loss=3.3011, lr=0.000578, tokens/sec=1377864.34, grad_norm=0.2461, duration=0.38s
Step 3860: loss=3.3135, lr=0.000578, tokens/sec=1377125.71, grad_norm=0.2620, duration=0.38s
Step 3861: loss=3.2559, lr=0.000578, tokens/sec=1378652.15, grad_norm=0.2849, duration=0.38s
Step 3862: loss=3.2922, lr=0.000578, tokens/sec=1375297.23, grad_norm=0.2767, duration=0.38s
Step 3863: loss=3.3034, lr=0.000578, tokens/sec=1376917.90, grad_norm=0.2833, duration=0.38s
Step 3864: loss=3.3161, lr=0.000578, tokens/sec=1380646.45, grad_norm=0.2958, duration=0.38s
Step 3865: loss=3.3583, lr=0.000578, tokens/sec=1376521.42, grad_norm=0.2823, duration=0.38s
Step 3866: loss=3.2822, lr=0.000578, tokens/sec=1375295.51, grad_norm=0.2686, duration=0.38s
Step 3867: loss=3.3852, lr=0.000577, tokens/sec=1375963.29, grad_norm=0.3149, duration=0.38s
Step 3868: loss=3.3354, lr=0.000577, tokens/sec=1376222.49, grad_norm=0.2761, duration=0.38s
Step 3869: loss=3.2546, lr=0.000577, tokens/sec=1373444.43, grad_norm=0.2945, duration=0.38s
Step 3870: loss=3.3144, lr=0.000577, tokens/sec=1377468.18, grad_norm=0.2737, duration=0.38s
Step 3871: loss=3.3130, lr=0.000577, tokens/sec=1373200.00, grad_norm=0.2562, duration=0.38s
Step 3872: loss=3.3310, lr=0.000577, tokens/sec=1378733.41, grad_norm=0.2791, duration=0.38s
Step 3873: loss=3.3471, lr=0.000577, tokens/sec=1378728.22, grad_norm=0.2690, duration=0.38s
Step 3874: loss=3.3393, lr=0.000577, tokens/sec=1376611.90, grad_norm=0.2798, duration=0.38s
Step 3875: loss=3.3393, lr=0.000577, tokens/sec=1374595.72, grad_norm=0.2935, duration=0.38s
Step 3876: loss=3.3358, lr=0.000577, tokens/sec=1377900.60, grad_norm=0.2547, duration=0.38s
Step 3877: loss=3.2826, lr=0.000577, tokens/sec=1376184.60, grad_norm=0.2621, duration=0.38s
Step 3878: loss=3.2960, lr=0.000577, tokens/sec=1377037.75, grad_norm=0.2754, duration=0.38s
Step 3879: loss=3.3289, lr=0.000577, tokens/sec=1377502.69, grad_norm=0.2695, duration=0.38s
Step 3880: loss=3.3451, lr=0.000577, tokens/sec=1378735.14, grad_norm=0.2675, duration=0.38s
Step 3881: loss=3.2547, lr=0.000577, tokens/sec=1378545.85, grad_norm=0.2748, duration=0.38s
Step 3882: loss=3.3540, lr=0.000577, tokens/sec=1374834.64, grad_norm=0.2690, duration=0.38s
Step 3883: loss=3.2546, lr=0.000577, tokens/sec=1376455.94, grad_norm=0.2544, duration=0.38s
Step 3884: loss=3.2825, lr=0.000577, tokens/sec=1376390.46, grad_norm=0.2587, duration=0.38s
Step 3885: loss=3.2198, lr=0.000577, tokens/sec=1376904.97, grad_norm=0.2807, duration=0.38s
Step 3886: loss=3.2551, lr=0.000577, tokens/sec=1375565.64, grad_norm=0.2698, duration=0.38s
Step 3887: loss=3.2971, lr=0.000577, tokens/sec=1376673.95, grad_norm=0.2826, duration=0.38s
Step 3888: loss=3.1931, lr=0.000577, tokens/sec=1377607.97, grad_norm=0.2634, duration=0.38s
Step 3889: loss=3.2466, lr=0.000577, tokens/sec=1376571.40, grad_norm=0.2985, duration=0.38s
Step 3890: loss=3.2044, lr=0.000577, tokens/sec=1374904.26, grad_norm=0.2554, duration=0.38s
Step 3891: loss=3.2838, lr=0.000577, tokens/sec=1372595.72, grad_norm=0.2691, duration=0.38s
Step 3892: loss=3.2601, lr=0.000577, tokens/sec=1375835.02, grad_norm=0.2520, duration=0.38s
Step 3893: loss=3.2793, lr=0.000577, tokens/sec=1378035.30, grad_norm=0.2657, duration=0.38s
Step 3894: loss=3.3444, lr=0.000577, tokens/sec=1375884.09, grad_norm=0.2995, duration=0.38s
Step 3895: loss=3.3442, lr=0.000577, tokens/sec=1377572.59, grad_norm=0.2996, duration=0.38s
Step 3896: loss=3.3558, lr=0.000577, tokens/sec=1377755.56, grad_norm=0.2708, duration=0.38s
Step 3897: loss=3.3641, lr=0.000577, tokens/sec=1377289.59, grad_norm=0.2861, duration=0.38s
Step 3898: loss=3.3106, lr=0.000577, tokens/sec=1374356.89, grad_norm=0.2965, duration=0.38s
Step 3899: loss=3.3493, lr=0.000577, tokens/sec=1372313.05, grad_norm=0.3259, duration=0.38s
Step 3900/19073 (20.4%), Elapsed time: 1557.86s, Steps per hour: 9012.35, Estimated hours remaining: 1.68
Step 3900: loss=3.3422, lr=0.000577, tokens/sec=1376922.21, grad_norm=0.3221, duration=0.38s
Step 3901: loss=3.3456, lr=0.000577, tokens/sec=1375711.08, grad_norm=0.2847, duration=0.38s
Step 3902: loss=3.3478, lr=0.000577, tokens/sec=1375589.74, grad_norm=0.3110, duration=0.38s
Step 3903: loss=3.3366, lr=0.000577, tokens/sec=1373133.98, grad_norm=0.2843, duration=0.38s
Step 3904: loss=3.3498, lr=0.000577, tokens/sec=1375499.39, grad_norm=0.2890, duration=0.38s
Step 3905: loss=3.2951, lr=0.000577, tokens/sec=1381165.01, grad_norm=0.2768, duration=0.38s
Step 3906: loss=3.3224, lr=0.000577, tokens/sec=1372405.55, grad_norm=0.2681, duration=0.38s
Step 3907: loss=3.3212, lr=0.000577, tokens/sec=1375333.36, grad_norm=0.2588, duration=0.38s
Step 3908: loss=3.3108, lr=0.000577, tokens/sec=1375931.44, grad_norm=0.2902, duration=0.38s
Step 3909: loss=3.2499, lr=0.000577, tokens/sec=1376070.06, grad_norm=0.2736, duration=0.38s
Step 3910: loss=3.3123, lr=0.000577, tokens/sec=1373246.31, grad_norm=0.3065, duration=0.38s
Step 3911: loss=3.2950, lr=0.000577, tokens/sec=1376724.80, grad_norm=0.2704, duration=0.38s
Step 3912: loss=3.3750, lr=0.000577, tokens/sec=1373369.81, grad_norm=0.2851, duration=0.38s
Step 3913: loss=3.3001, lr=0.000577, tokens/sec=1370544.30, grad_norm=0.2790, duration=0.38s
Step 3914: loss=3.3355, lr=0.000577, tokens/sec=1374384.38, grad_norm=0.2876, duration=0.38s
Step 3915: loss=3.2944, lr=0.000577, tokens/sec=1375765.30, grad_norm=0.2728, duration=0.38s
Step 3916: loss=3.3064, lr=0.000577, tokens/sec=1377098.11, grad_norm=0.2804, duration=0.38s
Step 3917: loss=3.3136, lr=0.000577, tokens/sec=1377246.46, grad_norm=0.2609, duration=0.38s
Step 3918: loss=3.3403, lr=0.000577, tokens/sec=1375878.06, grad_norm=0.2752, duration=0.38s
Step 3919: loss=3.3201, lr=0.000577, tokens/sec=1376380.12, grad_norm=0.2762, duration=0.38s
Step 3920: loss=3.3544, lr=0.000577, tokens/sec=1377165.38, grad_norm=0.2976, duration=0.38s
Step 3921: loss=3.3358, lr=0.000577, tokens/sec=1378301.33, grad_norm=0.2572, duration=0.38s
Step 3922: loss=3.3198, lr=0.000577, tokens/sec=1379370.79, grad_norm=0.2880, duration=0.38s
Step 3923: loss=3.3295, lr=0.000577, tokens/sec=1377158.48, grad_norm=0.2960, duration=0.38s
Step 3924: loss=3.3145, lr=0.000577, tokens/sec=1378790.46, grad_norm=0.2980, duration=0.38s
Step 3925: loss=3.3139, lr=0.000577, tokens/sec=1378059.48, grad_norm=0.2966, duration=0.38s
Step 3926: loss=3.3191, lr=0.000577, tokens/sec=1374897.39, grad_norm=0.2622, duration=0.38s
Step 3927: loss=3.2968, lr=0.000577, tokens/sec=1375766.16, grad_norm=0.2745, duration=0.38s
Step 3928: loss=3.2851, lr=0.000577, tokens/sec=1373746.45, grad_norm=0.2642, duration=0.38s
Step 3929: loss=3.2143, lr=0.000577, tokens/sec=1370399.10, grad_norm=0.2555, duration=0.38s
Step 3930: loss=3.2535, lr=0.000577, tokens/sec=1370834.79, grad_norm=0.2886, duration=0.38s
Step 3931: loss=3.1913, lr=0.000577, tokens/sec=1373975.62, grad_norm=0.2610, duration=0.38s
Step 3932: loss=3.2315, lr=0.000576, tokens/sec=1375176.82, grad_norm=0.2676, duration=0.38s
Step 3933: loss=3.2496, lr=0.000576, tokens/sec=1378777.49, grad_norm=0.3038, duration=0.38s
Step 3934: loss=3.2846, lr=0.000576, tokens/sec=1375021.19, grad_norm=0.2737, duration=0.38s
Step 3935: loss=3.2477, lr=0.000576, tokens/sec=1373406.69, grad_norm=0.2901, duration=0.38s
Step 3936: loss=3.2365, lr=0.000576, tokens/sec=1371258.78, grad_norm=0.2715, duration=0.38s
Step 3937: loss=3.2236, lr=0.000576, tokens/sec=1373826.27, grad_norm=0.3040, duration=0.38s
Step 3938: loss=3.2657, lr=0.000576, tokens/sec=1374710.87, grad_norm=0.2939, duration=0.38s
Step 3939: loss=3.3036, lr=0.000576, tokens/sec=1375175.10, grad_norm=0.2765, duration=0.38s
Step 3940: loss=3.3579, lr=0.000576, tokens/sec=1376188.04, grad_norm=0.2664, duration=0.38s
Step 3941: loss=3.3509, lr=0.000576, tokens/sec=1379194.30, grad_norm=0.2653, duration=0.38s
Step 3942: loss=3.3525, lr=0.000576, tokens/sec=1377930.82, grad_norm=0.2513, duration=0.38s
Step 3943: loss=3.3285, lr=0.000576, tokens/sec=1375434.00, grad_norm=0.2749, duration=0.38s
Step 3944: loss=3.3613, lr=0.000576, tokens/sec=1379189.98, grad_norm=0.3068, duration=0.38s
Step 3945: loss=3.2895, lr=0.000576, tokens/sec=1376813.59, grad_norm=0.2790, duration=0.38s
Step 3946: loss=3.3184, lr=0.000576, tokens/sec=1379128.57, grad_norm=0.3055, duration=0.38s
Step 3947: loss=3.3838, lr=0.000576, tokens/sec=1380402.05, grad_norm=0.3986, duration=0.38s
Step 3948: loss=3.4414, lr=0.000576, tokens/sec=1376499.88, grad_norm=0.3419, duration=0.38s
Step 3949: loss=3.3640, lr=0.000576, tokens/sec=1377940.31, grad_norm=0.3313, duration=0.38s
Step 3950: loss=3.3459, lr=0.000576, tokens/sec=1376206.99, grad_norm=0.3356, duration=0.38s
Step 3951: loss=3.3147, lr=0.000576, tokens/sec=1376248.33, grad_norm=0.3014, duration=0.38s
Step 3952: loss=3.3446, lr=0.000576, tokens/sec=1374929.19, grad_norm=0.2928, duration=0.38s
Step 3953: loss=3.2896, lr=0.000576, tokens/sec=1379617.42, grad_norm=0.2737, duration=0.38s
Step 3954: loss=3.2532, lr=0.000576, tokens/sec=1375849.66, grad_norm=0.3046, duration=0.38s
Step 3955: loss=3.3462, lr=0.000576, tokens/sec=1378337.61, grad_norm=0.2611, duration=0.38s
Step 3956: loss=3.2553, lr=0.000576, tokens/sec=1374587.99, grad_norm=0.3267, duration=0.38s
Step 3957: loss=3.3605, lr=0.000576, tokens/sec=1379330.12, grad_norm=0.2710, duration=0.38s
Step 3958: loss=3.3725, lr=0.000576, tokens/sec=1379048.13, grad_norm=0.2796, duration=0.38s
Step 3959: loss=3.2945, lr=0.000576, tokens/sec=1376089.00, grad_norm=0.2896, duration=0.38s
Step 3960: loss=3.2499, lr=0.000576, tokens/sec=1380418.51, grad_norm=0.2792, duration=0.38s
Step 3961: loss=3.3134, lr=0.000576, tokens/sec=1376288.81, grad_norm=0.2595, duration=0.38s
Step 3962: loss=3.3296, lr=0.000576, tokens/sec=1374673.06, grad_norm=0.2607, duration=0.38s
Step 3963: loss=3.2877, lr=0.000576, tokens/sec=1372915.37, grad_norm=0.2679, duration=0.38s
Step 3964: loss=3.1900, lr=0.000576, tokens/sec=1378432.65, grad_norm=0.2616, duration=0.38s
Step 3965: loss=3.2813, lr=0.000576, tokens/sec=1377243.01, grad_norm=0.2642, duration=0.38s
Step 3966: loss=3.3169, lr=0.000576, tokens/sec=1373925.83, grad_norm=0.2797, duration=0.38s
Step 3967: loss=3.3438, lr=0.000576, tokens/sec=1377335.31, grad_norm=0.2612, duration=0.38s
Step 3968: loss=3.3691, lr=0.000576, tokens/sec=1373927.55, grad_norm=0.2633, duration=0.38s
Step 3969: loss=3.2641, lr=0.000576, tokens/sec=1376590.36, grad_norm=0.2523, duration=0.38s
Step 3970: loss=3.2948, lr=0.000576, tokens/sec=1372064.74, grad_norm=0.2673, duration=0.38s
Step 3971: loss=3.2788, lr=0.000576, tokens/sec=1374003.10, grad_norm=0.2443, duration=0.38s
Step 3972: loss=3.2525, lr=0.000576, tokens/sec=1369967.11, grad_norm=0.2817, duration=0.38s
Step 3973: loss=3.2526, lr=0.000576, tokens/sec=1375943.49, grad_norm=0.2895, duration=0.38s
Step 3974: loss=3.2551, lr=0.000576, tokens/sec=1372629.14, grad_norm=0.3047, duration=0.38s
Step 3975: loss=3.2338, lr=0.000576, tokens/sec=1372925.65, grad_norm=0.2768, duration=0.38s
Step 3976: loss=3.2578, lr=0.000576, tokens/sec=1375551.88, grad_norm=0.2738, duration=0.38s
Step 3977: loss=3.3362, lr=0.000576, tokens/sec=1370869.83, grad_norm=0.2830, duration=0.38s
Step 3978: loss=3.2581, lr=0.000576, tokens/sec=1370893.76, grad_norm=0.2862, duration=0.38s
Step 3979: loss=3.2308, lr=0.000576, tokens/sec=1374070.06, grad_norm=0.2535, duration=0.38s
Step 3980: loss=3.2246, lr=0.000576, tokens/sec=1373867.47, grad_norm=0.2738, duration=0.38s
Step 3981: loss=3.2047, lr=0.000576, tokens/sec=1371554.70, grad_norm=0.2895, duration=0.38s
Step 3982: loss=3.2812, lr=0.000576, tokens/sec=1371460.61, grad_norm=0.2525, duration=0.38s
Step 3983: loss=3.2558, lr=0.000576, tokens/sec=1372314.77, grad_norm=0.2663, duration=0.38s
Step 3984: loss=3.3059, lr=0.000576, tokens/sec=1375995.15, grad_norm=0.3090, duration=0.38s
Step 3985: loss=3.3156, lr=0.000576, tokens/sec=1376786.00, grad_norm=0.2796, duration=0.38s
Step 3986: loss=3.3284, lr=0.000576, tokens/sec=1375705.05, grad_norm=0.3049, duration=0.38s
Step 3987: loss=3.3329, lr=0.000576, tokens/sec=1373341.50, grad_norm=0.3305, duration=0.38s
Step 3988: loss=3.3143, lr=0.000576, tokens/sec=1375901.31, grad_norm=0.3828, duration=0.38s
Step 3989: loss=3.3527, lr=0.000576, tokens/sec=1373471.88, grad_norm=0.3926, duration=0.38s
Step 3990: loss=3.3134, lr=0.000576, tokens/sec=1379548.18, grad_norm=0.3365, duration=0.38s
Step 3991: loss=3.3643, lr=0.000576, tokens/sec=1374538.16, grad_norm=0.3046, duration=0.38s
Step 3992: loss=3.3465, lr=0.000576, tokens/sec=1374560.50, grad_norm=0.3379, duration=0.38s
Step 3993: loss=3.4071, lr=0.000576, tokens/sec=1378109.57, grad_norm=0.2604, duration=0.38s
Step 3994: loss=3.3340, lr=0.000576, tokens/sec=1377216.27, grad_norm=0.3056, duration=0.38s
Step 3995: loss=3.3654, lr=0.000576, tokens/sec=1375194.88, grad_norm=0.2954, duration=0.38s
Step 3996: loss=3.4524, lr=0.000576, tokens/sec=1377223.17, grad_norm=0.3072, duration=0.38s
Step 3997: loss=3.3377, lr=0.000575, tokens/sec=1381362.83, grad_norm=0.2870, duration=0.38s
Step 3998: loss=3.3546, lr=0.000575, tokens/sec=1376255.22, grad_norm=0.2744, duration=0.38s
Step 3999: loss=3.3335, lr=0.000575, tokens/sec=1376784.28, grad_norm=0.2651, duration=0.38s
Step 4000/19073 (21.0%), Elapsed time: 1596.06s, Steps per hour: 9022.19, Estimated hours remaining: 1.67
Validation loss at step 4000: 3.698455333709717
Step 4000: loss=3.3694, lr=0.000575, tokens/sec=156937.62, grad_norm=0.2886, duration=3.34s
Step 4001: loss=3.3428, lr=0.000575, tokens/sec=1373881.20, grad_norm=0.2552, duration=0.38s
Step 4002: loss=3.3319, lr=0.000575, tokens/sec=1378680.68, grad_norm=0.2877, duration=0.38s
Step 4003: loss=3.2763, lr=0.000575, tokens/sec=1378468.08, grad_norm=0.2988, duration=0.38s
Step 4004: loss=3.3046, lr=0.000575, tokens/sec=1376316.38, grad_norm=0.2852, duration=0.38s
Step 4005: loss=3.3378, lr=0.000575, tokens/sec=1374933.49, grad_norm=0.2932, duration=0.38s
Step 4006: loss=3.2605, lr=0.000575, tokens/sec=1376967.91, grad_norm=0.3445, duration=0.38s
Step 4007: loss=3.3608, lr=0.000575, tokens/sec=1378024.94, grad_norm=0.3207, duration=0.38s
Step 4008: loss=3.2740, lr=0.000575, tokens/sec=1375492.51, grad_norm=0.2951, duration=0.38s
Step 4009: loss=3.2084, lr=0.000575, tokens/sec=1375306.69, grad_norm=0.4015, duration=0.38s
Step 4010: loss=3.3057, lr=0.000575, tokens/sec=1378519.06, grad_norm=0.2822, duration=0.38s
Step 4011: loss=3.3075, lr=0.000575, tokens/sec=1379445.20, grad_norm=0.2592, duration=0.38s
Step 4012: loss=3.3340, lr=0.000575, tokens/sec=1375368.63, grad_norm=0.2837, duration=0.38s
Step 4013: loss=3.2249, lr=0.000575, tokens/sec=1375192.30, grad_norm=0.2668, duration=0.38s
Step 4014: loss=3.3284, lr=0.000575, tokens/sec=1371900.39, grad_norm=0.2589, duration=0.38s
Step 4015: loss=3.3447, lr=0.000575, tokens/sec=1376302.60, grad_norm=0.2662, duration=0.38s
Step 4016: loss=3.2864, lr=0.000575, tokens/sec=1379945.54, grad_norm=0.2645, duration=0.38s
Step 4017: loss=3.3166, lr=0.000575, tokens/sec=1377880.74, grad_norm=0.2640, duration=0.38s
Step 4018: loss=3.3178, lr=0.000575, tokens/sec=1375591.46, grad_norm=0.2948, duration=0.38s
Step 4019: loss=3.3027, lr=0.000575, tokens/sec=1374273.58, grad_norm=0.2945, duration=0.38s
Step 4020: loss=3.3248, lr=0.000575, tokens/sec=1373831.42, grad_norm=0.2683, duration=0.38s
Step 4021: loss=3.3321, lr=0.000575, tokens/sec=1376685.16, grad_norm=0.2592, duration=0.38s
Step 4022: loss=3.3432, lr=0.000575, tokens/sec=1373700.97, grad_norm=0.3369, duration=0.38s
Step 4023: loss=3.3750, lr=0.000575, tokens/sec=1373963.61, grad_norm=0.2571, duration=0.38s
Step 4024: loss=3.2867, lr=0.000575, tokens/sec=1374472.86, grad_norm=0.2705, duration=0.38s
Step 4025: loss=3.2351, lr=0.000575, tokens/sec=1373139.98, grad_norm=0.2527, duration=0.38s
Step 4026: loss=3.3005, lr=0.000575, tokens/sec=1368383.19, grad_norm=0.2534, duration=0.38s
Step 4027: loss=3.2183, lr=0.000575, tokens/sec=1379481.55, grad_norm=0.2598, duration=0.38s
Step 4028: loss=3.2211, lr=0.000575, tokens/sec=1373533.65, grad_norm=0.2824, duration=0.38s
Step 4029: loss=3.2399, lr=0.000575, tokens/sec=1372457.80, grad_norm=0.2670, duration=0.38s
Step 4030: loss=3.2508, lr=0.000575, tokens/sec=1374860.42, grad_norm=0.2768, duration=0.38s
Step 4031: loss=3.2853, lr=0.000575, tokens/sec=1372755.96, grad_norm=0.2873, duration=0.38s
Step 4032: loss=3.2484, lr=0.000575, tokens/sec=1378105.25, grad_norm=0.2530, duration=0.38s
Step 4033: loss=3.2781, lr=0.000575, tokens/sec=1373849.44, grad_norm=0.2689, duration=0.38s
Step 4034: loss=3.2808, lr=0.000575, tokens/sec=1375702.47, grad_norm=0.2713, duration=0.38s
Step 4035: loss=3.2315, lr=0.000575, tokens/sec=1371363.11, grad_norm=0.2513, duration=0.38s
Step 4036: loss=3.2700, lr=0.000575, tokens/sec=1374076.07, grad_norm=0.2421, duration=0.38s
Step 4037: loss=3.3496, lr=0.000575, tokens/sec=1373304.62, grad_norm=0.2557, duration=0.38s
Step 4038: loss=3.3728, lr=0.000575, tokens/sec=1375451.21, grad_norm=0.2955, duration=0.38s
Step 4039: loss=3.3369, lr=0.000575, tokens/sec=1375404.76, grad_norm=0.2798, duration=0.38s
Step 4040: loss=3.3306, lr=0.000575, tokens/sec=1372298.49, grad_norm=0.2706, duration=0.38s
Step 4041: loss=3.3584, lr=0.000575, tokens/sec=1377455.23, grad_norm=0.2684, duration=0.38s
Step 4042: loss=3.3599, lr=0.000575, tokens/sec=1376690.33, grad_norm=0.2978, duration=0.38s
Step 4043: loss=3.3354, lr=0.000575, tokens/sec=1376308.63, grad_norm=0.2552, duration=0.38s
Step 4044: loss=3.2888, lr=0.000575, tokens/sec=1378944.36, grad_norm=0.2726, duration=0.38s
Step 4045: loss=3.3010, lr=0.000575, tokens/sec=1375785.10, grad_norm=0.2747, duration=0.38s
Step 4046: loss=3.3465, lr=0.000575, tokens/sec=1378083.66, grad_norm=0.2697, duration=0.38s
Step 4047: loss=3.2961, lr=0.000575, tokens/sec=1372742.24, grad_norm=0.2829, duration=0.38s
Step 4048: loss=3.2841, lr=0.000575, tokens/sec=1380558.04, grad_norm=0.2693, duration=0.38s
Step 4049: loss=3.2807, lr=0.000575, tokens/sec=1376444.74, grad_norm=0.2529, duration=0.38s
Step 4050: loss=3.3002, lr=0.000575, tokens/sec=1375604.37, grad_norm=0.2616, duration=0.38s
Step 4051: loss=3.2268, lr=0.000575, tokens/sec=1376863.59, grad_norm=0.2812, duration=0.38s
Step 4052: loss=3.2372, lr=0.000575, tokens/sec=1377184.36, grad_norm=0.2556, duration=0.38s
Step 4053: loss=3.2996, lr=0.000575, tokens/sec=1375464.12, grad_norm=0.2866, duration=0.38s
Step 4054: loss=3.2858, lr=0.000575, tokens/sec=1377745.21, grad_norm=0.2703, duration=0.38s
Step 4055: loss=3.3319, lr=0.000575, tokens/sec=1379419.24, grad_norm=0.2895, duration=0.38s
Step 4056: loss=3.2743, lr=0.000575, tokens/sec=1379590.59, grad_norm=0.3041, duration=0.38s
Step 4057: loss=3.3598, lr=0.000575, tokens/sec=1375031.50, grad_norm=0.2657, duration=0.38s
Step 4058: loss=3.2580, lr=0.000575, tokens/sec=1373707.83, grad_norm=0.2936, duration=0.38s
Step 4059: loss=3.2899, lr=0.000575, tokens/sec=1373743.88, grad_norm=0.2942, duration=0.38s
Step 4060: loss=3.2911, lr=0.000574, tokens/sec=1380561.51, grad_norm=0.3032, duration=0.38s
Step 4061: loss=3.3355, lr=0.000574, tokens/sec=1375708.50, grad_norm=0.2806, duration=0.38s
Step 4062: loss=3.3108, lr=0.000574, tokens/sec=1377706.36, grad_norm=0.3325, duration=0.38s
Step 4063: loss=3.3286, lr=0.000574, tokens/sec=1376549.00, grad_norm=0.2772, duration=0.38s
Step 4064: loss=3.3102, lr=0.000574, tokens/sec=1375018.61, grad_norm=0.2882, duration=0.38s
Step 4065: loss=3.3186, lr=0.000574, tokens/sec=1373423.85, grad_norm=0.3034, duration=0.38s
Step 4066: loss=3.2991, lr=0.000574, tokens/sec=1376289.67, grad_norm=0.2651, duration=0.38s
Step 4067: loss=3.2375, lr=0.000574, tokens/sec=1374322.54, grad_norm=0.2769, duration=0.38s
Step 4068: loss=3.3224, lr=0.000574, tokens/sec=1375736.90, grad_norm=0.2870, duration=0.38s
Step 4069: loss=3.3114, lr=0.000574, tokens/sec=1376438.71, grad_norm=0.2621, duration=0.38s
Step 4070: loss=3.3106, lr=0.000574, tokens/sec=1378704.88, grad_norm=0.2515, duration=0.38s
Step 4071: loss=3.2669, lr=0.000574, tokens/sec=1377681.33, grad_norm=0.2882, duration=0.38s
Step 4072: loss=3.2931, lr=0.000574, tokens/sec=1370862.99, grad_norm=0.2868, duration=0.38s
Step 4073: loss=3.2546, lr=0.000574, tokens/sec=1374574.24, grad_norm=0.2836, duration=0.38s
Step 4074: loss=3.2498, lr=0.000574, tokens/sec=1377610.56, grad_norm=0.2633, duration=0.38s
Step 4075: loss=3.1955, lr=0.000574, tokens/sec=1375986.54, grad_norm=0.2721, duration=0.38s
Step 4076: loss=3.2507, lr=0.000574, tokens/sec=1377255.09, grad_norm=0.2546, duration=0.38s
Step 4077: loss=3.2198, lr=0.000574, tokens/sec=1376651.55, grad_norm=0.2473, duration=0.38s
Step 4078: loss=3.1962, lr=0.000574, tokens/sec=1376238.86, grad_norm=0.2746, duration=0.38s
Step 4079: loss=3.2014, lr=0.000574, tokens/sec=1374862.14, grad_norm=0.2526, duration=0.38s
Step 4080: loss=3.2097, lr=0.000574, tokens/sec=1377900.60, grad_norm=0.2736, duration=0.38s
Step 4081: loss=3.2578, lr=0.000574, tokens/sec=1375802.31, grad_norm=0.2690, duration=0.38s
Step 4082: loss=3.2200, lr=0.000574, tokens/sec=1378204.58, grad_norm=0.2782, duration=0.38s
Step 4083: loss=3.2751, lr=0.000574, tokens/sec=1375837.60, grad_norm=0.2900, duration=0.38s
Step 4084: loss=3.3156, lr=0.000574, tokens/sec=1375071.91, grad_norm=0.3025, duration=0.38s
Step 4085: loss=3.3272, lr=0.000574, tokens/sec=1373806.53, grad_norm=0.2566, duration=0.38s
Step 4086: loss=3.3484, lr=0.000574, tokens/sec=1374878.48, grad_norm=0.3020, duration=0.38s
Step 4087: loss=3.3432, lr=0.000574, tokens/sec=1378820.72, grad_norm=0.2804, duration=0.38s
Step 4088: loss=3.2775, lr=0.000574, tokens/sec=1371687.31, grad_norm=0.3058, duration=0.38s
Step 4089: loss=3.3513, lr=0.000574, tokens/sec=1378080.21, grad_norm=0.3127, duration=0.38s
Step 4090: loss=3.2739, lr=0.000574, tokens/sec=1376538.66, grad_norm=0.3223, duration=0.38s
Step 4091: loss=3.3339, lr=0.000574, tokens/sec=1377895.42, grad_norm=0.2818, duration=0.38s
Step 4092: loss=3.3318, lr=0.000574, tokens/sec=1374855.27, grad_norm=0.3464, duration=0.38s
Step 4093: loss=3.3138, lr=0.000574, tokens/sec=1378202.85, grad_norm=0.2911, duration=0.38s
Step 4094: loss=3.3128, lr=0.000574, tokens/sec=1377926.50, grad_norm=0.3150, duration=0.38s
Step 4095: loss=3.2912, lr=0.000574, tokens/sec=1376130.34, grad_norm=0.2981, duration=0.38s
Step 4096: loss=3.3170, lr=0.000574, tokens/sec=1377143.82, grad_norm=0.2967, duration=0.38s
Step 4097: loss=3.2688, lr=0.000574, tokens/sec=1376066.62, grad_norm=0.2754, duration=0.38s
Step 4098: loss=3.2851, lr=0.000574, tokens/sec=1375648.25, grad_norm=0.2651, duration=0.38s
Step 4099: loss=3.2472, lr=0.000574, tokens/sec=1378468.08, grad_norm=0.2860, duration=0.38s
Step 4100/19073 (21.5%), Elapsed time: 1637.22s, Steps per hour: 9015.31, Estimated hours remaining: 1.66
Step 4100: loss=3.2877, lr=0.000574, tokens/sec=1374611.19, grad_norm=0.2857, duration=0.38s
Step 4101: loss=3.3476, lr=0.000574, tokens/sec=1375861.71, grad_norm=0.2762, duration=0.38s
Step 4102: loss=3.3331, lr=0.000574, tokens/sec=1376911.00, grad_norm=0.3081, duration=0.38s
Step 4103: loss=3.2426, lr=0.000574, tokens/sec=1372319.05, grad_norm=0.2976, duration=0.38s
Step 4104: loss=3.3237, lr=0.000574, tokens/sec=1373464.16, grad_norm=0.2862, duration=0.38s
Step 4105: loss=3.2421, lr=0.000574, tokens/sec=1374363.77, grad_norm=0.2659, duration=0.38s
Step 4106: loss=3.3266, lr=0.000574, tokens/sec=1375551.02, grad_norm=0.3053, duration=0.38s
Step 4107: loss=3.3043, lr=0.000574, tokens/sec=1373617.73, grad_norm=0.2580, duration=0.38s
Step 4108: loss=3.3230, lr=0.000574, tokens/sec=1372941.94, grad_norm=0.3074, duration=0.38s
Step 4109: loss=3.3326, lr=0.000574, tokens/sec=1378595.98, grad_norm=0.2936, duration=0.38s
Step 4110: loss=3.3186, lr=0.000574, tokens/sec=1377712.40, grad_norm=0.2740, duration=0.38s
Step 4111: loss=3.2821, lr=0.000574, tokens/sec=1373610.87, grad_norm=0.2526, duration=0.38s
Step 4112: loss=3.3343, lr=0.000574, tokens/sec=1377347.39, grad_norm=0.2716, duration=0.38s
Step 4113: loss=3.2756, lr=0.000574, tokens/sec=1376039.06, grad_norm=0.2655, duration=0.38s
Step 4114: loss=3.2865, lr=0.000574, tokens/sec=1374833.78, grad_norm=0.2965, duration=0.38s
Step 4115: loss=3.3199, lr=0.000574, tokens/sec=1374985.93, grad_norm=0.2816, duration=0.38s
Step 4116: loss=3.2881, lr=0.000574, tokens/sec=1372708.82, grad_norm=0.2850, duration=0.38s
Step 4117: loss=3.2703, lr=0.000574, tokens/sec=1372473.22, grad_norm=0.2834, duration=0.38s
Step 4118: loss=3.2493, lr=0.000574, tokens/sec=1373622.02, grad_norm=0.2598, duration=0.38s
Step 4119: loss=3.1904, lr=0.000574, tokens/sec=1375200.04, grad_norm=0.2875, duration=0.38s
Step 4120: loss=3.2502, lr=0.000574, tokens/sec=1373917.25, grad_norm=0.2880, duration=0.38s
Step 4121: loss=3.1718, lr=0.000574, tokens/sec=1375432.28, grad_norm=0.2738, duration=0.38s
Step 4122: loss=3.2352, lr=0.000573, tokens/sec=1377710.68, grad_norm=0.2712, duration=0.38s
Step 4123: loss=3.1977, lr=0.000573, tokens/sec=1376730.84, grad_norm=0.2793, duration=0.38s
Step 4124: loss=3.2600, lr=0.000573, tokens/sec=1375473.58, grad_norm=0.2785, duration=0.38s
Step 4125: loss=3.2445, lr=0.000573, tokens/sec=1370384.59, grad_norm=0.3094, duration=0.38s
Step 4126: loss=3.2119, lr=0.000573, tokens/sec=1375132.11, grad_norm=0.2513, duration=0.38s
Step 4127: loss=3.2140, lr=0.000573, tokens/sec=1377454.37, grad_norm=0.2806, duration=0.38s
Step 4128: loss=3.2415, lr=0.000573, tokens/sec=1369653.10, grad_norm=0.2555, duration=0.38s
Step 4129: loss=3.3048, lr=0.000573, tokens/sec=1372068.17, grad_norm=0.2971, duration=0.38s
Step 4130: loss=3.3206, lr=0.000573, tokens/sec=1375412.50, grad_norm=0.3083, duration=0.38s
Step 4131: loss=3.3224, lr=0.000573, tokens/sec=1374366.34, grad_norm=0.2985, duration=0.38s
Step 4132: loss=3.3371, lr=0.000573, tokens/sec=1374414.45, grad_norm=0.3142, duration=0.38s
Step 4133: loss=3.3135, lr=0.000573, tokens/sec=1377259.40, grad_norm=0.2992, duration=0.38s
Step 4134: loss=3.3232, lr=0.000573, tokens/sec=1376058.87, grad_norm=0.3008, duration=0.38s
Step 4135: loss=3.2729, lr=0.000573, tokens/sec=1374319.96, grad_norm=0.2662, duration=0.38s
Step 4136: loss=3.3096, lr=0.000573, tokens/sec=1373522.50, grad_norm=0.3247, duration=0.38s
Step 4137: loss=3.3582, lr=0.000573, tokens/sec=1374486.61, grad_norm=0.3365, duration=0.38s
Step 4138: loss=3.4094, lr=0.000573, tokens/sec=1371133.95, grad_norm=0.3088, duration=0.38s
Step 4139: loss=3.3366, lr=0.000573, tokens/sec=1374213.46, grad_norm=0.3611, duration=0.38s
Step 4140: loss=3.3556, lr=0.000573, tokens/sec=1376864.45, grad_norm=0.3134, duration=0.38s
Step 4141: loss=3.2915, lr=0.000573, tokens/sec=1367112.23, grad_norm=0.2847, duration=0.38s
Step 4142: loss=3.2986, lr=0.000573, tokens/sec=1373731.86, grad_norm=0.2812, duration=0.38s
Step 4143: loss=3.2616, lr=0.000573, tokens/sec=1376184.60, grad_norm=0.2994, duration=0.38s
Step 4144: loss=3.2400, lr=0.000573, tokens/sec=1365460.28, grad_norm=0.2856, duration=0.38s
Step 4145: loss=3.3187, lr=0.000573, tokens/sec=1370233.45, grad_norm=0.2777, duration=0.38s
Step 4146: loss=3.2507, lr=0.000573, tokens/sec=1365992.10, grad_norm=0.2728, duration=0.38s
Step 4147: loss=3.3208, lr=0.000573, tokens/sec=1374341.43, grad_norm=0.2873, duration=0.38s
Step 4148: loss=3.3324, lr=0.000573, tokens/sec=1373650.34, grad_norm=0.3033, duration=0.38s
Step 4149: loss=3.2903, lr=0.000573, tokens/sec=1377736.57, grad_norm=0.2688, duration=0.38s
Step 4150: loss=3.2227, lr=0.000573, tokens/sec=1375187.14, grad_norm=0.2600, duration=0.38s
Step 4151: loss=3.2980, lr=0.000573, tokens/sec=1373848.58, grad_norm=0.2607, duration=0.38s
Step 4152: loss=3.3070, lr=0.000573, tokens/sec=1379754.19, grad_norm=0.2517, duration=0.38s
Step 4153: loss=3.2400, lr=0.000573, tokens/sec=1375744.64, grad_norm=0.2564, duration=0.38s
Step 4154: loss=3.1910, lr=0.000573, tokens/sec=1379363.00, grad_norm=0.2619, duration=0.38s
Step 4155: loss=3.3019, lr=0.000573, tokens/sec=1375091.69, grad_norm=0.2513, duration=0.38s
Step 4156: loss=3.3014, lr=0.000573, tokens/sec=1377168.83, grad_norm=0.2669, duration=0.38s
Step 4157: loss=3.3129, lr=0.000573, tokens/sec=1375837.60, grad_norm=0.2602, duration=0.38s
Step 4158: loss=3.3251, lr=0.000573, tokens/sec=1375129.53, grad_norm=0.2822, duration=0.38s
Step 4159: loss=3.2634, lr=0.000573, tokens/sec=1372045.05, grad_norm=0.2510, duration=0.38s
Step 4160: loss=3.2574, lr=0.000573, tokens/sec=1373625.46, grad_norm=0.2693, duration=0.38s
Step 4161: loss=3.2818, lr=0.000573, tokens/sec=1375181.98, grad_norm=0.2771, duration=0.38s
Step 4162: loss=3.1884, lr=0.000573, tokens/sec=1375483.90, grad_norm=0.2695, duration=0.38s
Step 4163: loss=3.2704, lr=0.000573, tokens/sec=1376025.28, grad_norm=0.2645, duration=0.38s
Step 4164: loss=3.2213, lr=0.000573, tokens/sec=1376797.21, grad_norm=0.2685, duration=0.38s
Step 4165: loss=3.2373, lr=0.000573, tokens/sec=1373352.65, grad_norm=0.2745, duration=0.38s
Step 4166: loss=3.2468, lr=0.000573, tokens/sec=1375076.21, grad_norm=0.3028, duration=0.38s
Step 4167: loss=3.2911, lr=0.000573, tokens/sec=1373000.23, grad_norm=0.2682, duration=0.38s
Step 4168: loss=3.2345, lr=0.000573, tokens/sec=1372286.51, grad_norm=0.3092, duration=0.38s
Step 4169: loss=3.2376, lr=0.000573, tokens/sec=1374596.58, grad_norm=0.2567, duration=0.38s
Step 4170: loss=3.1432, lr=0.000573, tokens/sec=1375092.55, grad_norm=0.2678, duration=0.38s
Step 4171: loss=3.2326, lr=0.000573, tokens/sec=1375779.93, grad_norm=0.2736, duration=0.38s
Step 4172: loss=3.2517, lr=0.000573, tokens/sec=1372644.56, grad_norm=0.2652, duration=0.38s
Step 4173: loss=3.2630, lr=0.000573, tokens/sec=1376813.59, grad_norm=0.2714, duration=0.38s
Step 4174: loss=3.2429, lr=0.000573, tokens/sec=1372936.80, grad_norm=0.2587, duration=0.38s
Step 4175: loss=3.3052, lr=0.000573, tokens/sec=1375422.82, grad_norm=0.2799, duration=0.38s
Step 4176: loss=3.2981, lr=0.000573, tokens/sec=1376081.25, grad_norm=0.3102, duration=0.38s
Step 4177: loss=3.3234, lr=0.000573, tokens/sec=1373415.27, grad_norm=0.3047, duration=0.38s
Step 4178: loss=3.2935, lr=0.000573, tokens/sec=1371809.68, grad_norm=0.3018, duration=0.38s
Step 4179: loss=3.3062, lr=0.000573, tokens/sec=1374161.94, grad_norm=0.2745, duration=0.38s
Step 4180: loss=3.3012, lr=0.000573, tokens/sec=1373586.84, grad_norm=0.2893, duration=0.38s
Step 4181: loss=3.3162, lr=0.000573, tokens/sec=1372290.79, grad_norm=0.2747, duration=0.38s
Step 4182: loss=3.3259, lr=0.000572, tokens/sec=1371926.07, grad_norm=0.2820, duration=0.38s
Step 4183: loss=3.3748, lr=0.000572, tokens/sec=1372228.28, grad_norm=0.2677, duration=0.38s
Step 4184: loss=3.3207, lr=0.000572, tokens/sec=1370631.43, grad_norm=0.2888, duration=0.38s
Step 4185: loss=3.3757, lr=0.000572, tokens/sec=1374774.47, grad_norm=0.2718, duration=0.38s
Step 4186: loss=3.3902, lr=0.000572, tokens/sec=1373706.97, grad_norm=0.2983, duration=0.38s
Step 4187: loss=3.3105, lr=0.000572, tokens/sec=1372900.80, grad_norm=0.2707, duration=0.38s
Step 4188: loss=3.3432, lr=0.000572, tokens/sec=1377961.90, grad_norm=0.2966, duration=0.38s
Step 4189: loss=3.3268, lr=0.000572, tokens/sec=1377568.27, grad_norm=0.2659, duration=0.38s
Step 4190: loss=3.3647, lr=0.000572, tokens/sec=1375915.08, grad_norm=0.2784, duration=0.38s
Step 4191: loss=3.2910, lr=0.000572, tokens/sec=1376555.03, grad_norm=0.2724, duration=0.38s
Step 4192: loss=3.3219, lr=0.000572, tokens/sec=1374490.90, grad_norm=0.2958, duration=0.38s
Step 4193: loss=3.2255, lr=0.000572, tokens/sec=1375006.57, grad_norm=0.2780, duration=0.38s
Step 4194: loss=3.3081, lr=0.000572, tokens/sec=1374398.12, grad_norm=0.2713, duration=0.38s
Step 4195: loss=3.2775, lr=0.000572, tokens/sec=1374280.45, grad_norm=0.2881, duration=0.38s
Step 4196: loss=3.2744, lr=0.000572, tokens/sec=1373435.86, grad_norm=0.3311, duration=0.38s
Step 4197: loss=3.3239, lr=0.000572, tokens/sec=1370960.42, grad_norm=0.2880, duration=0.38s
Step 4198: loss=3.2318, lr=0.000572, tokens/sec=1373052.52, grad_norm=0.2965, duration=0.38s
Step 4199: loss=3.2289, lr=0.000572, tokens/sec=1374245.24, grad_norm=0.4462, duration=0.38s
Step 4200/19073 (22.0%), Elapsed time: 1675.46s, Steps per hour: 9024.37, Estimated hours remaining: 1.65
Step 4200: loss=3.2647, lr=0.000572, tokens/sec=1372087.86, grad_norm=0.2732, duration=0.38s
Step 4201: loss=3.2292, lr=0.000572, tokens/sec=1375486.49, grad_norm=0.2832, duration=0.38s
Step 4202: loss=3.3565, lr=0.000572, tokens/sec=1375354.86, grad_norm=0.2913, duration=0.38s
Step 4203: loss=3.2126, lr=0.000572, tokens/sec=1374701.42, grad_norm=0.3029, duration=0.38s
Step 4204: loss=3.3468, lr=0.000572, tokens/sec=1377779.73, grad_norm=0.2917, duration=0.38s
Step 4205: loss=3.3115, lr=0.000572, tokens/sec=1376773.94, grad_norm=0.2799, duration=0.38s
Step 4206: loss=3.2692, lr=0.000572, tokens/sec=1375274.87, grad_norm=0.2635, duration=0.38s
Step 4207: loss=3.2726, lr=0.000572, tokens/sec=1375071.91, grad_norm=0.2849, duration=0.38s
Step 4208: loss=3.3056, lr=0.000572, tokens/sec=1376242.30, grad_norm=0.2643, duration=0.38s
Step 4209: loss=3.3162, lr=0.000572, tokens/sec=1372516.91, grad_norm=0.2855, duration=0.38s
Step 4210: loss=3.2871, lr=0.000572, tokens/sec=1375374.65, grad_norm=0.2641, duration=0.38s
Step 4211: loss=3.2942, lr=0.000572, tokens/sec=1374610.33, grad_norm=0.2728, duration=0.38s
Step 4212: loss=3.3270, lr=0.000572, tokens/sec=1373036.24, grad_norm=0.3313, duration=0.38s
Step 4213: loss=3.3397, lr=0.000572, tokens/sec=1372888.80, grad_norm=0.2740, duration=0.38s
Step 4214: loss=3.2703, lr=0.000572, tokens/sec=1374368.06, grad_norm=0.2557, duration=0.38s
Step 4215: loss=3.2218, lr=0.000572, tokens/sec=1374700.56, grad_norm=0.2824, duration=0.38s
Step 4216: loss=3.2544, lr=0.000572, tokens/sec=1370856.15, grad_norm=0.2455, duration=0.38s
Step 4217: loss=3.2059, lr=0.000572, tokens/sec=1368184.82, grad_norm=0.2780, duration=0.38s
Step 4218: loss=3.1930, lr=0.000572, tokens/sec=1376837.72, grad_norm=0.2394, duration=0.38s
Step 4219: loss=3.2204, lr=0.000572, tokens/sec=1377633.00, grad_norm=0.2583, duration=0.38s
Step 4220: loss=3.2550, lr=0.000572, tokens/sec=1373803.09, grad_norm=0.2548, duration=0.38s
Step 4221: loss=3.2362, lr=0.000572, tokens/sec=1374301.06, grad_norm=0.2811, duration=0.38s
Step 4222: loss=3.2390, lr=0.000572, tokens/sec=1373882.06, grad_norm=0.2607, duration=0.38s
Step 4223: loss=3.2837, lr=0.000572, tokens/sec=1372554.60, grad_norm=0.2701, duration=0.38s
Step 4224: loss=3.2150, lr=0.000572, tokens/sec=1372824.51, grad_norm=0.2628, duration=0.38s
Step 4225: loss=3.2388, lr=0.000572, tokens/sec=1375979.65, grad_norm=0.2604, duration=0.38s
Step 4226: loss=3.2360, lr=0.000572, tokens/sec=1376275.89, grad_norm=0.2514, duration=0.38s
Step 4227: loss=3.3062, lr=0.000572, tokens/sec=1378433.51, grad_norm=0.2509, duration=0.38s
Step 4228: loss=3.3645, lr=0.000572, tokens/sec=1370262.48, grad_norm=0.2696, duration=0.38s
Step 4229: loss=3.3087, lr=0.000572, tokens/sec=1372814.23, grad_norm=0.2683, duration=0.38s
Step 4230: loss=3.3186, lr=0.000572, tokens/sec=1370926.23, grad_norm=0.2611, duration=0.38s
Step 4231: loss=3.3435, lr=0.000572, tokens/sec=1372858.80, grad_norm=0.2836, duration=0.38s
Step 4232: loss=3.3282, lr=0.000572, tokens/sec=1373422.13, grad_norm=0.2907, duration=0.38s
Step 4233: loss=3.3090, lr=0.000572, tokens/sec=1376236.27, grad_norm=0.2751, duration=0.38s
Step 4234: loss=3.2769, lr=0.000572, tokens/sec=1377302.53, grad_norm=0.2936, duration=0.38s
Step 4235: loss=3.3070, lr=0.000572, tokens/sec=1370904.01, grad_norm=0.2534, duration=0.38s
Step 4236: loss=3.2868, lr=0.000572, tokens/sec=1373428.99, grad_norm=0.2860, duration=0.38s
Step 4237: loss=3.2766, lr=0.000572, tokens/sec=1377426.76, grad_norm=0.3114, duration=0.38s
Step 4238: loss=3.2674, lr=0.000572, tokens/sec=1374530.42, grad_norm=0.2915, duration=0.38s
Step 4239: loss=3.2744, lr=0.000572, tokens/sec=1375959.85, grad_norm=0.2942, duration=0.38s
Step 4240: loss=3.2814, lr=0.000572, tokens/sec=1376079.53, grad_norm=0.2825, duration=0.38s
Step 4241: loss=3.1769, lr=0.000572, tokens/sec=1375894.42, grad_norm=0.2946, duration=0.38s
Step 4242: loss=3.2375, lr=0.000571, tokens/sec=1376661.03, grad_norm=0.2711, duration=0.38s
Step 4243: loss=3.2794, lr=0.000571, tokens/sec=1375278.31, grad_norm=0.3061, duration=0.38s
Step 4244: loss=3.2627, lr=0.000571, tokens/sec=1374994.53, grad_norm=0.2599, duration=0.38s
Step 4245: loss=3.3260, lr=0.000571, tokens/sec=1375183.70, grad_norm=0.2914, duration=0.38s
Step 4246: loss=3.2533, lr=0.000571, tokens/sec=1373362.09, grad_norm=0.2712, duration=0.38s
Step 4247: loss=3.2828, lr=0.000571, tokens/sec=1375256.81, grad_norm=0.2541, duration=0.38s
Step 4248: loss=3.2927, lr=0.000571, tokens/sec=1372887.08, grad_norm=0.3016, duration=0.38s
Step 4249: loss=3.2660, lr=0.000571, tokens/sec=1379462.51, grad_norm=0.3018, duration=0.38s
Validation loss at step 4250: 3.702240467071533
Step 4250: loss=3.3154, lr=0.000571, tokens/sec=156914.92, grad_norm=0.3053, duration=3.34s
Step 4251: loss=3.3122, lr=0.000571, tokens/sec=1378535.48, grad_norm=0.2952, duration=0.38s
Step 4252: loss=3.2944, lr=0.000571, tokens/sec=1373927.55, grad_norm=0.3402, duration=0.38s
Step 4253: loss=3.2964, lr=0.000571, tokens/sec=1375120.93, grad_norm=0.2937, duration=0.38s
Step 4254: loss=3.2934, lr=0.000571, tokens/sec=1372743.10, grad_norm=0.3165, duration=0.38s
Step 4255: loss=3.2837, lr=0.000571, tokens/sec=1380286.81, grad_norm=0.2695, duration=0.38s
Step 4256: loss=3.2585, lr=0.000571, tokens/sec=1373708.69, grad_norm=0.3115, duration=0.38s
Step 4257: loss=3.2652, lr=0.000571, tokens/sec=1376387.88, grad_norm=0.2781, duration=0.38s
Step 4258: loss=3.3063, lr=0.000571, tokens/sec=1376670.51, grad_norm=0.2996, duration=0.38s
Step 4259: loss=3.2795, lr=0.000571, tokens/sec=1374742.67, grad_norm=0.2584, duration=0.38s
Step 4260: loss=3.3236, lr=0.000571, tokens/sec=1375774.77, grad_norm=0.2706, duration=0.38s
Step 4261: loss=3.2089, lr=0.000571, tokens/sec=1375655.14, grad_norm=0.2755, duration=0.38s
Step 4262: loss=3.2929, lr=0.000571, tokens/sec=1379941.21, grad_norm=0.2755, duration=0.38s
Step 4263: loss=3.2210, lr=0.000571, tokens/sec=1376231.10, grad_norm=0.2668, duration=0.38s
Step 4264: loss=3.2275, lr=0.000571, tokens/sec=1369562.68, grad_norm=0.2651, duration=0.38s
Step 4265: loss=3.1943, lr=0.000571, tokens/sec=1379295.52, grad_norm=0.2929, duration=0.38s
Step 4266: loss=3.1787, lr=0.000571, tokens/sec=1375655.14, grad_norm=0.2850, duration=0.38s
Step 4267: loss=3.2271, lr=0.000571, tokens/sec=1371076.67, grad_norm=0.2680, duration=0.38s
Step 4268: loss=3.1602, lr=0.000571, tokens/sec=1378578.69, grad_norm=0.3074, duration=0.38s
Step 4269: loss=3.2120, lr=0.000571, tokens/sec=1374571.67, grad_norm=0.2906, duration=0.38s
Step 4270: loss=3.1850, lr=0.000571, tokens/sec=1376058.01, grad_norm=0.2822, duration=0.38s
Step 4271: loss=3.2178, lr=0.000571, tokens/sec=1377736.57, grad_norm=0.2820, duration=0.38s
Step 4272: loss=3.2171, lr=0.000571, tokens/sec=1373568.83, grad_norm=0.2567, duration=0.38s
Step 4273: loss=3.2479, lr=0.000571, tokens/sec=1374246.10, grad_norm=0.3092, duration=0.38s
Step 4274: loss=3.3020, lr=0.000571, tokens/sec=1373936.14, grad_norm=0.2688, duration=0.38s
Step 4275: loss=3.3246, lr=0.000571, tokens/sec=1374655.01, grad_norm=0.2993, duration=0.38s
Step 4276: loss=3.3270, lr=0.000571, tokens/sec=1375893.56, grad_norm=0.2969, duration=0.38s
Step 4277: loss=3.3163, lr=0.000571, tokens/sec=1374507.23, grad_norm=0.3054, duration=0.38s
Step 4278: loss=3.2870, lr=0.000571, tokens/sec=1375139.85, grad_norm=0.3143, duration=0.38s
Step 4279: loss=3.2866, lr=0.000571, tokens/sec=1378663.39, grad_norm=0.3183, duration=0.38s
Step 4280: loss=3.2659, lr=0.000571, tokens/sec=1372739.67, grad_norm=0.3398, duration=0.38s
Step 4281: loss=3.3231, lr=0.000571, tokens/sec=1372261.67, grad_norm=0.3289, duration=0.38s
Step 4282: loss=3.3107, lr=0.000571, tokens/sec=1376193.21, grad_norm=0.3389, duration=0.38s
Step 4283: loss=3.2804, lr=0.000571, tokens/sec=1376948.08, grad_norm=0.2951, duration=0.38s
Step 4284: loss=3.3095, lr=0.000571, tokens/sec=1375938.33, grad_norm=0.3144, duration=0.38s
Step 4285: loss=3.2863, lr=0.000571, tokens/sec=1376058.01, grad_norm=0.3212, duration=0.38s
Step 4286: loss=3.2641, lr=0.000571, tokens/sec=1375253.37, grad_norm=0.2821, duration=0.38s
Step 4287: loss=3.2475, lr=0.000571, tokens/sec=1375503.69, grad_norm=0.2834, duration=0.38s
Step 4288: loss=3.2862, lr=0.000571, tokens/sec=1374077.79, grad_norm=0.2685, duration=0.38s
Step 4289: loss=3.2201, lr=0.000571, tokens/sec=1373682.09, grad_norm=0.2839, duration=0.38s
Step 4290: loss=3.3396, lr=0.000571, tokens/sec=1374985.93, grad_norm=0.2867, duration=0.38s
Step 4291: loss=3.3057, lr=0.000571, tokens/sec=1377783.19, grad_norm=0.2898, duration=0.38s
Step 4292: loss=3.2748, lr=0.000571, tokens/sec=1379905.71, grad_norm=0.2898, duration=0.38s
Step 4293: loss=3.2297, lr=0.000571, tokens/sec=1375827.27, grad_norm=0.3087, duration=0.38s
Step 4294: loss=3.2764, lr=0.000571, tokens/sec=1378191.62, grad_norm=0.3012, duration=0.38s
Step 4295: loss=3.2624, lr=0.000571, tokens/sec=1378662.53, grad_norm=0.3064, duration=0.38s
Step 4296: loss=3.3176, lr=0.000571, tokens/sec=1376457.66, grad_norm=0.2987, duration=0.38s
Step 4297: loss=3.2836, lr=0.000571, tokens/sec=1376706.70, grad_norm=0.2631, duration=0.38s
Step 4298: loss=3.3317, lr=0.000571, tokens/sec=1380682.86, grad_norm=0.3055, duration=0.38s
Step 4299: loss=3.2909, lr=0.000571, tokens/sec=1375494.23, grad_norm=0.2629, duration=0.38s
Step 4300/19073 (22.5%), Elapsed time: 1716.65s, Steps per hour: 9017.58, Estimated hours remaining: 1.64
Step 4300: loss=3.2630, lr=0.000571, tokens/sec=1373130.55, grad_norm=0.2599, duration=0.38s
Step 4301: loss=3.3027, lr=0.000570, tokens/sec=1375640.51, grad_norm=0.2829, duration=0.38s
Step 4302: loss=3.2821, lr=0.000570, tokens/sec=1376770.49, grad_norm=0.2781, duration=0.38s
Step 4303: loss=3.2460, lr=0.000570, tokens/sec=1376902.38, grad_norm=0.2603, duration=0.38s
Step 4304: loss=3.2949, lr=0.000570, tokens/sec=1373296.91, grad_norm=0.3145, duration=0.38s
Step 4305: loss=3.2861, lr=0.000570, tokens/sec=1372243.69, grad_norm=0.2959, duration=0.38s
Step 4306: loss=3.2641, lr=0.000570, tokens/sec=1375252.51, grad_norm=0.3144, duration=0.38s
Step 4307: loss=3.2342, lr=0.000570, tokens/sec=1376611.04, grad_norm=0.2586, duration=0.38s
Step 4308: loss=3.2337, lr=0.000570, tokens/sec=1371924.36, grad_norm=0.3024, duration=0.38s
Step 4309: loss=3.1905, lr=0.000570, tokens/sec=1375427.12, grad_norm=0.3127, duration=0.38s
Step 4310: loss=3.2286, lr=0.000570, tokens/sec=1376295.70, grad_norm=0.2710, duration=0.38s
Step 4311: loss=3.1750, lr=0.000570, tokens/sec=1374325.97, grad_norm=0.2877, duration=0.38s
Step 4312: loss=3.1882, lr=0.000570, tokens/sec=1374777.05, grad_norm=0.2818, duration=0.38s
Step 4313: loss=3.1714, lr=0.000570, tokens/sec=1373628.03, grad_norm=0.2740, duration=0.38s
Step 4314: loss=3.2583, lr=0.000570, tokens/sec=1374824.32, grad_norm=0.3207, duration=0.38s
Step 4315: loss=3.2190, lr=0.000570, tokens/sec=1371198.93, grad_norm=0.2776, duration=0.38s
Step 4316: loss=3.2085, lr=0.000570, tokens/sec=1379999.23, grad_norm=0.2955, duration=0.38s
Step 4317: loss=3.1955, lr=0.000570, tokens/sec=1376865.31, grad_norm=0.3044, duration=0.38s
Step 4318: loss=3.2456, lr=0.000570, tokens/sec=1372648.84, grad_norm=0.2833, duration=0.38s
Step 4319: loss=3.2716, lr=0.000570, tokens/sec=1374787.36, grad_norm=0.3268, duration=0.38s
Step 4320: loss=3.2898, lr=0.000570, tokens/sec=1374234.07, grad_norm=0.2796, duration=0.38s
Step 4321: loss=3.3077, lr=0.000570, tokens/sec=1372559.74, grad_norm=0.3240, duration=0.38s
Step 4322: loss=3.3237, lr=0.000570, tokens/sec=1378385.99, grad_norm=0.3083, duration=0.38s
Step 4323: loss=3.2762, lr=0.000570, tokens/sec=1378608.94, grad_norm=0.2859, duration=0.38s
Step 4324: loss=3.3097, lr=0.000570, tokens/sec=1379227.17, grad_norm=0.3298, duration=0.38s
Step 4325: loss=3.2646, lr=0.000570, tokens/sec=1377290.45, grad_norm=0.3163, duration=0.38s
Step 4326: loss=3.2907, lr=0.000570, tokens/sec=1376716.19, grad_norm=0.3325, duration=0.38s
Step 4327: loss=3.3234, lr=0.000570, tokens/sec=1376472.31, grad_norm=0.3038, duration=0.38s
Step 4328: loss=3.3836, lr=0.000570, tokens/sec=1378468.94, grad_norm=0.3248, duration=0.38s
Step 4329: loss=3.3470, lr=0.000570, tokens/sec=1380037.34, grad_norm=0.3231, duration=0.38s
Step 4330: loss=3.3346, lr=0.000570, tokens/sec=1376694.64, grad_norm=0.3215, duration=0.38s
Step 4331: loss=3.2447, lr=0.000570, tokens/sec=1377546.70, grad_norm=0.2985, duration=0.38s
Step 4332: loss=3.2732, lr=0.000570, tokens/sec=1379607.04, grad_norm=0.3073, duration=0.38s
Step 4333: loss=3.2508, lr=0.000570, tokens/sec=1371022.82, grad_norm=0.2775, duration=0.38s
Step 4334: loss=3.2139, lr=0.000570, tokens/sec=1373700.11, grad_norm=0.2781, duration=0.38s
Step 4335: loss=3.3167, lr=0.000570, tokens/sec=1376767.90, grad_norm=0.2838, duration=0.38s
Step 4336: loss=3.2098, lr=0.000570, tokens/sec=1376550.72, grad_norm=0.2709, duration=0.38s
Step 4337: loss=3.2836, lr=0.000570, tokens/sec=1374733.22, grad_norm=0.2711, duration=0.38s
Step 4338: loss=3.3290, lr=0.000570, tokens/sec=1376774.80, grad_norm=0.3086, duration=0.38s
Step 4339: loss=3.2641, lr=0.000570, tokens/sec=1382430.96, grad_norm=0.2752, duration=0.38s
Step 4340: loss=3.2117, lr=0.000570, tokens/sec=1373628.89, grad_norm=0.2650, duration=0.38s
Step 4341: loss=3.2771, lr=0.000570, tokens/sec=1373836.57, grad_norm=0.2682, duration=0.38s
Step 4342: loss=3.2612, lr=0.000570, tokens/sec=1374553.62, grad_norm=0.2702, duration=0.38s
Step 4343: loss=3.2423, lr=0.000570, tokens/sec=1374117.29, grad_norm=0.2596, duration=0.38s
Step 4344: loss=3.2136, lr=0.000570, tokens/sec=1375518.32, grad_norm=0.2608, duration=0.38s
Step 4345: loss=3.2891, lr=0.000570, tokens/sec=1376654.99, grad_norm=0.2530, duration=0.38s
Step 4346: loss=3.2720, lr=0.000570, tokens/sec=1379200.36, grad_norm=0.2703, duration=0.38s
Step 4347: loss=3.2684, lr=0.000570, tokens/sec=1376704.12, grad_norm=0.2466, duration=0.38s
Step 4348: loss=3.3239, lr=0.000570, tokens/sec=1377547.56, grad_norm=0.2630, duration=0.38s
Step 4349: loss=3.2296, lr=0.000570, tokens/sec=1379457.32, grad_norm=0.2644, duration=0.38s
Step 4350: loss=3.2608, lr=0.000570, tokens/sec=1372081.86, grad_norm=0.2556, duration=0.38s
Step 4351: loss=3.2152, lr=0.000570, tokens/sec=1377005.85, grad_norm=0.2579, duration=0.38s
Step 4352: loss=3.2028, lr=0.000570, tokens/sec=1375394.43, grad_norm=0.2565, duration=0.38s
Step 4353: loss=3.2421, lr=0.000570, tokens/sec=1378864.81, grad_norm=0.2859, duration=0.38s
Step 4354: loss=3.2272, lr=0.000570, tokens/sec=1377186.95, grad_norm=0.2670, duration=0.38s
Step 4355: loss=3.2282, lr=0.000570, tokens/sec=1378161.39, grad_norm=0.2784, duration=0.38s
Step 4356: loss=3.2050, lr=0.000570, tokens/sec=1377158.48, grad_norm=0.2913, duration=0.38s
Step 4357: loss=3.2687, lr=0.000570, tokens/sec=1373495.90, grad_norm=0.2685, duration=0.38s
Step 4358: loss=3.2410, lr=0.000570, tokens/sec=1374336.28, grad_norm=0.3003, duration=0.38s
Step 4359: loss=3.1593, lr=0.000569, tokens/sec=1377194.71, grad_norm=0.2719, duration=0.38s
Step 4360: loss=3.1768, lr=0.000569, tokens/sec=1376501.60, grad_norm=0.3002, duration=0.38s
Step 4361: loss=3.2022, lr=0.000569, tokens/sec=1378849.25, grad_norm=0.2571, duration=0.38s
Step 4362: loss=3.2595, lr=0.000569, tokens/sec=1376423.20, grad_norm=0.2874, duration=0.38s
Step 4363: loss=3.2058, lr=0.000569, tokens/sec=1377331.00, grad_norm=0.2692, duration=0.38s
Step 4364: loss=3.2339, lr=0.000569, tokens/sec=1379897.05, grad_norm=0.2739, duration=0.38s
Step 4365: loss=3.2734, lr=0.000569, tokens/sec=1373628.03, grad_norm=0.3416, duration=0.38s
Step 4366: loss=3.2959, lr=0.000569, tokens/sec=1372003.11, grad_norm=0.3394, duration=0.38s
Step 4367: loss=3.3131, lr=0.000569, tokens/sec=1372655.70, grad_norm=0.3659, duration=0.38s
Step 4368: loss=3.2553, lr=0.000569, tokens/sec=1373543.95, grad_norm=0.3369, duration=0.38s
Step 4369: loss=3.3005, lr=0.000569, tokens/sec=1377814.26, grad_norm=0.2815, duration=0.38s
Step 4370: loss=3.2612, lr=0.000569, tokens/sec=1376090.73, grad_norm=0.3193, duration=0.38s
Step 4371: loss=3.3022, lr=0.000569, tokens/sec=1376245.75, grad_norm=0.3027, duration=0.38s
Step 4372: loss=3.2929, lr=0.000569, tokens/sec=1376546.41, grad_norm=0.2640, duration=0.38s
Step 4373: loss=3.3671, lr=0.000569, tokens/sec=1375169.08, grad_norm=0.3191, duration=0.38s
Step 4374: loss=3.3334, lr=0.000569, tokens/sec=1376303.46, grad_norm=0.3057, duration=0.38s
Step 4375: loss=3.3162, lr=0.000569, tokens/sec=1371914.09, grad_norm=0.2850, duration=0.38s
Step 4376: loss=3.3653, lr=0.000569, tokens/sec=1374289.04, grad_norm=0.3383, duration=0.38s
Step 4377: loss=3.3041, lr=0.000569, tokens/sec=1377518.22, grad_norm=0.2973, duration=0.38s
Step 4378: loss=3.3395, lr=0.000569, tokens/sec=1378653.88, grad_norm=0.2990, duration=0.38s
Step 4379: loss=3.3281, lr=0.000569, tokens/sec=1374961.00, grad_norm=0.2997, duration=0.38s
Step 4380: loss=3.3150, lr=0.000569, tokens/sec=1377581.22, grad_norm=0.3141, duration=0.38s
Step 4381: loss=3.2833, lr=0.000569, tokens/sec=1377834.12, grad_norm=0.2766, duration=0.38s
Step 4382: loss=3.2704, lr=0.000569, tokens/sec=1375017.75, grad_norm=0.2867, duration=0.38s
Step 4383: loss=3.2275, lr=0.000569, tokens/sec=1373210.29, grad_norm=0.2713, duration=0.38s
Step 4384: loss=3.2614, lr=0.000569, tokens/sec=1376073.50, grad_norm=0.2869, duration=0.38s
Step 4385: loss=3.2903, lr=0.000569, tokens/sec=1377704.64, grad_norm=0.2855, duration=0.38s
Step 4386: loss=3.2437, lr=0.000569, tokens/sec=1379179.60, grad_norm=0.3067, duration=0.38s
Step 4387: loss=3.2850, lr=0.000569, tokens/sec=1375971.04, grad_norm=0.2926, duration=0.38s
Step 4388: loss=3.2575, lr=0.000569, tokens/sec=1376120.87, grad_norm=0.3415, duration=0.38s
Step 4389: loss=3.1844, lr=0.000569, tokens/sec=1375464.12, grad_norm=0.3269, duration=0.38s
Step 4390: loss=3.1831, lr=0.000569, tokens/sec=1374394.69, grad_norm=0.3251, duration=0.38s
Step 4391: loss=3.2542, lr=0.000569, tokens/sec=1379470.30, grad_norm=0.2795, duration=0.38s
Step 4392: loss=3.3411, lr=0.000569, tokens/sec=1376939.45, grad_norm=0.3124, duration=0.38s
Step 4393: loss=3.2242, lr=0.000569, tokens/sec=1378007.67, grad_norm=0.2767, duration=0.38s
Step 4394: loss=3.3166, lr=0.000569, tokens/sec=1380130.88, grad_norm=0.2912, duration=0.38s
Step 4395: loss=3.2969, lr=0.000569, tokens/sec=1380306.74, grad_norm=0.2743, duration=0.38s
Step 4396: loss=3.2272, lr=0.000569, tokens/sec=1372745.67, grad_norm=0.2861, duration=0.38s
Step 4397: loss=3.2638, lr=0.000569, tokens/sec=1378651.29, grad_norm=0.2688, duration=0.38s
Step 4398: loss=3.3241, lr=0.000569, tokens/sec=1373227.44, grad_norm=0.3005, duration=0.38s
Step 4399: loss=3.2857, lr=0.000569, tokens/sec=1376646.37, grad_norm=0.2839, duration=0.38s
Step 4400/19073 (23.1%), Elapsed time: 1754.83s, Steps per hour: 9026.50, Estimated hours remaining: 1.63
Step 4400: loss=3.2520, lr=0.000569, tokens/sec=1377368.96, grad_norm=0.2677, duration=0.38s
Step 4401: loss=3.2853, lr=0.000569, tokens/sec=1379938.61, grad_norm=0.2712, duration=0.38s
Step 4402: loss=3.2847, lr=0.000569, tokens/sec=1376452.49, grad_norm=0.3272, duration=0.38s
Step 4403: loss=3.3250, lr=0.000569, tokens/sec=1377156.76, grad_norm=0.2557, duration=0.38s
Step 4404: loss=3.2600, lr=0.000569, tokens/sec=1374978.20, grad_norm=0.2565, duration=0.38s
Step 4405: loss=3.1787, lr=0.000569, tokens/sec=1377853.11, grad_norm=0.2610, duration=0.38s
Step 4406: loss=3.2440, lr=0.000569, tokens/sec=1378799.97, grad_norm=0.2547, duration=0.38s
Step 4407: loss=3.1861, lr=0.000569, tokens/sec=1378588.20, grad_norm=0.2789, duration=0.38s
Step 4408: loss=3.1767, lr=0.000569, tokens/sec=1378472.40, grad_norm=0.2630, duration=0.38s
Step 4409: loss=3.2285, lr=0.000569, tokens/sec=1376454.22, grad_norm=0.2763, duration=0.38s
Step 4410: loss=3.2124, lr=0.000569, tokens/sec=1372901.65, grad_norm=0.2933, duration=0.38s
Step 4411: loss=3.2282, lr=0.000569, tokens/sec=1375042.68, grad_norm=0.2712, duration=0.38s
Step 4412: loss=3.2483, lr=0.000569, tokens/sec=1379132.89, grad_norm=0.2774, duration=0.38s
Step 4413: loss=3.2218, lr=0.000569, tokens/sec=1373803.09, grad_norm=0.2732, duration=0.38s
Step 4414: loss=3.2283, lr=0.000569, tokens/sec=1377923.05, grad_norm=0.2838, duration=0.38s
Step 4415: loss=3.2065, lr=0.000569, tokens/sec=1375899.58, grad_norm=0.2794, duration=0.38s
Step 4416: loss=3.2033, lr=0.000568, tokens/sec=1379509.24, grad_norm=0.2716, duration=0.38s
Step 4417: loss=3.3017, lr=0.000568, tokens/sec=1378040.48, grad_norm=0.2393, duration=0.38s
Step 4418: loss=3.3390, lr=0.000568, tokens/sec=1372556.31, grad_norm=0.2720, duration=0.38s
Step 4419: loss=3.2982, lr=0.000568, tokens/sec=1377280.10, grad_norm=0.2525, duration=0.38s
Step 4420: loss=3.3047, lr=0.000568, tokens/sec=1377584.67, grad_norm=0.2962, duration=0.38s
Step 4421: loss=3.3132, lr=0.000568, tokens/sec=1379530.87, grad_norm=0.2922, duration=0.38s
Step 4422: loss=3.3001, lr=0.000568, tokens/sec=1377149.86, grad_norm=0.2687, duration=0.38s
Step 4423: loss=3.2980, lr=0.000568, tokens/sec=1374895.67, grad_norm=0.2934, duration=0.38s
Step 4424: loss=3.2886, lr=0.000568, tokens/sec=1374673.06, grad_norm=0.3303, duration=0.38s
Step 4425: loss=3.2502, lr=0.000568, tokens/sec=1378406.73, grad_norm=0.2906, duration=0.38s
Step 4426: loss=3.2695, lr=0.000568, tokens/sec=1376673.09, grad_norm=0.2961, duration=0.38s
Step 4427: loss=3.2582, lr=0.000568, tokens/sec=1378579.55, grad_norm=0.3025, duration=0.38s
Step 4428: loss=3.2576, lr=0.000568, tokens/sec=1378640.05, grad_norm=0.2710, duration=0.38s
Step 4429: loss=3.2541, lr=0.000568, tokens/sec=1375169.08, grad_norm=0.2877, duration=0.38s
Step 4430: loss=3.2317, lr=0.000568, tokens/sec=1376519.70, grad_norm=0.2943, duration=0.38s
Step 4431: loss=3.1772, lr=0.000568, tokens/sec=1378489.68, grad_norm=0.2813, duration=0.38s
Step 4432: loss=3.2168, lr=0.000568, tokens/sec=1379448.66, grad_norm=0.2917, duration=0.38s
Step 4433: loss=3.2553, lr=0.000568, tokens/sec=1375445.19, grad_norm=0.3210, duration=0.38s
Step 4434: loss=3.2556, lr=0.000568, tokens/sec=1376082.12, grad_norm=0.2636, duration=0.38s
Step 4435: loss=3.3091, lr=0.000568, tokens/sec=1375804.03, grad_norm=0.2984, duration=0.38s
Step 4436: loss=3.1749, lr=0.000568, tokens/sec=1374439.36, grad_norm=0.2750, duration=0.38s
Step 4437: loss=3.3197, lr=0.000568, tokens/sec=1377576.90, grad_norm=0.2857, duration=0.38s
Step 4438: loss=3.2736, lr=0.000568, tokens/sec=1377471.63, grad_norm=0.3251, duration=0.38s
Step 4439: loss=3.2872, lr=0.000568, tokens/sec=1374046.88, grad_norm=0.2693, duration=0.38s
Step 4440: loss=3.2926, lr=0.000568, tokens/sec=1378862.22, grad_norm=0.2876, duration=0.38s
Step 4441: loss=3.2974, lr=0.000568, tokens/sec=1375736.90, grad_norm=0.3426, duration=0.38s
Step 4442: loss=3.2618, lr=0.000568, tokens/sec=1377845.34, grad_norm=0.3162, duration=0.38s
Step 4443: loss=3.2790, lr=0.000568, tokens/sec=1377732.26, grad_norm=0.2958, duration=0.38s
Step 4444: loss=3.2630, lr=0.000568, tokens/sec=1378698.83, grad_norm=0.3433, duration=0.38s
Step 4445: loss=3.2398, lr=0.000568, tokens/sec=1376769.63, grad_norm=0.2620, duration=0.38s
Step 4446: loss=3.2887, lr=0.000568, tokens/sec=1374682.51, grad_norm=0.3287, duration=0.38s
Step 4447: loss=3.2489, lr=0.000568, tokens/sec=1377399.15, grad_norm=0.2867, duration=0.38s
Step 4448: loss=3.2746, lr=0.000568, tokens/sec=1377666.66, grad_norm=0.3044, duration=0.38s
Step 4449: loss=3.2945, lr=0.000568, tokens/sec=1378381.67, grad_norm=0.2830, duration=0.38s
Step 4450: loss=3.2697, lr=0.000568, tokens/sec=1376765.32, grad_norm=0.3050, duration=0.38s
Step 4451: loss=3.2112, lr=0.000568, tokens/sec=1378841.47, grad_norm=0.2753, duration=0.38s
Step 4452: loss=3.2638, lr=0.000568, tokens/sec=1377442.29, grad_norm=0.3018, duration=0.38s
Step 4453: loss=3.2010, lr=0.000568, tokens/sec=1378366.12, grad_norm=0.2714, duration=0.38s
Step 4454: loss=3.2307, lr=0.000568, tokens/sec=1372722.53, grad_norm=0.3178, duration=0.38s
Step 4455: loss=3.1207, lr=0.000568, tokens/sec=1373837.42, grad_norm=0.2946, duration=0.38s
Step 4456: loss=3.1866, lr=0.000568, tokens/sec=1374901.69, grad_norm=0.2789, duration=0.38s
Step 4457: loss=3.1877, lr=0.000568, tokens/sec=1376905.83, grad_norm=0.2824, duration=0.38s
Step 4458: loss=3.1689, lr=0.000568, tokens/sec=1376580.02, grad_norm=0.2999, duration=0.38s
Step 4459: loss=3.1868, lr=0.000568, tokens/sec=1377614.88, grad_norm=0.2990, duration=0.38s
Step 4460: loss=3.1479, lr=0.000568, tokens/sec=1371334.03, grad_norm=0.3202, duration=0.38s
Step 4461: loss=3.2205, lr=0.000568, tokens/sec=1377407.78, grad_norm=0.2962, duration=0.38s
Step 4462: loss=3.1947, lr=0.000568, tokens/sec=1372344.74, grad_norm=0.2920, duration=0.38s
Step 4463: loss=3.2404, lr=0.000568, tokens/sec=1374904.26, grad_norm=0.2993, duration=0.38s
Step 4464: loss=3.2955, lr=0.000568, tokens/sec=1377652.85, grad_norm=0.2817, duration=0.38s
Step 4465: loss=3.3049, lr=0.000568, tokens/sec=1376255.22, grad_norm=0.3104, duration=0.38s
Step 4466: loss=3.3020, lr=0.000568, tokens/sec=1374618.92, grad_norm=0.2987, duration=0.38s
Step 4467: loss=3.3239, lr=0.000568, tokens/sec=1373740.44, grad_norm=0.3030, duration=0.38s
Step 4468: loss=3.2205, lr=0.000568, tokens/sec=1379860.68, grad_norm=0.2859, duration=0.38s
Step 4469: loss=3.2756, lr=0.000568, tokens/sec=1376493.85, grad_norm=0.3069, duration=0.38s
Step 4470: loss=3.2487, lr=0.000568, tokens/sec=1375063.32, grad_norm=0.2946, duration=0.38s
Step 4471: loss=3.3041, lr=0.000568, tokens/sec=1373138.26, grad_norm=0.3388, duration=0.38s
Step 4472: loss=3.2757, lr=0.000567, tokens/sec=1376682.57, grad_norm=0.3339, duration=0.38s
Step 4473: loss=3.2797, lr=0.000567, tokens/sec=1378361.80, grad_norm=0.3111, duration=0.38s
Step 4474: loss=3.3071, lr=0.000567, tokens/sec=1374301.92, grad_norm=0.3420, duration=0.38s
Step 4475: loss=3.2314, lr=0.000567, tokens/sec=1376578.29, grad_norm=0.2704, duration=0.38s
Step 4476: loss=3.2415, lr=0.000567, tokens/sec=1377572.59, grad_norm=0.3064, duration=0.38s
Step 4477: loss=3.2448, lr=0.000567, tokens/sec=1373356.08, grad_norm=0.2634, duration=0.38s
Step 4478: loss=3.2623, lr=0.000567, tokens/sec=1374724.62, grad_norm=0.2933, duration=0.38s
Step 4479: loss=3.2722, lr=0.000567, tokens/sec=1377250.77, grad_norm=0.2883, duration=0.38s
Step 4480: loss=3.2980, lr=0.000567, tokens/sec=1376095.03, grad_norm=0.2679, duration=0.38s
Step 4481: loss=3.2523, lr=0.000567, tokens/sec=1377840.16, grad_norm=0.2975, duration=0.38s
Step 4482: loss=3.2623, lr=0.000567, tokens/sec=1376909.28, grad_norm=0.2683, duration=0.38s
Step 4483: loss=3.1809, lr=0.000567, tokens/sec=1375631.04, grad_norm=0.2795, duration=0.38s
Step 4484: loss=3.2950, lr=0.000567, tokens/sec=1373700.11, grad_norm=0.2721, duration=0.38s
Step 4485: loss=3.2543, lr=0.000567, tokens/sec=1376126.89, grad_norm=0.2913, duration=0.38s
Step 4486: loss=3.2961, lr=0.000567, tokens/sec=1376174.26, grad_norm=0.2851, duration=0.38s
Step 4487: loss=3.2959, lr=0.000567, tokens/sec=1374054.61, grad_norm=0.3030, duration=0.38s
Step 4488: loss=3.2948, lr=0.000567, tokens/sec=1379571.55, grad_norm=0.2689, duration=0.38s
Step 4489: loss=3.2419, lr=0.000567, tokens/sec=1375826.41, grad_norm=0.2852, duration=0.38s
Step 4490: loss=3.2839, lr=0.000567, tokens/sec=1375147.58, grad_norm=0.2918, duration=0.38s
Step 4491: loss=3.2486, lr=0.000567, tokens/sec=1375185.42, grad_norm=0.3041, duration=0.38s
Step 4492: loss=3.2534, lr=0.000567, tokens/sec=1375719.68, grad_norm=0.2572, duration=0.38s
Step 4493: loss=3.2573, lr=0.000567, tokens/sec=1379419.24, grad_norm=0.2994, duration=0.38s
Step 4494: loss=3.2616, lr=0.000567, tokens/sec=1375897.00, grad_norm=0.2798, duration=0.38s
Step 4495: loss=3.2604, lr=0.000567, tokens/sec=1376061.45, grad_norm=0.2832, duration=0.38s
Step 4496: loss=3.2320, lr=0.000567, tokens/sec=1373236.87, grad_norm=0.2793, duration=0.38s
Step 4497: loss=3.2144, lr=0.000567, tokens/sec=1375276.59, grad_norm=0.2781, duration=0.38s
Step 4498: loss=3.2294, lr=0.000567, tokens/sec=1377317.19, grad_norm=0.2790, duration=0.38s
Step 4499: loss=3.1714, lr=0.000567, tokens/sec=1379114.73, grad_norm=0.2856, duration=0.38s
Step 4500/19073 (23.6%), Elapsed time: 1793.01s, Steps per hour: 9035.09, Estimated hours remaining: 1.61
Validation loss at step 4500: 3.7018373012542725
Step 4500: loss=3.2339, lr=0.000567, tokens/sec=154047.64, grad_norm=0.2666, duration=3.40s
Step 4501: loss=3.1272, lr=0.000567, tokens/sec=1377779.73, grad_norm=0.2703, duration=0.38s
Step 4502: loss=3.1652, lr=0.000567, tokens/sec=1376406.83, grad_norm=0.2860, duration=0.38s
Step 4503: loss=3.1683, lr=0.000567, tokens/sec=1376549.00, grad_norm=0.2803, duration=0.38s
Step 4504: loss=3.2326, lr=0.000567, tokens/sec=1378526.84, grad_norm=0.3321, duration=0.38s
Step 4505: loss=3.2142, lr=0.000567, tokens/sec=1380023.48, grad_norm=0.2596, duration=0.38s
Step 4506: loss=3.1885, lr=0.000567, tokens/sec=1376690.33, grad_norm=0.3099, duration=0.38s
Step 4507: loss=3.1951, lr=0.000567, tokens/sec=1376302.60, grad_norm=0.2720, duration=0.38s
Step 4508: loss=3.2092, lr=0.000567, tokens/sec=1376574.85, grad_norm=0.2814, duration=0.38s
Step 4509: loss=3.2411, lr=0.000567, tokens/sec=1378903.72, grad_norm=0.2754, duration=0.38s
Step 4510: loss=3.2697, lr=0.000567, tokens/sec=1380488.71, grad_norm=0.2704, duration=0.38s
Step 4511: loss=3.2901, lr=0.000567, tokens/sec=1374704.00, grad_norm=0.2758, duration=0.38s
Step 4512: loss=3.2858, lr=0.000567, tokens/sec=1378031.85, grad_norm=0.2806, duration=0.38s
Step 4513: loss=3.2634, lr=0.000567, tokens/sec=1379415.78, grad_norm=0.3030, duration=0.38s
Step 4514: loss=3.2987, lr=0.000567, tokens/sec=1373694.96, grad_norm=0.3325, duration=0.38s
Step 4515: loss=3.2430, lr=0.000567, tokens/sec=1376581.74, grad_norm=0.2865, duration=0.38s
Step 4516: loss=3.2597, lr=0.000567, tokens/sec=1378375.62, grad_norm=0.3274, duration=0.38s
Step 4517: loss=3.2968, lr=0.000567, tokens/sec=1369549.03, grad_norm=0.2784, duration=0.38s
Step 4518: loss=3.3946, lr=0.000567, tokens/sec=1376352.56, grad_norm=0.3123, duration=0.38s
Step 4519: loss=3.3265, lr=0.000567, tokens/sec=1377292.18, grad_norm=0.3116, duration=0.38s
Step 4520: loss=3.2873, lr=0.000567, tokens/sec=1377620.92, grad_norm=0.2954, duration=0.38s
Step 4521: loss=3.2192, lr=0.000567, tokens/sec=1374245.24, grad_norm=0.2929, duration=0.38s
Step 4522: loss=3.2642, lr=0.000567, tokens/sec=1375680.96, grad_norm=0.3012, duration=0.38s
Step 4523: loss=3.2255, lr=0.000567, tokens/sec=1377358.60, grad_norm=0.2785, duration=0.38s
Step 4524: loss=3.2137, lr=0.000567, tokens/sec=1374704.86, grad_norm=0.2886, duration=0.38s
Step 4525: loss=3.2781, lr=0.000567, tokens/sec=1375793.70, grad_norm=0.2791, duration=0.38s
Step 4526: loss=3.1759, lr=0.000567, tokens/sec=1370174.54, grad_norm=0.2604, duration=0.38s
Step 4527: loss=3.2847, lr=0.000567, tokens/sec=1378240.86, grad_norm=0.2878, duration=0.38s
Step 4528: loss=3.3065, lr=0.000566, tokens/sec=1375305.83, grad_norm=0.2677, duration=0.38s
Step 4529: loss=3.2520, lr=0.000566, tokens/sec=1375742.06, grad_norm=0.2590, duration=0.38s
Step 4530: loss=3.1953, lr=0.000566, tokens/sec=1371466.60, grad_norm=0.2715, duration=0.38s
Step 4531: loss=3.2324, lr=0.000566, tokens/sec=1378808.62, grad_norm=0.2494, duration=0.38s
Step 4532: loss=3.2625, lr=0.000566, tokens/sec=1375570.81, grad_norm=0.2509, duration=0.38s
Step 4533: loss=3.2660, lr=0.000566, tokens/sec=1374236.65, grad_norm=0.2478, duration=0.38s
Step 4534: loss=3.2011, lr=0.000566, tokens/sec=1375067.61, grad_norm=0.2505, duration=0.38s
Step 4535: loss=3.2579, lr=0.000566, tokens/sec=1375965.01, grad_norm=0.2453, duration=0.38s
Step 4536: loss=3.2337, lr=0.000566, tokens/sec=1376072.64, grad_norm=0.2806, duration=0.38s
Step 4537: loss=3.2736, lr=0.000566, tokens/sec=1373177.71, grad_norm=0.2590, duration=0.38s
Step 4538: loss=3.2917, lr=0.000566, tokens/sec=1375299.81, grad_norm=0.2630, duration=0.38s
Step 4539: loss=3.2313, lr=0.000566, tokens/sec=1375690.42, grad_norm=0.2716, duration=0.38s
Step 4540: loss=3.1965, lr=0.000566, tokens/sec=1378365.26, grad_norm=0.2607, duration=0.38s
Step 4541: loss=3.2337, lr=0.000566, tokens/sec=1377201.61, grad_norm=0.2726, duration=0.38s
Step 4542: loss=3.1793, lr=0.000566, tokens/sec=1375671.49, grad_norm=0.2897, duration=0.38s
Step 4543: loss=3.2500, lr=0.000566, tokens/sec=1376913.59, grad_norm=0.3080, duration=0.38s
Step 4544: loss=3.2200, lr=0.000566, tokens/sec=1377853.11, grad_norm=0.2912, duration=0.38s
Step 4545: loss=3.1872, lr=0.000566, tokens/sec=1375382.39, grad_norm=0.2856, duration=0.38s
Step 4546: loss=3.1856, lr=0.000566, tokens/sec=1378134.62, grad_norm=0.2911, duration=0.38s
Step 4547: loss=3.2745, lr=0.000566, tokens/sec=1378717.85, grad_norm=0.2626, duration=0.38s
Step 4548: loss=3.1613, lr=0.000566, tokens/sec=1376348.25, grad_norm=0.2909, duration=0.38s
Step 4549: loss=3.1909, lr=0.000566, tokens/sec=1374714.31, grad_norm=0.2689, duration=0.38s
Step 4550: loss=3.1477, lr=0.000566, tokens/sec=1371511.93, grad_norm=0.2953, duration=0.38s
Step 4551: loss=3.2087, lr=0.000566, tokens/sec=1374711.73, grad_norm=0.3021, duration=0.38s
Step 4552: loss=3.2028, lr=0.000566, tokens/sec=1376533.49, grad_norm=0.2838, duration=0.38s
Step 4553: loss=3.2014, lr=0.000566, tokens/sec=1376588.64, grad_norm=0.3010, duration=0.38s
Step 4554: loss=3.2029, lr=0.000566, tokens/sec=1374650.72, grad_norm=0.3161, duration=0.38s
Step 4555: loss=3.2771, lr=0.000566, tokens/sec=1378708.34, grad_norm=0.3303, duration=0.38s
Step 4556: loss=3.2812, lr=0.000566, tokens/sec=1379041.21, grad_norm=0.2787, duration=0.38s
Step 4557: loss=3.2754, lr=0.000566, tokens/sec=1374987.65, grad_norm=0.3158, duration=0.38s
Step 4558: loss=3.2511, lr=0.000566, tokens/sec=1374596.58, grad_norm=0.3233, duration=0.38s
Step 4559: loss=3.2609, lr=0.000566, tokens/sec=1372614.57, grad_norm=0.2943, duration=0.38s
Step 4560: loss=3.2475, lr=0.000566, tokens/sec=1376990.33, grad_norm=0.2890, duration=0.38s
Step 4561: loss=3.2707, lr=0.000566, tokens/sec=1380610.91, grad_norm=0.2812, duration=0.38s
Step 4562: loss=3.2870, lr=0.000566, tokens/sec=1377710.68, grad_norm=0.2635, duration=0.38s
Step 4563: loss=3.3787, lr=0.000566, tokens/sec=1378330.70, grad_norm=0.2708, duration=0.38s
Step 4564: loss=3.2765, lr=0.000566, tokens/sec=1377965.35, grad_norm=0.2708, duration=0.38s
Step 4565: loss=3.2933, lr=0.000566, tokens/sec=1375982.23, grad_norm=0.2838, duration=0.38s
Step 4566: loss=3.3562, lr=0.000566, tokens/sec=1372703.68, grad_norm=0.2918, duration=0.38s
Step 4567: loss=3.3010, lr=0.000566, tokens/sec=1375469.28, grad_norm=0.3067, duration=0.38s
Step 4568: loss=3.3413, lr=0.000566, tokens/sec=1377477.67, grad_norm=0.2996, duration=0.38s
Step 4569: loss=3.2768, lr=0.000566, tokens/sec=1374532.14, grad_norm=0.3043, duration=0.38s
Step 4570: loss=3.3062, lr=0.000566, tokens/sec=1377614.01, grad_norm=0.3126, duration=0.38s
Step 4571: loss=3.2351, lr=0.000566, tokens/sec=1377273.20, grad_norm=0.2879, duration=0.38s
Step 4572: loss=3.2762, lr=0.000566, tokens/sec=1379120.78, grad_norm=0.3113, duration=0.38s
Step 4573: loss=3.1771, lr=0.000566, tokens/sec=1377701.18, grad_norm=0.2810, duration=0.38s
Step 4574: loss=3.2803, lr=0.000566, tokens/sec=1373156.27, grad_norm=0.2927, duration=0.38s
Step 4575: loss=3.2657, lr=0.000566, tokens/sec=1374244.38, grad_norm=0.3077, duration=0.38s
Step 4576: loss=3.2037, lr=0.000566, tokens/sec=1380175.92, grad_norm=0.3268, duration=0.38s
Step 4577: loss=3.3129, lr=0.000566, tokens/sec=1378512.15, grad_norm=0.3136, duration=0.38s
Step 4578: loss=3.2213, lr=0.000566, tokens/sec=1372795.38, grad_norm=0.3556, duration=0.38s
Step 4579: loss=3.1087, lr=0.000566, tokens/sec=1380712.33, grad_norm=0.4112, duration=0.38s
Step 4580: loss=3.2124, lr=0.000566, tokens/sec=1376343.08, grad_norm=0.3009, duration=0.38s
Step 4581: loss=3.2456, lr=0.000566, tokens/sec=1377797.00, grad_norm=0.2962, duration=0.38s
Step 4582: loss=3.3648, lr=0.000566, tokens/sec=1372536.61, grad_norm=0.2884, duration=0.38s
Step 4583: loss=3.1988, lr=0.000565, tokens/sec=1373335.50, grad_norm=0.2856, duration=0.38s
Step 4584: loss=3.3064, lr=0.000565, tokens/sec=1377419.86, grad_norm=0.2948, duration=0.38s
Step 4585: loss=3.2584, lr=0.000565, tokens/sec=1375116.63, grad_norm=0.2829, duration=0.38s
Step 4586: loss=3.2227, lr=0.000565, tokens/sec=1377371.54, grad_norm=0.2773, duration=0.38s
Step 4587: loss=3.2843, lr=0.000565, tokens/sec=1375410.78, grad_norm=0.2798, duration=0.38s
Step 4588: loss=3.2916, lr=0.000565, tokens/sec=1379369.92, grad_norm=0.2589, duration=0.38s
Step 4589: loss=3.2518, lr=0.000565, tokens/sec=1372325.90, grad_norm=0.2920, duration=0.38s
Step 4590: loss=3.2450, lr=0.000565, tokens/sec=1379222.85, grad_norm=0.2867, duration=0.38s
Step 4591: loss=3.2540, lr=0.000565, tokens/sec=1376530.90, grad_norm=0.2733, duration=0.38s
Step 4592: loss=3.2836, lr=0.000565, tokens/sec=1375909.05, grad_norm=0.3113, duration=0.38s
Step 4593: loss=3.3137, lr=0.000565, tokens/sec=1377333.59, grad_norm=0.2596, duration=0.38s
Step 4594: loss=3.2187, lr=0.000565, tokens/sec=1376341.36, grad_norm=0.2626, duration=0.38s
Step 4595: loss=3.1706, lr=0.000565, tokens/sec=1373767.05, grad_norm=0.2683, duration=0.38s
Step 4596: loss=3.2244, lr=0.000565, tokens/sec=1374802.83, grad_norm=0.2473, duration=0.38s
Step 4597: loss=3.1698, lr=0.000565, tokens/sec=1374283.03, grad_norm=0.2763, duration=0.38s
Step 4598: loss=3.1897, lr=0.000565, tokens/sec=1376705.84, grad_norm=0.2747, duration=0.38s
Step 4599: loss=3.1823, lr=0.000565, tokens/sec=1380246.09, grad_norm=0.2847, duration=0.38s
Step 4600/19073 (24.1%), Elapsed time: 1834.21s, Steps per hour: 9028.39, Estimated hours remaining: 1.60
Step 4600: loss=3.2046, lr=0.000565, tokens/sec=1376012.37, grad_norm=0.2856, duration=0.38s
Step 4601: loss=3.2367, lr=0.000565, tokens/sec=1374157.65, grad_norm=0.2740, duration=0.38s
Step 4602: loss=3.1875, lr=0.000565, tokens/sec=1374034.86, grad_norm=0.2855, duration=0.38s
Step 4603: loss=3.2339, lr=0.000565, tokens/sec=1375301.53, grad_norm=0.2777, duration=0.38s
Step 4604: loss=3.1971, lr=0.000565, tokens/sec=1377998.17, grad_norm=0.2699, duration=0.38s
Step 4605: loss=3.1735, lr=0.000565, tokens/sec=1376039.06, grad_norm=0.2979, duration=0.38s
Step 4606: loss=3.2008, lr=0.000565, tokens/sec=1372322.47, grad_norm=0.2991, duration=0.38s
Step 4607: loss=3.2813, lr=0.000565, tokens/sec=1376785.14, grad_norm=0.2593, duration=0.38s
Step 4608: loss=3.3342, lr=0.000565, tokens/sec=1374740.09, grad_norm=0.3086, duration=0.38s
Step 4609: loss=3.2901, lr=0.000565, tokens/sec=1374970.46, grad_norm=0.3087, duration=0.38s
Step 4610: loss=3.2764, lr=0.000565, tokens/sec=1376526.59, grad_norm=0.3044, duration=0.38s
Step 4611: loss=3.2877, lr=0.000565, tokens/sec=1374742.67, grad_norm=0.2794, duration=0.38s
Step 4612: loss=3.2912, lr=0.000565, tokens/sec=1373797.08, grad_norm=0.3037, duration=0.38s
Step 4613: loss=3.3095, lr=0.000565, tokens/sec=1377416.41, grad_norm=0.3047, duration=0.38s
Step 4614: loss=3.2282, lr=0.000565, tokens/sec=1378972.03, grad_norm=0.2939, duration=0.38s
Step 4615: loss=3.2371, lr=0.000565, tokens/sec=1380290.28, grad_norm=0.2970, duration=0.38s
Step 4616: loss=3.2555, lr=0.000565, tokens/sec=1374527.85, grad_norm=0.3161, duration=0.38s
Step 4617: loss=3.2525, lr=0.000565, tokens/sec=1377607.97, grad_norm=0.3097, duration=0.38s
Step 4618: loss=3.2427, lr=0.000565, tokens/sec=1377740.89, grad_norm=0.3150, duration=0.38s
Step 4619: loss=3.2071, lr=0.000565, tokens/sec=1375276.59, grad_norm=0.2902, duration=0.38s
Step 4620: loss=3.2318, lr=0.000565, tokens/sec=1378570.91, grad_norm=0.2771, duration=0.38s
Step 4621: loss=3.1532, lr=0.000565, tokens/sec=1377342.21, grad_norm=0.2927, duration=0.38s
Step 4622: loss=3.1948, lr=0.000565, tokens/sec=1376184.60, grad_norm=0.2888, duration=0.38s
Step 4623: loss=3.2472, lr=0.000565, tokens/sec=1377524.26, grad_norm=0.2815, duration=0.38s
Step 4624: loss=3.2418, lr=0.000565, tokens/sec=1376902.38, grad_norm=0.2891, duration=0.38s
Step 4625: loss=3.2307, lr=0.000565, tokens/sec=1374586.27, grad_norm=0.2851, duration=0.38s
Step 4626: loss=3.2140, lr=0.000565, tokens/sec=1376133.78, grad_norm=0.2758, duration=0.38s
Step 4627: loss=3.2979, lr=0.000565, tokens/sec=1378528.57, grad_norm=0.2930, duration=0.38s
Step 4628: loss=3.2954, lr=0.000565, tokens/sec=1376222.49, grad_norm=0.2793, duration=0.38s
Step 4629: loss=3.2699, lr=0.000565, tokens/sec=1377860.88, grad_norm=0.2976, duration=0.38s
Step 4630: loss=3.2748, lr=0.000565, tokens/sec=1373706.12, grad_norm=0.2798, duration=0.38s
Step 4631: loss=3.2708, lr=0.000565, tokens/sec=1376847.21, grad_norm=0.3491, duration=0.38s
Step 4632: loss=3.2443, lr=0.000565, tokens/sec=1374624.94, grad_norm=0.3071, duration=0.38s
Step 4633: loss=3.2483, lr=0.000565, tokens/sec=1377899.73, grad_norm=0.2820, duration=0.38s
Step 4634: loss=3.2196, lr=0.000565, tokens/sec=1379423.57, grad_norm=0.3156, duration=0.38s
Step 4635: loss=3.2690, lr=0.000565, tokens/sec=1375656.86, grad_norm=0.2807, duration=0.38s
Step 4636: loss=3.2705, lr=0.000565, tokens/sec=1371850.75, grad_norm=0.3046, duration=0.38s
Step 4637: loss=3.2210, lr=0.000564, tokens/sec=1376560.20, grad_norm=0.2743, duration=0.38s
Step 4638: loss=3.2885, lr=0.000564, tokens/sec=1372989.94, grad_norm=0.2998, duration=0.38s
Step 4639: loss=3.2392, lr=0.000564, tokens/sec=1377671.84, grad_norm=0.2832, duration=0.38s
Step 4640: loss=3.2722, lr=0.000564, tokens/sec=1377815.13, grad_norm=0.2849, duration=0.38s
Step 4641: loss=3.1828, lr=0.000564, tokens/sec=1377274.93, grad_norm=0.2819, duration=0.38s
Step 4642: loss=3.2421, lr=0.000564, tokens/sec=1376078.67, grad_norm=0.2869, duration=0.38s
Step 4643: loss=3.2005, lr=0.000564, tokens/sec=1371439.23, grad_norm=0.2639, duration=0.38s
Step 4644: loss=3.1567, lr=0.000564, tokens/sec=1375626.74, grad_norm=0.2833, duration=0.38s
Step 4645: loss=3.1308, lr=0.000564, tokens/sec=1376942.90, grad_norm=0.2595, duration=0.38s
Step 4646: loss=3.1497, lr=0.000564, tokens/sec=1376571.40, grad_norm=0.2751, duration=0.38s
Step 4647: loss=3.1994, lr=0.000564, tokens/sec=1375382.39, grad_norm=0.2750, duration=0.38s
Step 4648: loss=3.1458, lr=0.000564, tokens/sec=1377393.98, grad_norm=0.2893, duration=0.38s
Step 4649: loss=3.1456, lr=0.000564, tokens/sec=1375745.50, grad_norm=0.2837, duration=0.38s
Step 4650: loss=3.1478, lr=0.000564, tokens/sec=1375906.47, grad_norm=0.3249, duration=0.38s
Step 4651: loss=3.1946, lr=0.000564, tokens/sec=1376895.48, grad_norm=0.2859, duration=0.38s
Step 4652: loss=3.1903, lr=0.000564, tokens/sec=1375592.32, grad_norm=0.2983, duration=0.38s
Step 4653: loss=3.2351, lr=0.000564, tokens/sec=1377475.94, grad_norm=0.2829, duration=0.38s
Step 4654: loss=3.2818, lr=0.000564, tokens/sec=1376786.86, grad_norm=0.2998, duration=0.38s
Step 4655: loss=3.2788, lr=0.000564, tokens/sec=1375733.45, grad_norm=0.2951, duration=0.38s
Step 4656: loss=3.3087, lr=0.000564, tokens/sec=1378497.46, grad_norm=0.2983, duration=0.38s
Step 4657: loss=3.2608, lr=0.000564, tokens/sec=1377186.95, grad_norm=0.2938, duration=0.38s
Step 4658: loss=3.2142, lr=0.000564, tokens/sec=1377065.34, grad_norm=0.2860, duration=0.38s
Step 4659: loss=3.2647, lr=0.000564, tokens/sec=1374453.10, grad_norm=0.3108, duration=0.38s
Step 4660: loss=3.2299, lr=0.000564, tokens/sec=1375046.98, grad_norm=0.2854, duration=0.38s
Step 4661: loss=3.2721, lr=0.000564, tokens/sec=1375849.66, grad_norm=0.3448, duration=0.38s
Step 4662: loss=3.2751, lr=0.000564, tokens/sec=1375835.88, grad_norm=0.3204, duration=0.38s
Step 4663: loss=3.2775, lr=0.000564, tokens/sec=1375887.53, grad_norm=0.3209, duration=0.38s
Step 4664: loss=3.2571, lr=0.000564, tokens/sec=1372402.98, grad_norm=0.3292, duration=0.38s
Step 4665: loss=3.2107, lr=0.000564, tokens/sec=1379544.72, grad_norm=0.3007, duration=0.38s
Step 4666: loss=3.2418, lr=0.000564, tokens/sec=1378565.73, grad_norm=0.3141, duration=0.38s
Step 4667: loss=3.2226, lr=0.000564, tokens/sec=1375211.22, grad_norm=0.2933, duration=0.38s
Step 4668: loss=3.3182, lr=0.000564, tokens/sec=1376008.93, grad_norm=0.2946, duration=0.38s
Step 4669: loss=3.2352, lr=0.000564, tokens/sec=1376852.38, grad_norm=0.3109, duration=0.38s
Step 4670: loss=3.2429, lr=0.000564, tokens/sec=1373679.51, grad_norm=0.2678, duration=0.38s
Step 4671: loss=3.2429, lr=0.000564, tokens/sec=1375156.18, grad_norm=0.3076, duration=0.38s
Step 4672: loss=3.2177, lr=0.000564, tokens/sec=1374503.79, grad_norm=0.2693, duration=0.38s
Step 4673: loss=3.2024, lr=0.000564, tokens/sec=1375551.88, grad_norm=0.2870, duration=0.38s
Step 4674: loss=3.2902, lr=0.000564, tokens/sec=1376687.74, grad_norm=0.2945, duration=0.38s
Step 4675: loss=3.2354, lr=0.000564, tokens/sec=1375307.55, grad_norm=0.3133, duration=0.38s
Step 4676: loss=3.3069, lr=0.000564, tokens/sec=1373943.00, grad_norm=0.2777, duration=0.38s
Step 4677: loss=3.2617, lr=0.000564, tokens/sec=1374939.51, grad_norm=0.3062, duration=0.38s
Step 4678: loss=3.2471, lr=0.000564, tokens/sec=1377557.05, grad_norm=0.2792, duration=0.38s
Step 4679: loss=3.2646, lr=0.000564, tokens/sec=1375549.29, grad_norm=0.3096, duration=0.38s
Step 4680: loss=3.2349, lr=0.000564, tokens/sec=1376464.56, grad_norm=0.3045, duration=0.38s
Step 4681: loss=3.2244, lr=0.000564, tokens/sec=1375229.28, grad_norm=0.2858, duration=0.38s
Step 4682: loss=3.2642, lr=0.000564, tokens/sec=1379327.53, grad_norm=0.2911, duration=0.38s
Step 4683: loss=3.2282, lr=0.000564, tokens/sec=1377282.69, grad_norm=0.2933, duration=0.38s
Step 4684: loss=3.2391, lr=0.000564, tokens/sec=1374506.37, grad_norm=0.2825, duration=0.38s
Step 4685: loss=3.2286, lr=0.000564, tokens/sec=1375902.17, grad_norm=0.2796, duration=0.38s
Step 4686: loss=3.2107, lr=0.000564, tokens/sec=1378691.05, grad_norm=0.2792, duration=0.38s
Step 4687: loss=3.2157, lr=0.000564, tokens/sec=1376091.59, grad_norm=0.2845, duration=0.38s
Step 4688: loss=3.2126, lr=0.000564, tokens/sec=1375752.39, grad_norm=0.2890, duration=0.38s
Step 4689: loss=3.1775, lr=0.000564, tokens/sec=1379715.24, grad_norm=0.2783, duration=0.38s
Step 4690: loss=3.1858, lr=0.000563, tokens/sec=1374803.69, grad_norm=0.2720, duration=0.38s
Step 4691: loss=3.1045, lr=0.000563, tokens/sec=1376855.83, grad_norm=0.2946, duration=0.38s
Step 4692: loss=3.1581, lr=0.000563, tokens/sec=1374879.34, grad_norm=0.2411, duration=0.38s
Step 4693: loss=3.1472, lr=0.000563, tokens/sec=1375021.19, grad_norm=0.2941, duration=0.38s
Step 4694: loss=3.2269, lr=0.000563, tokens/sec=1376370.65, grad_norm=0.2684, duration=0.38s
Step 4695: loss=3.1971, lr=0.000563, tokens/sec=1380029.54, grad_norm=0.2854, duration=0.38s
Step 4696: loss=3.1963, lr=0.000563, tokens/sec=1374643.84, grad_norm=0.3136, duration=0.38s
Step 4697: loss=3.1587, lr=0.000563, tokens/sec=1376711.88, grad_norm=0.2617, duration=0.38s
Step 4698: loss=3.1851, lr=0.000563, tokens/sec=1375372.07, grad_norm=0.2834, duration=0.38s
Step 4699: loss=3.2244, lr=0.000563, tokens/sec=1378360.94, grad_norm=0.2722, duration=0.38s
Step 4700/19073 (24.6%), Elapsed time: 1872.39s, Steps per hour: 9036.57, Estimated hours remaining: 1.59
Step 4700: loss=3.2606, lr=0.000563, tokens/sec=1378715.25, grad_norm=0.2796, duration=0.38s
Step 4701: loss=3.2540, lr=0.000563, tokens/sec=1373573.98, grad_norm=0.2836, duration=0.38s
Step 4702: loss=3.2729, lr=0.000563, tokens/sec=1377910.96, grad_norm=0.2906, duration=0.38s
Step 4703: loss=3.2530, lr=0.000563, tokens/sec=1376284.51, grad_norm=0.2839, duration=0.38s
Step 4704: loss=3.2778, lr=0.000563, tokens/sec=1376794.62, grad_norm=0.2704, duration=0.38s
Step 4705: loss=3.2140, lr=0.000563, tokens/sec=1374063.19, grad_norm=0.3136, duration=0.38s
Step 4706: loss=3.2350, lr=0.000563, tokens/sec=1376121.73, grad_norm=0.3014, duration=0.38s
Step 4707: loss=3.3123, lr=0.000563, tokens/sec=1375534.67, grad_norm=0.3019, duration=0.38s
Step 4708: loss=3.3730, lr=0.000563, tokens/sec=1378842.33, grad_norm=0.3050, duration=0.38s
Step 4709: loss=3.2809, lr=0.000563, tokens/sec=1377909.23, grad_norm=0.2949, duration=0.38s
Step 4710: loss=3.2648, lr=0.000563, tokens/sec=1375349.70, grad_norm=0.2986, duration=0.38s
Step 4711: loss=3.2121, lr=0.000563, tokens/sec=1376231.97, grad_norm=0.2919, duration=0.38s
Step 4712: loss=3.2377, lr=0.000563, tokens/sec=1378468.08, grad_norm=0.2729, duration=0.38s
Step 4713: loss=3.2276, lr=0.000563, tokens/sec=1375857.40, grad_norm=0.2781, duration=0.38s
Step 4714: loss=3.1758, lr=0.000563, tokens/sec=1372103.27, grad_norm=0.3069, duration=0.38s
Step 4715: loss=3.2441, lr=0.000563, tokens/sec=1373349.22, grad_norm=0.2692, duration=0.38s
Step 4716: loss=3.1775, lr=0.000563, tokens/sec=1375298.09, grad_norm=0.2870, duration=0.38s
Step 4717: loss=3.2624, lr=0.000563, tokens/sec=1374225.49, grad_norm=0.2756, duration=0.38s
Step 4718: loss=3.2984, lr=0.000563, tokens/sec=1374138.75, grad_norm=0.2805, duration=0.38s
Step 4719: loss=3.2358, lr=0.000563, tokens/sec=1376618.80, grad_norm=0.2768, duration=0.38s
Step 4720: loss=3.1489, lr=0.000563, tokens/sec=1374222.05, grad_norm=0.2488, duration=0.38s
Step 4721: loss=3.2367, lr=0.000563, tokens/sec=1375813.50, grad_norm=0.2774, duration=0.38s
Step 4722: loss=3.2885, lr=0.000563, tokens/sec=1375727.43, grad_norm=0.2643, duration=0.38s
Step 4723: loss=3.2562, lr=0.000563, tokens/sec=1376226.80, grad_norm=0.2611, duration=0.38s
Step 4724: loss=3.1760, lr=0.000563, tokens/sec=1369206.23, grad_norm=0.2560, duration=0.38s
Step 4725: loss=3.2208, lr=0.000563, tokens/sec=1376888.59, grad_norm=0.2691, duration=0.38s
Step 4726: loss=3.2360, lr=0.000563, tokens/sec=1374946.39, grad_norm=0.2717, duration=0.38s
Step 4727: loss=3.2426, lr=0.000563, tokens/sec=1375237.88, grad_norm=0.2669, duration=0.38s
Step 4728: loss=3.2975, lr=0.000563, tokens/sec=1373837.42, grad_norm=0.2874, duration=0.38s
Step 4729: loss=3.1690, lr=0.000563, tokens/sec=1378302.19, grad_norm=0.2653, duration=0.38s
Step 4730: loss=3.2150, lr=0.000563, tokens/sec=1375758.41, grad_norm=0.2713, duration=0.38s
Step 4731: loss=3.2059, lr=0.000563, tokens/sec=1375600.92, grad_norm=0.2631, duration=0.38s
Step 4732: loss=3.1864, lr=0.000563, tokens/sec=1375778.21, grad_norm=0.2652, duration=0.38s
Step 4733: loss=3.2417, lr=0.000563, tokens/sec=1375119.21, grad_norm=0.2937, duration=0.38s
Step 4734: loss=3.1790, lr=0.000563, tokens/sec=1375659.44, grad_norm=0.2796, duration=0.38s
Step 4735: loss=3.1670, lr=0.000563, tokens/sec=1371914.09, grad_norm=0.2816, duration=0.38s
Step 4736: loss=3.1927, lr=0.000563, tokens/sec=1374425.61, grad_norm=0.2652, duration=0.38s
Step 4737: loss=3.1988, lr=0.000563, tokens/sec=1373760.18, grad_norm=0.2995, duration=0.38s
Step 4738: loss=3.1934, lr=0.000563, tokens/sec=1377570.00, grad_norm=0.2721, duration=0.38s
Step 4739: loss=3.1655, lr=0.000563, tokens/sec=1376536.93, grad_norm=0.3054, duration=0.38s
Step 4740: loss=3.1574, lr=0.000563, tokens/sec=1374997.97, grad_norm=0.2851, duration=0.38s
Step 4741: loss=3.1575, lr=0.000563, tokens/sec=1376588.64, grad_norm=0.3080, duration=0.38s
Step 4742: loss=3.1980, lr=0.000563, tokens/sec=1377263.71, grad_norm=0.2722, duration=0.38s
Step 4743: loss=3.1686, lr=0.000562, tokens/sec=1371463.18, grad_norm=0.3267, duration=0.38s
Step 4744: loss=3.2100, lr=0.000562, tokens/sec=1371506.80, grad_norm=0.3202, duration=0.38s
Step 4745: loss=3.2632, lr=0.000562, tokens/sec=1373223.15, grad_norm=0.3305, duration=0.38s
Step 4746: loss=3.2456, lr=0.000562, tokens/sec=1372451.80, grad_norm=0.2878, duration=0.38s
Step 4747: loss=3.2749, lr=0.000562, tokens/sec=1373150.27, grad_norm=0.3440, duration=0.38s
Step 4748: loss=3.2156, lr=0.000562, tokens/sec=1373230.01, grad_norm=0.3178, duration=0.38s
Step 4749: loss=3.2485, lr=0.000562, tokens/sec=1373261.74, grad_norm=0.2896, duration=0.38s
Validation loss at step 4750: 3.708381414413452
Step 4750: loss=3.2215, lr=0.000562, tokens/sec=153047.87, grad_norm=0.3171, duration=3.43s
Step 4751: loss=3.2647, lr=0.000562, tokens/sec=1375989.98, grad_norm=0.2817, duration=0.38s
Step 4752: loss=3.3010, lr=0.000562, tokens/sec=1378177.80, grad_norm=0.2854, duration=0.38s
Step 4753: loss=3.3234, lr=0.000562, tokens/sec=1380740.08, grad_norm=0.2828, duration=0.38s
Step 4754: loss=3.2521, lr=0.000562, tokens/sec=1377223.17, grad_norm=0.2631, duration=0.38s
Step 4755: loss=3.2900, lr=0.000562, tokens/sec=1378100.93, grad_norm=0.2730, duration=0.38s
Step 4756: loss=3.3510, lr=0.000562, tokens/sec=1375884.09, grad_norm=0.2799, duration=0.38s
Step 4757: loss=3.2997, lr=0.000562, tokens/sec=1373112.54, grad_norm=0.2841, duration=0.38s
Step 4758: loss=3.2876, lr=0.000562, tokens/sec=1379042.08, grad_norm=0.2904, duration=0.38s
Step 4759: loss=3.2692, lr=0.000562, tokens/sec=1373865.75, grad_norm=0.2985, duration=0.38s
Step 4760: loss=3.2575, lr=0.000562, tokens/sec=1375765.30, grad_norm=0.2859, duration=0.38s
Step 4761: loss=3.2386, lr=0.000562, tokens/sec=1378148.43, grad_norm=0.2838, duration=0.38s
Step 4762: loss=3.2286, lr=0.000562, tokens/sec=1376769.63, grad_norm=0.2933, duration=0.38s
Step 4763: loss=3.1964, lr=0.000562, tokens/sec=1372875.94, grad_norm=0.2818, duration=0.38s
Step 4764: loss=3.2517, lr=0.000562, tokens/sec=1374226.35, grad_norm=0.2747, duration=0.38s
Step 4765: loss=3.2266, lr=0.000562, tokens/sec=1375802.31, grad_norm=0.3032, duration=0.38s
Step 4766: loss=3.2281, lr=0.000562, tokens/sec=1375940.05, grad_norm=0.2899, duration=0.38s
Step 4767: loss=3.2775, lr=0.000562, tokens/sec=1376215.60, grad_norm=0.3343, duration=0.38s
Step 4768: loss=3.1417, lr=0.000562, tokens/sec=1366986.45, grad_norm=0.3215, duration=0.38s
Step 4769: loss=3.1386, lr=0.000562, tokens/sec=1373145.12, grad_norm=0.3672, duration=0.38s
Step 4770: loss=3.2028, lr=0.000562, tokens/sec=1374398.98, grad_norm=0.3593, duration=0.38s
Step 4771: loss=3.2646, lr=0.000562, tokens/sec=1372604.29, grad_norm=0.2993, duration=0.38s
Step 4772: loss=3.3360, lr=0.000562, tokens/sec=1376380.12, grad_norm=0.3085, duration=0.38s
Step 4773: loss=3.1912, lr=0.000562, tokens/sec=1376058.87, grad_norm=0.3275, duration=0.38s
Step 4774: loss=3.2630, lr=0.000562, tokens/sec=1377035.16, grad_norm=0.2829, duration=0.38s
Step 4775: loss=3.2557, lr=0.000562, tokens/sec=1375038.38, grad_norm=0.3002, duration=0.38s
Step 4776: loss=3.2419, lr=0.000562, tokens/sec=1374267.57, grad_norm=0.2777, duration=0.38s
Step 4777: loss=3.2548, lr=0.000562, tokens/sec=1371816.52, grad_norm=0.3058, duration=0.38s
Step 4778: loss=3.2627, lr=0.000562, tokens/sec=1375168.22, grad_norm=0.2774, duration=0.38s
Step 4779: loss=3.2456, lr=0.000562, tokens/sec=1373137.41, grad_norm=0.3039, duration=0.38s
Step 4780: loss=3.2160, lr=0.000562, tokens/sec=1373778.20, grad_norm=0.2645, duration=0.38s
Step 4781: loss=3.2451, lr=0.000562, tokens/sec=1375498.53, grad_norm=0.2966, duration=0.38s
Step 4782: loss=3.2732, lr=0.000562, tokens/sec=1376226.80, grad_norm=0.3111, duration=0.38s
Step 4783: loss=3.2737, lr=0.000562, tokens/sec=1374861.28, grad_norm=0.2839, duration=0.38s
Step 4784: loss=3.2105, lr=0.000562, tokens/sec=1376295.70, grad_norm=0.2712, duration=0.38s
Step 4785: loss=3.1543, lr=0.000562, tokens/sec=1379419.24, grad_norm=0.2768, duration=0.38s
Step 4786: loss=3.2095, lr=0.000562, tokens/sec=1375854.82, grad_norm=0.2641, duration=0.38s
Step 4787: loss=3.1810, lr=0.000562, tokens/sec=1374533.86, grad_norm=0.2770, duration=0.38s
Step 4788: loss=3.1493, lr=0.000562, tokens/sec=1376356.00, grad_norm=0.2982, duration=0.38s
Step 4789: loss=3.1792, lr=0.000562, tokens/sec=1374437.64, grad_norm=0.2808, duration=0.38s
Step 4790: loss=3.2143, lr=0.000562, tokens/sec=1375999.45, grad_norm=0.2783, duration=0.38s
Step 4791: loss=3.1773, lr=0.000562, tokens/sec=1378223.58, grad_norm=0.2881, duration=0.38s
Step 4792: loss=3.2025, lr=0.000562, tokens/sec=1373531.94, grad_norm=0.2806, duration=0.38s
Step 4793: loss=3.2076, lr=0.000562, tokens/sec=1375181.12, grad_norm=0.2959, duration=0.38s
Step 4794: loss=3.1664, lr=0.000562, tokens/sec=1376160.48, grad_norm=0.2866, duration=0.38s
Step 4795: loss=3.1769, lr=0.000561, tokens/sec=1372679.69, grad_norm=0.3265, duration=0.38s
Step 4796: loss=3.1768, lr=0.000561, tokens/sec=1374222.91, grad_norm=0.2716, duration=0.38s
Step 4797: loss=3.2754, lr=0.000561, tokens/sec=1372462.08, grad_norm=0.2987, duration=0.38s
Step 4798: loss=3.3248, lr=0.000561, tokens/sec=1375584.57, grad_norm=0.3319, duration=0.38s
Step 4799: loss=3.2622, lr=0.000561, tokens/sec=1376793.76, grad_norm=0.2830, duration=0.38s
Step 4800/19073 (25.2%), Elapsed time: 1913.65s, Steps per hour: 9029.88, Estimated hours remaining: 1.58
Step 4800: loss=3.2526, lr=0.000561, tokens/sec=1376692.91, grad_norm=0.3050, duration=0.38s
Step 4801: loss=3.2808, lr=0.000561, tokens/sec=1373116.83, grad_norm=0.2862, duration=0.38s
Step 4802: loss=3.3041, lr=0.000561, tokens/sec=1375477.02, grad_norm=0.2997, duration=0.38s
Step 4803: loss=3.2537, lr=0.000561, tokens/sec=1379812.20, grad_norm=0.2860, duration=0.38s
Step 4804: loss=3.2173, lr=0.000561, tokens/sec=1379691.86, grad_norm=0.2895, duration=0.38s
Step 4805: loss=3.2213, lr=0.000561, tokens/sec=1374406.71, grad_norm=0.3070, duration=0.38s
Step 4806: loss=3.2503, lr=0.000561, tokens/sec=1377512.18, grad_norm=0.3188, duration=0.38s
Step 4807: loss=3.2373, lr=0.000561, tokens/sec=1377614.88, grad_norm=0.3027, duration=0.38s
Step 4808: loss=3.1937, lr=0.000561, tokens/sec=1376145.84, grad_norm=0.2807, duration=0.38s
Step 4809: loss=3.2097, lr=0.000561, tokens/sec=1376290.54, grad_norm=0.2941, duration=0.38s
Step 4810: loss=3.2094, lr=0.000561, tokens/sec=1378760.20, grad_norm=0.2635, duration=0.38s
Step 4811: loss=3.1324, lr=0.000561, tokens/sec=1378227.90, grad_norm=0.3031, duration=0.38s
Step 4812: loss=3.1889, lr=0.000561, tokens/sec=1376873.07, grad_norm=0.2839, duration=0.38s
Step 4813: loss=3.2334, lr=0.000561, tokens/sec=1377306.84, grad_norm=0.2690, duration=0.38s
Step 4814: loss=3.1672, lr=0.000561, tokens/sec=1377023.09, grad_norm=0.3060, duration=0.38s
Step 4815: loss=3.2698, lr=0.000561, tokens/sec=1380314.53, grad_norm=0.2764, duration=0.38s
Step 4816: loss=3.1948, lr=0.000561, tokens/sec=1374860.42, grad_norm=0.2776, duration=0.38s
Step 4817: loss=3.3199, lr=0.000561, tokens/sec=1375963.29, grad_norm=0.2873, duration=0.38s
Step 4818: loss=3.2757, lr=0.000561, tokens/sec=1380296.34, grad_norm=0.2674, duration=0.38s
Step 4819: loss=3.2560, lr=0.000561, tokens/sec=1376362.90, grad_norm=0.3006, duration=0.38s
Step 4820: loss=3.2485, lr=0.000561, tokens/sec=1377218.86, grad_norm=0.2751, duration=0.38s
Step 4821: loss=3.2525, lr=0.000561, tokens/sec=1375106.31, grad_norm=0.2852, duration=0.38s
Step 4822: loss=3.2145, lr=0.000561, tokens/sec=1377932.54, grad_norm=0.2825, duration=0.38s
Step 4823: loss=3.2084, lr=0.000561, tokens/sec=1379561.16, grad_norm=0.2930, duration=0.38s
Step 4824: loss=3.2497, lr=0.000561, tokens/sec=1375793.70, grad_norm=0.3066, duration=0.38s
Step 4825: loss=3.2561, lr=0.000561, tokens/sec=1377012.74, grad_norm=0.3034, duration=0.38s
Step 4826: loss=3.2461, lr=0.000561, tokens/sec=1378252.09, grad_norm=0.3205, duration=0.38s
Step 4827: loss=3.2374, lr=0.000561, tokens/sec=1378805.16, grad_norm=0.3094, duration=0.38s
Step 4828: loss=3.2352, lr=0.000561, tokens/sec=1377841.03, grad_norm=0.3032, duration=0.38s
Step 4829: loss=3.2452, lr=0.000561, tokens/sec=1377031.71, grad_norm=0.3009, duration=0.38s
Step 4830: loss=3.2450, lr=0.000561, tokens/sec=1371633.41, grad_norm=0.3045, duration=0.38s
Step 4831: loss=3.1619, lr=0.000561, tokens/sec=1379477.22, grad_norm=0.2971, duration=0.38s
Step 4832: loss=3.2417, lr=0.000561, tokens/sec=1372649.70, grad_norm=0.2716, duration=0.38s
Step 4833: loss=3.1327, lr=0.000561, tokens/sec=1378453.39, grad_norm=0.3227, duration=0.38s
Step 4834: loss=3.1671, lr=0.000561, tokens/sec=1378712.66, grad_norm=0.2668, duration=0.38s
Step 4835: loss=3.0940, lr=0.000561, tokens/sec=1375708.50, grad_norm=0.2878, duration=0.38s
Step 4836: loss=3.1627, lr=0.000561, tokens/sec=1376392.19, grad_norm=0.2805, duration=0.38s
Step 4837: loss=3.1773, lr=0.000561, tokens/sec=1372423.54, grad_norm=0.2816, duration=0.38s
Step 4838: loss=3.1081, lr=0.000561, tokens/sec=1375946.93, grad_norm=0.2876, duration=0.38s
Step 4839: loss=3.1517, lr=0.000561, tokens/sec=1376935.14, grad_norm=0.2989, duration=0.38s
Step 4840: loss=3.1227, lr=0.000561, tokens/sec=1373768.76, grad_norm=0.3043, duration=0.38s
Step 4841: loss=3.1877, lr=0.000561, tokens/sec=1374187.70, grad_norm=0.2755, duration=0.38s
Step 4842: loss=3.1865, lr=0.000561, tokens/sec=1378011.12, grad_norm=0.2972, duration=0.38s
Step 4843: loss=3.2203, lr=0.000561, tokens/sec=1378241.72, grad_norm=0.2911, duration=0.38s
Step 4844: loss=3.2559, lr=0.000561, tokens/sec=1376669.64, grad_norm=0.3092, duration=0.38s
Step 4845: loss=3.2863, lr=0.000561, tokens/sec=1376416.31, grad_norm=0.3014, duration=0.38s
Step 4846: loss=3.2479, lr=0.000561, tokens/sec=1375592.32, grad_norm=0.3093, duration=0.38s
Step 4847: loss=3.2568, lr=0.000560, tokens/sec=1373658.06, grad_norm=0.3196, duration=0.38s
Step 4848: loss=3.2041, lr=0.000560, tokens/sec=1376310.35, grad_norm=0.3026, duration=0.38s
Step 4849: loss=3.2442, lr=0.000560, tokens/sec=1374868.16, grad_norm=0.3032, duration=0.38s
Step 4850: loss=3.1978, lr=0.000560, tokens/sec=1373055.95, grad_norm=0.2959, duration=0.38s
Step 4851: loss=3.2697, lr=0.000560, tokens/sec=1375464.98, grad_norm=0.3183, duration=0.38s
Step 4852: loss=3.2704, lr=0.000560, tokens/sec=1377112.78, grad_norm=0.2746, duration=0.38s
Step 4853: loss=3.2258, lr=0.000560, tokens/sec=1373119.40, grad_norm=0.2991, duration=0.38s
Step 4854: loss=3.2345, lr=0.000560, tokens/sec=1372154.64, grad_norm=0.2861, duration=0.38s
Step 4855: loss=3.2118, lr=0.000560, tokens/sec=1375845.35, grad_norm=0.2927, duration=0.38s
Step 4856: loss=3.2171, lr=0.000560, tokens/sec=1377780.60, grad_norm=0.3057, duration=0.38s
Step 4857: loss=3.2781, lr=0.000560, tokens/sec=1376052.84, grad_norm=0.2887, duration=0.38s
Step 4858: loss=3.2788, lr=0.000560, tokens/sec=1373455.58, grad_norm=0.3074, duration=0.38s
Step 4859: loss=3.1814, lr=0.000560, tokens/sec=1376739.46, grad_norm=0.2913, duration=0.38s
Step 4860: loss=3.2353, lr=0.000560, tokens/sec=1376116.56, grad_norm=0.2967, duration=0.38s
Step 4861: loss=3.1965, lr=0.000560, tokens/sec=1378735.14, grad_norm=0.2797, duration=0.38s
Step 4862: loss=3.2431, lr=0.000560, tokens/sec=1376062.31, grad_norm=0.2944, duration=0.38s
Step 4863: loss=3.2004, lr=0.000560, tokens/sec=1375053.86, grad_norm=0.2885, duration=0.38s
Step 4864: loss=3.2719, lr=0.000560, tokens/sec=1372428.68, grad_norm=0.2990, duration=0.38s
Step 4865: loss=3.2515, lr=0.000560, tokens/sec=1375239.60, grad_norm=0.3103, duration=0.38s
Step 4866: loss=3.2768, lr=0.000560, tokens/sec=1374160.22, grad_norm=0.2858, duration=0.38s
Step 4867: loss=3.2101, lr=0.000560, tokens/sec=1371422.12, grad_norm=0.3000, duration=0.38s
Step 4868: loss=3.2695, lr=0.000560, tokens/sec=1373076.53, grad_norm=0.2953, duration=0.38s
Step 4869: loss=3.2162, lr=0.000560, tokens/sec=1377729.67, grad_norm=0.3396, duration=0.38s
Step 4870: loss=3.2113, lr=0.000560, tokens/sec=1373852.87, grad_norm=0.2865, duration=0.38s
Step 4871: loss=3.2390, lr=0.000560, tokens/sec=1371412.71, grad_norm=0.3064, duration=0.38s
Step 4872: loss=3.2365, lr=0.000560, tokens/sec=1375542.41, grad_norm=0.2984, duration=0.38s
Step 4873: loss=3.2065, lr=0.000560, tokens/sec=1372844.23, grad_norm=0.2859, duration=0.38s
Step 4874: loss=3.2061, lr=0.000560, tokens/sec=1373178.56, grad_norm=0.2815, duration=0.38s
Step 4875: loss=3.2110, lr=0.000560, tokens/sec=1374654.16, grad_norm=0.2748, duration=0.38s
Step 4876: loss=3.2136, lr=0.000560, tokens/sec=1376355.14, grad_norm=0.2810, duration=0.38s
Step 4877: loss=3.2006, lr=0.000560, tokens/sec=1373953.30, grad_norm=0.2546, duration=0.38s
Step 4878: loss=3.2187, lr=0.000560, tokens/sec=1374609.47, grad_norm=0.2650, duration=0.38s
Step 4879: loss=3.1311, lr=0.000560, tokens/sec=1374317.38, grad_norm=0.2517, duration=0.38s
Step 4880: loss=3.1650, lr=0.000560, tokens/sec=1376888.59, grad_norm=0.2659, duration=0.38s
Step 4881: loss=3.0997, lr=0.000560, tokens/sec=1375878.06, grad_norm=0.2598, duration=0.38s
Step 4882: loss=3.1361, lr=0.000560, tokens/sec=1373519.07, grad_norm=0.2512, duration=0.38s
Step 4883: loss=3.1494, lr=0.000560, tokens/sec=1371410.15, grad_norm=0.2799, duration=0.38s
Step 4884: loss=3.2082, lr=0.000560, tokens/sec=1372408.98, grad_norm=0.2492, duration=0.38s
Step 4885: loss=3.2051, lr=0.000560, tokens/sec=1378018.03, grad_norm=0.2784, duration=0.38s
Step 4886: loss=3.1559, lr=0.000560, tokens/sec=1380175.92, grad_norm=0.2528, duration=0.38s
Step 4887: loss=3.1346, lr=0.000560, tokens/sec=1373001.09, grad_norm=0.2565, duration=0.38s
Step 4888: loss=3.1674, lr=0.000560, tokens/sec=1377451.78, grad_norm=0.2590, duration=0.38s
Step 4889: loss=3.2087, lr=0.000560, tokens/sec=1375684.40, grad_norm=0.2405, duration=0.38s
Step 4890: loss=3.2248, lr=0.000560, tokens/sec=1376621.38, grad_norm=0.2721, duration=0.38s
Step 4891: loss=3.2390, lr=0.000560, tokens/sec=1373370.67, grad_norm=0.2751, duration=0.38s
Step 4892: loss=3.2616, lr=0.000560, tokens/sec=1375650.83, grad_norm=0.3036, duration=0.38s
Step 4893: loss=3.2361, lr=0.000560, tokens/sec=1376568.82, grad_norm=0.2672, duration=0.38s
Step 4894: loss=3.2503, lr=0.000560, tokens/sec=1372685.69, grad_norm=0.2713, duration=0.38s
Step 4895: loss=3.1888, lr=0.000560, tokens/sec=1375741.20, grad_norm=0.2677, duration=0.38s
Step 4896: loss=3.2467, lr=0.000560, tokens/sec=1378424.87, grad_norm=0.2620, duration=0.38s
Step 4897: loss=3.2935, lr=0.000560, tokens/sec=1375526.92, grad_norm=0.2734, duration=0.38s
Step 4898: loss=3.3291, lr=0.000559, tokens/sec=1373334.64, grad_norm=0.3021, duration=0.38s
Step 4899: loss=3.2548, lr=0.000559, tokens/sec=1375959.85, grad_norm=0.2679, duration=0.38s
Step 4900/19073 (25.7%), Elapsed time: 1951.85s, Steps per hour: 9037.60, Estimated hours remaining: 1.57
Step 4900: loss=3.2569, lr=0.000559, tokens/sec=1373694.10, grad_norm=0.2671, duration=0.38s
Step 4901: loss=3.1888, lr=0.000559, tokens/sec=1372358.44, grad_norm=0.2799, duration=0.38s
Step 4902: loss=3.2402, lr=0.000559, tokens/sec=1375854.82, grad_norm=0.2719, duration=0.38s
Step 4903: loss=3.1891, lr=0.000559, tokens/sec=1374537.30, grad_norm=0.2777, duration=0.38s
Step 4904: loss=3.1447, lr=0.000559, tokens/sec=1375421.10, grad_norm=0.2934, duration=0.38s
Step 4905: loss=3.2469, lr=0.000559, tokens/sec=1376867.90, grad_norm=0.2613, duration=0.38s
Step 4906: loss=3.1572, lr=0.000559, tokens/sec=1377967.08, grad_norm=0.2696, duration=0.38s
Step 4907: loss=3.2546, lr=0.000559, tokens/sec=1373293.47, grad_norm=0.2739, duration=0.38s
Step 4908: loss=3.2804, lr=0.000559, tokens/sec=1374494.34, grad_norm=0.2668, duration=0.38s
Step 4909: loss=3.1920, lr=0.000559, tokens/sec=1373495.90, grad_norm=0.2797, duration=0.38s
Step 4910: loss=3.1535, lr=0.000559, tokens/sec=1368999.10, grad_norm=0.2711, duration=0.38s
Step 4911: loss=3.2595, lr=0.000559, tokens/sec=1374057.18, grad_norm=0.2678, duration=0.38s
Step 4912: loss=3.2807, lr=0.000559, tokens/sec=1378514.74, grad_norm=0.2598, duration=0.38s
Step 4913: loss=3.2306, lr=0.000559, tokens/sec=1376501.60, grad_norm=0.2619, duration=0.38s
Step 4914: loss=3.1363, lr=0.000559, tokens/sec=1374513.24, grad_norm=0.2414, duration=0.38s
Step 4915: loss=3.2263, lr=0.000559, tokens/sec=1373139.12, grad_norm=0.2546, duration=0.38s
Step 4916: loss=3.2053, lr=0.000559, tokens/sec=1376643.79, grad_norm=0.2611, duration=0.38s
Step 4917: loss=3.2470, lr=0.000559, tokens/sec=1377880.74, grad_norm=0.2565, duration=0.38s
Step 4918: loss=3.2357, lr=0.000559, tokens/sec=1375594.90, grad_norm=0.2601, duration=0.38s
Step 4919: loss=3.1896, lr=0.000559, tokens/sec=1373483.04, grad_norm=0.2673, duration=0.38s
Step 4920: loss=3.1928, lr=0.000559, tokens/sec=1376474.03, grad_norm=0.2565, duration=0.38s
Step 4921: loss=3.2158, lr=0.000559, tokens/sec=1378795.65, grad_norm=0.2636, duration=0.38s
Step 4922: loss=3.1818, lr=0.000559, tokens/sec=1368483.67, grad_norm=0.2793, duration=0.38s
Step 4923: loss=3.2033, lr=0.000559, tokens/sec=1370458.89, grad_norm=0.2792, duration=0.38s
Step 4924: loss=3.1616, lr=0.000559, tokens/sec=1376317.24, grad_norm=0.2848, duration=0.38s
Step 4925: loss=3.1794, lr=0.000559, tokens/sec=1372642.85, grad_norm=0.3188, duration=0.38s
Step 4926: loss=3.1176, lr=0.000559, tokens/sec=1373055.95, grad_norm=0.2601, duration=0.38s
Step 4927: loss=3.2339, lr=0.000559, tokens/sec=1373804.81, grad_norm=0.3042, duration=0.38s
Step 4928: loss=3.1681, lr=0.000559, tokens/sec=1369668.46, grad_norm=0.2674, duration=0.38s
Step 4929: loss=3.1739, lr=0.000559, tokens/sec=1369196.00, grad_norm=0.3026, duration=0.38s
Step 4930: loss=3.1007, lr=0.000559, tokens/sec=1374649.00, grad_norm=0.2684, duration=0.38s
Step 4931: loss=3.1522, lr=0.000559, tokens/sec=1372743.96, grad_norm=0.2951, duration=0.38s
Step 4932: loss=3.1643, lr=0.000559, tokens/sec=1373946.44, grad_norm=0.3168, duration=0.38s
Step 4933: loss=3.1754, lr=0.000559, tokens/sec=1374008.25, grad_norm=0.3204, duration=0.38s
Step 4934: loss=3.1984, lr=0.000559, tokens/sec=1375907.33, grad_norm=0.3080, duration=0.38s
Step 4935: loss=3.2285, lr=0.000559, tokens/sec=1375557.04, grad_norm=0.2991, duration=0.38s
Step 4936: loss=3.2458, lr=0.000559, tokens/sec=1375765.30, grad_norm=0.3000, duration=0.38s
Step 4937: loss=3.2330, lr=0.000559, tokens/sec=1372362.73, grad_norm=0.3055, duration=0.38s
Step 4938: loss=3.2024, lr=0.000559, tokens/sec=1373416.13, grad_norm=0.3109, duration=0.38s
Step 4939: loss=3.2201, lr=0.000559, tokens/sec=1376306.04, grad_norm=0.2868, duration=0.38s
Step 4940: loss=3.2146, lr=0.000559, tokens/sec=1370167.71, grad_norm=0.3020, duration=0.38s
Step 4941: loss=3.2791, lr=0.000559, tokens/sec=1374381.80, grad_norm=0.2863, duration=0.38s
Step 4942: loss=3.2476, lr=0.000559, tokens/sec=1372798.80, grad_norm=0.2835, duration=0.38s
Step 4943: loss=3.3023, lr=0.000559, tokens/sec=1371447.78, grad_norm=0.2918, duration=0.38s
Step 4944: loss=3.2497, lr=0.000559, tokens/sec=1378081.93, grad_norm=0.2821, duration=0.38s
Step 4945: loss=3.2858, lr=0.000559, tokens/sec=1373041.38, grad_norm=0.2727, duration=0.38s
Step 4946: loss=3.3553, lr=0.000559, tokens/sec=1375673.21, grad_norm=0.3087, duration=0.38s
Step 4947: loss=3.2502, lr=0.000559, tokens/sec=1371825.08, grad_norm=0.2805, duration=0.38s
Step 4948: loss=3.2804, lr=0.000558, tokens/sec=1371004.01, grad_norm=0.2946, duration=0.38s
Step 4949: loss=3.2211, lr=0.000558, tokens/sec=1373049.95, grad_norm=0.2834, duration=0.38s
Step 4950: loss=3.2637, lr=0.000558, tokens/sec=1371371.66, grad_norm=0.2888, duration=0.38s
Step 4951: loss=3.1948, lr=0.000558, tokens/sec=1374833.78, grad_norm=0.2909, duration=0.38s
Step 4952: loss=3.2476, lr=0.000558, tokens/sec=1376537.79, grad_norm=0.2687, duration=0.38s
Step 4953: loss=3.1719, lr=0.000558, tokens/sec=1376225.94, grad_norm=0.2917, duration=0.38s
Step 4954: loss=3.2163, lr=0.000558, tokens/sec=1372281.37, grad_norm=0.2667, duration=0.38s
Step 4955: loss=3.2527, lr=0.000558, tokens/sec=1375172.52, grad_norm=0.3093, duration=0.38s
Step 4956: loss=3.1930, lr=0.000558, tokens/sec=1375495.09, grad_norm=0.3010, duration=0.38s
Step 4957: loss=3.2015, lr=0.000558, tokens/sec=1377358.60, grad_norm=0.3125, duration=0.38s
Step 4958: loss=3.1739, lr=0.000558, tokens/sec=1371566.68, grad_norm=0.2903, duration=0.38s
Step 4959: loss=3.1276, lr=0.000558, tokens/sec=1373533.65, grad_norm=0.3580, duration=0.38s
Step 4960: loss=3.2228, lr=0.000558, tokens/sec=1374523.55, grad_norm=0.2858, duration=0.38s
Step 4961: loss=3.2359, lr=0.000558, tokens/sec=1372719.96, grad_norm=0.3005, duration=0.38s
Step 4962: loss=3.3248, lr=0.000558, tokens/sec=1373996.23, grad_norm=0.2922, duration=0.38s
Step 4963: loss=3.1521, lr=0.000558, tokens/sec=1370505.86, grad_norm=0.3142, duration=0.38s
Step 4964: loss=3.2614, lr=0.000558, tokens/sec=1375445.19, grad_norm=0.2779, duration=0.38s
Step 4965: loss=3.2754, lr=0.000558, tokens/sec=1374022.84, grad_norm=0.2999, duration=0.38s
Step 4966: loss=3.2126, lr=0.000558, tokens/sec=1373012.23, grad_norm=0.2928, duration=0.38s
Step 4967: loss=3.2213, lr=0.000558, tokens/sec=1372250.54, grad_norm=0.2750, duration=0.38s
Step 4968: loss=3.2566, lr=0.000558, tokens/sec=1369659.93, grad_norm=0.2829, duration=0.38s
Step 4969: loss=3.2143, lr=0.000558, tokens/sec=1373837.42, grad_norm=0.2755, duration=0.38s
Step 4970: loss=3.2073, lr=0.000558, tokens/sec=1373909.53, grad_norm=0.2855, duration=0.38s
Step 4971: loss=3.2354, lr=0.000558, tokens/sec=1373951.59, grad_norm=0.2781, duration=0.38s
Step 4972: loss=3.2339, lr=0.000558, tokens/sec=1373013.09, grad_norm=0.3429, duration=0.38s
Step 4973: loss=3.2688, lr=0.000558, tokens/sec=1373172.56, grad_norm=0.2854, duration=0.38s
Step 4974: loss=3.1957, lr=0.000558, tokens/sec=1374901.69, grad_norm=0.2674, duration=0.38s
Step 4975: loss=3.1404, lr=0.000558, tokens/sec=1377802.18, grad_norm=0.2744, duration=0.38s
Step 4976: loss=3.2230, lr=0.000558, tokens/sec=1373212.01, grad_norm=0.2720, duration=0.38s
Step 4977: loss=3.1392, lr=0.000558, tokens/sec=1374464.27, grad_norm=0.2821, duration=0.38s
Step 4978: loss=3.1435, lr=0.000558, tokens/sec=1372413.26, grad_norm=0.2736, duration=0.38s
Step 4979: loss=3.1878, lr=0.000558, tokens/sec=1373169.13, grad_norm=0.2945, duration=0.38s
Step 4980: loss=3.1549, lr=0.000558, tokens/sec=1378232.22, grad_norm=0.2874, duration=0.38s
Step 4981: loss=3.1934, lr=0.000558, tokens/sec=1375463.26, grad_norm=0.2897, duration=0.38s
Step 4982: loss=3.1766, lr=0.000558, tokens/sec=1375276.59, grad_norm=0.2715, duration=0.38s
Step 4983: loss=3.1763, lr=0.000558, tokens/sec=1373801.38, grad_norm=0.2926, duration=0.38s
Step 4984: loss=3.1639, lr=0.000558, tokens/sec=1377920.46, grad_norm=0.2671, duration=0.38s
Step 4985: loss=3.1563, lr=0.000558, tokens/sec=1372886.22, grad_norm=0.3238, duration=0.38s
Step 4986: loss=3.1725, lr=0.000558, tokens/sec=1371600.90, grad_norm=0.2765, duration=0.38s
Step 4987: loss=3.2642, lr=0.000558, tokens/sec=1373475.32, grad_norm=0.3289, duration=0.38s
Step 4988: loss=3.2960, lr=0.000558, tokens/sec=1376650.68, grad_norm=0.3063, duration=0.38s
Step 4989: loss=3.2421, lr=0.000558, tokens/sec=1376259.53, grad_norm=0.3277, duration=0.38s
Step 4990: loss=3.2467, lr=0.000558, tokens/sec=1374416.16, grad_norm=0.3351, duration=0.38s
Step 4991: loss=3.2942, lr=0.000558, tokens/sec=1375458.95, grad_norm=0.2888, duration=0.38s
Step 4992: loss=3.2536, lr=0.000558, tokens/sec=1375284.33, grad_norm=0.3306, duration=0.38s
Step 4993: loss=3.2422, lr=0.000558, tokens/sec=1372343.03, grad_norm=0.2824, duration=0.38s
Step 4994: loss=3.2022, lr=0.000558, tokens/sec=1372981.37, grad_norm=0.2959, duration=0.38s
Step 4995: loss=3.2164, lr=0.000558, tokens/sec=1371242.53, grad_norm=0.3039, duration=0.38s
Step 4996: loss=3.2371, lr=0.000558, tokens/sec=1375807.48, grad_norm=0.3137, duration=0.38s
Step 4997: loss=3.1915, lr=0.000558, tokens/sec=1372218.86, grad_norm=0.3035, duration=0.38s
Step 4998: loss=3.2001, lr=0.000557, tokens/sec=1374144.77, grad_norm=0.2871, duration=0.38s
Step 4999: loss=3.1910, lr=0.000557, tokens/sec=1373370.67, grad_norm=0.3020, duration=0.38s
Step 5000/19073 (26.2%), Elapsed time: 1990.10s, Steps per hour: 9044.79, Estimated hours remaining: 1.56
Validation loss at step 5000: 3.700774669647217
Checkpoint saved to model_checkpoints/model_05000.pt
Step 5000: loss=3.1910, lr=0.000557, tokens/sec=77255.11, grad_norm=0.2734, duration=6.79s
Step 5001: loss=3.1306, lr=0.000557, tokens/sec=1384458.88, grad_norm=0.2993, duration=0.38s
Step 5002: loss=3.1783, lr=0.000557, tokens/sec=1378338.47, grad_norm=0.3077, duration=0.38s
Step 5003: loss=3.1592, lr=0.000557, tokens/sec=1382583.07, grad_norm=0.2714, duration=0.38s
Step 5004: loss=3.2087, lr=0.000557, tokens/sec=1382319.73, grad_norm=0.3405, duration=0.38s
Step 5005: loss=3.2533, lr=0.000557, tokens/sec=1375582.85, grad_norm=0.2786, duration=0.38s
Step 5006: loss=3.2220, lr=0.000557, tokens/sec=1379289.46, grad_norm=0.3070, duration=0.38s
Step 5007: loss=3.3018, lr=0.000557, tokens/sec=1376178.57, grad_norm=0.2991, duration=0.38s
Step 5008: loss=3.2620, lr=0.000557, tokens/sec=1375656.00, grad_norm=0.2837, duration=0.38s
Step 5009: loss=3.2325, lr=0.000557, tokens/sec=1374791.66, grad_norm=0.3099, duration=0.38s
Step 5010: loss=3.2358, lr=0.000557, tokens/sec=1375484.76, grad_norm=0.3037, duration=0.38s
Step 5011: loss=3.2239, lr=0.000557, tokens/sec=1377458.68, grad_norm=0.3127, duration=0.38s
Step 5012: loss=3.1762, lr=0.000557, tokens/sec=1377124.85, grad_norm=0.2847, duration=0.38s
Step 5013: loss=3.2413, lr=0.000557, tokens/sec=1374705.72, grad_norm=0.3036, duration=0.38s
Step 5014: loss=3.2389, lr=0.000557, tokens/sec=1375279.17, grad_norm=0.2817, duration=0.38s
Step 5015: loss=3.2284, lr=0.000557, tokens/sec=1374854.41, grad_norm=0.2787, duration=0.38s
Step 5016: loss=3.2597, lr=0.000557, tokens/sec=1371705.28, grad_norm=0.3092, duration=0.38s
Step 5017: loss=3.1833, lr=0.000557, tokens/sec=1376949.80, grad_norm=0.2834, duration=0.38s
Step 5018: loss=3.2408, lr=0.000557, tokens/sec=1368357.64, grad_norm=0.2976, duration=0.38s
Step 5019: loss=3.2174, lr=0.000557, tokens/sec=1372145.22, grad_norm=0.2783, duration=0.38s
Step 5020: loss=3.2247, lr=0.000557, tokens/sec=1375249.93, grad_norm=0.3070, duration=0.38s
Step 5021: loss=3.1664, lr=0.000557, tokens/sec=1376948.08, grad_norm=0.2963, duration=0.38s
Step 5022: loss=3.1753, lr=0.000557, tokens/sec=1374795.96, grad_norm=0.3012, duration=0.38s
Step 5023: loss=3.1442, lr=0.000557, tokens/sec=1372414.97, grad_norm=0.2983, duration=0.38s
Step 5024: loss=3.1297, lr=0.000557, tokens/sec=1377082.59, grad_norm=0.2793, duration=0.38s
Step 5025: loss=3.1091, lr=0.000557, tokens/sec=1374950.69, grad_norm=0.2963, duration=0.38s
Step 5026: loss=3.1409, lr=0.000557, tokens/sec=1376008.93, grad_norm=0.2741, duration=0.38s
Step 5027: loss=3.1411, lr=0.000557, tokens/sec=1373350.08, grad_norm=0.2955, duration=0.38s
Step 5028: loss=3.1147, lr=0.000557, tokens/sec=1374233.22, grad_norm=0.2905, duration=0.38s
Step 5029: loss=3.1273, lr=0.000557, tokens/sec=1374759.86, grad_norm=0.2895, duration=0.38s
Step 5030: loss=3.1208, lr=0.000557, tokens/sec=1376563.64, grad_norm=0.3036, duration=0.38s
Step 5031: loss=3.1869, lr=0.000557, tokens/sec=1373465.88, grad_norm=0.2690, duration=0.38s
Step 5032: loss=3.1711, lr=0.000557, tokens/sec=1377501.83, grad_norm=0.3083, duration=0.38s
Step 5033: loss=3.1936, lr=0.000557, tokens/sec=1375996.87, grad_norm=0.2581, duration=0.38s
Step 5034: loss=3.2678, lr=0.000557, tokens/sec=1375176.82, grad_norm=0.3240, duration=0.38s
Step 5035: loss=3.2252, lr=0.000557, tokens/sec=1377173.15, grad_norm=0.2925, duration=0.38s
Step 5036: loss=3.2424, lr=0.000557, tokens/sec=1376840.31, grad_norm=0.3079, duration=0.38s
Step 5037: loss=3.2434, lr=0.000557, tokens/sec=1370153.19, grad_norm=0.2984, duration=0.38s
Step 5038: loss=3.1864, lr=0.000557, tokens/sec=1373927.55, grad_norm=0.2915, duration=0.38s
Step 5039: loss=3.2115, lr=0.000557, tokens/sec=1374567.37, grad_norm=0.2906, duration=0.38s
Step 5040: loss=3.1987, lr=0.000557, tokens/sec=1378234.81, grad_norm=0.2860, duration=0.38s
Step 5041: loss=3.2656, lr=0.000557, tokens/sec=1379747.27, grad_norm=0.2859, duration=0.38s
Step 5042: loss=3.2201, lr=0.000557, tokens/sec=1376027.01, grad_norm=0.2587, duration=0.38s
Step 5043: loss=3.2064, lr=0.000557, tokens/sec=1375410.78, grad_norm=0.2961, duration=0.38s
Step 5044: loss=3.2336, lr=0.000557, tokens/sec=1375607.81, grad_norm=0.2791, duration=0.38s
Step 5045: loss=3.1881, lr=0.000557, tokens/sec=1379145.00, grad_norm=0.2676, duration=0.38s
Step 5046: loss=3.2743, lr=0.000557, tokens/sec=1373630.60, grad_norm=0.2899, duration=0.38s
Step 5047: loss=3.2423, lr=0.000556, tokens/sec=1374757.28, grad_norm=0.2823, duration=0.38s
Step 5048: loss=3.2266, lr=0.000556, tokens/sec=1374979.06, grad_norm=0.3125, duration=0.38s
Step 5049: loss=3.1759, lr=0.000556, tokens/sec=1378239.99, grad_norm=0.2685, duration=0.38s
Step 5050: loss=3.1931, lr=0.000556, tokens/sec=1376157.90, grad_norm=0.3182, duration=0.38s
Step 5051: loss=3.2199, lr=0.000556, tokens/sec=1373428.14, grad_norm=0.2769, duration=0.38s
Step 5052: loss=3.2408, lr=0.000556, tokens/sec=1376888.59, grad_norm=0.3126, duration=0.38s
Step 5053: loss=3.1805, lr=0.000556, tokens/sec=1373862.32, grad_norm=0.2911, duration=0.38s
Step 5054: loss=3.2856, lr=0.000556, tokens/sec=1375493.37, grad_norm=0.3014, duration=0.38s
Step 5055: loss=3.2186, lr=0.000556, tokens/sec=1374675.64, grad_norm=0.3143, duration=0.38s
Step 5056: loss=3.2284, lr=0.000556, tokens/sec=1376163.06, grad_norm=0.2988, duration=0.38s
Step 5057: loss=3.2339, lr=0.000556, tokens/sec=1375040.10, grad_norm=0.2980, duration=0.38s
Step 5058: loss=3.2217, lr=0.000556, tokens/sec=1375842.77, grad_norm=0.3505, duration=0.38s
Step 5059: loss=3.1919, lr=0.000556, tokens/sec=1377116.22, grad_norm=0.2890, duration=0.38s
Step 5060: loss=3.2287, lr=0.000556, tokens/sec=1372959.94, grad_norm=0.3647, duration=0.38s
Step 5061: loss=3.2083, lr=0.000556, tokens/sec=1376195.79, grad_norm=0.2924, duration=0.38s
Step 5062: loss=3.2172, lr=0.000556, tokens/sec=1376248.33, grad_norm=0.3235, duration=0.38s
Step 5063: loss=3.1840, lr=0.000556, tokens/sec=1377911.82, grad_norm=0.3532, duration=0.38s
Step 5064: loss=3.1924, lr=0.000556, tokens/sec=1377628.68, grad_norm=0.3002, duration=0.38s
Step 5065: loss=3.2163, lr=0.000556, tokens/sec=1374855.27, grad_norm=0.2827, duration=0.38s
Step 5066: loss=3.1997, lr=0.000556, tokens/sec=1375126.95, grad_norm=0.2694, duration=0.38s
Step 5067: loss=3.2095, lr=0.000556, tokens/sec=1378351.43, grad_norm=0.2686, duration=0.38s
Step 5068: loss=3.1738, lr=0.000556, tokens/sec=1373215.44, grad_norm=0.2480, duration=0.38s
Step 5069: loss=3.1123, lr=0.000556, tokens/sec=1378647.83, grad_norm=0.2502, duration=0.38s
Step 5070: loss=3.1653, lr=0.000556, tokens/sec=1376696.36, grad_norm=0.2725, duration=0.38s
Step 5071: loss=3.0795, lr=0.000556, tokens/sec=1377486.30, grad_norm=0.2524, duration=0.38s
Step 5072: loss=3.1386, lr=0.000556, tokens/sec=1369360.56, grad_norm=0.2663, duration=0.38s
Step 5073: loss=3.1301, lr=0.000556, tokens/sec=1373594.57, grad_norm=0.2493, duration=0.38s
Step 5074: loss=3.2172, lr=0.000556, tokens/sec=1377893.69, grad_norm=0.2559, duration=0.38s
Step 5075: loss=3.1683, lr=0.000556, tokens/sec=1372463.80, grad_norm=0.2626, duration=0.38s
Step 5076: loss=3.1340, lr=0.000556, tokens/sec=1370439.24, grad_norm=0.2376, duration=0.38s
Step 5077: loss=3.1220, lr=0.000556, tokens/sec=1377370.68, grad_norm=0.2741, duration=0.38s
Step 5078: loss=3.1581, lr=0.000556, tokens/sec=1365186.47, grad_norm=0.2484, duration=0.38s
Step 5079: loss=3.1813, lr=0.000556, tokens/sec=1375388.41, grad_norm=0.2881, duration=0.38s
Step 5080: loss=3.2168, lr=0.000556, tokens/sec=1374921.46, grad_norm=0.2900, duration=0.38s
Step 5081: loss=3.2337, lr=0.000556, tokens/sec=1374414.45, grad_norm=0.2837, duration=0.38s
Step 5082: loss=3.2437, lr=0.000556, tokens/sec=1375705.91, grad_norm=0.2604, duration=0.38s
Step 5083: loss=3.2105, lr=0.000556, tokens/sec=1374046.88, grad_norm=0.3017, duration=0.38s
Step 5084: loss=3.2271, lr=0.000556, tokens/sec=1375816.08, grad_norm=0.2657, duration=0.38s
Step 5085: loss=3.2052, lr=0.000556, tokens/sec=1369804.97, grad_norm=0.2809, duration=0.38s
Step 5086: loss=3.2309, lr=0.000556, tokens/sec=1369624.10, grad_norm=0.2778, duration=0.38s
Step 5087: loss=3.2520, lr=0.000556, tokens/sec=1373416.98, grad_norm=0.2997, duration=0.38s
Step 5088: loss=3.3063, lr=0.000556, tokens/sec=1367329.84, grad_norm=0.2710, duration=0.38s
Step 5089: loss=3.2528, lr=0.000556, tokens/sec=1375544.13, grad_norm=0.2877, duration=0.38s
Step 5090: loss=3.2345, lr=0.000556, tokens/sec=1373307.20, grad_norm=0.2843, duration=0.38s
Step 5091: loss=3.1923, lr=0.000556, tokens/sec=1374234.07, grad_norm=0.2739, duration=0.38s
Step 5092: loss=3.2047, lr=0.000556, tokens/sec=1372419.26, grad_norm=0.2785, duration=0.38s
Step 5093: loss=3.1579, lr=0.000556, tokens/sec=1376271.59, grad_norm=0.2601, duration=0.38s
Step 5094: loss=3.1462, lr=0.000556, tokens/sec=1375208.64, grad_norm=0.2921, duration=0.38s
Step 5095: loss=3.2284, lr=0.000556, tokens/sec=1368073.31, grad_norm=0.2683, duration=0.38s
Step 5096: loss=3.1483, lr=0.000555, tokens/sec=1375915.94, grad_norm=0.2785, duration=0.38s
Step 5097: loss=3.2391, lr=0.000555, tokens/sec=1374841.51, grad_norm=0.2677, duration=0.38s
Step 5098: loss=3.2381, lr=0.000555, tokens/sec=1372057.04, grad_norm=0.2838, duration=0.38s
Step 5099: loss=3.1964, lr=0.000555, tokens/sec=1378731.68, grad_norm=0.2842, duration=0.38s
Step 5100/19073 (26.7%), Elapsed time: 2034.72s, Steps per hour: 9023.37, Estimated hours remaining: 1.55
Step 5100: loss=3.1794, lr=0.000555, tokens/sec=1376613.63, grad_norm=0.2576, duration=0.38s
Step 5101: loss=3.2516, lr=0.000555, tokens/sec=1374375.79, grad_norm=0.2874, duration=0.38s
Step 5102: loss=3.2537, lr=0.000555, tokens/sec=1372896.51, grad_norm=0.2623, duration=0.38s
Step 5103: loss=3.1934, lr=0.000555, tokens/sec=1374997.97, grad_norm=0.2707, duration=0.38s
Step 5104: loss=3.1424, lr=0.000555, tokens/sec=1378681.54, grad_norm=0.2519, duration=0.38s
Step 5105: loss=3.1971, lr=0.000555, tokens/sec=1375767.02, grad_norm=0.2825, duration=0.38s
Step 5106: loss=3.2108, lr=0.000555, tokens/sec=1377518.22, grad_norm=0.2629, duration=0.38s
Step 5107: loss=3.1889, lr=0.000555, tokens/sec=1373234.30, grad_norm=0.2597, duration=0.38s
Step 5108: loss=3.2575, lr=0.000555, tokens/sec=1376311.21, grad_norm=0.2879, duration=0.38s
Step 5109: loss=3.1654, lr=0.000555, tokens/sec=1371993.69, grad_norm=0.2670, duration=0.38s
Step 5110: loss=3.2053, lr=0.000555, tokens/sec=1375042.68, grad_norm=0.2812, duration=0.38s
Step 5111: loss=3.2109, lr=0.000555, tokens/sec=1378174.35, grad_norm=0.2779, duration=0.38s
Step 5112: loss=3.1456, lr=0.000555, tokens/sec=1374983.36, grad_norm=0.3095, duration=0.38s
Step 5113: loss=3.1863, lr=0.000555, tokens/sec=1376234.55, grad_norm=0.2830, duration=0.38s
Step 5114: loss=3.1719, lr=0.000555, tokens/sec=1374682.51, grad_norm=0.2908, duration=0.38s
Step 5115: loss=3.1021, lr=0.000555, tokens/sec=1362770.22, grad_norm=0.2979, duration=0.38s
Step 5116: loss=3.1533, lr=0.000555, tokens/sec=1376562.78, grad_norm=0.2878, duration=0.38s
Step 5117: loss=3.2122, lr=0.000555, tokens/sec=1379111.27, grad_norm=0.3009, duration=0.38s
Step 5118: loss=3.1788, lr=0.000555, tokens/sec=1375080.51, grad_norm=0.2919, duration=0.38s
Step 5119: loss=3.1215, lr=0.000555, tokens/sec=1374720.33, grad_norm=0.2903, duration=0.38s
Step 5120: loss=3.0972, lr=0.000555, tokens/sec=1379069.75, grad_norm=0.2820, duration=0.38s
Step 5121: loss=3.1161, lr=0.000555, tokens/sec=1372269.38, grad_norm=0.3007, duration=0.38s
Step 5122: loss=3.1736, lr=0.000555, tokens/sec=1376367.20, grad_norm=0.3116, duration=0.38s
Step 5123: loss=3.1662, lr=0.000555, tokens/sec=1376644.65, grad_norm=0.3027, duration=0.38s
Step 5124: loss=3.1634, lr=0.000555, tokens/sec=1378273.68, grad_norm=0.2862, duration=0.38s
Step 5125: loss=3.2298, lr=0.000555, tokens/sec=1373150.27, grad_norm=0.2857, duration=0.38s
Step 5126: loss=3.2054, lr=0.000555, tokens/sec=1375526.92, grad_norm=0.2902, duration=0.38s
Step 5127: loss=3.2239, lr=0.000555, tokens/sec=1369994.42, grad_norm=0.2973, duration=0.38s
Step 5128: loss=3.1743, lr=0.000555, tokens/sec=1376181.15, grad_norm=0.2956, duration=0.38s
Step 5129: loss=3.2164, lr=0.000555, tokens/sec=1373758.46, grad_norm=0.3124, duration=0.38s
Step 5130: loss=3.2261, lr=0.000555, tokens/sec=1376922.21, grad_norm=0.2878, duration=0.38s
Step 5131: loss=3.2278, lr=0.000555, tokens/sec=1377741.75, grad_norm=0.2914, duration=0.38s
Step 5132: loss=3.2271, lr=0.000555, tokens/sec=1372151.21, grad_norm=0.2782, duration=0.38s
Step 5133: loss=3.3002, lr=0.000555, tokens/sec=1374098.40, grad_norm=0.2907, duration=0.38s
Step 5134: loss=3.2475, lr=0.000555, tokens/sec=1375607.81, grad_norm=0.2798, duration=0.38s
Step 5135: loss=3.2904, lr=0.000555, tokens/sec=1377206.78, grad_norm=0.2752, duration=0.38s
Step 5136: loss=3.3079, lr=0.000555, tokens/sec=1372677.98, grad_norm=0.2900, duration=0.38s
Step 5137: loss=3.2466, lr=0.000555, tokens/sec=1371998.83, grad_norm=0.2842, duration=0.38s
Step 5138: loss=3.2363, lr=0.000555, tokens/sec=1369537.95, grad_norm=0.2740, duration=0.38s
Step 5139: loss=3.2290, lr=0.000555, tokens/sec=1374488.33, grad_norm=0.2739, duration=0.38s
Step 5140: loss=3.2202, lr=0.000555, tokens/sec=1373086.82, grad_norm=0.2920, duration=0.38s
Step 5141: loss=3.2155, lr=0.000555, tokens/sec=1377318.92, grad_norm=0.2718, duration=0.38s
Step 5142: loss=3.2218, lr=0.000555, tokens/sec=1375737.76, grad_norm=0.2779, duration=0.38s
Step 5143: loss=3.1372, lr=0.000555, tokens/sec=1373414.41, grad_norm=0.2832, duration=0.38s
Step 5144: loss=3.2438, lr=0.000555, tokens/sec=1371163.87, grad_norm=0.2852, duration=0.38s
Step 5145: loss=3.2198, lr=0.000554, tokens/sec=1375749.81, grad_norm=0.3031, duration=0.38s
Step 5146: loss=3.1175, lr=0.000554, tokens/sec=1377853.11, grad_norm=0.3137, duration=0.38s
Step 5147: loss=3.2320, lr=0.000554, tokens/sec=1377076.56, grad_norm=0.3195, duration=0.38s
Step 5148: loss=3.1603, lr=0.000554, tokens/sec=1375849.66, grad_norm=0.2961, duration=0.38s
Step 5149: loss=3.1520, lr=0.000554, tokens/sec=1378822.45, grad_norm=0.3703, duration=0.38s
Step 5150: loss=3.1946, lr=0.000554, tokens/sec=1377745.21, grad_norm=0.2969, duration=0.38s
Step 5151: loss=3.2291, lr=0.000554, tokens/sec=1374448.81, grad_norm=0.3194, duration=0.38s
Step 5152: loss=3.2848, lr=0.000554, tokens/sec=1378385.99, grad_norm=0.2885, duration=0.38s
Step 5153: loss=3.1483, lr=0.000554, tokens/sec=1374620.64, grad_norm=0.3071, duration=0.38s
Step 5154: loss=3.2832, lr=0.000554, tokens/sec=1374166.23, grad_norm=0.3141, duration=0.38s
Step 5155: loss=3.2475, lr=0.000554, tokens/sec=1370416.18, grad_norm=0.3011, duration=0.38s
Step 5156: loss=3.1830, lr=0.000554, tokens/sec=1377097.25, grad_norm=0.3321, duration=0.38s
Step 5157: loss=3.2185, lr=0.000554, tokens/sec=1377167.97, grad_norm=0.2937, duration=0.38s
Step 5158: loss=3.2301, lr=0.000554, tokens/sec=1377125.71, grad_norm=0.2962, duration=0.38s
Step 5159: loss=3.2077, lr=0.000554, tokens/sec=1377137.79, grad_norm=0.2813, duration=0.38s
Step 5160: loss=3.1981, lr=0.000554, tokens/sec=1374932.63, grad_norm=0.2865, duration=0.38s
Step 5161: loss=3.1979, lr=0.000554, tokens/sec=1373944.72, grad_norm=0.2671, duration=0.38s
Step 5162: loss=3.2273, lr=0.000554, tokens/sec=1379044.67, grad_norm=0.3315, duration=0.38s
Step 5163: loss=3.2517, lr=0.000554, tokens/sec=1376523.15, grad_norm=0.2670, duration=0.38s
Step 5164: loss=3.1801, lr=0.000554, tokens/sec=1371025.38, grad_norm=0.2691, duration=0.38s
Step 5165: loss=3.1528, lr=0.000554, tokens/sec=1376872.21, grad_norm=0.2624, duration=0.38s
Step 5166: loss=3.1802, lr=0.000554, tokens/sec=1373956.74, grad_norm=0.2691, duration=0.38s
Step 5167: loss=3.1366, lr=0.000554, tokens/sec=1377267.16, grad_norm=0.2649, duration=0.38s
Step 5168: loss=3.1541, lr=0.000554, tokens/sec=1375261.97, grad_norm=0.2899, duration=0.38s
Step 5169: loss=3.1273, lr=0.000554, tokens/sec=1376394.77, grad_norm=0.2732, duration=0.38s
Step 5170: loss=3.1685, lr=0.000554, tokens/sec=1378205.44, grad_norm=0.2941, duration=0.38s
Step 5171: loss=3.1664, lr=0.000554, tokens/sec=1376902.38, grad_norm=0.2824, duration=0.38s
Step 5172: loss=3.1437, lr=0.000554, tokens/sec=1376748.08, grad_norm=0.2940, duration=0.38s
Step 5173: loss=3.1751, lr=0.000554, tokens/sec=1374838.94, grad_norm=0.3110, duration=0.38s
Step 5174: loss=3.1429, lr=0.000554, tokens/sec=1374923.18, grad_norm=0.2933, duration=0.38s
Step 5175: loss=3.1519, lr=0.000554, tokens/sec=1377876.42, grad_norm=0.3386, duration=0.38s
Step 5176: loss=3.1625, lr=0.000554, tokens/sec=1374904.26, grad_norm=0.2848, duration=0.38s
Step 5177: loss=3.2403, lr=0.000554, tokens/sec=1372870.80, grad_norm=0.3507, duration=0.38s
Step 5178: loss=3.2718, lr=0.000554, tokens/sec=1372230.84, grad_norm=0.2663, duration=0.38s
Step 5179: loss=3.2349, lr=0.000554, tokens/sec=1374544.17, grad_norm=0.3703, duration=0.38s
Step 5180: loss=3.2559, lr=0.000554, tokens/sec=1375629.32, grad_norm=0.2932, duration=0.38s
Step 5181: loss=3.2401, lr=0.000554, tokens/sec=1377778.01, grad_norm=0.3363, duration=0.38s
Step 5182: loss=3.2366, lr=0.000554, tokens/sec=1375060.74, grad_norm=0.3097, duration=0.38s
Step 5183: loss=3.2275, lr=0.000554, tokens/sec=1374961.86, grad_norm=0.2975, duration=0.38s
Step 5184: loss=3.1987, lr=0.000554, tokens/sec=1377633.86, grad_norm=0.3356, duration=0.38s
Step 5185: loss=3.2012, lr=0.000554, tokens/sec=1374603.46, grad_norm=0.3159, duration=0.38s
Step 5186: loss=3.1915, lr=0.000554, tokens/sec=1376506.77, grad_norm=0.2999, duration=0.38s
Step 5187: loss=3.1941, lr=0.000554, tokens/sec=1377686.51, grad_norm=0.3231, duration=0.38s
Step 5188: loss=3.1765, lr=0.000554, tokens/sec=1373539.66, grad_norm=0.2961, duration=0.38s
Step 5189: loss=3.1700, lr=0.000554, tokens/sec=1375946.07, grad_norm=0.2882, duration=0.38s
Step 5190: loss=3.1900, lr=0.000554, tokens/sec=1375083.09, grad_norm=0.3229, duration=0.38s
Step 5191: loss=3.1163, lr=0.000554, tokens/sec=1379146.73, grad_norm=0.2623, duration=0.38s
Step 5192: loss=3.1033, lr=0.000554, tokens/sec=1378654.75, grad_norm=0.3150, duration=0.38s
Step 5193: loss=3.1998, lr=0.000553, tokens/sec=1375588.02, grad_norm=0.2789, duration=0.38s
Step 5194: loss=3.1879, lr=0.000553, tokens/sec=1377886.78, grad_norm=0.2821, duration=0.38s
Step 5195: loss=3.2806, lr=0.000553, tokens/sec=1373870.04, grad_norm=0.3084, duration=0.38s
Step 5196: loss=3.2043, lr=0.000553, tokens/sec=1377688.24, grad_norm=0.3056, duration=0.38s
Step 5197: loss=3.2882, lr=0.000553, tokens/sec=1374081.23, grad_norm=0.3114, duration=0.38s
Step 5198: loss=3.2356, lr=0.000553, tokens/sec=1376567.95, grad_norm=0.2909, duration=0.38s
Step 5199: loss=3.2191, lr=0.000553, tokens/sec=1374706.58, grad_norm=0.3068, duration=0.38s
Step 5200/19073 (27.3%), Elapsed time: 2072.92s, Steps per hour: 9030.74, Estimated hours remaining: 1.54
Step 5200: loss=3.2065, lr=0.000553, tokens/sec=1377458.68, grad_norm=0.2734, duration=0.38s
Step 5201: loss=3.1850, lr=0.000553, tokens/sec=1377073.11, grad_norm=0.2912, duration=0.38s
Step 5202: loss=3.2063, lr=0.000553, tokens/sec=1374311.37, grad_norm=0.2757, duration=0.38s
Step 5203: loss=3.2289, lr=0.000553, tokens/sec=1372679.69, grad_norm=0.3299, duration=0.38s
Step 5204: loss=3.2092, lr=0.000553, tokens/sec=1377180.91, grad_norm=0.2612, duration=0.38s
Step 5205: loss=3.2486, lr=0.000553, tokens/sec=1376667.06, grad_norm=0.3284, duration=0.38s
Step 5206: loss=3.2061, lr=0.000553, tokens/sec=1374394.69, grad_norm=0.3018, duration=0.38s
Step 5207: loss=3.1892, lr=0.000553, tokens/sec=1376872.21, grad_norm=0.2989, duration=0.38s
Step 5208: loss=3.2153, lr=0.000553, tokens/sec=1379049.86, grad_norm=0.2981, duration=0.38s
Step 5209: loss=3.1977, lr=0.000553, tokens/sec=1376503.33, grad_norm=0.2976, duration=0.38s
Step 5210: loss=3.2278, lr=0.000553, tokens/sec=1373951.59, grad_norm=0.3075, duration=0.38s
Step 5211: loss=3.0977, lr=0.000553, tokens/sec=1372075.02, grad_norm=0.2969, duration=0.38s
Step 5212: loss=3.1887, lr=0.000553, tokens/sec=1374958.42, grad_norm=0.3101, duration=0.38s
Step 5213: loss=3.1072, lr=0.000553, tokens/sec=1378233.95, grad_norm=0.3008, duration=0.38s
Step 5214: loss=3.1470, lr=0.000553, tokens/sec=1374589.71, grad_norm=0.2948, duration=0.38s
Step 5215: loss=3.0907, lr=0.000553, tokens/sec=1377251.64, grad_norm=0.3051, duration=0.38s
Step 5216: loss=3.1035, lr=0.000553, tokens/sec=1376836.86, grad_norm=0.2673, duration=0.38s
Step 5217: loss=3.1471, lr=0.000553, tokens/sec=1377526.85, grad_norm=0.3103, duration=0.38s
Step 5218: loss=3.0881, lr=0.000553, tokens/sec=1377434.53, grad_norm=0.2700, duration=0.38s
Step 5219: loss=3.1274, lr=0.000553, tokens/sec=1375284.33, grad_norm=0.2788, duration=0.38s
Step 5220: loss=3.1213, lr=0.000553, tokens/sec=1375188.86, grad_norm=0.2977, duration=0.38s
Step 5221: loss=3.1722, lr=0.000553, tokens/sec=1378297.87, grad_norm=0.2682, duration=0.38s
Step 5222: loss=3.1468, lr=0.000553, tokens/sec=1377430.21, grad_norm=0.2992, duration=0.38s
Step 5223: loss=3.2052, lr=0.000553, tokens/sec=1378005.94, grad_norm=0.2585, duration=0.38s
Step 5224: loss=3.2071, lr=0.000553, tokens/sec=1378809.48, grad_norm=0.3187, duration=0.38s
Step 5225: loss=3.2180, lr=0.000553, tokens/sec=1379237.55, grad_norm=0.2609, duration=0.38s
Step 5226: loss=3.2336, lr=0.000553, tokens/sec=1375556.18, grad_norm=0.3242, duration=0.38s
Step 5227: loss=3.2251, lr=0.000553, tokens/sec=1371892.69, grad_norm=0.2890, duration=0.38s
Step 5228: loss=3.1523, lr=0.000553, tokens/sec=1372685.69, grad_norm=0.3065, duration=0.38s
Step 5229: loss=3.2158, lr=0.000553, tokens/sec=1376828.24, grad_norm=0.3201, duration=0.38s
Step 5230: loss=3.1981, lr=0.000553, tokens/sec=1372902.51, grad_norm=0.2938, duration=0.38s
Step 5231: loss=3.2212, lr=0.000553, tokens/sec=1373352.65, grad_norm=0.3118, duration=0.38s
Step 5232: loss=3.2015, lr=0.000553, tokens/sec=1375593.18, grad_norm=0.2720, duration=0.38s
Step 5233: loss=3.2071, lr=0.000553, tokens/sec=1373777.35, grad_norm=0.3064, duration=0.38s
Step 5234: loss=3.2143, lr=0.000553, tokens/sec=1375261.11, grad_norm=0.2793, duration=0.38s
Step 5235: loss=3.2506, lr=0.000553, tokens/sec=1376452.49, grad_norm=0.3017, duration=0.38s
Step 5236: loss=3.2376, lr=0.000553, tokens/sec=1372867.37, grad_norm=0.3006, duration=0.38s
Step 5237: loss=3.1899, lr=0.000553, tokens/sec=1374556.20, grad_norm=0.2798, duration=0.38s
Step 5238: loss=3.2186, lr=0.000553, tokens/sec=1376056.28, grad_norm=0.3066, duration=0.38s
Step 5239: loss=3.1300, lr=0.000553, tokens/sec=1375148.44, grad_norm=0.2957, duration=0.38s
Step 5240: loss=3.2148, lr=0.000552, tokens/sec=1374618.92, grad_norm=0.3240, duration=0.38s
Step 5241: loss=3.2201, lr=0.000552, tokens/sec=1376368.06, grad_norm=0.3061, duration=0.38s
Step 5242: loss=3.2205, lr=0.000552, tokens/sec=1378225.31, grad_norm=0.2880, duration=0.38s
Step 5243: loss=3.1979, lr=0.000552, tokens/sec=1377166.25, grad_norm=0.2937, duration=0.38s
Step 5244: loss=3.2564, lr=0.000552, tokens/sec=1379633.00, grad_norm=0.2942, duration=0.38s
Step 5245: loss=3.1682, lr=0.000552, tokens/sec=1374775.33, grad_norm=0.2865, duration=0.38s
Step 5246: loss=3.2508, lr=0.000552, tokens/sec=1375867.73, grad_norm=0.2839, duration=0.38s
Step 5247: loss=3.1873, lr=0.000552, tokens/sec=1374820.03, grad_norm=0.3131, duration=0.38s
Step 5248: loss=3.1979, lr=0.000552, tokens/sec=1377023.95, grad_norm=0.3059, duration=0.38s
Step 5249: loss=3.2046, lr=0.000552, tokens/sec=1372289.93, grad_norm=0.2848, duration=0.38s
Validation loss at step 5250: 3.701730728149414
Step 5250: loss=3.2001, lr=0.000552, tokens/sec=154288.83, grad_norm=0.3361, duration=3.40s
Step 5251: loss=3.1874, lr=0.000552, tokens/sec=1375338.52, grad_norm=0.2795, duration=0.38s
Step 5252: loss=3.1943, lr=0.000552, tokens/sec=1376394.77, grad_norm=0.3053, duration=0.38s
Step 5253: loss=3.1616, lr=0.000552, tokens/sec=1378714.39, grad_norm=0.2895, duration=0.38s
Step 5254: loss=3.1955, lr=0.000552, tokens/sec=1376088.14, grad_norm=0.2710, duration=0.38s
Step 5255: loss=3.2002, lr=0.000552, tokens/sec=1372170.91, grad_norm=0.2713, duration=0.38s
Step 5256: loss=3.2073, lr=0.000552, tokens/sec=1376464.56, grad_norm=0.2562, duration=0.38s
Step 5257: loss=3.1664, lr=0.000552, tokens/sec=1373930.13, grad_norm=0.2583, duration=0.38s
Step 5258: loss=3.1561, lr=0.000552, tokens/sec=1371092.06, grad_norm=0.2611, duration=0.38s
Step 5259: loss=3.1139, lr=0.000552, tokens/sec=1374844.09, grad_norm=0.2594, duration=0.38s
Step 5260: loss=3.1448, lr=0.000552, tokens/sec=1378452.52, grad_norm=0.2812, duration=0.38s
Step 5261: loss=3.0785, lr=0.000552, tokens/sec=1377902.32, grad_norm=0.2517, duration=0.38s
Step 5262: loss=3.1230, lr=0.000552, tokens/sec=1370662.19, grad_norm=0.2794, duration=0.38s
Step 5263: loss=3.1388, lr=0.000552, tokens/sec=1373641.76, grad_norm=0.2440, duration=0.38s
Step 5264: loss=3.1834, lr=0.000552, tokens/sec=1374482.31, grad_norm=0.2564, duration=0.38s
Step 5265: loss=3.1479, lr=0.000552, tokens/sec=1375296.37, grad_norm=0.2620, duration=0.38s
Step 5266: loss=3.1207, lr=0.000552, tokens/sec=1375168.22, grad_norm=0.2481, duration=0.38s
Step 5267: loss=3.1115, lr=0.000552, tokens/sec=1375900.45, grad_norm=0.2494, duration=0.38s
Step 5268: loss=3.1258, lr=0.000552, tokens/sec=1376457.66, grad_norm=0.2497, duration=0.38s
Step 5269: loss=3.1725, lr=0.000552, tokens/sec=1374490.90, grad_norm=0.2795, duration=0.38s
Step 5270: loss=3.2060, lr=0.000552, tokens/sec=1375616.41, grad_norm=0.2579, duration=0.38s
Step 5271: loss=3.2197, lr=0.000552, tokens/sec=1374188.56, grad_norm=0.3178, duration=0.38s
Step 5272: loss=3.2204, lr=0.000552, tokens/sec=1374202.30, grad_norm=0.3183, duration=0.38s
Step 5273: loss=3.1888, lr=0.000552, tokens/sec=1375071.91, grad_norm=0.3042, duration=0.38s
Step 5274: loss=3.2452, lr=0.000552, tokens/sec=1374922.32, grad_norm=0.2901, duration=0.38s
Step 5275: loss=3.1903, lr=0.000552, tokens/sec=1377517.36, grad_norm=0.2948, duration=0.38s
Step 5276: loss=3.1878, lr=0.000552, tokens/sec=1375448.63, grad_norm=0.2663, duration=0.38s
Step 5277: loss=3.2298, lr=0.000552, tokens/sec=1372982.23, grad_norm=0.2931, duration=0.38s
Step 5278: loss=3.3013, lr=0.000552, tokens/sec=1377938.59, grad_norm=0.2776, duration=0.38s
Step 5279: loss=3.2296, lr=0.000552, tokens/sec=1376647.24, grad_norm=0.2865, duration=0.38s
Step 5280: loss=3.2386, lr=0.000552, tokens/sec=1376442.15, grad_norm=0.2874, duration=0.38s
Step 5281: loss=3.1585, lr=0.000552, tokens/sec=1375584.57, grad_norm=0.2636, duration=0.38s
Step 5282: loss=3.1776, lr=0.000552, tokens/sec=1375622.44, grad_norm=0.2828, duration=0.38s
Step 5283: loss=3.1621, lr=0.000552, tokens/sec=1372610.29, grad_norm=0.2895, duration=0.38s
Step 5284: loss=3.1267, lr=0.000552, tokens/sec=1377173.15, grad_norm=0.3032, duration=0.38s
Step 5285: loss=3.2204, lr=0.000552, tokens/sec=1373185.42, grad_norm=0.2671, duration=0.38s
Step 5286: loss=3.1370, lr=0.000552, tokens/sec=1376383.57, grad_norm=0.3047, duration=0.38s
Step 5287: loss=3.1950, lr=0.000551, tokens/sec=1373939.57, grad_norm=0.2567, duration=0.38s
Step 5288: loss=3.2450, lr=0.000551, tokens/sec=1375590.60, grad_norm=0.3079, duration=0.38s
Step 5289: loss=3.2248, lr=0.000551, tokens/sec=1373782.49, grad_norm=0.2892, duration=0.38s
Step 5290: loss=3.1724, lr=0.000551, tokens/sec=1378816.40, grad_norm=0.2588, duration=0.38s
Step 5291: loss=3.2282, lr=0.000551, tokens/sec=1374482.31, grad_norm=0.3056, duration=0.38s
Step 5292: loss=3.2221, lr=0.000551, tokens/sec=1376140.67, grad_norm=0.2965, duration=0.38s
Step 5293: loss=3.2001, lr=0.000551, tokens/sec=1377674.43, grad_norm=0.2618, duration=0.38s
Step 5294: loss=3.1170, lr=0.000551, tokens/sec=1374840.65, grad_norm=0.2851, duration=0.38s
Step 5295: loss=3.2051, lr=0.000551, tokens/sec=1374296.77, grad_norm=0.2958, duration=0.38s
Step 5296: loss=3.1566, lr=0.000551, tokens/sec=1378481.90, grad_norm=0.2758, duration=0.38s
Step 5297: loss=3.2116, lr=0.000551, tokens/sec=1375299.81, grad_norm=0.2819, duration=0.38s
Step 5298: loss=3.2344, lr=0.000551, tokens/sec=1373359.52, grad_norm=0.3088, duration=0.38s
Step 5299: loss=3.1773, lr=0.000551, tokens/sec=1375406.48, grad_norm=0.2728, duration=0.38s
Step 5300/19073 (27.8%), Elapsed time: 2114.14s, Steps per hour: 9024.96, Estimated hours remaining: 1.53
Step 5300: loss=3.2011, lr=0.000551, tokens/sec=1375071.05, grad_norm=0.3038, duration=0.38s
Step 5301: loss=3.1763, lr=0.000551, tokens/sec=1377634.73, grad_norm=0.2586, duration=0.38s
Step 5302: loss=3.1292, lr=0.000551, tokens/sec=1375109.75, grad_norm=0.2902, duration=0.38s
Step 5303: loss=3.2000, lr=0.000551, tokens/sec=1374261.56, grad_norm=0.2774, duration=0.38s
Step 5304: loss=3.0955, lr=0.000551, tokens/sec=1376765.32, grad_norm=0.2698, duration=0.38s
Step 5305: loss=3.1383, lr=0.000551, tokens/sec=1377697.73, grad_norm=0.2908, duration=0.38s
Step 5306: loss=3.1285, lr=0.000551, tokens/sec=1374581.98, grad_norm=0.2787, duration=0.38s
Step 5307: loss=3.2233, lr=0.000551, tokens/sec=1374887.07, grad_norm=0.2869, duration=0.38s
Step 5308: loss=3.1265, lr=0.000551, tokens/sec=1374918.02, grad_norm=0.2840, duration=0.38s
Step 5309: loss=3.1188, lr=0.000551, tokens/sec=1378618.45, grad_norm=0.2725, duration=0.38s
Step 5310: loss=3.0638, lr=0.000551, tokens/sec=1373156.27, grad_norm=0.3293, duration=0.38s
Step 5311: loss=3.1269, lr=0.000551, tokens/sec=1375333.36, grad_norm=0.2960, duration=0.38s
Step 5312: loss=3.1640, lr=0.000551, tokens/sec=1374741.81, grad_norm=0.3065, duration=0.38s
Step 5313: loss=3.1309, lr=0.000551, tokens/sec=1378111.30, grad_norm=0.2906, duration=0.38s
Step 5314: loss=3.1648, lr=0.000551, tokens/sec=1373850.30, grad_norm=0.2774, duration=0.38s
Step 5315: loss=3.1893, lr=0.000551, tokens/sec=1373969.62, grad_norm=0.2863, duration=0.38s
Step 5316: loss=3.1957, lr=0.000551, tokens/sec=1376726.53, grad_norm=0.2766, duration=0.38s
Step 5317: loss=3.1975, lr=0.000551, tokens/sec=1377772.83, grad_norm=0.3180, duration=0.38s
Step 5318: loss=3.1709, lr=0.000551, tokens/sec=1376397.35, grad_norm=0.2665, duration=0.38s
Step 5319: loss=3.2340, lr=0.000551, tokens/sec=1375577.69, grad_norm=0.3127, duration=0.38s
Step 5320: loss=3.1723, lr=0.000551, tokens/sec=1373950.73, grad_norm=0.2596, duration=0.38s
Step 5321: loss=3.2067, lr=0.000551, tokens/sec=1377132.61, grad_norm=0.3118, duration=0.38s
Step 5322: loss=3.2251, lr=0.000551, tokens/sec=1375303.25, grad_norm=0.2967, duration=0.38s
Step 5323: loss=3.3014, lr=0.000551, tokens/sec=1374628.38, grad_norm=0.2956, duration=0.38s
Step 5324: loss=3.2505, lr=0.000551, tokens/sec=1376087.28, grad_norm=0.2859, duration=0.38s
Step 5325: loss=3.2431, lr=0.000551, tokens/sec=1373455.58, grad_norm=0.2984, duration=0.38s
Step 5326: loss=3.3008, lr=0.000551, tokens/sec=1375912.50, grad_norm=0.2821, duration=0.38s
Step 5327: loss=3.2019, lr=0.000551, tokens/sec=1375120.93, grad_norm=0.3119, duration=0.38s
Step 5328: loss=3.2445, lr=0.000551, tokens/sec=1376599.84, grad_norm=0.2718, duration=0.38s
Step 5329: loss=3.1849, lr=0.000551, tokens/sec=1376477.48, grad_norm=0.2763, duration=0.38s
Step 5330: loss=3.2417, lr=0.000551, tokens/sec=1373679.51, grad_norm=0.2985, duration=0.38s
Step 5331: loss=3.1901, lr=0.000551, tokens/sec=1375077.07, grad_norm=0.2476, duration=0.38s
Step 5332: loss=3.1876, lr=0.000551, tokens/sec=1373812.53, grad_norm=0.2767, duration=0.38s
Step 5333: loss=3.1639, lr=0.000551, tokens/sec=1372932.51, grad_norm=0.2812, duration=0.38s
Step 5334: loss=3.2122, lr=0.000550, tokens/sec=1373458.16, grad_norm=0.2712, duration=0.38s
Step 5335: loss=3.1421, lr=0.000550, tokens/sec=1375366.04, grad_norm=0.3447, duration=0.38s
Step 5336: loss=3.1490, lr=0.000550, tokens/sec=1374240.09, grad_norm=0.3003, duration=0.38s
Step 5337: loss=3.2186, lr=0.000550, tokens/sec=1375980.51, grad_norm=0.3260, duration=0.38s
Step 5338: loss=3.1849, lr=0.000550, tokens/sec=1374918.02, grad_norm=0.3096, duration=0.38s
Step 5339: loss=3.1255, lr=0.000550, tokens/sec=1375423.68, grad_norm=0.3895, duration=0.38s
Step 5340: loss=3.1858, lr=0.000550, tokens/sec=1375159.62, grad_norm=0.3000, duration=0.38s
Step 5341: loss=3.1907, lr=0.000550, tokens/sec=1377956.72, grad_norm=0.3243, duration=0.38s
Step 5342: loss=3.2829, lr=0.000550, tokens/sec=1375142.43, grad_norm=0.3037, duration=0.38s
Step 5343: loss=3.1677, lr=0.000550, tokens/sec=1373894.93, grad_norm=0.3060, duration=0.38s
Step 5344: loss=3.2559, lr=0.000550, tokens/sec=1376852.38, grad_norm=0.3116, duration=0.38s
Step 5345: loss=3.2154, lr=0.000550, tokens/sec=1374308.79, grad_norm=0.2919, duration=0.38s
Step 5346: loss=3.1775, lr=0.000550, tokens/sec=1376605.01, grad_norm=0.3049, duration=0.38s
Step 5347: loss=3.1919, lr=0.000550, tokens/sec=1377081.73, grad_norm=0.2842, duration=0.38s
Step 5348: loss=3.2238, lr=0.000550, tokens/sec=1378532.89, grad_norm=0.2976, duration=0.38s
Step 5349: loss=3.1980, lr=0.000550, tokens/sec=1376811.86, grad_norm=0.2717, duration=0.38s
Step 5350: loss=3.1578, lr=0.000550, tokens/sec=1378174.35, grad_norm=0.2810, duration=0.38s
Step 5351: loss=3.1938, lr=0.000550, tokens/sec=1375163.06, grad_norm=0.2803, duration=0.38s
Step 5352: loss=3.2115, lr=0.000550, tokens/sec=1377877.29, grad_norm=0.3239, duration=0.38s
Step 5353: loss=3.2357, lr=0.000550, tokens/sec=1379651.18, grad_norm=0.2713, duration=0.38s
Step 5354: loss=3.1934, lr=0.000550, tokens/sec=1372105.84, grad_norm=0.2690, duration=0.38s
Step 5355: loss=3.1118, lr=0.000550, tokens/sec=1375437.45, grad_norm=0.2745, duration=0.38s
Step 5356: loss=3.1779, lr=0.000550, tokens/sec=1376937.73, grad_norm=0.2781, duration=0.38s
Step 5357: loss=3.1459, lr=0.000550, tokens/sec=1378473.26, grad_norm=0.2645, duration=0.38s
Step 5358: loss=3.0978, lr=0.000550, tokens/sec=1376528.32, grad_norm=0.2970, duration=0.38s
Step 5359: loss=3.1457, lr=0.000550, tokens/sec=1375553.60, grad_norm=0.2793, duration=0.38s
Step 5360: loss=3.1401, lr=0.000550, tokens/sec=1374397.27, grad_norm=0.2950, duration=0.38s
Step 5361: loss=3.1336, lr=0.000550, tokens/sec=1376924.80, grad_norm=0.3153, duration=0.38s
Step 5362: loss=3.1455, lr=0.000550, tokens/sec=1377435.39, grad_norm=0.3001, duration=0.38s
Step 5363: loss=3.1553, lr=0.000550, tokens/sec=1374355.18, grad_norm=0.3139, duration=0.38s
Step 5364: loss=3.1412, lr=0.000550, tokens/sec=1375104.59, grad_norm=0.2886, duration=0.38s
Step 5365: loss=3.1397, lr=0.000550, tokens/sec=1376759.28, grad_norm=0.3357, duration=0.38s
Step 5366: loss=3.1385, lr=0.000550, tokens/sec=1378309.10, grad_norm=0.2878, duration=0.38s
Step 5367: loss=3.2177, lr=0.000550, tokens/sec=1374834.64, grad_norm=0.3606, duration=0.38s
Step 5368: loss=3.2663, lr=0.000550, tokens/sec=1374650.72, grad_norm=0.2901, duration=0.38s
Step 5369: loss=3.2479, lr=0.000550, tokens/sec=1378022.35, grad_norm=0.3929, duration=0.38s
Step 5370: loss=3.2020, lr=0.000550, tokens/sec=1377768.51, grad_norm=0.2728, duration=0.38s
Step 5371: loss=3.2272, lr=0.000550, tokens/sec=1373344.93, grad_norm=0.3399, duration=0.38s
Step 5372: loss=3.2252, lr=0.000550, tokens/sec=1376959.28, grad_norm=0.2989, duration=0.38s
Step 5373: loss=3.2275, lr=0.000550, tokens/sec=1377367.23, grad_norm=0.3101, duration=0.38s
Step 5374: loss=3.1863, lr=0.000550, tokens/sec=1375897.00, grad_norm=0.3146, duration=0.38s
Step 5375: loss=3.1551, lr=0.000550, tokens/sec=1379760.25, grad_norm=0.2938, duration=0.38s
Step 5376: loss=3.1998, lr=0.000550, tokens/sec=1375458.09, grad_norm=0.3439, duration=0.38s
Step 5377: loss=3.1739, lr=0.000550, tokens/sec=1373336.36, grad_norm=0.2931, duration=0.38s
Step 5378: loss=3.1618, lr=0.000550, tokens/sec=1376662.75, grad_norm=0.3255, duration=0.38s
Step 5379: loss=3.1670, lr=0.000550, tokens/sec=1374956.70, grad_norm=0.2806, duration=0.38s
Step 5380: loss=3.1758, lr=0.000549, tokens/sec=1373613.44, grad_norm=0.3078, duration=0.38s
Step 5381: loss=3.0445, lr=0.000549, tokens/sec=1372352.45, grad_norm=0.3144, duration=0.38s
Step 5382: loss=3.1459, lr=0.000549, tokens/sec=1375983.10, grad_norm=0.2821, duration=0.38s
Step 5383: loss=3.1860, lr=0.000549, tokens/sec=1376053.70, grad_norm=0.3428, duration=0.38s
Step 5384: loss=3.2148, lr=0.000549, tokens/sec=1377348.25, grad_norm=0.2706, duration=0.38s
Step 5385: loss=3.2597, lr=0.000549, tokens/sec=1377224.03, grad_norm=0.3119, duration=0.38s
Step 5386: loss=3.1906, lr=0.000549, tokens/sec=1371382.78, grad_norm=0.3037, duration=0.38s
Step 5387: loss=3.2628, lr=0.000549, tokens/sec=1373246.31, grad_norm=0.2746, duration=0.38s
Step 5388: loss=3.2242, lr=0.000549, tokens/sec=1373703.54, grad_norm=0.3103, duration=0.38s
Step 5389: loss=3.1867, lr=0.000549, tokens/sec=1373296.05, grad_norm=0.2688, duration=0.38s
Step 5390: loss=3.1698, lr=0.000549, tokens/sec=1374075.22, grad_norm=0.3008, duration=0.38s
Step 5391: loss=3.2154, lr=0.000549, tokens/sec=1377596.75, grad_norm=0.2573, duration=0.38s
Step 5392: loss=3.1966, lr=0.000549, tokens/sec=1378157.07, grad_norm=0.3008, duration=0.38s
Step 5393: loss=3.1991, lr=0.000549, tokens/sec=1378539.80, grad_norm=0.2682, duration=0.38s
Step 5394: loss=3.2267, lr=0.000549, tokens/sec=1378043.07, grad_norm=0.2860, duration=0.38s
Step 5395: loss=3.1957, lr=0.000549, tokens/sec=1374740.95, grad_norm=0.3053, duration=0.38s
Step 5396: loss=3.2128, lr=0.000549, tokens/sec=1373241.16, grad_norm=0.2958, duration=0.38s
Step 5397: loss=3.1670, lr=0.000549, tokens/sec=1376126.89, grad_norm=0.3159, duration=0.38s
Step 5398: loss=3.1965, lr=0.000549, tokens/sec=1375336.80, grad_norm=0.3178, duration=0.38s
Step 5399: loss=3.2054, lr=0.000549, tokens/sec=1376902.38, grad_norm=0.3258, duration=0.38s
Step 5400/19073 (28.3%), Elapsed time: 2152.34s, Steps per hour: 9032.03, Estimated hours remaining: 1.51
Step 5400: loss=3.1611, lr=0.000549, tokens/sec=1378854.44, grad_norm=0.3573, duration=0.38s
Step 5401: loss=3.1116, lr=0.000549, tokens/sec=1376169.09, grad_norm=0.3111, duration=0.38s
Step 5402: loss=3.1541, lr=0.000549, tokens/sec=1377019.64, grad_norm=0.3374, duration=0.38s
Step 5403: loss=3.1234, lr=0.000549, tokens/sec=1376168.23, grad_norm=0.2861, duration=0.38s
Step 5404: loss=3.1312, lr=0.000549, tokens/sec=1376892.90, grad_norm=0.3447, duration=0.38s
Step 5405: loss=3.0533, lr=0.000549, tokens/sec=1373827.98, grad_norm=0.2810, duration=0.38s
Step 5406: loss=3.1151, lr=0.000549, tokens/sec=1376066.62, grad_norm=0.3191, duration=0.38s
Step 5407: loss=3.1236, lr=0.000549, tokens/sec=1374028.85, grad_norm=0.2838, duration=0.38s
Step 5408: loss=3.0912, lr=0.000549, tokens/sec=1381023.63, grad_norm=0.2867, duration=0.38s
Step 5409: loss=3.1272, lr=0.000549, tokens/sec=1376578.29, grad_norm=0.3004, duration=0.38s
Step 5410: loss=3.1058, lr=0.000549, tokens/sec=1374139.61, grad_norm=0.2839, duration=0.38s
Step 5411: loss=3.1494, lr=0.000549, tokens/sec=1378693.64, grad_norm=0.2873, duration=0.38s
Step 5412: loss=3.1571, lr=0.000549, tokens/sec=1373880.34, grad_norm=0.2744, duration=0.38s
Step 5413: loss=3.1484, lr=0.000549, tokens/sec=1376585.19, grad_norm=0.2916, duration=0.38s
Step 5414: loss=3.2010, lr=0.000549, tokens/sec=1377577.77, grad_norm=0.3076, duration=0.38s
Step 5415: loss=3.2106, lr=0.000549, tokens/sec=1377444.02, grad_norm=0.2683, duration=0.38s
Step 5416: loss=3.2179, lr=0.000549, tokens/sec=1372401.27, grad_norm=0.3305, duration=0.38s
Step 5417: loss=3.1919, lr=0.000549, tokens/sec=1377106.74, grad_norm=0.2886, duration=0.38s
Step 5418: loss=3.1562, lr=0.000549, tokens/sec=1377179.18, grad_norm=0.3165, duration=0.38s
Step 5419: loss=3.2141, lr=0.000549, tokens/sec=1376198.38, grad_norm=0.3219, duration=0.38s
Step 5420: loss=3.1535, lr=0.000549, tokens/sec=1376816.17, grad_norm=0.2968, duration=0.38s
Step 5421: loss=3.2034, lr=0.000549, tokens/sec=1374768.46, grad_norm=0.3250, duration=0.38s
Step 5422: loss=3.2052, lr=0.000549, tokens/sec=1378481.90, grad_norm=0.2672, duration=0.38s
Step 5423: loss=3.1861, lr=0.000549, tokens/sec=1376856.69, grad_norm=0.3030, duration=0.38s
Step 5424: loss=3.2698, lr=0.000549, tokens/sec=1374564.79, grad_norm=0.2814, duration=0.38s
Step 5425: loss=3.2143, lr=0.000549, tokens/sec=1375817.81, grad_norm=0.3040, duration=0.38s
Step 5426: loss=3.1848, lr=0.000548, tokens/sec=1379602.71, grad_norm=0.2822, duration=0.38s
Step 5427: loss=3.1820, lr=0.000548, tokens/sec=1375506.27, grad_norm=0.2874, duration=0.38s
Step 5428: loss=3.1717, lr=0.000548, tokens/sec=1374848.39, grad_norm=0.2805, duration=0.38s
Step 5429: loss=3.1574, lr=0.000548, tokens/sec=1375766.16, grad_norm=0.3121, duration=0.38s
Step 5430: loss=3.2143, lr=0.000548, tokens/sec=1377693.42, grad_norm=0.2774, duration=0.38s
Step 5431: loss=3.1994, lr=0.000548, tokens/sec=1376900.66, grad_norm=0.2957, duration=0.38s
Step 5432: loss=3.2378, lr=0.000548, tokens/sec=1377464.72, grad_norm=0.2990, duration=0.38s
Step 5433: loss=3.1640, lr=0.000548, tokens/sec=1379483.28, grad_norm=0.3023, duration=0.38s
Step 5434: loss=3.2065, lr=0.000548, tokens/sec=1377075.69, grad_norm=0.3001, duration=0.38s
Step 5435: loss=3.1917, lr=0.000548, tokens/sec=1377641.63, grad_norm=0.3038, duration=0.38s
Step 5436: loss=3.2035, lr=0.000548, tokens/sec=1381506.02, grad_norm=0.3019, duration=0.38s
Step 5437: loss=3.1644, lr=0.000548, tokens/sec=1374065.77, grad_norm=0.2963, duration=0.38s
Step 5438: loss=3.2118, lr=0.000548, tokens/sec=1376598.11, grad_norm=0.3043, duration=0.38s
Step 5439: loss=3.1785, lr=0.000548, tokens/sec=1375678.37, grad_norm=0.2933, duration=0.38s
Step 5440: loss=3.1795, lr=0.000548, tokens/sec=1372283.94, grad_norm=0.3413, duration=0.38s
Step 5441: loss=3.1638, lr=0.000548, tokens/sec=1375469.28, grad_norm=0.3059, duration=0.38s
Step 5442: loss=3.1725, lr=0.000548, tokens/sec=1378508.69, grad_norm=0.3358, duration=0.38s
Step 5443: loss=3.1675, lr=0.000548, tokens/sec=1377809.95, grad_norm=0.3070, duration=0.38s
Step 5444: loss=3.1814, lr=0.000548, tokens/sec=1372971.08, grad_norm=0.2969, duration=0.38s
Step 5445: loss=3.2104, lr=0.000548, tokens/sec=1371813.10, grad_norm=0.3134, duration=0.38s
Step 5446: loss=3.1649, lr=0.000548, tokens/sec=1377463.00, grad_norm=0.2944, duration=0.38s
Step 5447: loss=3.1465, lr=0.000548, tokens/sec=1377942.04, grad_norm=0.2683, duration=0.38s
Step 5448: loss=3.1556, lr=0.000548, tokens/sec=1379534.34, grad_norm=0.3022, duration=0.38s
Step 5449: loss=3.0943, lr=0.000548, tokens/sec=1378035.30, grad_norm=0.2658, duration=0.38s
Step 5450: loss=3.1473, lr=0.000548, tokens/sec=1376945.49, grad_norm=0.2993, duration=0.38s
Step 5451: loss=3.0650, lr=0.000548, tokens/sec=1378113.02, grad_norm=0.2828, duration=0.38s
Step 5452: loss=3.1316, lr=0.000548, tokens/sec=1376499.88, grad_norm=0.2800, duration=0.38s
Step 5453: loss=3.1061, lr=0.000548, tokens/sec=1375648.25, grad_norm=0.2678, duration=0.38s
Step 5454: loss=3.1634, lr=0.000548, tokens/sec=1378271.95, grad_norm=0.2577, duration=0.38s
Step 5455: loss=3.1357, lr=0.000548, tokens/sec=1374857.85, grad_norm=0.2602, duration=0.38s
Step 5456: loss=3.1125, lr=0.000548, tokens/sec=1378589.93, grad_norm=0.2523, duration=0.38s
Step 5457: loss=3.0824, lr=0.000548, tokens/sec=1378680.68, grad_norm=0.2516, duration=0.38s
Step 5458: loss=3.1173, lr=0.000548, tokens/sec=1372069.88, grad_norm=0.2508, duration=0.38s
Step 5459: loss=3.1652, lr=0.000548, tokens/sec=1372355.87, grad_norm=0.2620, duration=0.38s
Step 5460: loss=3.1904, lr=0.000548, tokens/sec=1376318.96, grad_norm=0.2557, duration=0.38s
Step 5461: loss=3.1927, lr=0.000548, tokens/sec=1373998.80, grad_norm=0.2828, duration=0.38s
Step 5462: loss=3.1974, lr=0.000548, tokens/sec=1377589.85, grad_norm=0.2871, duration=0.38s
Step 5463: loss=3.2038, lr=0.000548, tokens/sec=1377960.17, grad_norm=0.2779, duration=0.38s
Step 5464: loss=3.2327, lr=0.000548, tokens/sec=1374722.05, grad_norm=0.3252, duration=0.38s
Step 5465: loss=3.1473, lr=0.000548, tokens/sec=1377000.67, grad_norm=0.2743, duration=0.38s
Step 5466: loss=3.1679, lr=0.000548, tokens/sec=1378167.44, grad_norm=0.2952, duration=0.38s
Step 5467: loss=3.2259, lr=0.000548, tokens/sec=1378736.86, grad_norm=0.2762, duration=0.38s
Step 5468: loss=3.2790, lr=0.000548, tokens/sec=1378213.22, grad_norm=0.2841, duration=0.38s
Step 5469: loss=3.2353, lr=0.000548, tokens/sec=1376929.97, grad_norm=0.2921, duration=0.38s
Step 5470: loss=3.2048, lr=0.000548, tokens/sec=1374725.48, grad_norm=0.2878, duration=0.38s
Step 5471: loss=3.1303, lr=0.000548, tokens/sec=1374357.75, grad_norm=0.3072, duration=0.38s
Step 5472: loss=3.1778, lr=0.000547, tokens/sec=1377255.95, grad_norm=0.2562, duration=0.38s
Step 5473: loss=3.1463, lr=0.000547, tokens/sec=1375455.51, grad_norm=0.3243, duration=0.38s
Step 5474: loss=3.1193, lr=0.000547, tokens/sec=1375764.44, grad_norm=0.2794, duration=0.38s
Step 5475: loss=3.2091, lr=0.000547, tokens/sec=1376443.88, grad_norm=0.3079, duration=0.38s
Step 5476: loss=3.0938, lr=0.000547, tokens/sec=1373121.11, grad_norm=0.2866, duration=0.38s
Step 5477: loss=3.2045, lr=0.000547, tokens/sec=1374277.87, grad_norm=0.2937, duration=0.38s
Step 5478: loss=3.2732, lr=0.000547, tokens/sec=1375368.63, grad_norm=0.3090, duration=0.38s
Step 5479: loss=3.2175, lr=0.000547, tokens/sec=1378208.03, grad_norm=0.2947, duration=0.38s
Step 5480: loss=3.1515, lr=0.000547, tokens/sec=1374079.51, grad_norm=0.2939, duration=0.38s
Step 5481: loss=3.1917, lr=0.000547, tokens/sec=1373897.51, grad_norm=0.3044, duration=0.38s
Step 5482: loss=3.2295, lr=0.000547, tokens/sec=1377047.24, grad_norm=0.2996, duration=0.38s
Step 5483: loss=3.1742, lr=0.000547, tokens/sec=1376611.90, grad_norm=0.2760, duration=0.38s
Step 5484: loss=3.1237, lr=0.000547, tokens/sec=1376786.86, grad_norm=0.2880, duration=0.38s
Step 5485: loss=3.1479, lr=0.000547, tokens/sec=1378576.96, grad_norm=0.2835, duration=0.38s
Step 5486: loss=3.1758, lr=0.000547, tokens/sec=1375766.16, grad_norm=0.2761, duration=0.38s
Step 5487: loss=3.1916, lr=0.000547, tokens/sec=1376585.19, grad_norm=0.2880, duration=0.38s
Step 5488: loss=3.2460, lr=0.000547, tokens/sec=1374688.53, grad_norm=0.2752, duration=0.38s
Step 5489: loss=3.1736, lr=0.000547, tokens/sec=1373362.09, grad_norm=0.2816, duration=0.38s
Step 5490: loss=3.1621, lr=0.000547, tokens/sec=1377387.94, grad_norm=0.2839, duration=0.38s
Step 5491: loss=3.1615, lr=0.000547, tokens/sec=1378049.12, grad_norm=0.2808, duration=0.38s
Step 5492: loss=3.1448, lr=0.000547, tokens/sec=1381077.40, grad_norm=0.2997, duration=0.38s
Step 5493: loss=3.1230, lr=0.000547, tokens/sec=1375349.70, grad_norm=0.2853, duration=0.38s
Step 5494: loss=3.1339, lr=0.000547, tokens/sec=1374600.88, grad_norm=0.2963, duration=0.38s
Step 5495: loss=3.1201, lr=0.000547, tokens/sec=1373326.92, grad_norm=0.2792, duration=0.38s
Step 5496: loss=3.1454, lr=0.000547, tokens/sec=1372420.97, grad_norm=0.3298, duration=0.38s
Step 5497: loss=3.1682, lr=0.000547, tokens/sec=1375748.09, grad_norm=0.2837, duration=0.38s
Step 5498: loss=3.1315, lr=0.000547, tokens/sec=1376921.35, grad_norm=0.3079, duration=0.38s
Step 5499: loss=3.0855, lr=0.000547, tokens/sec=1376822.21, grad_norm=0.3178, duration=0.38s
Step 5500/19073 (28.8%), Elapsed time: 2190.52s, Steps per hour: 9038.95, Estimated hours remaining: 1.50
Validation loss at step 5500: 3.7163782119750977
Step 5500: loss=3.0803, lr=0.000547, tokens/sec=153689.30, grad_norm=0.3063, duration=3.41s
Step 5501: loss=3.1204, lr=0.000547, tokens/sec=1376855.83, grad_norm=0.3058, duration=0.38s
Step 5502: loss=3.1345, lr=0.000547, tokens/sec=1375845.35, grad_norm=0.3056, duration=0.38s
Step 5503: loss=3.1369, lr=0.000547, tokens/sec=1376384.43, grad_norm=0.3042, duration=0.38s
Step 5504: loss=3.1317, lr=0.000547, tokens/sec=1375868.59, grad_norm=0.2986, duration=0.38s
Step 5505: loss=3.1837, lr=0.000547, tokens/sec=1378494.00, grad_norm=0.2854, duration=0.38s
Step 5506: loss=3.1725, lr=0.000547, tokens/sec=1378296.14, grad_norm=0.2939, duration=0.38s
Step 5507: loss=3.1943, lr=0.000547, tokens/sec=1376992.91, grad_norm=0.3115, duration=0.38s
Step 5508: loss=3.1908, lr=0.000547, tokens/sec=1374652.44, grad_norm=0.2937, duration=0.38s
Step 5509: loss=3.1843, lr=0.000547, tokens/sec=1368189.07, grad_norm=0.3241, duration=0.38s
Step 5510: loss=3.1566, lr=0.000547, tokens/sec=1375996.87, grad_norm=0.3042, duration=0.38s
Step 5511: loss=3.2048, lr=0.000547, tokens/sec=1373758.46, grad_norm=0.3092, duration=0.38s
Step 5512: loss=3.2255, lr=0.000547, tokens/sec=1372571.74, grad_norm=0.2991, duration=0.38s
Step 5513: loss=3.3066, lr=0.000547, tokens/sec=1372555.46, grad_norm=0.3135, duration=0.38s
Step 5514: loss=3.2081, lr=0.000547, tokens/sec=1372953.08, grad_norm=0.3000, duration=0.38s
Step 5515: loss=3.2426, lr=0.000547, tokens/sec=1374057.18, grad_norm=0.3039, duration=0.38s
Step 5516: loss=3.2610, lr=0.000547, tokens/sec=1369011.88, grad_norm=0.3499, duration=0.38s
Step 5517: loss=3.2114, lr=0.000546, tokens/sec=1377559.64, grad_norm=0.2920, duration=0.38s
Step 5518: loss=3.2039, lr=0.000546, tokens/sec=1374020.27, grad_norm=0.2912, duration=0.38s
Step 5519: loss=3.2086, lr=0.000546, tokens/sec=1375084.81, grad_norm=0.2915, duration=0.38s
Step 5520: loss=3.2173, lr=0.000546, tokens/sec=1371497.39, grad_norm=0.2950, duration=0.38s
Step 5521: loss=3.1597, lr=0.000546, tokens/sec=1372149.50, grad_norm=0.2885, duration=0.38s
Step 5522: loss=3.2190, lr=0.000546, tokens/sec=1373107.40, grad_norm=0.2830, duration=0.38s
Step 5523: loss=3.1340, lr=0.000546, tokens/sec=1370341.03, grad_norm=0.2921, duration=0.38s
Step 5524: loss=3.1346, lr=0.000546, tokens/sec=1373663.21, grad_norm=0.3029, duration=0.38s
Step 5525: loss=3.1783, lr=0.000546, tokens/sec=1375989.98, grad_norm=0.3128, duration=0.38s
Step 5526: loss=3.1388, lr=0.000546, tokens/sec=1375843.63, grad_norm=0.3079, duration=0.38s
Step 5527: loss=3.2428, lr=0.000546, tokens/sec=1374209.17, grad_norm=0.3236, duration=0.38s
Step 5528: loss=3.1573, lr=0.000546, tokens/sec=1376720.49, grad_norm=0.3136, duration=0.38s
Step 5529: loss=3.1179, lr=0.000546, tokens/sec=1377688.24, grad_norm=0.3871, duration=0.38s
Step 5530: loss=3.1497, lr=0.000546, tokens/sec=1377986.08, grad_norm=0.3247, duration=0.38s
Step 5531: loss=3.1883, lr=0.000546, tokens/sec=1368841.45, grad_norm=0.3498, duration=0.38s
Step 5532: loss=3.3052, lr=0.000546, tokens/sec=1374121.58, grad_norm=0.3068, duration=0.38s
Step 5533: loss=3.1408, lr=0.000546, tokens/sec=1374917.16, grad_norm=0.3309, duration=0.38s
Step 5534: loss=3.2248, lr=0.000546, tokens/sec=1376140.67, grad_norm=0.2970, duration=0.38s
Step 5535: loss=3.2128, lr=0.000546, tokens/sec=1375743.78, grad_norm=0.3323, duration=0.38s
Step 5536: loss=3.1513, lr=0.000546, tokens/sec=1376102.78, grad_norm=0.3156, duration=0.38s
Step 5537: loss=3.1862, lr=0.000546, tokens/sec=1377060.17, grad_norm=0.2995, duration=0.38s
Step 5538: loss=3.2131, lr=0.000546, tokens/sec=1379267.83, grad_norm=0.2952, duration=0.38s
Step 5539: loss=3.1650, lr=0.000546, tokens/sec=1376280.20, grad_norm=0.2830, duration=0.38s
Step 5540: loss=3.1516, lr=0.000546, tokens/sec=1373198.29, grad_norm=0.2963, duration=0.38s
Step 5541: loss=3.1760, lr=0.000546, tokens/sec=1376318.10, grad_norm=0.2660, duration=0.38s
Step 5542: loss=3.1999, lr=0.000546, tokens/sec=1376050.26, grad_norm=0.3550, duration=0.38s
Step 5543: loss=3.2518, lr=0.000546, tokens/sec=1373538.80, grad_norm=0.2761, duration=0.38s
Step 5544: loss=3.1531, lr=0.000546, tokens/sec=1376535.21, grad_norm=0.2664, duration=0.38s
Step 5545: loss=3.1097, lr=0.000546, tokens/sec=1372339.60, grad_norm=0.2732, duration=0.38s
Step 5546: loss=3.1889, lr=0.000546, tokens/sec=1373283.18, grad_norm=0.2895, duration=0.38s
Step 5547: loss=3.0917, lr=0.000546, tokens/sec=1375761.00, grad_norm=0.2765, duration=0.38s
Step 5548: loss=3.1151, lr=0.000546, tokens/sec=1374746.97, grad_norm=0.2961, duration=0.38s
Step 5549: loss=3.1188, lr=0.000546, tokens/sec=1376376.68, grad_norm=0.2837, duration=0.38s
Step 5550: loss=3.1124, lr=0.000546, tokens/sec=1376656.72, grad_norm=0.2952, duration=0.38s
Step 5551: loss=3.1359, lr=0.000546, tokens/sec=1375029.78, grad_norm=0.2980, duration=0.38s
Step 5552: loss=3.1280, lr=0.000546, tokens/sec=1374807.13, grad_norm=0.3066, duration=0.38s
Step 5553: loss=3.1536, lr=0.000546, tokens/sec=1374813.15, grad_norm=0.3008, duration=0.38s
Step 5554: loss=3.1330, lr=0.000546, tokens/sec=1376431.82, grad_norm=0.3291, duration=0.38s
Step 5555: loss=3.1184, lr=0.000546, tokens/sec=1371418.70, grad_norm=0.2966, duration=0.38s
Step 5556: loss=3.1181, lr=0.000546, tokens/sec=1377439.70, grad_norm=0.3253, duration=0.38s
Step 5557: loss=3.2094, lr=0.000546, tokens/sec=1375699.03, grad_norm=0.3064, duration=0.38s
Step 5558: loss=3.2823, lr=0.000546, tokens/sec=1377362.05, grad_norm=0.3100, duration=0.38s
Step 5559: loss=3.1954, lr=0.000546, tokens/sec=1376688.60, grad_norm=0.2989, duration=0.38s
Step 5560: loss=3.1925, lr=0.000546, tokens/sec=1376198.38, grad_norm=0.3137, duration=0.38s
Step 5561: loss=3.2116, lr=0.000546, tokens/sec=1376102.78, grad_norm=0.2880, duration=0.38s
Step 5562: loss=3.2232, lr=0.000545, tokens/sec=1377063.62, grad_norm=0.3250, duration=0.38s
Step 5563: loss=3.2142, lr=0.000545, tokens/sec=1375045.26, grad_norm=0.2844, duration=0.38s
Step 5564: loss=3.1436, lr=0.000545, tokens/sec=1372812.52, grad_norm=0.3221, duration=0.38s
Step 5565: loss=3.1604, lr=0.000545, tokens/sec=1376454.22, grad_norm=0.2964, duration=0.38s
Step 5566: loss=3.1772, lr=0.000545, tokens/sec=1378454.25, grad_norm=0.3058, duration=0.38s
Step 5567: loss=3.1614, lr=0.000545, tokens/sec=1376418.89, grad_norm=0.3130, duration=0.38s
Step 5568: loss=3.1584, lr=0.000545, tokens/sec=1378384.26, grad_norm=0.3140, duration=0.38s
Step 5569: loss=3.1583, lr=0.000545, tokens/sec=1373946.44, grad_norm=0.2960, duration=0.38s
Step 5570: loss=3.1021, lr=0.000545, tokens/sec=1379161.43, grad_norm=0.3267, duration=0.38s
Step 5571: loss=3.0841, lr=0.000545, tokens/sec=1375204.34, grad_norm=0.3179, duration=0.38s
Step 5572: loss=3.1297, lr=0.000545, tokens/sec=1372428.68, grad_norm=0.2821, duration=0.38s
Step 5573: loss=3.2114, lr=0.000545, tokens/sec=1374722.05, grad_norm=0.3134, duration=0.38s
Step 5574: loss=3.1975, lr=0.000545, tokens/sec=1377702.05, grad_norm=0.2812, duration=0.38s
Step 5575: loss=3.2458, lr=0.000545, tokens/sec=1378586.47, grad_norm=0.3066, duration=0.38s
Step 5576: loss=3.1677, lr=0.000545, tokens/sec=1374759.86, grad_norm=0.2762, duration=0.38s
Step 5577: loss=3.2509, lr=0.000545, tokens/sec=1376245.75, grad_norm=0.2978, duration=0.38s
Step 5578: loss=3.1923, lr=0.000545, tokens/sec=1377865.20, grad_norm=0.2996, duration=0.38s
Step 5579: loss=3.1500, lr=0.000545, tokens/sec=1373013.09, grad_norm=0.2564, duration=0.38s
Step 5580: loss=3.2039, lr=0.000545, tokens/sec=1375277.45, grad_norm=0.3178, duration=0.38s
Step 5581: loss=3.2039, lr=0.000545, tokens/sec=1375453.79, grad_norm=0.2527, duration=0.38s
Step 5582: loss=3.1705, lr=0.000545, tokens/sec=1374271.00, grad_norm=0.2837, duration=0.38s
Step 5583: loss=3.2186, lr=0.000545, tokens/sec=1375118.35, grad_norm=0.2662, duration=0.38s
Step 5584: loss=3.1784, lr=0.000545, tokens/sec=1376426.65, grad_norm=0.2791, duration=0.38s
Step 5585: loss=3.2025, lr=0.000545, tokens/sec=1375850.52, grad_norm=0.2822, duration=0.38s
Step 5586: loss=3.1872, lr=0.000545, tokens/sec=1375429.70, grad_norm=0.2840, duration=0.38s
Step 5587: loss=3.1460, lr=0.000545, tokens/sec=1375776.49, grad_norm=0.2885, duration=0.38s
Step 5588: loss=3.2007, lr=0.000545, tokens/sec=1375200.04, grad_norm=0.3170, duration=0.38s
Step 5589: loss=3.1315, lr=0.000545, tokens/sec=1377911.82, grad_norm=0.2777, duration=0.38s
Step 5590: loss=3.1742, lr=0.000545, tokens/sec=1379017.86, grad_norm=0.3492, duration=0.38s
Step 5591: loss=3.0751, lr=0.000545, tokens/sec=1372826.23, grad_norm=0.3097, duration=0.38s
Step 5592: loss=3.1711, lr=0.000545, tokens/sec=1377395.70, grad_norm=0.3295, duration=0.38s
Step 5593: loss=3.1059, lr=0.000545, tokens/sec=1376028.73, grad_norm=0.3062, duration=0.38s
Step 5594: loss=3.0939, lr=0.000545, tokens/sec=1376628.28, grad_norm=0.3457, duration=0.38s
Step 5595: loss=3.0596, lr=0.000545, tokens/sec=1378550.17, grad_norm=0.2743, duration=0.38s
Step 5596: loss=3.0911, lr=0.000545, tokens/sec=1377598.48, grad_norm=0.3300, duration=0.38s
Step 5597: loss=3.1229, lr=0.000545, tokens/sec=1375826.41, grad_norm=0.2709, duration=0.38s
Step 5598: loss=3.0902, lr=0.000545, tokens/sec=1375011.73, grad_norm=0.2933, duration=0.38s
Step 5599: loss=3.1141, lr=0.000545, tokens/sec=1373826.27, grad_norm=0.2870, duration=0.38s
Step 5600/19073 (29.4%), Elapsed time: 2231.76s, Steps per hour: 9033.25, Estimated hours remaining: 1.49
Step 5600: loss=3.0817, lr=0.000545, tokens/sec=1375484.76, grad_norm=0.2614, duration=0.38s
Step 5601: loss=3.1635, lr=0.000545, tokens/sec=1375150.16, grad_norm=0.2763, duration=0.38s
Step 5602: loss=3.0991, lr=0.000545, tokens/sec=1373985.93, grad_norm=0.2605, duration=0.38s
Step 5603: loss=3.1393, lr=0.000545, tokens/sec=1376634.31, grad_norm=0.2717, duration=0.38s
Step 5604: loss=3.1924, lr=0.000545, tokens/sec=1376663.61, grad_norm=0.2809, duration=0.38s
Step 5605: loss=3.1985, lr=0.000545, tokens/sec=1374370.64, grad_norm=0.2846, duration=0.38s
Step 5606: loss=3.1830, lr=0.000545, tokens/sec=1377110.19, grad_norm=0.2868, duration=0.38s
Step 5607: loss=3.1977, lr=0.000544, tokens/sec=1373603.15, grad_norm=0.2809, duration=0.38s
Step 5608: loss=3.1539, lr=0.000544, tokens/sec=1376538.66, grad_norm=0.3023, duration=0.38s
Step 5609: loss=3.1684, lr=0.000544, tokens/sec=1375175.10, grad_norm=0.2875, duration=0.38s
Step 5610: loss=3.1308, lr=0.000544, tokens/sec=1380414.18, grad_norm=0.2943, duration=0.38s
Step 5611: loss=3.2033, lr=0.000544, tokens/sec=1375497.67, grad_norm=0.3026, duration=0.38s
Step 5612: loss=3.1852, lr=0.000544, tokens/sec=1371407.58, grad_norm=0.2826, duration=0.38s
Step 5613: loss=3.2417, lr=0.000544, tokens/sec=1377867.79, grad_norm=0.2813, duration=0.38s
Step 5614: loss=3.2355, lr=0.000544, tokens/sec=1379255.72, grad_norm=0.2899, duration=0.38s
Step 5615: loss=3.1603, lr=0.000544, tokens/sec=1377614.01, grad_norm=0.2810, duration=0.38s
Step 5616: loss=3.1790, lr=0.000544, tokens/sec=1376770.49, grad_norm=0.2928, duration=0.38s
Step 5617: loss=3.1401, lr=0.000544, tokens/sec=1377213.68, grad_norm=0.2911, duration=0.38s
Step 5618: loss=3.1971, lr=0.000544, tokens/sec=1378544.12, grad_norm=0.2903, duration=0.38s
Step 5619: loss=3.1571, lr=0.000544, tokens/sec=1370698.07, grad_norm=0.2930, duration=0.38s
Step 5620: loss=3.1970, lr=0.000544, tokens/sec=1374667.05, grad_norm=0.3007, duration=0.38s
Step 5621: loss=3.2205, lr=0.000544, tokens/sec=1375239.60, grad_norm=0.2996, duration=0.38s
Step 5622: loss=3.2088, lr=0.000544, tokens/sec=1378557.95, grad_norm=0.3091, duration=0.38s
Step 5623: loss=3.1200, lr=0.000544, tokens/sec=1376711.01, grad_norm=0.3030, duration=0.38s
Step 5624: loss=3.2323, lr=0.000544, tokens/sec=1375631.90, grad_norm=0.3031, duration=0.38s
Step 5625: loss=3.1430, lr=0.000544, tokens/sec=1373841.72, grad_norm=0.2919, duration=0.38s
Step 5626: loss=3.1861, lr=0.000544, tokens/sec=1376472.31, grad_norm=0.2988, duration=0.38s
Step 5627: loss=3.1811, lr=0.000544, tokens/sec=1376643.79, grad_norm=0.2919, duration=0.38s
Step 5628: loss=3.1865, lr=0.000544, tokens/sec=1377917.00, grad_norm=0.3061, duration=0.38s
Step 5629: loss=3.1620, lr=0.000544, tokens/sec=1375205.20, grad_norm=0.3069, duration=0.38s
Step 5630: loss=3.1525, lr=0.000544, tokens/sec=1376630.00, grad_norm=0.3175, duration=0.38s
Step 5631: loss=3.1433, lr=0.000544, tokens/sec=1371523.05, grad_norm=0.2932, duration=0.38s
Step 5632: loss=3.1754, lr=0.000544, tokens/sec=1376967.91, grad_norm=0.3026, duration=0.38s
Step 5633: loss=3.1547, lr=0.000544, tokens/sec=1380383.85, grad_norm=0.3082, duration=0.38s
Step 5634: loss=3.1914, lr=0.000544, tokens/sec=1376973.08, grad_norm=0.2820, duration=0.38s
Step 5635: loss=3.1644, lr=0.000544, tokens/sec=1375726.57, grad_norm=0.2876, duration=0.38s
Step 5636: loss=3.1466, lr=0.000544, tokens/sec=1378041.34, grad_norm=0.2730, duration=0.38s
Step 5637: loss=3.1496, lr=0.000544, tokens/sec=1375893.56, grad_norm=0.2842, duration=0.38s
Step 5638: loss=3.1355, lr=0.000544, tokens/sec=1378496.59, grad_norm=0.2869, duration=0.38s
Step 5639: loss=3.0946, lr=0.000544, tokens/sec=1374547.61, grad_norm=0.2703, duration=0.38s
Step 5640: loss=3.1318, lr=0.000544, tokens/sec=1376511.08, grad_norm=0.2888, duration=0.38s
Step 5641: loss=3.0752, lr=0.000544, tokens/sec=1380588.38, grad_norm=0.2902, duration=0.38s
Step 5642: loss=3.0966, lr=0.000544, tokens/sec=1374886.21, grad_norm=0.2648, duration=0.38s
Step 5643: loss=3.0867, lr=0.000544, tokens/sec=1375934.88, grad_norm=0.3136, duration=0.38s
Step 5644: loss=3.1522, lr=0.000544, tokens/sec=1375786.82, grad_norm=0.2494, duration=0.38s
Step 5645: loss=3.1281, lr=0.000544, tokens/sec=1364887.36, grad_norm=0.2782, duration=0.38s
Step 5646: loss=3.0841, lr=0.000544, tokens/sec=1373471.88, grad_norm=0.2592, duration=0.38s
Step 5647: loss=3.0756, lr=0.000544, tokens/sec=1374075.22, grad_norm=0.2660, duration=0.38s
Step 5648: loss=3.1142, lr=0.000544, tokens/sec=1374504.65, grad_norm=0.2734, duration=0.38s
Step 5649: loss=3.1529, lr=0.000544, tokens/sec=1375766.16, grad_norm=0.2755, duration=0.38s
Step 5650: loss=3.1676, lr=0.000544, tokens/sec=1375102.01, grad_norm=0.2592, duration=0.38s
Step 5651: loss=3.1737, lr=0.000543, tokens/sec=1377353.43, grad_norm=0.2868, duration=0.38s
Step 5652: loss=3.2165, lr=0.000543, tokens/sec=1374969.60, grad_norm=0.2743, duration=0.38s
Step 5653: loss=3.1900, lr=0.000543, tokens/sec=1375575.97, grad_norm=0.2934, duration=0.38s
Step 5654: loss=3.1909, lr=0.000543, tokens/sec=1380173.32, grad_norm=0.2956, duration=0.38s
Step 5655: loss=3.1283, lr=0.000543, tokens/sec=1373405.83, grad_norm=0.2880, duration=0.38s
Step 5656: loss=3.1701, lr=0.000543, tokens/sec=1377979.17, grad_norm=0.3123, duration=0.38s
Step 5657: loss=3.2057, lr=0.000543, tokens/sec=1376059.73, grad_norm=0.2742, duration=0.38s
Step 5658: loss=3.2873, lr=0.000543, tokens/sec=1380651.65, grad_norm=0.3019, duration=0.38s
Step 5659: loss=3.2029, lr=0.000543, tokens/sec=1370739.09, grad_norm=0.3006, duration=0.38s
Step 5660: loss=3.1742, lr=0.000543, tokens/sec=1373764.47, grad_norm=0.2585, duration=0.38s
Step 5661: loss=3.1372, lr=0.000543, tokens/sec=1375687.84, grad_norm=0.2934, duration=0.38s
Step 5662: loss=3.1640, lr=0.000543, tokens/sec=1375319.59, grad_norm=0.2689, duration=0.38s
Step 5663: loss=3.1390, lr=0.000543, tokens/sec=1376720.49, grad_norm=0.3054, duration=0.38s
Step 5664: loss=3.1066, lr=0.000543, tokens/sec=1374590.57, grad_norm=0.2773, duration=0.38s
Step 5665: loss=3.1696, lr=0.000543, tokens/sec=1377429.35, grad_norm=0.3106, duration=0.38s
Step 5666: loss=3.1011, lr=0.000543, tokens/sec=1379183.06, grad_norm=0.2738, duration=0.38s
Step 5667: loss=3.2331, lr=0.000543, tokens/sec=1373934.42, grad_norm=0.3125, duration=0.38s
Step 5668: loss=3.2654, lr=0.000543, tokens/sec=1379250.53, grad_norm=0.2878, duration=0.38s
Step 5669: loss=3.1971, lr=0.000543, tokens/sec=1377472.49, grad_norm=0.3184, duration=0.38s
Step 5670: loss=3.1153, lr=0.000543, tokens/sec=1373662.35, grad_norm=0.3085, duration=0.38s
Step 5671: loss=3.1996, lr=0.000543, tokens/sec=1375813.50, grad_norm=0.3068, duration=0.38s
Step 5672: loss=3.2045, lr=0.000543, tokens/sec=1377671.84, grad_norm=0.3091, duration=0.38s
Step 5673: loss=3.1846, lr=0.000543, tokens/sec=1376696.36, grad_norm=0.2901, duration=0.38s
Step 5674: loss=3.0688, lr=0.000543, tokens/sec=1373802.23, grad_norm=0.2882, duration=0.38s
Step 5675: loss=3.1687, lr=0.000543, tokens/sec=1377496.65, grad_norm=0.2747, duration=0.38s
Step 5676: loss=3.1573, lr=0.000543, tokens/sec=1374071.78, grad_norm=0.3159, duration=0.38s
Step 5677: loss=3.2047, lr=0.000543, tokens/sec=1372748.24, grad_norm=0.2732, duration=0.38s
Step 5678: loss=3.2431, lr=0.000543, tokens/sec=1371769.46, grad_norm=0.2989, duration=0.38s
Step 5679: loss=3.1389, lr=0.000543, tokens/sec=1373726.71, grad_norm=0.2846, duration=0.38s
Step 5680: loss=3.1460, lr=0.000543, tokens/sec=1378240.86, grad_norm=0.2673, duration=0.38s
Step 5681: loss=3.1730, lr=0.000543, tokens/sec=1375948.66, grad_norm=0.2743, duration=0.38s
Step 5682: loss=3.0699, lr=0.000543, tokens/sec=1378190.76, grad_norm=0.2854, duration=0.38s
Step 5683: loss=3.1616, lr=0.000543, tokens/sec=1376854.10, grad_norm=0.2897, duration=0.38s
Step 5684: loss=3.1131, lr=0.000543, tokens/sec=1374701.42, grad_norm=0.2798, duration=0.38s
Step 5685: loss=3.1266, lr=0.000543, tokens/sec=1372486.92, grad_norm=0.2719, duration=0.38s
Step 5686: loss=3.0913, lr=0.000543, tokens/sec=1378215.81, grad_norm=0.2932, duration=0.38s
Step 5687: loss=3.1653, lr=0.000543, tokens/sec=1374382.66, grad_norm=0.2621, duration=0.38s
Step 5688: loss=3.0926, lr=0.000543, tokens/sec=1370132.70, grad_norm=0.3307, duration=0.38s
Step 5689: loss=3.0983, lr=0.000543, tokens/sec=1374056.33, grad_norm=0.2909, duration=0.38s
Step 5690: loss=3.0729, lr=0.000543, tokens/sec=1378752.42, grad_norm=0.2889, duration=0.38s
Step 5691: loss=3.0891, lr=0.000543, tokens/sec=1377714.13, grad_norm=0.3000, duration=0.38s
Step 5692: loss=3.1346, lr=0.000543, tokens/sec=1375893.56, grad_norm=0.2815, duration=0.38s
Step 5693: loss=3.0995, lr=0.000543, tokens/sec=1374136.18, grad_norm=0.2853, duration=0.38s
Step 5694: loss=3.1244, lr=0.000543, tokens/sec=1374365.48, grad_norm=0.3010, duration=0.38s
Step 5695: loss=3.1557, lr=0.000542, tokens/sec=1376356.00, grad_norm=0.2634, duration=0.38s
Step 5696: loss=3.1720, lr=0.000542, tokens/sec=1377033.44, grad_norm=0.3034, duration=0.38s
Step 5697: loss=3.2103, lr=0.000542, tokens/sec=1374563.07, grad_norm=0.2743, duration=0.38s
Step 5698: loss=3.1371, lr=0.000542, tokens/sec=1375713.66, grad_norm=0.2830, duration=0.38s
Step 5699: loss=3.1668, lr=0.000542, tokens/sec=1375578.55, grad_norm=0.2981, duration=0.38s
Step 5700/19073 (29.9%), Elapsed time: 2269.95s, Steps per hour: 9039.86, Estimated hours remaining: 1.48
Step 5700: loss=3.1534, lr=0.000542, tokens/sec=1378169.16, grad_norm=0.2847, duration=0.38s
Step 5701: loss=3.2040, lr=0.000542, tokens/sec=1376265.56, grad_norm=0.2879, duration=0.38s
Step 5702: loss=3.2314, lr=0.000542, tokens/sec=1375453.79, grad_norm=0.2738, duration=0.38s
Step 5703: loss=3.2604, lr=0.000542, tokens/sec=1378269.36, grad_norm=0.2904, duration=0.38s
Step 5704: loss=3.2015, lr=0.000542, tokens/sec=1376881.69, grad_norm=0.2810, duration=0.38s
Step 5705: loss=3.1971, lr=0.000542, tokens/sec=1377325.82, grad_norm=0.3158, duration=0.38s
Step 5706: loss=3.2675, lr=0.000542, tokens/sec=1376097.62, grad_norm=0.3024, duration=0.38s
Step 5707: loss=3.1686, lr=0.000542, tokens/sec=1375460.67, grad_norm=0.2680, duration=0.38s
Step 5708: loss=3.2263, lr=0.000542, tokens/sec=1378036.16, grad_norm=0.2970, duration=0.38s
Step 5709: loss=3.1856, lr=0.000542, tokens/sec=1374226.35, grad_norm=0.2843, duration=0.38s
Step 5710: loss=3.1845, lr=0.000542, tokens/sec=1378442.15, grad_norm=0.3011, duration=0.38s
Step 5711: loss=3.1891, lr=0.000542, tokens/sec=1375995.15, grad_norm=0.2887, duration=0.38s
Step 5712: loss=3.1886, lr=0.000542, tokens/sec=1375102.01, grad_norm=0.2806, duration=0.38s
Step 5713: loss=3.0603, lr=0.000542, tokens/sec=1370352.99, grad_norm=0.3197, duration=0.38s
Step 5714: loss=3.1735, lr=0.000542, tokens/sec=1377257.67, grad_norm=0.2818, duration=0.38s
Step 5715: loss=3.1730, lr=0.000542, tokens/sec=1376644.65, grad_norm=0.3381, duration=0.38s
Step 5716: loss=3.1649, lr=0.000542, tokens/sec=1374506.37, grad_norm=0.3166, duration=0.38s
Step 5717: loss=3.2192, lr=0.000542, tokens/sec=1377752.97, grad_norm=0.3089, duration=0.38s
Step 5718: loss=3.1527, lr=0.000542, tokens/sec=1378645.24, grad_norm=0.3208, duration=0.38s
Step 5719: loss=3.0838, lr=0.000542, tokens/sec=1377029.99, grad_norm=0.3682, duration=0.38s
Step 5720: loss=3.1494, lr=0.000542, tokens/sec=1371654.80, grad_norm=0.3124, duration=0.38s
Step 5721: loss=3.2107, lr=0.000542, tokens/sec=1375317.87, grad_norm=0.3047, duration=0.38s
Step 5722: loss=3.2795, lr=0.000542, tokens/sec=1374807.99, grad_norm=0.3219, duration=0.38s
Step 5723: loss=3.1099, lr=0.000542, tokens/sec=1376285.37, grad_norm=0.2947, duration=0.38s
Step 5724: loss=3.2225, lr=0.000542, tokens/sec=1375756.69, grad_norm=0.3016, duration=0.38s
Step 5725: loss=3.1865, lr=0.000542, tokens/sec=1370874.95, grad_norm=0.3123, duration=0.38s
Step 5726: loss=3.1477, lr=0.000542, tokens/sec=1374550.19, grad_norm=0.2999, duration=0.38s
Step 5727: loss=3.1792, lr=0.000542, tokens/sec=1373477.03, grad_norm=0.3014, duration=0.38s
Step 5728: loss=3.1774, lr=0.000542, tokens/sec=1375986.54, grad_norm=0.2908, duration=0.38s
Step 5729: loss=3.1624, lr=0.000542, tokens/sec=1375650.83, grad_norm=0.2980, duration=0.38s
Step 5730: loss=3.1381, lr=0.000542, tokens/sec=1378407.59, grad_norm=0.2895, duration=0.38s
Step 5731: loss=3.1642, lr=0.000542, tokens/sec=1376265.56, grad_norm=0.2837, duration=0.38s
Step 5732: loss=3.2102, lr=0.000542, tokens/sec=1375914.22, grad_norm=0.3122, duration=0.38s
Step 5733: loss=3.2107, lr=0.000542, tokens/sec=1372091.28, grad_norm=0.2810, duration=0.38s
Step 5734: loss=3.1521, lr=0.000542, tokens/sec=1373524.21, grad_norm=0.2857, duration=0.38s
Step 5735: loss=3.1201, lr=0.000542, tokens/sec=1372587.16, grad_norm=0.2766, duration=0.38s
Step 5736: loss=3.1304, lr=0.000542, tokens/sec=1373943.86, grad_norm=0.2958, duration=0.38s
Step 5737: loss=3.1068, lr=0.000542, tokens/sec=1376854.10, grad_norm=0.2583, duration=0.38s
Step 5738: loss=3.0888, lr=0.000541, tokens/sec=1378532.02, grad_norm=0.2954, duration=0.38s
Step 5739: loss=3.0872, lr=0.000541, tokens/sec=1376625.69, grad_norm=0.2529, duration=0.38s
Step 5740: loss=3.1128, lr=0.000541, tokens/sec=1373314.92, grad_norm=0.2932, duration=0.38s
Step 5741: loss=3.1139, lr=0.000541, tokens/sec=1375700.75, grad_norm=0.2842, duration=0.38s
Step 5742: loss=3.1286, lr=0.000541, tokens/sec=1378519.06, grad_norm=0.2959, duration=0.38s
Step 5743: loss=3.1435, lr=0.000541, tokens/sec=1371576.94, grad_norm=0.3084, duration=0.38s
Step 5744: loss=3.1097, lr=0.000541, tokens/sec=1376662.75, grad_norm=0.3058, duration=0.38s
Step 5745: loss=3.0966, lr=0.000541, tokens/sec=1376868.76, grad_norm=0.3156, duration=0.38s
Step 5746: loss=3.1123, lr=0.000541, tokens/sec=1375697.31, grad_norm=0.3153, duration=0.38s
Step 5747: loss=3.2217, lr=0.000541, tokens/sec=1377656.30, grad_norm=0.3160, duration=0.38s
Step 5748: loss=3.2300, lr=0.000541, tokens/sec=1373025.95, grad_norm=0.3249, duration=0.38s
Step 5749: loss=3.1830, lr=0.000541, tokens/sec=1375674.93, grad_norm=0.2858, duration=0.38s
Validation loss at step 5750: 3.707378625869751
Step 5750: loss=3.1776, lr=0.000541, tokens/sec=156323.89, grad_norm=0.2946, duration=3.35s
Step 5751: loss=3.2079, lr=0.000541, tokens/sec=1376400.80, grad_norm=0.2803, duration=0.38s
Step 5752: loss=3.2068, lr=0.000541, tokens/sec=1377005.85, grad_norm=0.2955, duration=0.38s
Step 5753: loss=3.1704, lr=0.000541, tokens/sec=1378182.12, grad_norm=0.3057, duration=0.38s
Step 5754: loss=3.1470, lr=0.000541, tokens/sec=1376675.68, grad_norm=0.2732, duration=0.38s
Step 5755: loss=3.1393, lr=0.000541, tokens/sec=1376287.09, grad_norm=0.2926, duration=0.38s
Step 5756: loss=3.1571, lr=0.000541, tokens/sec=1373666.64, grad_norm=0.2773, duration=0.38s
Step 5757: loss=3.1552, lr=0.000541, tokens/sec=1377917.87, grad_norm=0.2932, duration=0.38s
Step 5758: loss=3.1466, lr=0.000541, tokens/sec=1375370.35, grad_norm=0.2793, duration=0.38s
Step 5759: loss=3.0817, lr=0.000541, tokens/sec=1374690.25, grad_norm=0.2872, duration=0.38s
Step 5760: loss=3.1458, lr=0.000541, tokens/sec=1375049.56, grad_norm=0.3137, duration=0.38s
Step 5761: loss=3.0653, lr=0.000541, tokens/sec=1374905.98, grad_norm=0.2890, duration=0.38s
Step 5762: loss=3.1568, lr=0.000541, tokens/sec=1375953.82, grad_norm=0.2943, duration=0.38s
Step 5763: loss=3.1927, lr=0.000541, tokens/sec=1376011.51, grad_norm=0.2787, duration=0.38s
Step 5764: loss=3.1844, lr=0.000541, tokens/sec=1371403.30, grad_norm=0.2835, duration=0.38s
Step 5765: loss=3.2212, lr=0.000541, tokens/sec=1373383.53, grad_norm=0.2634, duration=0.38s
Step 5766: loss=3.1584, lr=0.000541, tokens/sec=1370747.63, grad_norm=0.2865, duration=0.38s
Step 5767: loss=3.2195, lr=0.000541, tokens/sec=1369764.87, grad_norm=0.2690, duration=0.38s
Step 5768: loss=3.1563, lr=0.000541, tokens/sec=1373505.34, grad_norm=0.2858, duration=0.38s
Step 5769: loss=3.1815, lr=0.000541, tokens/sec=1375150.16, grad_norm=0.2605, duration=0.38s
Step 5770: loss=3.1914, lr=0.000541, tokens/sec=1377180.05, grad_norm=0.2850, duration=0.38s
Step 5771: loss=3.1781, lr=0.000541, tokens/sec=1375656.00, grad_norm=0.2470, duration=0.38s
Step 5772: loss=3.1891, lr=0.000541, tokens/sec=1371093.77, grad_norm=0.2932, duration=0.38s
Step 5773: loss=3.1682, lr=0.000541, tokens/sec=1375352.28, grad_norm=0.2614, duration=0.38s
Step 5774: loss=3.1864, lr=0.000541, tokens/sec=1377698.59, grad_norm=0.2845, duration=0.38s
Step 5775: loss=3.1767, lr=0.000541, tokens/sec=1374216.04, grad_norm=0.2693, duration=0.38s
Step 5776: loss=3.1686, lr=0.000541, tokens/sec=1374744.39, grad_norm=0.2893, duration=0.38s
Step 5777: loss=3.1522, lr=0.000541, tokens/sec=1375713.66, grad_norm=0.3075, duration=0.38s
Step 5778: loss=3.1306, lr=0.000541, tokens/sec=1372949.65, grad_norm=0.2829, duration=0.38s
Step 5779: loss=3.1489, lr=0.000541, tokens/sec=1379258.32, grad_norm=0.2979, duration=0.38s
Step 5780: loss=3.1357, lr=0.000541, tokens/sec=1371454.62, grad_norm=0.3150, duration=0.38s
Step 5781: loss=3.0943, lr=0.000541, tokens/sec=1381275.19, grad_norm=0.2994, duration=0.38s
Step 5782: loss=3.1532, lr=0.000540, tokens/sec=1377425.04, grad_norm=0.3111, duration=0.38s
Step 5783: loss=3.0668, lr=0.000540, tokens/sec=1374718.61, grad_norm=0.3027, duration=0.38s
Step 5784: loss=3.0997, lr=0.000540, tokens/sec=1377368.96, grad_norm=0.3086, duration=0.38s
Step 5785: loss=3.0390, lr=0.000540, tokens/sec=1378386.86, grad_norm=0.2742, duration=0.38s
Step 5786: loss=3.0919, lr=0.000540, tokens/sec=1376667.92, grad_norm=0.3276, duration=0.38s
Step 5787: loss=3.1225, lr=0.000540, tokens/sec=1377197.30, grad_norm=0.2548, duration=0.38s
Step 5788: loss=3.0782, lr=0.000540, tokens/sec=1377262.85, grad_norm=0.3085, duration=0.38s
Step 5789: loss=3.0857, lr=0.000540, tokens/sec=1377381.03, grad_norm=0.2598, duration=0.38s
Step 5790: loss=3.0987, lr=0.000540, tokens/sec=1373348.36, grad_norm=0.3068, duration=0.38s
Step 5791: loss=3.1049, lr=0.000540, tokens/sec=1375827.27, grad_norm=0.2673, duration=0.38s
Step 5792: loss=3.0976, lr=0.000540, tokens/sec=1380579.71, grad_norm=0.2837, duration=0.38s
Step 5793: loss=3.1331, lr=0.000540, tokens/sec=1377654.58, grad_norm=0.2617, duration=0.38s
Step 5794: loss=3.1780, lr=0.000540, tokens/sec=1373991.94, grad_norm=0.2917, duration=0.38s
Step 5795: loss=3.1659, lr=0.000540, tokens/sec=1376075.23, grad_norm=0.2771, duration=0.38s
Step 5796: loss=3.1889, lr=0.000540, tokens/sec=1372071.59, grad_norm=0.3050, duration=0.38s
Step 5797: loss=3.1987, lr=0.000540, tokens/sec=1378240.86, grad_norm=0.2632, duration=0.38s
Step 5798: loss=3.1114, lr=0.000540, tokens/sec=1376121.73, grad_norm=0.3266, duration=0.38s
Step 5799: loss=3.1472, lr=0.000540, tokens/sec=1379415.78, grad_norm=0.2801, duration=0.38s
Step 5800/19073 (30.4%), Elapsed time: 2311.11s, Steps per hour: 9034.61, Estimated hours remaining: 1.47
Step 5800: loss=3.1348, lr=0.000540, tokens/sec=1376888.59, grad_norm=0.3158, duration=0.38s
Step 5801: loss=3.1834, lr=0.000540, tokens/sec=1374139.61, grad_norm=0.2901, duration=0.38s
Step 5802: loss=3.2452, lr=0.000540, tokens/sec=1376593.81, grad_norm=0.2841, duration=0.38s
Step 5803: loss=3.2103, lr=0.000540, tokens/sec=1376645.51, grad_norm=0.3086, duration=0.38s
Step 5804: loss=3.1852, lr=0.000540, tokens/sec=1376037.34, grad_norm=0.2752, duration=0.38s
Step 5805: loss=3.1553, lr=0.000540, tokens/sec=1374022.84, grad_norm=0.3041, duration=0.38s
Step 5806: loss=3.1331, lr=0.000540, tokens/sec=1377744.34, grad_norm=0.2552, duration=0.38s
Step 5807: loss=3.1688, lr=0.000540, tokens/sec=1375697.31, grad_norm=0.3001, duration=0.38s
Step 5808: loss=3.1958, lr=0.000540, tokens/sec=1375779.93, grad_norm=0.2765, duration=0.38s
Step 5809: loss=3.1387, lr=0.000540, tokens/sec=1378383.40, grad_norm=0.3051, duration=0.38s
Step 5810: loss=3.2139, lr=0.000540, tokens/sec=1373941.29, grad_norm=0.2713, duration=0.38s
Step 5811: loss=3.1898, lr=0.000540, tokens/sec=1375986.54, grad_norm=0.3250, duration=0.38s
Step 5812: loss=3.1621, lr=0.000540, tokens/sec=1377028.27, grad_norm=0.2564, duration=0.38s
Step 5813: loss=3.1444, lr=0.000540, tokens/sec=1377469.90, grad_norm=0.3073, duration=0.38s
Step 5814: loss=3.1800, lr=0.000540, tokens/sec=1372694.26, grad_norm=0.2598, duration=0.38s
Step 5815: loss=3.1249, lr=0.000540, tokens/sec=1376521.42, grad_norm=0.3044, duration=0.38s
Step 5816: loss=3.1997, lr=0.000540, tokens/sec=1375713.66, grad_norm=0.2784, duration=0.38s
Step 5817: loss=3.1517, lr=0.000540, tokens/sec=1377784.05, grad_norm=0.2790, duration=0.38s
Step 5818: loss=3.1666, lr=0.000540, tokens/sec=1375139.85, grad_norm=0.2960, duration=0.38s
Step 5819: loss=3.1320, lr=0.000540, tokens/sec=1378791.33, grad_norm=0.2877, duration=0.38s
Step 5820: loss=3.1370, lr=0.000540, tokens/sec=1378101.80, grad_norm=0.3145, duration=0.38s
Step 5821: loss=3.1524, lr=0.000540, tokens/sec=1372679.69, grad_norm=0.3005, duration=0.38s
Step 5822: loss=3.1613, lr=0.000540, tokens/sec=1374031.43, grad_norm=0.2892, duration=0.38s
Step 5823: loss=3.1641, lr=0.000540, tokens/sec=1375329.92, grad_norm=0.3095, duration=0.38s
Step 5824: loss=3.1507, lr=0.000540, tokens/sec=1375301.53, grad_norm=0.3349, duration=0.38s
Step 5825: loss=3.1474, lr=0.000539, tokens/sec=1374092.39, grad_norm=0.2817, duration=0.38s
Step 5826: loss=3.1472, lr=0.000539, tokens/sec=1376044.23, grad_norm=0.2882, duration=0.38s
Step 5827: loss=3.1315, lr=0.000539, tokens/sec=1378968.57, grad_norm=0.3197, duration=0.38s
Step 5828: loss=3.1383, lr=0.000539, tokens/sec=1375519.18, grad_norm=0.2458, duration=0.38s
Step 5829: loss=3.0829, lr=0.000539, tokens/sec=1375761.00, grad_norm=0.2976, duration=0.38s
Step 5830: loss=3.1411, lr=0.000539, tokens/sec=1372083.58, grad_norm=0.2575, duration=0.38s
Step 5831: loss=3.0447, lr=0.000539, tokens/sec=1378673.76, grad_norm=0.3087, duration=0.38s
Step 5832: loss=3.0769, lr=0.000539, tokens/sec=1371517.06, grad_norm=0.2627, duration=0.38s
Step 5833: loss=3.0775, lr=0.000539, tokens/sec=1372660.84, grad_norm=0.3033, duration=0.38s
Step 5834: loss=3.1450, lr=0.000539, tokens/sec=1377162.80, grad_norm=0.2647, duration=0.38s
Step 5835: loss=3.1008, lr=0.000539, tokens/sec=1377886.78, grad_norm=0.2905, duration=0.38s
Step 5836: loss=3.0773, lr=0.000539, tokens/sec=1373426.42, grad_norm=0.2643, duration=0.38s
Step 5837: loss=3.0706, lr=0.000539, tokens/sec=1376082.98, grad_norm=0.2853, duration=0.38s
Step 5838: loss=3.1020, lr=0.000539, tokens/sec=1376597.25, grad_norm=0.2712, duration=0.38s
Step 5839: loss=3.1304, lr=0.000539, tokens/sec=1376655.85, grad_norm=0.2811, duration=0.38s
Step 5840: loss=3.1521, lr=0.000539, tokens/sec=1377299.94, grad_norm=0.2957, duration=0.38s
Step 5841: loss=3.1928, lr=0.000539, tokens/sec=1373975.62, grad_norm=0.2636, duration=0.38s
Step 5842: loss=3.2045, lr=0.000539, tokens/sec=1377866.93, grad_norm=0.3028, duration=0.38s
Step 5843: loss=3.1499, lr=0.000539, tokens/sec=1377896.28, grad_norm=0.2866, duration=0.38s
Step 5844: loss=3.1743, lr=0.000539, tokens/sec=1375857.40, grad_norm=0.3060, duration=0.38s
Step 5845: loss=3.1272, lr=0.000539, tokens/sec=1376525.73, grad_norm=0.2919, duration=0.38s
Step 5846: loss=3.1494, lr=0.000539, tokens/sec=1375603.50, grad_norm=0.3023, duration=0.38s
Step 5847: loss=3.2145, lr=0.000539, tokens/sec=1374304.50, grad_norm=0.3094, duration=0.38s
Step 5848: loss=3.2570, lr=0.000539, tokens/sec=1376220.77, grad_norm=0.3182, duration=0.38s
Step 5849: loss=3.1754, lr=0.000539, tokens/sec=1372936.80, grad_norm=0.3021, duration=0.38s
Step 5850: loss=3.1833, lr=0.000539, tokens/sec=1375377.23, grad_norm=0.3215, duration=0.38s
Step 5851: loss=3.1231, lr=0.000539, tokens/sec=1379511.83, grad_norm=0.2938, duration=0.38s
Step 5852: loss=3.1615, lr=0.000539, tokens/sec=1375071.91, grad_norm=0.3088, duration=0.38s
Step 5853: loss=3.1305, lr=0.000539, tokens/sec=1375187.14, grad_norm=0.3054, duration=0.38s
Step 5854: loss=3.0676, lr=0.000539, tokens/sec=1376482.65, grad_norm=0.3054, duration=0.38s
Step 5855: loss=3.1796, lr=0.000539, tokens/sec=1375372.93, grad_norm=0.3224, duration=0.38s
Step 5856: loss=3.1327, lr=0.000539, tokens/sec=1375415.08, grad_norm=0.2818, duration=0.38s
Step 5857: loss=3.2321, lr=0.000539, tokens/sec=1372873.37, grad_norm=0.3446, duration=0.38s
Step 5858: loss=3.2420, lr=0.000539, tokens/sec=1372869.08, grad_norm=0.2982, duration=0.38s
Step 5859: loss=3.1666, lr=0.000539, tokens/sec=1375864.29, grad_norm=0.3338, duration=0.38s
Step 5860: loss=3.1257, lr=0.000539, tokens/sec=1373335.50, grad_norm=0.2922, duration=0.38s
Step 5861: loss=3.1738, lr=0.000539, tokens/sec=1374337.14, grad_norm=0.3379, duration=0.38s
Step 5862: loss=3.2127, lr=0.000539, tokens/sec=1379411.45, grad_norm=0.3103, duration=0.38s
Step 5863: loss=3.1334, lr=0.000539, tokens/sec=1376866.17, grad_norm=0.3125, duration=0.38s
Step 5864: loss=3.0918, lr=0.000539, tokens/sec=1371734.37, grad_norm=0.2920, duration=0.38s
Step 5865: loss=3.1484, lr=0.000539, tokens/sec=1374869.02, grad_norm=0.2970, duration=0.38s
Step 5866: loss=3.1715, lr=0.000539, tokens/sec=1374176.54, grad_norm=0.2906, duration=0.38s
Step 5867: loss=3.2037, lr=0.000538, tokens/sec=1374580.26, grad_norm=0.3238, duration=0.38s
Step 5868: loss=3.2128, lr=0.000538, tokens/sec=1371324.62, grad_norm=0.3220, duration=0.38s
Step 5869: loss=3.1252, lr=0.000538, tokens/sec=1376393.91, grad_norm=0.3027, duration=0.38s
Step 5870: loss=3.1609, lr=0.000538, tokens/sec=1377865.20, grad_norm=0.2749, duration=0.38s
Step 5871: loss=3.1028, lr=0.000538, tokens/sec=1376892.04, grad_norm=0.2913, duration=0.38s
Step 5872: loss=3.1067, lr=0.000538, tokens/sec=1378309.10, grad_norm=0.2800, duration=0.38s
Step 5873: loss=3.1435, lr=0.000538, tokens/sec=1376909.28, grad_norm=0.2941, duration=0.38s
Step 5874: loss=3.1265, lr=0.000538, tokens/sec=1377066.21, grad_norm=0.2938, duration=0.38s
Step 5875: loss=3.0754, lr=0.000538, tokens/sec=1376413.72, grad_norm=0.2776, duration=0.38s
Step 5876: loss=3.0915, lr=0.000538, tokens/sec=1377948.95, grad_norm=0.2941, duration=0.38s
Step 5877: loss=3.1271, lr=0.000538, tokens/sec=1376402.52, grad_norm=0.2990, duration=0.38s
Step 5878: loss=3.1101, lr=0.000538, tokens/sec=1380517.31, grad_norm=0.3249, duration=0.38s
Step 5879: loss=3.0905, lr=0.000538, tokens/sec=1378801.70, grad_norm=0.2809, duration=0.38s
Step 5880: loss=3.0417, lr=0.000538, tokens/sec=1378877.78, grad_norm=0.2993, duration=0.38s
Step 5881: loss=3.0902, lr=0.000538, tokens/sec=1375501.11, grad_norm=0.3002, duration=0.38s
Step 5882: loss=3.1017, lr=0.000538, tokens/sec=1375415.08, grad_norm=0.2836, duration=0.38s
Step 5883: loss=3.0903, lr=0.000538, tokens/sec=1378261.59, grad_norm=0.2743, duration=0.38s
Step 5884: loss=3.0995, lr=0.000538, tokens/sec=1376973.94, grad_norm=0.2971, duration=0.38s
Step 5885: loss=3.1566, lr=0.000538, tokens/sec=1376985.15, grad_norm=0.2693, duration=0.38s
Step 5886: loss=3.1871, lr=0.000538, tokens/sec=1376998.09, grad_norm=0.2909, duration=0.38s
Step 5887: loss=3.1623, lr=0.000538, tokens/sec=1372574.31, grad_norm=0.2820, duration=0.38s
Step 5888: loss=3.1214, lr=0.000538, tokens/sec=1374378.37, grad_norm=0.3032, duration=0.38s
Step 5889: loss=3.1640, lr=0.000538, tokens/sec=1376624.83, grad_norm=0.3002, duration=0.38s
Step 5890: loss=3.1573, lr=0.000538, tokens/sec=1376419.75, grad_norm=0.3008, duration=0.38s
Step 5891: loss=3.2109, lr=0.000538, tokens/sec=1378341.07, grad_norm=0.3065, duration=0.38s
Step 5892: loss=3.1817, lr=0.000538, tokens/sec=1375394.43, grad_norm=0.2838, duration=0.38s
Step 5893: loss=3.2586, lr=0.000538, tokens/sec=1373397.26, grad_norm=0.3012, duration=0.38s
Step 5894: loss=3.1615, lr=0.000538, tokens/sec=1374452.24, grad_norm=0.2918, duration=0.38s
Step 5895: loss=3.2061, lr=0.000538, tokens/sec=1371985.13, grad_norm=0.3102, duration=0.38s
Step 5896: loss=3.2290, lr=0.000538, tokens/sec=1374906.84, grad_norm=0.3084, duration=0.38s
Step 5897: loss=3.1940, lr=0.000538, tokens/sec=1379582.80, grad_norm=0.3229, duration=0.38s
Step 5898: loss=3.2052, lr=0.000538, tokens/sec=1374171.39, grad_norm=0.3041, duration=0.38s
Step 5899: loss=3.1574, lr=0.000538, tokens/sec=1376128.62, grad_norm=0.2944, duration=0.38s
Step 5900/19073 (30.9%), Elapsed time: 2349.30s, Steps per hour: 9040.97, Estimated hours remaining: 1.46
Step 5900: loss=3.2147, lr=0.000538, tokens/sec=1377196.43, grad_norm=0.3170, duration=0.38s
Step 5901: loss=3.1606, lr=0.000538, tokens/sec=1377021.37, grad_norm=0.2911, duration=0.38s
Step 5902: loss=3.1151, lr=0.000538, tokens/sec=1376336.19, grad_norm=0.3147, duration=0.38s
Step 5903: loss=3.0979, lr=0.000538, tokens/sec=1378478.45, grad_norm=0.3073, duration=0.38s
Step 5904: loss=3.1677, lr=0.000538, tokens/sec=1373028.52, grad_norm=0.3059, duration=0.38s
Step 5905: loss=3.1973, lr=0.000538, tokens/sec=1372792.81, grad_norm=0.3347, duration=0.38s
Step 5906: loss=3.1447, lr=0.000538, tokens/sec=1370372.63, grad_norm=0.3263, duration=0.38s
Step 5907: loss=3.2151, lr=0.000538, tokens/sec=1373734.44, grad_norm=0.3189, duration=0.38s
Step 5908: loss=3.1179, lr=0.000538, tokens/sec=1373690.67, grad_norm=0.3219, duration=0.38s
Step 5909: loss=3.0855, lr=0.000538, tokens/sec=1373926.69, grad_norm=0.3925, duration=0.38s
Step 5910: loss=3.1746, lr=0.000537, tokens/sec=1371724.96, grad_norm=0.3142, duration=0.38s
Step 5911: loss=3.1866, lr=0.000537, tokens/sec=1371039.91, grad_norm=0.3124, duration=0.38s
Step 5912: loss=3.2535, lr=0.000537, tokens/sec=1372235.13, grad_norm=0.3290, duration=0.38s
Step 5913: loss=3.1076, lr=0.000537, tokens/sec=1376623.97, grad_norm=0.3031, duration=0.38s
Step 5914: loss=3.1998, lr=0.000537, tokens/sec=1377563.96, grad_norm=0.3214, duration=0.38s
Step 5915: loss=3.1848, lr=0.000537, tokens/sec=1376378.40, grad_norm=0.3055, duration=0.38s
Step 5916: loss=3.1402, lr=0.000537, tokens/sec=1376223.35, grad_norm=0.3021, duration=0.38s
Step 5917: loss=3.1463, lr=0.000537, tokens/sec=1371821.66, grad_norm=0.3017, duration=0.38s
Step 5918: loss=3.1784, lr=0.000537, tokens/sec=1378213.22, grad_norm=0.2941, duration=0.38s
Step 5919: loss=3.1499, lr=0.000537, tokens/sec=1374826.04, grad_norm=0.2817, duration=0.38s
Step 5920: loss=3.1278, lr=0.000537, tokens/sec=1375383.25, grad_norm=0.3088, duration=0.38s
Step 5921: loss=3.1826, lr=0.000537, tokens/sec=1377385.35, grad_norm=0.3030, duration=0.38s
Step 5922: loss=3.1815, lr=0.000537, tokens/sec=1378259.86, grad_norm=0.3378, duration=0.38s
Step 5923: loss=3.2135, lr=0.000537, tokens/sec=1380261.69, grad_norm=0.3201, duration=0.38s
Step 5924: loss=3.1667, lr=0.000537, tokens/sec=1375836.74, grad_norm=0.3041, duration=0.38s
Step 5925: loss=3.0677, lr=0.000537, tokens/sec=1373740.44, grad_norm=0.2960, duration=0.38s
Step 5926: loss=3.1521, lr=0.000537, tokens/sec=1373947.30, grad_norm=0.2935, duration=0.38s
Step 5927: loss=3.0849, lr=0.000537, tokens/sec=1374441.08, grad_norm=0.2936, duration=0.38s
Step 5928: loss=3.0623, lr=0.000537, tokens/sec=1369521.74, grad_norm=0.2742, duration=0.38s
Step 5929: loss=3.0920, lr=0.000537, tokens/sec=1365356.00, grad_norm=0.2877, duration=0.38s
Step 5930: loss=3.0947, lr=0.000537, tokens/sec=1375923.69, grad_norm=0.2689, duration=0.38s
Step 5931: loss=3.1141, lr=0.000537, tokens/sec=1374075.22, grad_norm=0.2908, duration=0.38s
Step 5932: loss=3.1178, lr=0.000537, tokens/sec=1373128.83, grad_norm=0.2968, duration=0.38s
Step 5933: loss=3.1208, lr=0.000537, tokens/sec=1372888.80, grad_norm=0.3007, duration=0.38s
Step 5934: loss=3.0883, lr=0.000537, tokens/sec=1376087.28, grad_norm=0.3250, duration=0.38s
Step 5935: loss=3.0921, lr=0.000537, tokens/sec=1377123.99, grad_norm=0.3121, duration=0.38s
Step 5936: loss=3.1302, lr=0.000537, tokens/sec=1374861.28, grad_norm=0.3462, duration=0.38s
Step 5937: loss=3.1704, lr=0.000537, tokens/sec=1372585.44, grad_norm=0.2997, duration=0.38s
Step 5938: loss=3.2178, lr=0.000537, tokens/sec=1376120.87, grad_norm=0.3408, duration=0.38s
Step 5939: loss=3.1716, lr=0.000537, tokens/sec=1374234.07, grad_norm=0.3502, duration=0.38s
Step 5940: loss=3.1753, lr=0.000537, tokens/sec=1375964.15, grad_norm=0.3006, duration=0.38s
Step 5941: loss=3.1987, lr=0.000537, tokens/sec=1372117.82, grad_norm=0.2983, duration=0.38s
Step 5942: loss=3.1628, lr=0.000537, tokens/sec=1378998.84, grad_norm=0.2907, duration=0.38s
Step 5943: loss=3.1749, lr=0.000537, tokens/sec=1374490.04, grad_norm=0.2941, duration=0.38s
Step 5944: loss=3.1299, lr=0.000537, tokens/sec=1375218.10, grad_norm=0.2827, duration=0.38s
Step 5945: loss=3.1233, lr=0.000537, tokens/sec=1379416.65, grad_norm=0.2876, duration=0.38s
Step 5946: loss=3.1569, lr=0.000537, tokens/sec=1376742.04, grad_norm=0.2916, duration=0.38s
Step 5947: loss=3.1456, lr=0.000537, tokens/sec=1372291.64, grad_norm=0.2844, duration=0.38s
Step 5948: loss=3.0746, lr=0.000537, tokens/sec=1378667.71, grad_norm=0.2743, duration=0.38s
Step 5949: loss=3.1266, lr=0.000537, tokens/sec=1373802.23, grad_norm=0.2903, duration=0.38s
Step 5950: loss=3.1297, lr=0.000537, tokens/sec=1374949.83, grad_norm=0.2841, duration=0.38s
Step 5951: loss=3.0910, lr=0.000537, tokens/sec=1379331.85, grad_norm=0.2788, duration=0.38s
Step 5952: loss=3.1401, lr=0.000536, tokens/sec=1377030.85, grad_norm=0.3094, duration=0.38s
Step 5953: loss=3.1792, lr=0.000536, tokens/sec=1378340.20, grad_norm=0.2731, duration=0.38s
Step 5954: loss=3.1614, lr=0.000536, tokens/sec=1372386.71, grad_norm=0.2942, duration=0.38s
Step 5955: loss=3.2097, lr=0.000536, tokens/sec=1374733.22, grad_norm=0.2828, duration=0.38s
Step 5956: loss=3.1257, lr=0.000536, tokens/sec=1379478.08, grad_norm=0.2759, duration=0.38s
Step 5957: loss=3.1836, lr=0.000536, tokens/sec=1377120.54, grad_norm=0.2870, duration=0.38s
Step 5958: loss=3.1882, lr=0.000536, tokens/sec=1378315.15, grad_norm=0.2825, duration=0.38s
Step 5959: loss=3.1728, lr=0.000536, tokens/sec=1376335.33, grad_norm=0.3016, duration=0.38s
Step 5960: loss=3.1661, lr=0.000536, tokens/sec=1373967.04, grad_norm=0.2751, duration=0.38s
Step 5961: loss=3.1984, lr=0.000536, tokens/sec=1375746.36, grad_norm=0.3132, duration=0.38s
Step 5962: loss=3.1406, lr=0.000536, tokens/sec=1376671.37, grad_norm=0.2940, duration=0.38s
Step 5963: loss=3.1781, lr=0.000536, tokens/sec=1376754.11, grad_norm=0.2799, duration=0.38s
Step 5964: loss=3.1655, lr=0.000536, tokens/sec=1376508.50, grad_norm=0.2997, duration=0.38s
Step 5965: loss=3.1582, lr=0.000536, tokens/sec=1370247.96, grad_norm=0.2749, duration=0.38s
Step 5966: loss=3.1780, lr=0.000536, tokens/sec=1378643.51, grad_norm=0.3097, duration=0.38s
Step 5967: loss=3.0838, lr=0.000536, tokens/sec=1374117.29, grad_norm=0.2850, duration=0.38s
Step 5968: loss=3.1476, lr=0.000536, tokens/sec=1369907.37, grad_norm=0.2781, duration=0.38s
Step 5969: loss=3.1129, lr=0.000536, tokens/sec=1375354.00, grad_norm=0.3068, duration=0.38s
Step 5970: loss=3.1535, lr=0.000536, tokens/sec=1376088.14, grad_norm=0.2890, duration=0.38s
Step 5971: loss=3.0763, lr=0.000536, tokens/sec=1373621.17, grad_norm=0.2841, duration=0.38s
Step 5972: loss=3.1132, lr=0.000536, tokens/sec=1373997.09, grad_norm=0.3044, duration=0.38s
Step 5973: loss=3.0802, lr=0.000536, tokens/sec=1376562.78, grad_norm=0.2988, duration=0.38s
Step 5974: loss=3.0803, lr=0.000536, tokens/sec=1375364.32, grad_norm=0.3292, duration=0.38s
Step 5975: loss=3.0400, lr=0.000536, tokens/sec=1369691.49, grad_norm=0.2891, duration=0.38s
Step 5976: loss=3.0916, lr=0.000536, tokens/sec=1379246.21, grad_norm=0.3240, duration=0.38s
Step 5977: loss=3.1151, lr=0.000536, tokens/sec=1376873.93, grad_norm=0.2947, duration=0.38s
Step 5978: loss=3.0559, lr=0.000536, tokens/sec=1368767.32, grad_norm=0.3081, duration=0.38s
Step 5979: loss=3.1038, lr=0.000536, tokens/sec=1378236.54, grad_norm=0.2918, duration=0.38s
Step 5980: loss=3.0402, lr=0.000536, tokens/sec=1377042.92, grad_norm=0.3055, duration=0.38s
Step 5981: loss=3.1039, lr=0.000536, tokens/sec=1375749.81, grad_norm=0.2894, duration=0.38s
Step 5982: loss=3.0923, lr=0.000536, tokens/sec=1375016.03, grad_norm=0.2952, duration=0.38s
Step 5983: loss=3.1227, lr=0.000536, tokens/sec=1377354.29, grad_norm=0.2802, duration=0.38s
Step 5984: loss=3.1470, lr=0.000536, tokens/sec=1375292.07, grad_norm=0.2921, duration=0.38s
Step 5985: loss=3.1729, lr=0.000536, tokens/sec=1375629.32, grad_norm=0.2917, duration=0.38s
Step 5986: loss=3.1877, lr=0.000536, tokens/sec=1374453.10, grad_norm=0.2769, duration=0.38s
Step 5987: loss=3.1562, lr=0.000536, tokens/sec=1379395.88, grad_norm=0.3071, duration=0.38s
Step 5988: loss=3.0905, lr=0.000536, tokens/sec=1376858.41, grad_norm=0.2950, duration=0.38s
Step 5989: loss=3.1546, lr=0.000536, tokens/sec=1374747.83, grad_norm=0.3308, duration=0.38s
Step 5990: loss=3.1171, lr=0.000536, tokens/sec=1376095.03, grad_norm=0.3026, duration=0.38s
Step 5991: loss=3.2445, lr=0.000536, tokens/sec=1374764.16, grad_norm=0.3404, duration=0.38s
Step 5992: loss=3.2156, lr=0.000536, tokens/sec=1373175.13, grad_norm=0.3330, duration=0.38s
Step 5993: loss=3.1618, lr=0.000536, tokens/sec=1376865.31, grad_norm=0.3044, duration=0.38s
Step 5994: loss=3.1855, lr=0.000535, tokens/sec=1375897.86, grad_norm=0.3251, duration=0.38s
Step 5995: loss=3.1157, lr=0.000535, tokens/sec=1378350.57, grad_norm=0.3176, duration=0.38s
Step 5996: loss=3.1659, lr=0.000535, tokens/sec=1379511.83, grad_norm=0.3001, duration=0.38s
Step 5997: loss=3.1694, lr=0.000535, tokens/sec=1377708.09, grad_norm=0.2983, duration=0.38s
Step 5998: loss=3.1820, lr=0.000535, tokens/sec=1375716.24, grad_norm=0.3132, duration=0.38s
Step 5999: loss=3.1598, lr=0.000535, tokens/sec=1375082.23, grad_norm=0.3091, duration=0.38s
Step 6000/19073 (31.5%), Elapsed time: 2387.52s, Steps per hour: 9047.03, Estimated hours remaining: 1.45
Validation loss at step 6000: 3.706906318664551
Step 6000: loss=3.1879, lr=0.000535, tokens/sec=156814.47, grad_norm=0.2765, duration=3.34s
Step 6001: loss=3.1473, lr=0.000535, tokens/sec=1377908.37, grad_norm=0.3327, duration=0.38s
Step 6002: loss=3.1871, lr=0.000535, tokens/sec=1378088.84, grad_norm=0.2703, duration=0.38s
Step 6003: loss=3.0991, lr=0.000535, tokens/sec=1377950.68, grad_norm=0.3254, duration=0.38s
Step 6004: loss=3.1647, lr=0.000535, tokens/sec=1378047.39, grad_norm=0.2802, duration=0.38s
Step 6005: loss=3.1438, lr=0.000535, tokens/sec=1373133.98, grad_norm=0.3425, duration=0.38s
Step 6006: loss=3.1743, lr=0.000535, tokens/sec=1376722.22, grad_norm=0.2742, duration=0.38s
Step 6007: loss=3.1397, lr=0.000535, tokens/sec=1377641.63, grad_norm=0.3187, duration=0.38s
Step 6008: loss=3.1407, lr=0.000535, tokens/sec=1378802.56, grad_norm=0.3092, duration=0.38s
Step 6009: loss=3.1203, lr=0.000535, tokens/sec=1374723.76, grad_norm=0.3124, duration=0.38s
Step 6010: loss=3.1438, lr=0.000535, tokens/sec=1377337.90, grad_norm=0.2950, duration=0.38s
Step 6011: loss=3.1426, lr=0.000535, tokens/sec=1377421.58, grad_norm=0.3176, duration=0.38s
Step 6012: loss=3.1738, lr=0.000535, tokens/sec=1379252.26, grad_norm=0.2904, duration=0.38s
Step 6013: loss=3.1256, lr=0.000535, tokens/sec=1375593.18, grad_norm=0.3181, duration=0.38s
Step 6014: loss=3.1355, lr=0.000535, tokens/sec=1377401.74, grad_norm=0.2673, duration=0.38s
Step 6015: loss=3.1503, lr=0.000535, tokens/sec=1376691.19, grad_norm=0.2975, duration=0.38s
Step 6016: loss=3.1290, lr=0.000535, tokens/sec=1374324.25, grad_norm=0.2801, duration=0.38s
Step 6017: loss=3.1353, lr=0.000535, tokens/sec=1374435.92, grad_norm=0.2880, duration=0.38s
Step 6018: loss=3.1268, lr=0.000535, tokens/sec=1380207.97, grad_norm=0.2887, duration=0.38s
Step 6019: loss=3.0934, lr=0.000535, tokens/sec=1379164.03, grad_norm=0.3246, duration=0.38s
Step 6020: loss=3.1077, lr=0.000535, tokens/sec=1375163.92, grad_norm=0.2487, duration=0.38s
Step 6021: loss=3.0265, lr=0.000535, tokens/sec=1374133.60, grad_norm=0.3321, duration=0.38s
Step 6022: loss=3.0682, lr=0.000535, tokens/sec=1378302.19, grad_norm=0.2782, duration=0.38s
Step 6023: loss=3.0707, lr=0.000535, tokens/sec=1375890.98, grad_norm=0.3015, duration=0.38s
Step 6024: loss=3.1188, lr=0.000535, tokens/sec=1374405.00, grad_norm=0.2969, duration=0.38s
Step 6025: loss=3.0962, lr=0.000535, tokens/sec=1378563.13, grad_norm=0.2993, duration=0.38s
Step 6026: loss=3.0741, lr=0.000535, tokens/sec=1377741.75, grad_norm=0.2966, duration=0.38s
Step 6027: loss=3.0583, lr=0.000535, tokens/sec=1376317.24, grad_norm=0.2700, duration=0.38s
Step 6028: loss=3.0806, lr=0.000535, tokens/sec=1376455.94, grad_norm=0.2957, duration=0.38s
Step 6029: loss=3.1092, lr=0.000535, tokens/sec=1378094.02, grad_norm=0.2617, duration=0.38s
Step 6030: loss=3.1727, lr=0.000535, tokens/sec=1375889.25, grad_norm=0.3128, duration=0.38s
Step 6031: loss=3.1792, lr=0.000535, tokens/sec=1374172.24, grad_norm=0.2763, duration=0.38s
Step 6032: loss=3.1634, lr=0.000535, tokens/sec=1377374.13, grad_norm=0.2934, duration=0.38s
Step 6033: loss=3.1314, lr=0.000535, tokens/sec=1377504.42, grad_norm=0.2873, duration=0.38s
Step 6034: loss=3.1729, lr=0.000535, tokens/sec=1375700.75, grad_norm=0.2991, duration=0.38s
Step 6035: loss=3.1110, lr=0.000534, tokens/sec=1373729.29, grad_norm=0.3274, duration=0.38s
Step 6036: loss=3.1524, lr=0.000534, tokens/sec=1373777.35, grad_norm=0.3005, duration=0.38s
Step 6037: loss=3.1838, lr=0.000534, tokens/sec=1378037.89, grad_norm=0.3263, duration=0.38s
Step 6038: loss=3.2294, lr=0.000534, tokens/sec=1379592.32, grad_norm=0.3177, duration=0.38s
Step 6039: loss=3.1808, lr=0.000534, tokens/sec=1375139.85, grad_norm=0.3257, duration=0.38s
Step 6040: loss=3.1686, lr=0.000534, tokens/sec=1377262.85, grad_norm=0.3367, duration=0.38s
Step 6041: loss=3.1180, lr=0.000534, tokens/sec=1374797.68, grad_norm=0.3086, duration=0.38s
Step 6042: loss=3.1528, lr=0.000534, tokens/sec=1375889.25, grad_norm=0.3260, duration=0.38s
Step 6043: loss=3.0905, lr=0.000534, tokens/sec=1376706.70, grad_norm=0.3232, duration=0.38s
Step 6044: loss=3.0786, lr=0.000534, tokens/sec=1370795.48, grad_norm=0.3497, duration=0.38s
Step 6045: loss=3.2085, lr=0.000534, tokens/sec=1371854.18, grad_norm=0.3110, duration=0.38s
Step 6046: loss=3.1283, lr=0.000534, tokens/sec=1373513.92, grad_norm=0.3434, duration=0.38s
Step 6047: loss=3.2036, lr=0.000534, tokens/sec=1376356.00, grad_norm=0.3080, duration=0.38s
Step 6048: loss=3.2115, lr=0.000534, tokens/sec=1373284.90, grad_norm=0.3253, duration=0.38s
Step 6049: loss=3.1718, lr=0.000534, tokens/sec=1371645.39, grad_norm=0.3206, duration=0.38s
Step 6050: loss=3.1011, lr=0.000534, tokens/sec=1375996.01, grad_norm=0.3038, duration=0.38s
Step 6051: loss=3.1822, lr=0.000534, tokens/sec=1377388.80, grad_norm=0.3401, duration=0.38s
Step 6052: loss=3.1589, lr=0.000534, tokens/sec=1375934.88, grad_norm=0.2954, duration=0.38s
Step 6053: loss=3.1558, lr=0.000534, tokens/sec=1373218.87, grad_norm=0.3164, duration=0.38s
Step 6054: loss=3.0704, lr=0.000534, tokens/sec=1375388.41, grad_norm=0.2802, duration=0.38s
Step 6055: loss=3.1638, lr=0.000534, tokens/sec=1375211.22, grad_norm=0.2984, duration=0.38s
Step 6056: loss=3.1704, lr=0.000534, tokens/sec=1377712.40, grad_norm=0.2901, duration=0.38s
Step 6057: loss=3.1703, lr=0.000534, tokens/sec=1378486.22, grad_norm=0.3156, duration=0.38s
Step 6058: loss=3.1934, lr=0.000534, tokens/sec=1375504.55, grad_norm=0.2822, duration=0.38s
Step 6059: loss=3.1419, lr=0.000534, tokens/sec=1375272.29, grad_norm=0.3149, duration=0.38s
Step 6060: loss=3.0872, lr=0.000534, tokens/sec=1377564.82, grad_norm=0.2636, duration=0.38s
Step 6061: loss=3.1384, lr=0.000534, tokens/sec=1373793.65, grad_norm=0.2956, duration=0.38s
Step 6062: loss=3.0891, lr=0.000534, tokens/sec=1378024.07, grad_norm=0.2858, duration=0.38s
Step 6063: loss=3.1524, lr=0.000534, tokens/sec=1379359.54, grad_norm=0.2723, duration=0.38s
Step 6064: loss=3.0759, lr=0.000534, tokens/sec=1377903.19, grad_norm=0.3034, duration=0.38s
Step 6065: loss=3.0741, lr=0.000534, tokens/sec=1375969.32, grad_norm=0.2711, duration=0.38s
Step 6066: loss=3.0504, lr=0.000534, tokens/sec=1378005.08, grad_norm=0.3121, duration=0.38s
Step 6067: loss=3.1458, lr=0.000534, tokens/sec=1374961.00, grad_norm=0.2695, duration=0.38s
Step 6068: loss=3.0995, lr=0.000534, tokens/sec=1377658.89, grad_norm=0.3043, duration=0.38s
Step 6069: loss=3.0569, lr=0.000534, tokens/sec=1375581.99, grad_norm=0.2751, duration=0.38s
Step 6070: loss=3.0429, lr=0.000534, tokens/sec=1378285.78, grad_norm=0.2699, duration=0.38s
Step 6071: loss=3.0545, lr=0.000534, tokens/sec=1373265.17, grad_norm=0.2918, duration=0.38s
Step 6072: loss=3.0916, lr=0.000534, tokens/sec=1370442.66, grad_norm=0.2758, duration=0.38s
Step 6073: loss=3.0660, lr=0.000534, tokens/sec=1378736.86, grad_norm=0.2613, duration=0.38s
Step 6074: loss=3.0972, lr=0.000534, tokens/sec=1373688.95, grad_norm=0.2986, duration=0.38s
Step 6075: loss=3.1728, lr=0.000534, tokens/sec=1375957.27, grad_norm=0.2595, duration=0.38s
Step 6076: loss=3.1378, lr=0.000534, tokens/sec=1377252.50, grad_norm=0.2914, duration=0.38s
Step 6077: loss=3.1421, lr=0.000533, tokens/sec=1374185.12, grad_norm=0.2970, duration=0.38s
Step 6078: loss=3.1190, lr=0.000533, tokens/sec=1378247.77, grad_norm=0.2828, duration=0.38s
Step 6079: loss=3.1665, lr=0.000533, tokens/sec=1379233.23, grad_norm=0.2890, duration=0.38s
Step 6080: loss=3.1614, lr=0.000533, tokens/sec=1374549.33, grad_norm=0.3065, duration=0.38s
Step 6081: loss=3.1628, lr=0.000533, tokens/sec=1373551.67, grad_norm=0.2892, duration=0.38s
Step 6082: loss=3.1816, lr=0.000533, tokens/sec=1376578.29, grad_norm=0.3052, duration=0.38s
Step 6083: loss=3.2140, lr=0.000533, tokens/sec=1377436.25, grad_norm=0.2844, duration=0.38s
Step 6084: loss=3.1716, lr=0.000533, tokens/sec=1374739.23, grad_norm=0.2945, duration=0.38s
Step 6085: loss=3.1669, lr=0.000533, tokens/sec=1378358.35, grad_norm=0.3262, duration=0.38s
Step 6086: loss=3.2496, lr=0.000533, tokens/sec=1372378.14, grad_norm=0.2887, duration=0.38s
Step 6087: loss=3.1738, lr=0.000533, tokens/sec=1379425.30, grad_norm=0.3453, duration=0.38s
Step 6088: loss=3.1709, lr=0.000533, tokens/sec=1375588.88, grad_norm=0.2884, duration=0.38s
Step 6089: loss=3.1863, lr=0.000533, tokens/sec=1377073.97, grad_norm=0.3181, duration=0.38s
Step 6090: loss=3.1861, lr=0.000533, tokens/sec=1378396.36, grad_norm=0.3038, duration=0.38s
Step 6091: loss=3.0865, lr=0.000533, tokens/sec=1376518.84, grad_norm=0.3159, duration=0.38s
Step 6092: loss=3.1518, lr=0.000533, tokens/sec=1376430.09, grad_norm=0.2994, duration=0.38s
Step 6093: loss=3.0885, lr=0.000533, tokens/sec=1376748.94, grad_norm=0.2874, duration=0.38s
Step 6094: loss=3.1919, lr=0.000533, tokens/sec=1374206.59, grad_norm=0.3045, duration=0.38s
Step 6095: loss=3.1696, lr=0.000533, tokens/sec=1378384.26, grad_norm=0.3123, duration=0.38s
Step 6096: loss=3.1350, lr=0.000533, tokens/sec=1372512.62, grad_norm=0.2965, duration=0.38s
Step 6097: loss=3.1806, lr=0.000533, tokens/sec=1375565.64, grad_norm=0.3355, duration=0.38s
Step 6098: loss=3.1116, lr=0.000533, tokens/sec=1376799.79, grad_norm=0.3261, duration=0.38s
Step 6099: loss=3.1043, lr=0.000533, tokens/sec=1375525.20, grad_norm=0.3379, duration=0.38s
Step 6100/19073 (32.0%), Elapsed time: 2428.66s, Steps per hour: 9042.01, Estimated hours remaining: 1.43
Step 6100: loss=3.1482, lr=0.000533, tokens/sec=1376664.47, grad_norm=0.3315, duration=0.38s
Step 6101: loss=3.1589, lr=0.000533, tokens/sec=1379703.12, grad_norm=0.3017, duration=0.38s
Step 6102: loss=3.2457, lr=0.000533, tokens/sec=1375466.70, grad_norm=0.3262, duration=0.38s
Step 6103: loss=3.0824, lr=0.000533, tokens/sec=1376758.42, grad_norm=0.2936, duration=0.38s
Step 6104: loss=3.1970, lr=0.000533, tokens/sec=1378902.86, grad_norm=0.3395, duration=0.38s
Step 6105: loss=3.1798, lr=0.000533, tokens/sec=1376590.36, grad_norm=0.3023, duration=0.38s
Step 6106: loss=3.1050, lr=0.000533, tokens/sec=1376665.33, grad_norm=0.3112, duration=0.38s
Step 6107: loss=3.1448, lr=0.000533, tokens/sec=1374781.35, grad_norm=0.2810, duration=0.38s
Step 6108: loss=3.1635, lr=0.000533, tokens/sec=1373451.30, grad_norm=0.3000, duration=0.38s
Step 6109: loss=3.1376, lr=0.000533, tokens/sec=1376712.74, grad_norm=0.2893, duration=0.38s
Step 6110: loss=3.1405, lr=0.000533, tokens/sec=1378840.60, grad_norm=0.2829, duration=0.38s
Step 6111: loss=3.1461, lr=0.000533, tokens/sec=1378242.58, grad_norm=0.2997, duration=0.38s
Step 6112: loss=3.1679, lr=0.000533, tokens/sec=1375406.48, grad_norm=0.3162, duration=0.38s
Step 6113: loss=3.2222, lr=0.000533, tokens/sec=1373139.12, grad_norm=0.3323, duration=0.38s
Step 6114: loss=3.1111, lr=0.000533, tokens/sec=1376498.16, grad_norm=0.2810, duration=0.38s
Step 6115: loss=3.0856, lr=0.000533, tokens/sec=1373943.86, grad_norm=0.3001, duration=0.38s
Step 6116: loss=3.1241, lr=0.000533, tokens/sec=1377512.18, grad_norm=0.2732, duration=0.38s
Step 6117: loss=3.0547, lr=0.000533, tokens/sec=1376551.58, grad_norm=0.2987, duration=0.38s
Step 6118: loss=3.0647, lr=0.000532, tokens/sec=1376531.76, grad_norm=0.2716, duration=0.38s
Step 6119: loss=3.0737, lr=0.000532, tokens/sec=1374749.55, grad_norm=0.2873, duration=0.38s
Step 6120: loss=3.0951, lr=0.000532, tokens/sec=1375523.48, grad_norm=0.2825, duration=0.38s
Step 6121: loss=3.1021, lr=0.000532, tokens/sec=1376262.97, grad_norm=0.2782, duration=0.38s
Step 6122: loss=3.0949, lr=0.000532, tokens/sec=1377611.42, grad_norm=0.2873, duration=0.38s
Step 6123: loss=3.1005, lr=0.000532, tokens/sec=1373845.15, grad_norm=0.2847, duration=0.38s
Step 6124: loss=3.0828, lr=0.000532, tokens/sec=1375403.03, grad_norm=0.2823, duration=0.38s
Step 6125: loss=3.1066, lr=0.000532, tokens/sec=1377481.98, grad_norm=0.2998, duration=0.38s
Step 6126: loss=3.0785, lr=0.000532, tokens/sec=1379842.50, grad_norm=0.2863, duration=0.38s
Step 6127: loss=3.1616, lr=0.000532, tokens/sec=1374961.00, grad_norm=0.3159, duration=0.38s
Step 6128: loss=3.2011, lr=0.000532, tokens/sec=1378759.34, grad_norm=0.2973, duration=0.38s
Step 6129: loss=3.1700, lr=0.000532, tokens/sec=1378448.20, grad_norm=0.3479, duration=0.38s
Step 6130: loss=3.1648, lr=0.000532, tokens/sec=1375132.11, grad_norm=0.3060, duration=0.38s
Step 6131: loss=3.1570, lr=0.000532, tokens/sec=1374881.05, grad_norm=0.3182, duration=0.38s
Step 6132: loss=3.1672, lr=0.000532, tokens/sec=1374876.76, grad_norm=0.2850, duration=0.38s
Step 6133: loss=3.1567, lr=0.000532, tokens/sec=1377125.71, grad_norm=0.3103, duration=0.38s
Step 6134: loss=3.1120, lr=0.000532, tokens/sec=1374898.25, grad_norm=0.2732, duration=0.38s
Step 6135: loss=3.1242, lr=0.000532, tokens/sec=1375655.14, grad_norm=0.3189, duration=0.38s
Step 6136: loss=3.1477, lr=0.000532, tokens/sec=1372673.69, grad_norm=0.2920, duration=0.38s
Step 6137: loss=3.0741, lr=0.000532, tokens/sec=1378430.92, grad_norm=0.2942, duration=0.38s
Step 6138: loss=3.1196, lr=0.000532, tokens/sec=1375117.49, grad_norm=0.2927, duration=0.38s
Step 6139: loss=3.1114, lr=0.000532, tokens/sec=1377274.06, grad_norm=0.2736, duration=0.38s
Step 6140: loss=3.1558, lr=0.000532, tokens/sec=1377584.67, grad_norm=0.2823, duration=0.38s
Step 6141: loss=3.0759, lr=0.000532, tokens/sec=1372916.22, grad_norm=0.2727, duration=0.38s
Step 6142: loss=3.1285, lr=0.000532, tokens/sec=1379197.76, grad_norm=0.2874, duration=0.38s
Step 6143: loss=3.1585, lr=0.000532, tokens/sec=1375933.16, grad_norm=0.2789, duration=0.38s
Step 6144: loss=3.1527, lr=0.000532, tokens/sec=1381879.32, grad_norm=0.2733, duration=0.38s
Step 6145: loss=3.1813, lr=0.000532, tokens/sec=1374416.16, grad_norm=0.2794, duration=0.38s
Step 6146: loss=3.0905, lr=0.000532, tokens/sec=1375549.29, grad_norm=0.2988, duration=0.38s
Step 6147: loss=3.2177, lr=0.000532, tokens/sec=1377252.50, grad_norm=0.2974, duration=0.38s
Step 6148: loss=3.1800, lr=0.000532, tokens/sec=1379360.40, grad_norm=0.2888, duration=0.38s
Step 6149: loss=3.1505, lr=0.000532, tokens/sec=1375074.49, grad_norm=0.2890, duration=0.38s
Step 6150: loss=3.1849, lr=0.000532, tokens/sec=1376429.23, grad_norm=0.2724, duration=0.38s
Step 6151: loss=3.1500, lr=0.000532, tokens/sec=1374728.92, grad_norm=0.2916, duration=0.38s
Step 6152: loss=3.1480, lr=0.000532, tokens/sec=1377265.44, grad_norm=0.2804, duration=0.38s
Step 6153: loss=3.1573, lr=0.000532, tokens/sec=1375062.46, grad_norm=0.2957, duration=0.38s
Step 6154: loss=3.1435, lr=0.000532, tokens/sec=1374010.82, grad_norm=0.2743, duration=0.38s
Step 6155: loss=3.1638, lr=0.000532, tokens/sec=1376193.21, grad_norm=0.2926, duration=0.38s
Step 6156: loss=3.1093, lr=0.000532, tokens/sec=1377256.81, grad_norm=0.2955, duration=0.38s
Step 6157: loss=3.0962, lr=0.000532, tokens/sec=1372183.75, grad_norm=0.2679, duration=0.38s
Step 6158: loss=3.1105, lr=0.000532, tokens/sec=1372469.79, grad_norm=0.2924, duration=0.38s
Step 6159: loss=3.1303, lr=0.000531, tokens/sec=1371158.74, grad_norm=0.2942, duration=0.38s
Step 6160: loss=3.1365, lr=0.000531, tokens/sec=1373116.83, grad_norm=0.2835, duration=0.38s
Step 6161: loss=3.0392, lr=0.000531, tokens/sec=1377111.05, grad_norm=0.2948, duration=0.38s
Step 6162: loss=3.1251, lr=0.000531, tokens/sec=1372757.67, grad_norm=0.2912, duration=0.38s
Step 6163: loss=3.0562, lr=0.000531, tokens/sec=1374362.91, grad_norm=0.2848, duration=0.38s
Step 6164: loss=3.0812, lr=0.000531, tokens/sec=1381292.54, grad_norm=0.2890, duration=0.38s
Step 6165: loss=3.0448, lr=0.000531, tokens/sec=1376244.02, grad_norm=0.3011, duration=0.38s
Step 6166: loss=3.0787, lr=0.000531, tokens/sec=1375161.34, grad_norm=0.2866, duration=0.38s
Step 6167: loss=3.0896, lr=0.000531, tokens/sec=1377976.58, grad_norm=0.3048, duration=0.38s
Step 6168: loss=3.0694, lr=0.000531, tokens/sec=1374621.50, grad_norm=0.2835, duration=0.38s
Step 6169: loss=3.0468, lr=0.000531, tokens/sec=1375170.80, grad_norm=0.3180, duration=0.38s
Step 6170: loss=3.0359, lr=0.000531, tokens/sec=1374167.09, grad_norm=0.2826, duration=0.38s
Step 6171: loss=3.0996, lr=0.000531, tokens/sec=1374728.92, grad_norm=0.3236, duration=0.38s
Step 6172: loss=3.0822, lr=0.000531, tokens/sec=1378892.48, grad_norm=0.2542, duration=0.38s
Step 6173: loss=3.0930, lr=0.000531, tokens/sec=1375969.32, grad_norm=0.3122, duration=0.38s
Step 6174: loss=3.1527, lr=0.000531, tokens/sec=1374514.96, grad_norm=0.2619, duration=0.38s
Step 6175: loss=3.1735, lr=0.000531, tokens/sec=1377866.93, grad_norm=0.2997, duration=0.38s
Step 6176: loss=3.1471, lr=0.000531, tokens/sec=1377111.91, grad_norm=0.2736, duration=0.38s
Step 6177: loss=3.1374, lr=0.000531, tokens/sec=1377436.25, grad_norm=0.2914, duration=0.38s
Step 6178: loss=3.0969, lr=0.000531, tokens/sec=1374211.75, grad_norm=0.2795, duration=0.38s
Step 6179: loss=3.1369, lr=0.000531, tokens/sec=1377219.72, grad_norm=0.3162, duration=0.38s
Step 6180: loss=3.1743, lr=0.000531, tokens/sec=1376358.59, grad_norm=0.2935, duration=0.38s
Step 6181: loss=3.2158, lr=0.000531, tokens/sec=1373995.37, grad_norm=0.3293, duration=0.38s
Step 6182: loss=3.1645, lr=0.000531, tokens/sec=1374981.64, grad_norm=0.3021, duration=0.38s
Step 6183: loss=3.1581, lr=0.000531, tokens/sec=1371418.70, grad_norm=0.2841, duration=0.38s
Step 6184: loss=3.1381, lr=0.000531, tokens/sec=1376856.69, grad_norm=0.3103, duration=0.38s
Step 6185: loss=3.1427, lr=0.000531, tokens/sec=1378607.21, grad_norm=0.2773, duration=0.38s
Step 6186: loss=3.1655, lr=0.000531, tokens/sec=1379505.78, grad_norm=0.2951, duration=0.38s
Step 6187: loss=3.1519, lr=0.000531, tokens/sec=1374099.26, grad_norm=0.2840, duration=0.38s
Step 6188: loss=3.2008, lr=0.000531, tokens/sec=1376065.75, grad_norm=0.2896, duration=0.38s
Step 6189: loss=3.1308, lr=0.000531, tokens/sec=1379011.81, grad_norm=0.3097, duration=0.38s
Step 6190: loss=3.1430, lr=0.000531, tokens/sec=1377253.36, grad_norm=0.2654, duration=0.38s
Step 6191: loss=3.1731, lr=0.000531, tokens/sec=1375857.40, grad_norm=0.3053, duration=0.38s
Step 6192: loss=3.1425, lr=0.000531, tokens/sec=1377411.23, grad_norm=0.2675, duration=0.38s
Step 6193: loss=3.0820, lr=0.000531, tokens/sec=1376743.77, grad_norm=0.2985, duration=0.38s
Step 6194: loss=3.1790, lr=0.000531, tokens/sec=1374038.30, grad_norm=0.2706, duration=0.38s
Step 6195: loss=3.1164, lr=0.000531, tokens/sec=1375285.19, grad_norm=0.3203, duration=0.38s
Step 6196: loss=3.1570, lr=0.000531, tokens/sec=1377001.53, grad_norm=0.2629, duration=0.38s
Step 6197: loss=3.1087, lr=0.000531, tokens/sec=1377396.56, grad_norm=0.3063, duration=0.38s
Step 6198: loss=3.1273, lr=0.000531, tokens/sec=1372408.98, grad_norm=0.2703, duration=0.38s
Step 6199: loss=3.1277, lr=0.000531, tokens/sec=1375540.69, grad_norm=0.3039, duration=0.38s
Step 6200/19073 (32.5%), Elapsed time: 2466.85s, Steps per hour: 9047.97, Estimated hours remaining: 1.42
Step 6200: loss=3.1324, lr=0.000530, tokens/sec=1376998.95, grad_norm=0.2738, duration=0.38s
Step 6201: loss=3.1541, lr=0.000530, tokens/sec=1376839.45, grad_norm=0.3122, duration=0.38s
Step 6202: loss=3.1334, lr=0.000530, tokens/sec=1376703.26, grad_norm=0.2667, duration=0.38s
Step 6203: loss=3.1070, lr=0.000530, tokens/sec=1373533.65, grad_norm=0.3107, duration=0.38s
Step 6204: loss=3.1343, lr=0.000530, tokens/sec=1377343.94, grad_norm=0.2754, duration=0.38s
Step 6205: loss=3.1311, lr=0.000530, tokens/sec=1377664.07, grad_norm=0.3127, duration=0.38s
Step 6206: loss=3.1341, lr=0.000530, tokens/sec=1375911.64, grad_norm=0.2779, duration=0.38s
Step 6207: loss=3.1187, lr=0.000530, tokens/sec=1372585.44, grad_norm=0.2743, duration=0.38s
Step 6208: loss=3.1398, lr=0.000530, tokens/sec=1377399.15, grad_norm=0.2805, duration=0.38s
Step 6209: loss=3.0605, lr=0.000530, tokens/sec=1377771.96, grad_norm=0.2707, duration=0.38s
Step 6210: loss=3.0927, lr=0.000530, tokens/sec=1375514.88, grad_norm=0.2744, duration=0.38s
Step 6211: loss=3.0134, lr=0.000530, tokens/sec=1377946.36, grad_norm=0.2877, duration=0.38s
Step 6212: loss=3.0640, lr=0.000530, tokens/sec=1376831.69, grad_norm=0.2943, duration=0.38s
Step 6213: loss=3.0426, lr=0.000530, tokens/sec=1372959.08, grad_norm=0.2879, duration=0.38s
Step 6214: loss=3.1161, lr=0.000530, tokens/sec=1376391.32, grad_norm=0.3320, duration=0.38s
Step 6215: loss=3.0915, lr=0.000530, tokens/sec=1375250.79, grad_norm=0.2755, duration=0.38s
Step 6216: loss=3.0646, lr=0.000530, tokens/sec=1373652.06, grad_norm=0.3053, duration=0.38s
Step 6217: loss=3.0364, lr=0.000530, tokens/sec=1375630.18, grad_norm=0.2691, duration=0.38s
Step 6218: loss=3.0634, lr=0.000530, tokens/sec=1377959.31, grad_norm=0.3124, duration=0.38s
Step 6219: loss=3.1350, lr=0.000530, tokens/sec=1374223.77, grad_norm=0.2958, duration=0.38s
Step 6220: loss=3.1559, lr=0.000530, tokens/sec=1373907.81, grad_norm=0.2985, duration=0.38s
Step 6221: loss=3.1444, lr=0.000530, tokens/sec=1375686.12, grad_norm=0.3062, duration=0.38s
Step 6222: loss=3.1424, lr=0.000530, tokens/sec=1375556.18, grad_norm=0.2799, duration=0.38s
Step 6223: loss=3.1328, lr=0.000530, tokens/sec=1377779.73, grad_norm=0.2962, duration=0.38s
Step 6224: loss=3.1524, lr=0.000530, tokens/sec=1376370.65, grad_norm=0.2903, duration=0.38s
Step 6225: loss=3.1204, lr=0.000530, tokens/sec=1373771.34, grad_norm=0.3110, duration=0.38s
Step 6226: loss=3.1217, lr=0.000530, tokens/sec=1374661.03, grad_norm=0.2903, duration=0.38s
Step 6227: loss=3.1547, lr=0.000530, tokens/sec=1375440.89, grad_norm=0.3112, duration=0.38s
Step 6228: loss=3.2347, lr=0.000530, tokens/sec=1375718.82, grad_norm=0.3010, duration=0.38s
Step 6229: loss=3.1675, lr=0.000530, tokens/sec=1373459.87, grad_norm=0.3205, duration=0.38s
Step 6230: loss=3.1643, lr=0.000530, tokens/sec=1373882.06, grad_norm=0.3049, duration=0.38s
Step 6231: loss=3.1071, lr=0.000530, tokens/sec=1375912.50, grad_norm=0.2991, duration=0.38s
Step 6232: loss=3.1112, lr=0.000530, tokens/sec=1375937.46, grad_norm=0.3201, duration=0.38s
Step 6233: loss=3.0967, lr=0.000530, tokens/sec=1378561.41, grad_norm=0.2893, duration=0.38s
Step 6234: loss=3.1086, lr=0.000530, tokens/sec=1377667.52, grad_norm=0.3282, duration=0.38s
Step 6235: loss=3.2006, lr=0.000530, tokens/sec=1372807.37, grad_norm=0.2782, duration=0.38s
Step 6236: loss=3.1054, lr=0.000530, tokens/sec=1378837.15, grad_norm=0.3155, duration=0.38s
Step 6237: loss=3.1736, lr=0.000530, tokens/sec=1374431.63, grad_norm=0.2941, duration=0.38s
Step 6238: loss=3.2206, lr=0.000530, tokens/sec=1373475.32, grad_norm=0.3163, duration=0.38s
Step 6239: loss=3.1463, lr=0.000530, tokens/sec=1376306.90, grad_norm=0.2883, duration=0.38s
Step 6240: loss=3.1118, lr=0.000529, tokens/sec=1377025.68, grad_norm=0.2941, duration=0.38s
Step 6241: loss=3.1261, lr=0.000529, tokens/sec=1375023.76, grad_norm=0.3015, duration=0.38s
Step 6242: loss=3.1785, lr=0.000529, tokens/sec=1374479.73, grad_norm=0.2973, duration=0.38s
Step 6243: loss=3.1333, lr=0.000529, tokens/sec=1371742.93, grad_norm=0.3038, duration=0.38s
Step 6244: loss=3.0878, lr=0.000529, tokens/sec=1376389.60, grad_norm=0.2851, duration=0.38s
Step 6245: loss=3.1618, lr=0.000529, tokens/sec=1376906.69, grad_norm=0.2949, duration=0.38s
Step 6246: loss=3.1360, lr=0.000529, tokens/sec=1375984.82, grad_norm=0.3024, duration=0.38s
Step 6247: loss=3.1531, lr=0.000529, tokens/sec=1373079.96, grad_norm=0.2914, duration=0.38s
Step 6248: loss=3.2110, lr=0.000529, tokens/sec=1371480.28, grad_norm=0.3045, duration=0.38s
Step 6249: loss=3.0673, lr=0.000529, tokens/sec=1376923.07, grad_norm=0.3031, duration=0.38s
Validation loss at step 6250: 3.718623399734497
Step 6250: loss=3.1254, lr=0.000529, tokens/sec=153832.11, grad_norm=0.2868, duration=3.41s
Step 6251: loss=3.1195, lr=0.000529, tokens/sec=1376259.53, grad_norm=0.3090, duration=0.38s
Step 6252: loss=3.1003, lr=0.000529, tokens/sec=1378430.06, grad_norm=0.3001, duration=0.38s
Step 6253: loss=3.1039, lr=0.000529, tokens/sec=1379577.61, grad_norm=0.2984, duration=0.38s
Step 6254: loss=3.0774, lr=0.000529, tokens/sec=1379736.88, grad_norm=0.2952, duration=0.38s
Step 6255: loss=3.0365, lr=0.000529, tokens/sec=1376061.45, grad_norm=0.3357, duration=0.38s
Step 6256: loss=3.0747, lr=0.000529, tokens/sec=1376706.70, grad_norm=0.3347, duration=0.38s
Step 6257: loss=3.1401, lr=0.000529, tokens/sec=1373572.26, grad_norm=0.2832, duration=0.38s
Step 6258: loss=3.0691, lr=0.000529, tokens/sec=1373471.03, grad_norm=0.2860, duration=0.38s
Step 6259: loss=3.0644, lr=0.000529, tokens/sec=1373349.22, grad_norm=0.2754, duration=0.38s
Step 6260: loss=3.0095, lr=0.000529, tokens/sec=1372059.61, grad_norm=0.2596, duration=0.38s
Step 6261: loss=3.0480, lr=0.000529, tokens/sec=1369484.21, grad_norm=0.2739, duration=0.38s
Step 6262: loss=3.0724, lr=0.000529, tokens/sec=1377645.95, grad_norm=0.2997, duration=0.38s
Step 6263: loss=3.0669, lr=0.000529, tokens/sec=1375606.95, grad_norm=0.2438, duration=0.38s
Step 6264: loss=3.1174, lr=0.000529, tokens/sec=1377101.56, grad_norm=0.2876, duration=0.38s
Step 6265: loss=3.1245, lr=0.000529, tokens/sec=1377656.30, grad_norm=0.2650, duration=0.38s
Step 6266: loss=3.1203, lr=0.000529, tokens/sec=1375976.21, grad_norm=0.2786, duration=0.38s
Step 6267: loss=3.1410, lr=0.000529, tokens/sec=1373904.37, grad_norm=0.2717, duration=0.38s
Step 6268: loss=3.1246, lr=0.000529, tokens/sec=1372253.96, grad_norm=0.2865, duration=0.38s
Step 6269: loss=3.1712, lr=0.000529, tokens/sec=1375860.85, grad_norm=0.2689, duration=0.38s
Step 6270: loss=3.1156, lr=0.000529, tokens/sec=1376114.84, grad_norm=0.2872, duration=0.38s
Step 6271: loss=3.1617, lr=0.000529, tokens/sec=1373425.56, grad_norm=0.2810, duration=0.38s
Step 6272: loss=3.1387, lr=0.000529, tokens/sec=1375463.26, grad_norm=0.3116, duration=0.38s
Step 6273: loss=3.2250, lr=0.000529, tokens/sec=1370853.59, grad_norm=0.2735, duration=0.38s
Step 6274: loss=3.1304, lr=0.000529, tokens/sec=1377307.71, grad_norm=0.3032, duration=0.38s
Step 6275: loss=3.1895, lr=0.000529, tokens/sec=1374852.69, grad_norm=0.2993, duration=0.38s
Step 6276: loss=3.2271, lr=0.000529, tokens/sec=1372585.44, grad_norm=0.2968, duration=0.38s
Step 6277: loss=3.1403, lr=0.000529, tokens/sec=1372917.94, grad_norm=0.3256, duration=0.38s
Step 6278: loss=3.1995, lr=0.000529, tokens/sec=1372832.23, grad_norm=0.2791, duration=0.38s
Step 6279: loss=3.1594, lr=0.000529, tokens/sec=1377041.20, grad_norm=0.3218, duration=0.38s
Step 6280: loss=3.1085, lr=0.000528, tokens/sec=1375856.54, grad_norm=0.2976, duration=0.38s
Step 6281: loss=3.1256, lr=0.000528, tokens/sec=1375084.81, grad_norm=0.3206, duration=0.38s
Step 6282: loss=3.1440, lr=0.000528, tokens/sec=1378022.35, grad_norm=0.3113, duration=0.38s
Step 6283: loss=3.1150, lr=0.000528, tokens/sec=1376430.09, grad_norm=0.3282, duration=0.38s
Step 6284: loss=3.1670, lr=0.000528, tokens/sec=1380220.10, grad_norm=0.2857, duration=0.38s
Step 6285: loss=3.1662, lr=0.000528, tokens/sec=1376869.62, grad_norm=0.3692, duration=0.38s
Step 6286: loss=3.1001, lr=0.000528, tokens/sec=1372806.52, grad_norm=0.3192, duration=0.38s
Step 6287: loss=3.1807, lr=0.000528, tokens/sec=1371609.45, grad_norm=0.3725, duration=0.38s
Step 6288: loss=3.1367, lr=0.000528, tokens/sec=1373918.11, grad_norm=0.3243, duration=0.38s
Step 6289: loss=3.0802, lr=0.000528, tokens/sec=1377532.89, grad_norm=0.3489, duration=0.38s
Step 6290: loss=3.1229, lr=0.000528, tokens/sec=1374548.47, grad_norm=0.3645, duration=0.38s
Step 6291: loss=3.1548, lr=0.000528, tokens/sec=1380439.31, grad_norm=0.3332, duration=0.38s
Step 6292: loss=3.2215, lr=0.000528, tokens/sec=1376980.84, grad_norm=0.3344, duration=0.38s
Step 6293: loss=3.0790, lr=0.000528, tokens/sec=1369372.50, grad_norm=0.3195, duration=0.38s
Step 6294: loss=3.1917, lr=0.000528, tokens/sec=1372141.79, grad_norm=0.3521, duration=0.38s
Step 6295: loss=3.1429, lr=0.000528, tokens/sec=1373768.76, grad_norm=0.2960, duration=0.38s
Step 6296: loss=3.1058, lr=0.000528, tokens/sec=1378889.02, grad_norm=0.3325, duration=0.38s
Step 6297: loss=3.1307, lr=0.000528, tokens/sec=1376701.53, grad_norm=0.3131, duration=0.38s
Step 6298: loss=3.1542, lr=0.000528, tokens/sec=1376947.21, grad_norm=0.3110, duration=0.38s
Step 6299: loss=3.1565, lr=0.000528, tokens/sec=1372155.49, grad_norm=0.3315, duration=0.38s
Step 6300/19073 (33.0%), Elapsed time: 2508.09s, Steps per hour: 9042.75, Estimated hours remaining: 1.41
Step 6300: loss=3.1037, lr=0.000528, tokens/sec=1373652.91, grad_norm=0.2950, duration=0.38s
Step 6301: loss=3.1456, lr=0.000528, tokens/sec=1373260.89, grad_norm=0.3472, duration=0.38s
Step 6302: loss=3.1788, lr=0.000528, tokens/sec=1374909.42, grad_norm=0.3255, duration=0.38s
Step 6303: loss=3.1719, lr=0.000528, tokens/sec=1378238.27, grad_norm=0.3315, duration=0.38s
Step 6304: loss=3.1332, lr=0.000528, tokens/sec=1379055.05, grad_norm=0.3325, duration=0.38s
Step 6305: loss=3.0627, lr=0.000528, tokens/sec=1375881.51, grad_norm=0.3069, duration=0.38s
Step 6306: loss=3.0969, lr=0.000528, tokens/sec=1377057.58, grad_norm=0.3173, duration=0.38s
Step 6307: loss=3.0604, lr=0.000528, tokens/sec=1376724.80, grad_norm=0.2994, duration=0.38s
Step 6308: loss=3.0483, lr=0.000528, tokens/sec=1376063.17, grad_norm=0.3125, duration=0.38s
Step 6309: loss=3.0757, lr=0.000528, tokens/sec=1375752.39, grad_norm=0.2974, duration=0.38s
Step 6310: loss=3.0872, lr=0.000528, tokens/sec=1377351.70, grad_norm=0.3051, duration=0.38s
Step 6311: loss=3.0830, lr=0.000528, tokens/sec=1378125.98, grad_norm=0.3018, duration=0.38s
Step 6312: loss=3.0759, lr=0.000528, tokens/sec=1373743.02, grad_norm=0.2830, duration=0.38s
Step 6313: loss=3.0983, lr=0.000528, tokens/sec=1376175.98, grad_norm=0.3243, duration=0.38s
Step 6314: loss=3.1002, lr=0.000528, tokens/sec=1376856.69, grad_norm=0.2835, duration=0.38s
Step 6315: loss=3.0588, lr=0.000528, tokens/sec=1376333.61, grad_norm=0.3197, duration=0.38s
Step 6316: loss=3.0722, lr=0.000528, tokens/sec=1375908.19, grad_norm=0.2792, duration=0.38s
Step 6317: loss=3.1489, lr=0.000528, tokens/sec=1379940.34, grad_norm=0.3246, duration=0.38s
Step 6318: loss=3.2012, lr=0.000528, tokens/sec=1377748.66, grad_norm=0.2888, duration=0.38s
Step 6319: loss=3.1600, lr=0.000528, tokens/sec=1376783.42, grad_norm=0.3395, duration=0.38s
Step 6320: loss=3.1254, lr=0.000527, tokens/sec=1378092.30, grad_norm=0.2851, duration=0.38s
Step 6321: loss=3.1627, lr=0.000527, tokens/sec=1375196.60, grad_norm=0.3122, duration=0.38s
Step 6322: loss=3.1492, lr=0.000527, tokens/sec=1378106.98, grad_norm=0.2795, duration=0.38s
Step 6323: loss=3.1422, lr=0.000527, tokens/sec=1378120.80, grad_norm=0.3116, duration=0.38s
Step 6324: loss=3.1094, lr=0.000527, tokens/sec=1375583.71, grad_norm=0.2769, duration=0.38s
Step 6325: loss=3.1178, lr=0.000527, tokens/sec=1375576.83, grad_norm=0.3212, duration=0.38s
Step 6326: loss=3.0744, lr=0.000527, tokens/sec=1376435.26, grad_norm=0.2881, duration=0.38s
Step 6327: loss=3.1173, lr=0.000527, tokens/sec=1374268.43, grad_norm=0.3126, duration=0.38s
Step 6328: loss=3.1054, lr=0.000527, tokens/sec=1375689.56, grad_norm=0.2827, duration=0.38s
Step 6329: loss=3.1418, lr=0.000527, tokens/sec=1380459.24, grad_norm=0.3004, duration=0.38s
Step 6330: loss=3.1401, lr=0.000527, tokens/sec=1375028.06, grad_norm=0.2898, duration=0.38s
Step 6331: loss=3.0657, lr=0.000527, tokens/sec=1376542.96, grad_norm=0.2869, duration=0.38s
Step 6332: loss=3.1073, lr=0.000527, tokens/sec=1379602.71, grad_norm=0.2895, duration=0.38s
Step 6333: loss=3.1517, lr=0.000527, tokens/sec=1375065.90, grad_norm=0.2744, duration=0.38s
Step 6334: loss=3.1274, lr=0.000527, tokens/sec=1374905.12, grad_norm=0.2860, duration=0.38s
Step 6335: loss=3.1469, lr=0.000527, tokens/sec=1377619.19, grad_norm=0.2767, duration=0.38s
Step 6336: loss=3.1269, lr=0.000527, tokens/sec=1378593.38, grad_norm=0.3127, duration=0.38s
Step 6337: loss=3.2084, lr=0.000527, tokens/sec=1375495.95, grad_norm=0.2783, duration=0.38s
Step 6338: loss=3.1582, lr=0.000527, tokens/sec=1372369.58, grad_norm=0.3174, duration=0.38s
Step 6339: loss=3.1692, lr=0.000527, tokens/sec=1374652.44, grad_norm=0.2735, duration=0.38s
Step 6340: loss=3.1391, lr=0.000527, tokens/sec=1376308.63, grad_norm=0.3187, duration=0.38s
Step 6341: loss=3.1592, lr=0.000527, tokens/sec=1378549.31, grad_norm=0.2609, duration=0.38s
Step 6342: loss=3.1298, lr=0.000527, tokens/sec=1372956.51, grad_norm=0.3035, duration=0.38s
Step 6343: loss=3.1381, lr=0.000527, tokens/sec=1377573.45, grad_norm=0.2981, duration=0.38s
Step 6344: loss=3.1507, lr=0.000527, tokens/sec=1379678.01, grad_norm=0.2720, duration=0.38s
Step 6345: loss=3.0972, lr=0.000527, tokens/sec=1379512.70, grad_norm=0.3040, duration=0.38s
Step 6346: loss=3.1249, lr=0.000527, tokens/sec=1376183.73, grad_norm=0.2868, duration=0.38s
Step 6347: loss=3.0631, lr=0.000527, tokens/sec=1379234.09, grad_norm=0.2899, duration=0.38s
Step 6348: loss=3.1330, lr=0.000527, tokens/sec=1375063.32, grad_norm=0.2868, duration=0.38s
Step 6349: loss=3.1171, lr=0.000527, tokens/sec=1376596.39, grad_norm=0.2955, duration=0.38s
Step 6350: loss=3.0991, lr=0.000527, tokens/sec=1377899.73, grad_norm=0.2997, duration=0.38s
Step 6351: loss=3.0515, lr=0.000527, tokens/sec=1377061.90, grad_norm=0.3155, duration=0.38s
Step 6352: loss=3.1028, lr=0.000527, tokens/sec=1377070.52, grad_norm=0.2915, duration=0.38s
Step 6353: loss=3.0636, lr=0.000527, tokens/sec=1375300.67, grad_norm=0.3079, duration=0.38s
Step 6354: loss=3.0831, lr=0.000527, tokens/sec=1372920.51, grad_norm=0.2817, duration=0.38s
Step 6355: loss=3.0360, lr=0.000527, tokens/sec=1373700.11, grad_norm=0.3146, duration=0.38s
Step 6356: loss=3.0582, lr=0.000527, tokens/sec=1377771.10, grad_norm=0.2860, duration=0.38s
Step 6357: loss=3.1065, lr=0.000527, tokens/sec=1378106.11, grad_norm=0.3160, duration=0.38s
Step 6358: loss=3.0154, lr=0.000527, tokens/sec=1377887.65, grad_norm=0.2767, duration=0.38s
Step 6359: loss=3.0448, lr=0.000527, tokens/sec=1377771.96, grad_norm=0.3281, duration=0.38s
Step 6360: loss=3.0315, lr=0.000526, tokens/sec=1375174.24, grad_norm=0.2650, duration=0.38s
Step 6361: loss=3.0876, lr=0.000526, tokens/sec=1377227.48, grad_norm=0.3036, duration=0.38s
Step 6362: loss=3.0522, lr=0.000526, tokens/sec=1373946.44, grad_norm=0.2768, duration=0.38s
Step 6363: loss=3.0994, lr=0.000526, tokens/sec=1375276.59, grad_norm=0.2877, duration=0.38s
Step 6364: loss=3.1552, lr=0.000526, tokens/sec=1378330.70, grad_norm=0.2938, duration=0.38s
Step 6365: loss=3.1314, lr=0.000526, tokens/sec=1375584.57, grad_norm=0.2770, duration=0.38s
Step 6366: loss=3.1303, lr=0.000526, tokens/sec=1379487.60, grad_norm=0.2889, duration=0.38s
Step 6367: loss=3.1449, lr=0.000526, tokens/sec=1377726.22, grad_norm=0.2777, duration=0.38s
Step 6368: loss=3.0783, lr=0.000526, tokens/sec=1375822.11, grad_norm=0.2862, duration=0.38s
Step 6369: loss=3.1959, lr=0.000526, tokens/sec=1377368.09, grad_norm=0.3141, duration=0.38s
Step 6370: loss=3.1502, lr=0.000526, tokens/sec=1374954.98, grad_norm=0.3076, duration=0.38s
Step 6371: loss=3.1675, lr=0.000526, tokens/sec=1378631.41, grad_norm=0.2889, duration=0.38s
Step 6372: loss=3.1620, lr=0.000526, tokens/sec=1377250.77, grad_norm=0.3035, duration=0.38s
Step 6373: loss=3.1149, lr=0.000526, tokens/sec=1376582.60, grad_norm=0.2854, duration=0.38s
Step 6374: loss=3.1663, lr=0.000526, tokens/sec=1378931.39, grad_norm=0.2985, duration=0.38s
Step 6375: loss=3.1440, lr=0.000526, tokens/sec=1376175.12, grad_norm=0.3031, duration=0.38s
Step 6376: loss=3.1503, lr=0.000526, tokens/sec=1374616.35, grad_norm=0.3071, duration=0.38s
Step 6377: loss=3.1736, lr=0.000526, tokens/sec=1375114.05, grad_norm=0.3042, duration=0.38s
Step 6378: loss=3.1723, lr=0.000526, tokens/sec=1375040.96, grad_norm=0.3014, duration=0.38s
Step 6379: loss=3.0887, lr=0.000526, tokens/sec=1378881.24, grad_norm=0.3075, duration=0.38s
Step 6380: loss=3.1693, lr=0.000526, tokens/sec=1371459.75, grad_norm=0.2866, duration=0.38s
Step 6381: loss=3.1248, lr=0.000526, tokens/sec=1377695.14, grad_norm=0.3163, duration=0.38s
Step 6382: loss=3.1277, lr=0.000526, tokens/sec=1377563.96, grad_norm=0.2842, duration=0.38s
Step 6383: loss=3.0976, lr=0.000526, tokens/sec=1377521.67, grad_norm=0.3084, duration=0.38s
Step 6384: loss=3.1547, lr=0.000526, tokens/sec=1378685.86, grad_norm=0.2959, duration=0.38s
Step 6385: loss=3.0978, lr=0.000526, tokens/sec=1373715.56, grad_norm=0.3180, duration=0.38s
Step 6386: loss=3.1279, lr=0.000526, tokens/sec=1374707.44, grad_norm=0.2897, duration=0.38s
Step 6387: loss=3.0950, lr=0.000526, tokens/sec=1376796.35, grad_norm=0.2849, duration=0.38s
Step 6388: loss=3.1346, lr=0.000526, tokens/sec=1372869.94, grad_norm=0.2647, duration=0.38s
Step 6389: loss=3.1132, lr=0.000526, tokens/sec=1372700.26, grad_norm=0.2839, duration=0.38s
Step 6390: loss=3.1426, lr=0.000526, tokens/sec=1375067.61, grad_norm=0.2724, duration=0.38s
Step 6391: loss=3.1106, lr=0.000526, tokens/sec=1374134.46, grad_norm=0.2861, duration=0.38s
Step 6392: loss=3.1159, lr=0.000526, tokens/sec=1375257.67, grad_norm=0.2844, duration=0.38s
Step 6393: loss=3.1132, lr=0.000526, tokens/sec=1377330.13, grad_norm=0.3395, duration=0.38s
Step 6394: loss=3.1168, lr=0.000526, tokens/sec=1377098.98, grad_norm=0.2946, duration=0.38s
Step 6395: loss=3.1375, lr=0.000526, tokens/sec=1376977.39, grad_norm=0.3087, duration=0.38s
Step 6396: loss=3.1199, lr=0.000526, tokens/sec=1374997.11, grad_norm=0.2699, duration=0.38s
Step 6397: loss=3.1335, lr=0.000526, tokens/sec=1377282.69, grad_norm=0.2788, duration=0.38s
Step 6398: loss=3.1099, lr=0.000526, tokens/sec=1372833.09, grad_norm=0.2770, duration=0.38s
Step 6399: loss=3.0427, lr=0.000526, tokens/sec=1375756.69, grad_norm=0.2678, duration=0.38s
Step 6400/19073 (33.6%), Elapsed time: 2546.26s, Steps per hour: 9048.56, Estimated hours remaining: 1.40
Step 6400: loss=3.0850, lr=0.000525, tokens/sec=1375948.66, grad_norm=0.2985, duration=0.38s
Step 6401: loss=3.0106, lr=0.000525, tokens/sec=1379167.49, grad_norm=0.2758, duration=0.38s
Step 6402: loss=3.0371, lr=0.000525, tokens/sec=1378826.77, grad_norm=0.3041, duration=0.38s
Step 6403: loss=3.0371, lr=0.000525, tokens/sec=1377204.20, grad_norm=0.3138, duration=0.38s
Step 6404: loss=3.1103, lr=0.000525, tokens/sec=1374073.50, grad_norm=0.3188, duration=0.38s
Step 6405: loss=3.0799, lr=0.000525, tokens/sec=1374613.77, grad_norm=0.2839, duration=0.38s
Step 6406: loss=3.0463, lr=0.000525, tokens/sec=1376648.96, grad_norm=0.3338, duration=0.38s
Step 6407: loss=3.0212, lr=0.000525, tokens/sec=1374025.42, grad_norm=0.2867, duration=0.38s
Step 6408: loss=3.0829, lr=0.000525, tokens/sec=1377793.55, grad_norm=0.3112, duration=0.38s
Step 6409: loss=3.1232, lr=0.000525, tokens/sec=1379617.42, grad_norm=0.3152, duration=0.38s
Step 6410: loss=3.1197, lr=0.000525, tokens/sec=1377840.16, grad_norm=0.3247, duration=0.38s
Step 6411: loss=3.1264, lr=0.000525, tokens/sec=1374526.13, grad_norm=0.3255, duration=0.38s
Step 6412: loss=3.1466, lr=0.000525, tokens/sec=1371169.86, grad_norm=0.3287, duration=0.38s
Step 6413: loss=3.1150, lr=0.000525, tokens/sec=1374781.35, grad_norm=0.3029, duration=0.38s
Step 6414: loss=3.1638, lr=0.000525, tokens/sec=1374502.07, grad_norm=0.3251, duration=0.38s
Step 6415: loss=3.0902, lr=0.000525, tokens/sec=1375502.83, grad_norm=0.3156, duration=0.38s
Step 6416: loss=3.0988, lr=0.000525, tokens/sec=1377673.56, grad_norm=0.3124, duration=0.38s
Step 6417: loss=3.1615, lr=0.000525, tokens/sec=1372816.80, grad_norm=0.3194, duration=0.38s
Step 6418: loss=3.2224, lr=0.000525, tokens/sec=1374857.85, grad_norm=0.3073, duration=0.38s
Step 6419: loss=3.1647, lr=0.000525, tokens/sec=1377551.88, grad_norm=0.3114, duration=0.38s
Step 6420: loss=3.1524, lr=0.000525, tokens/sec=1377406.05, grad_norm=0.2979, duration=0.38s
Step 6421: loss=3.0706, lr=0.000525, tokens/sec=1374318.24, grad_norm=0.3012, duration=0.38s
Step 6422: loss=3.1252, lr=0.000525, tokens/sec=1376039.06, grad_norm=0.3164, duration=0.38s
Step 6423: loss=3.1313, lr=0.000525, tokens/sec=1374834.64, grad_norm=0.3159, duration=0.38s
Step 6424: loss=3.0996, lr=0.000525, tokens/sec=1373155.41, grad_norm=0.2997, duration=0.38s
Step 6425: loss=3.1795, lr=0.000525, tokens/sec=1375638.79, grad_norm=0.3076, duration=0.38s
Step 6426: loss=3.0750, lr=0.000525, tokens/sec=1377542.38, grad_norm=0.3062, duration=0.38s
Step 6427: loss=3.1825, lr=0.000525, tokens/sec=1377594.16, grad_norm=0.2860, duration=0.38s
Step 6428: loss=3.1922, lr=0.000525, tokens/sec=1374672.20, grad_norm=0.3153, duration=0.38s
Step 6429: loss=3.1568, lr=0.000525, tokens/sec=1374178.25, grad_norm=0.2971, duration=0.38s
Step 6430: loss=3.0591, lr=0.000525, tokens/sec=1373517.35, grad_norm=0.2945, duration=0.38s
Step 6431: loss=3.1501, lr=0.000525, tokens/sec=1376665.33, grad_norm=0.3092, duration=0.38s
Step 6432: loss=3.1598, lr=0.000525, tokens/sec=1373520.78, grad_norm=0.2903, duration=0.38s
Step 6433: loss=3.1516, lr=0.000525, tokens/sec=1373000.23, grad_norm=0.3128, duration=0.38s
Step 6434: loss=3.0849, lr=0.000525, tokens/sec=1377190.40, grad_norm=0.2870, duration=0.38s
Step 6435: loss=3.1301, lr=0.000525, tokens/sec=1372779.95, grad_norm=0.3224, duration=0.38s
Step 6436: loss=3.1238, lr=0.000525, tokens/sec=1376382.71, grad_norm=0.3088, duration=0.38s
Step 6437: loss=3.1739, lr=0.000525, tokens/sec=1375112.33, grad_norm=0.2980, duration=0.38s
Step 6438: loss=3.1399, lr=0.000525, tokens/sec=1377695.14, grad_norm=0.3379, duration=0.38s
Step 6439: loss=3.1040, lr=0.000524, tokens/sec=1373807.38, grad_norm=0.2931, duration=0.38s
Step 6440: loss=3.1095, lr=0.000524, tokens/sec=1376277.62, grad_norm=0.3267, duration=0.38s
Step 6441: loss=3.1374, lr=0.000524, tokens/sec=1378066.39, grad_norm=0.3134, duration=0.38s
Step 6442: loss=3.0513, lr=0.000524, tokens/sec=1376594.67, grad_norm=0.3146, duration=0.38s
Step 6443: loss=3.1073, lr=0.000524, tokens/sec=1376132.92, grad_norm=0.3015, duration=0.38s
Step 6444: loss=3.0397, lr=0.000524, tokens/sec=1376492.13, grad_norm=0.3752, duration=0.38s
Step 6445: loss=3.0614, lr=0.000524, tokens/sec=1377113.64, grad_norm=0.3179, duration=0.38s
Step 6446: loss=3.0676, lr=0.000524, tokens/sec=1374865.58, grad_norm=0.3485, duration=0.38s
Step 6447: loss=3.1128, lr=0.000524, tokens/sec=1376306.04, grad_norm=0.3043, duration=0.38s
Step 6448: loss=3.0756, lr=0.000524, tokens/sec=1374389.53, grad_norm=0.3468, duration=0.38s
Step 6449: loss=3.0328, lr=0.000524, tokens/sec=1376157.04, grad_norm=0.2811, duration=0.38s
Step 6450: loss=3.0054, lr=0.000524, tokens/sec=1373771.34, grad_norm=0.2953, duration=0.38s
Step 6451: loss=3.0303, lr=0.000524, tokens/sec=1375446.05, grad_norm=0.2905, duration=0.38s
Step 6452: loss=3.0713, lr=0.000524, tokens/sec=1374111.28, grad_norm=0.2921, duration=0.38s
Step 6453: loss=3.0890, lr=0.000524, tokens/sec=1373810.82, grad_norm=0.2673, duration=0.38s
Step 6454: loss=3.0717, lr=0.000524, tokens/sec=1379081.00, grad_norm=0.2941, duration=0.38s
Step 6455: loss=3.1128, lr=0.000524, tokens/sec=1375500.25, grad_norm=0.2955, duration=0.38s
Step 6456: loss=3.1239, lr=0.000524, tokens/sec=1373171.70, grad_norm=0.2825, duration=0.38s
Step 6457: loss=3.1461, lr=0.000524, tokens/sec=1377260.26, grad_norm=0.3069, duration=0.38s
Step 6458: loss=3.1307, lr=0.000524, tokens/sec=1376077.81, grad_norm=0.2616, duration=0.38s
Step 6459: loss=3.1292, lr=0.000524, tokens/sec=1371789.14, grad_norm=0.3045, duration=0.38s
Step 6460: loss=3.1152, lr=0.000524, tokens/sec=1372081.86, grad_norm=0.2880, duration=0.38s
Step 6461: loss=3.1187, lr=0.000524, tokens/sec=1375841.05, grad_norm=0.2797, duration=0.38s
Step 6462: loss=3.1513, lr=0.000524, tokens/sec=1377197.30, grad_norm=0.3058, duration=0.38s
Step 6463: loss=3.1869, lr=0.000524, tokens/sec=1376299.15, grad_norm=0.3030, duration=0.38s
Step 6464: loss=3.1547, lr=0.000524, tokens/sec=1372844.23, grad_norm=0.2931, duration=0.38s
Step 6465: loss=3.1693, lr=0.000524, tokens/sec=1371062.14, grad_norm=0.3163, duration=0.38s
Step 6466: loss=3.1963, lr=0.000524, tokens/sec=1375803.17, grad_norm=0.2995, duration=0.38s
Step 6467: loss=3.1714, lr=0.000524, tokens/sec=1375725.71, grad_norm=0.3054, duration=0.38s
Step 6468: loss=3.1731, lr=0.000524, tokens/sec=1370525.51, grad_norm=0.3025, duration=0.38s
Step 6469: loss=3.0852, lr=0.000524, tokens/sec=1372382.43, grad_norm=0.3101, duration=0.38s
Step 6470: loss=3.1524, lr=0.000524, tokens/sec=1375807.48, grad_norm=0.3168, duration=0.38s
Step 6471: loss=3.1162, lr=0.000524, tokens/sec=1376749.80, grad_norm=0.3268, duration=0.38s
Step 6472: loss=3.1681, lr=0.000524, tokens/sec=1376082.98, grad_norm=0.3192, duration=0.38s
Step 6473: loss=3.0954, lr=0.000524, tokens/sec=1377177.46, grad_norm=0.3330, duration=0.38s
Step 6474: loss=3.1653, lr=0.000524, tokens/sec=1373022.52, grad_norm=0.3518, duration=0.38s
Step 6475: loss=3.1316, lr=0.000524, tokens/sec=1377986.08, grad_norm=0.3266, duration=0.38s
Step 6476: loss=3.1060, lr=0.000524, tokens/sec=1376753.25, grad_norm=0.3646, duration=0.38s
Step 6477: loss=3.2078, lr=0.000524, tokens/sec=1373250.60, grad_norm=0.3809, duration=0.38s
Step 6478: loss=3.1118, lr=0.000524, tokens/sec=1377375.86, grad_norm=0.3341, duration=0.38s
Step 6479: loss=3.0563, lr=0.000523, tokens/sec=1374493.48, grad_norm=0.4078, duration=0.38s
Step 6480: loss=3.1175, lr=0.000523, tokens/sec=1375996.01, grad_norm=0.3647, duration=0.38s
Step 6481: loss=3.1329, lr=0.000523, tokens/sec=1380080.64, grad_norm=0.3117, duration=0.38s
Step 6482: loss=3.2208, lr=0.000523, tokens/sec=1370864.70, grad_norm=0.3406, duration=0.38s
Step 6483: loss=3.0769, lr=0.000523, tokens/sec=1373244.59, grad_norm=0.3311, duration=0.38s
Step 6484: loss=3.1567, lr=0.000523, tokens/sec=1375443.47, grad_norm=0.3287, duration=0.38s
Step 6485: loss=3.1482, lr=0.000523, tokens/sec=1375942.63, grad_norm=0.3498, duration=0.38s
Step 6486: loss=3.0925, lr=0.000523, tokens/sec=1379082.73, grad_norm=0.3219, duration=0.38s
Step 6487: loss=3.1214, lr=0.000523, tokens/sec=1374062.34, grad_norm=0.3366, duration=0.38s
Step 6488: loss=3.1721, lr=0.000523, tokens/sec=1376205.27, grad_norm=0.2977, duration=0.38s
Step 6489: loss=3.1212, lr=0.000523, tokens/sec=1379060.24, grad_norm=0.3602, duration=0.38s
Step 6490: loss=3.1023, lr=0.000523, tokens/sec=1375656.86, grad_norm=0.2917, duration=0.38s
Step 6491: loss=3.1565, lr=0.000523, tokens/sec=1375157.04, grad_norm=0.3252, duration=0.38s
Step 6492: loss=3.1332, lr=0.000523, tokens/sec=1379271.29, grad_norm=0.3542, duration=0.38s
Step 6493: loss=3.1897, lr=0.000523, tokens/sec=1374629.24, grad_norm=0.2957, duration=0.38s
Step 6494: loss=3.1099, lr=0.000523, tokens/sec=1376088.14, grad_norm=0.3290, duration=0.38s
Step 6495: loss=3.0360, lr=0.000523, tokens/sec=1375436.59, grad_norm=0.2976, duration=0.38s
Step 6496: loss=3.1008, lr=0.000523, tokens/sec=1377412.09, grad_norm=0.2953, duration=0.38s
Step 6497: loss=3.0416, lr=0.000523, tokens/sec=1375243.90, grad_norm=0.3028, duration=0.38s
Step 6498: loss=3.0495, lr=0.000523, tokens/sec=1374168.81, grad_norm=0.3206, duration=0.38s
Step 6499: loss=3.0679, lr=0.000523, tokens/sec=1374520.97, grad_norm=0.2792, duration=0.38s
Step 6500/19073 (34.1%), Elapsed time: 2584.47s, Steps per hour: 9054.08, Estimated hours remaining: 1.39
Validation loss at step 6500: 3.726940155029297
Step 6500: loss=3.0670, lr=0.000523, tokens/sec=156311.16, grad_norm=0.2908, duration=3.35s
Step 6501: loss=3.0632, lr=0.000523, tokens/sec=1378380.81, grad_norm=0.3040, duration=0.38s
Step 6502: loss=3.0715, lr=0.000523, tokens/sec=1375804.89, grad_norm=0.2868, duration=0.38s
Step 6503: loss=3.1155, lr=0.000523, tokens/sec=1377783.19, grad_norm=0.2972, duration=0.38s
Step 6504: loss=3.0510, lr=0.000523, tokens/sec=1377058.45, grad_norm=0.2923, duration=0.38s
Step 6505: loss=3.0490, lr=0.000523, tokens/sec=1373249.74, grad_norm=0.2821, duration=0.38s
Step 6506: loss=3.0595, lr=0.000523, tokens/sec=1373957.60, grad_norm=0.2972, duration=0.38s
Step 6507: loss=3.1481, lr=0.000523, tokens/sec=1374806.27, grad_norm=0.2921, duration=0.38s
Step 6508: loss=3.1909, lr=0.000523, tokens/sec=1375857.40, grad_norm=0.2880, duration=0.38s
Step 6509: loss=3.1156, lr=0.000523, tokens/sec=1376340.50, grad_norm=0.2983, duration=0.38s
Step 6510: loss=3.1320, lr=0.000523, tokens/sec=1375828.14, grad_norm=0.3093, duration=0.38s
Step 6511: loss=3.1440, lr=0.000523, tokens/sec=1371888.41, grad_norm=0.2878, duration=0.38s
Step 6512: loss=3.1344, lr=0.000523, tokens/sec=1372681.40, grad_norm=0.3058, duration=0.38s
Step 6513: loss=3.1388, lr=0.000523, tokens/sec=1372653.13, grad_norm=0.2979, duration=0.38s
Step 6514: loss=3.1043, lr=0.000523, tokens/sec=1375587.16, grad_norm=0.2873, duration=0.38s
Step 6515: loss=3.0407, lr=0.000523, tokens/sec=1379128.57, grad_norm=0.3009, duration=0.38s
Step 6516: loss=3.1199, lr=0.000523, tokens/sec=1376012.37, grad_norm=0.2909, duration=0.38s
Step 6517: loss=3.1060, lr=0.000523, tokens/sec=1377512.18, grad_norm=0.3032, duration=0.38s
Step 6518: loss=3.1308, lr=0.000522, tokens/sec=1374879.34, grad_norm=0.2797, duration=0.38s
Step 6519: loss=3.1259, lr=0.000522, tokens/sec=1376917.04, grad_norm=0.3005, duration=0.38s
Step 6520: loss=3.1281, lr=0.000522, tokens/sec=1376291.40, grad_norm=0.2740, duration=0.38s
Step 6521: loss=3.0477, lr=0.000522, tokens/sec=1378710.07, grad_norm=0.3293, duration=0.38s
Step 6522: loss=3.0979, lr=0.000522, tokens/sec=1373607.44, grad_norm=0.2690, duration=0.38s
Step 6523: loss=3.1244, lr=0.000522, tokens/sec=1378852.71, grad_norm=0.3028, duration=0.38s
Step 6524: loss=3.0911, lr=0.000522, tokens/sec=1372601.72, grad_norm=0.2698, duration=0.38s
Step 6525: loss=3.1829, lr=0.000522, tokens/sec=1378199.39, grad_norm=0.2899, duration=0.38s
Step 6526: loss=3.1181, lr=0.000522, tokens/sec=1378692.78, grad_norm=0.2951, duration=0.38s
Step 6527: loss=3.1882, lr=0.000522, tokens/sec=1378024.07, grad_norm=0.2826, duration=0.38s
Step 6528: loss=3.1775, lr=0.000522, tokens/sec=1375190.58, grad_norm=0.3157, duration=0.38s
Step 6529: loss=3.1233, lr=0.000522, tokens/sec=1377231.80, grad_norm=0.2880, duration=0.38s
Step 6530: loss=3.1479, lr=0.000522, tokens/sec=1375255.09, grad_norm=0.2832, duration=0.38s
Step 6531: loss=3.1408, lr=0.000522, tokens/sec=1375525.20, grad_norm=0.2920, duration=0.38s
Step 6532: loss=3.1102, lr=0.000522, tokens/sec=1374080.37, grad_norm=0.2719, duration=0.38s
Step 6533: loss=3.1449, lr=0.000522, tokens/sec=1373533.65, grad_norm=0.3013, duration=0.38s
Step 6534: loss=3.0842, lr=0.000522, tokens/sec=1379138.08, grad_norm=0.2637, duration=0.38s
Step 6535: loss=3.1170, lr=0.000522, tokens/sec=1376739.46, grad_norm=0.3113, duration=0.38s
Step 6536: loss=3.0913, lr=0.000522, tokens/sec=1375655.14, grad_norm=0.2745, duration=0.38s
Step 6537: loss=3.0838, lr=0.000522, tokens/sec=1374762.44, grad_norm=0.2816, duration=0.38s
Step 6538: loss=3.1137, lr=0.000522, tokens/sec=1376623.97, grad_norm=0.2578, duration=0.38s
Step 6539: loss=3.0782, lr=0.000522, tokens/sec=1375022.04, grad_norm=0.2834, duration=0.38s
Step 6540: loss=3.1112, lr=0.000522, tokens/sec=1375954.68, grad_norm=0.2804, duration=0.38s
Step 6541: loss=3.0305, lr=0.000522, tokens/sec=1376411.14, grad_norm=0.3177, duration=0.38s
Step 6542: loss=3.1102, lr=0.000522, tokens/sec=1380687.19, grad_norm=0.2827, duration=0.38s
Step 6543: loss=3.0667, lr=0.000522, tokens/sec=1371302.39, grad_norm=0.3328, duration=0.38s
Step 6544: loss=3.0719, lr=0.000522, tokens/sec=1377954.99, grad_norm=0.2776, duration=0.38s
Step 6545: loss=3.0167, lr=0.000522, tokens/sec=1375064.18, grad_norm=0.3511, duration=0.38s
Step 6546: loss=3.0746, lr=0.000522, tokens/sec=1374865.58, grad_norm=0.2764, duration=0.38s
Step 6547: loss=3.0514, lr=0.000522, tokens/sec=1377946.36, grad_norm=0.3264, duration=0.38s
Step 6548: loss=3.0126, lr=0.000522, tokens/sec=1376193.21, grad_norm=0.2846, duration=0.38s
Step 6549: loss=3.0418, lr=0.000522, tokens/sec=1377312.88, grad_norm=0.3090, duration=0.38s
Step 6550: loss=3.0236, lr=0.000522, tokens/sec=1377157.62, grad_norm=0.3097, duration=0.38s
Step 6551: loss=3.0561, lr=0.000522, tokens/sec=1380750.48, grad_norm=0.2831, duration=0.38s
Step 6552: loss=3.0598, lr=0.000522, tokens/sec=1376681.71, grad_norm=0.3184, duration=0.38s
Step 6553: loss=3.1000, lr=0.000522, tokens/sec=1374732.36, grad_norm=0.2587, duration=0.38s
Step 6554: loss=3.1156, lr=0.000522, tokens/sec=1375837.60, grad_norm=0.3120, duration=0.38s
Step 6555: loss=3.1150, lr=0.000522, tokens/sec=1374356.89, grad_norm=0.2832, duration=0.38s
Step 6556: loss=3.1368, lr=0.000522, tokens/sec=1370379.46, grad_norm=0.2961, duration=0.38s
Step 6557: loss=3.1268, lr=0.000521, tokens/sec=1373132.26, grad_norm=0.2945, duration=0.38s
Step 6558: loss=3.1389, lr=0.000521, tokens/sec=1377789.23, grad_norm=0.3005, duration=0.38s
Step 6559: loss=3.1665, lr=0.000521, tokens/sec=1375133.83, grad_norm=0.3224, duration=0.38s
Step 6560: loss=3.0979, lr=0.000521, tokens/sec=1379009.22, grad_norm=0.3300, duration=0.38s
Step 6561: loss=3.1617, lr=0.000521, tokens/sec=1373970.47, grad_norm=0.3173, duration=0.38s
Step 6562: loss=3.1181, lr=0.000521, tokens/sec=1372811.66, grad_norm=0.2995, duration=0.38s
Step 6563: loss=3.1469, lr=0.000521, tokens/sec=1373367.23, grad_norm=0.3277, duration=0.38s
Step 6564: loss=3.1674, lr=0.000521, tokens/sec=1375553.60, grad_norm=0.2916, duration=0.38s
Step 6565: loss=3.1284, lr=0.000521, tokens/sec=1373449.58, grad_norm=0.3229, duration=0.38s
Step 6566: loss=3.1675, lr=0.000521, tokens/sec=1373483.89, grad_norm=0.3043, duration=0.38s
Step 6567: loss=3.1460, lr=0.000521, tokens/sec=1375387.55, grad_norm=0.2936, duration=0.38s
Step 6568: loss=3.1277, lr=0.000521, tokens/sec=1372102.41, grad_norm=0.2958, duration=0.38s
Step 6569: loss=3.1126, lr=0.000521, tokens/sec=1374933.49, grad_norm=0.3036, duration=0.38s
Step 6570: loss=3.1255, lr=0.000521, tokens/sec=1377180.91, grad_norm=0.2898, duration=0.38s
Step 6571: loss=3.1092, lr=0.000521, tokens/sec=1378154.48, grad_norm=0.2867, duration=0.38s
Step 6572: loss=3.1441, lr=0.000521, tokens/sec=1376230.24, grad_norm=0.3312, duration=0.38s
Step 6573: loss=3.0716, lr=0.000521, tokens/sec=1374742.67, grad_norm=0.2825, duration=0.38s
Step 6574: loss=3.1397, lr=0.000521, tokens/sec=1375558.76, grad_norm=0.3144, duration=0.38s
Step 6575: loss=3.0686, lr=0.000521, tokens/sec=1376748.08, grad_norm=0.3002, duration=0.38s
Step 6576: loss=3.1167, lr=0.000521, tokens/sec=1373326.92, grad_norm=0.3061, duration=0.38s
Step 6577: loss=3.1064, lr=0.000521, tokens/sec=1378354.03, grad_norm=0.2825, duration=0.38s
Step 6578: loss=3.1244, lr=0.000521, tokens/sec=1376605.87, grad_norm=0.3073, duration=0.38s
Step 6579: loss=3.1264, lr=0.000521, tokens/sec=1375335.08, grad_norm=0.2878, duration=0.38s
Step 6580: loss=3.1025, lr=0.000521, tokens/sec=1375766.16, grad_norm=0.2950, duration=0.38s
Step 6581: loss=3.0974, lr=0.000521, tokens/sec=1375799.73, grad_norm=0.3028, duration=0.38s
Step 6582: loss=3.1202, lr=0.000521, tokens/sec=1374775.33, grad_norm=0.2885, duration=0.38s
Step 6583: loss=3.0930, lr=0.000521, tokens/sec=1371212.61, grad_norm=0.3280, duration=0.38s
Step 6584: loss=3.1252, lr=0.000521, tokens/sec=1373220.58, grad_norm=0.2792, duration=0.38s
Step 6585: loss=3.1230, lr=0.000521, tokens/sec=1373587.70, grad_norm=0.3105, duration=0.38s
Step 6586: loss=3.1351, lr=0.000521, tokens/sec=1375120.93, grad_norm=0.2912, duration=0.38s
Step 6587: loss=3.1011, lr=0.000521, tokens/sec=1374429.91, grad_norm=0.2789, duration=0.38s
Step 6588: loss=3.0922, lr=0.000521, tokens/sec=1375022.04, grad_norm=0.2913, duration=0.38s
Step 6589: loss=3.0336, lr=0.000521, tokens/sec=1377597.62, grad_norm=0.2699, duration=0.38s
Step 6590: loss=3.0771, lr=0.000521, tokens/sec=1376475.76, grad_norm=0.2772, duration=0.38s
Step 6591: loss=2.9829, lr=0.000521, tokens/sec=1379546.45, grad_norm=0.2932, duration=0.38s
Step 6592: loss=3.0333, lr=0.000521, tokens/sec=1373462.45, grad_norm=0.2939, duration=0.38s
Step 6593: loss=3.0322, lr=0.000521, tokens/sec=1375036.66, grad_norm=0.2986, duration=0.38s
Step 6594: loss=3.0999, lr=0.000521, tokens/sec=1375597.48, grad_norm=0.2902, duration=0.38s
Step 6595: loss=3.0615, lr=0.000520, tokens/sec=1378310.83, grad_norm=0.3217, duration=0.38s
Step 6596: loss=3.0253, lr=0.000520, tokens/sec=1371606.03, grad_norm=0.2751, duration=0.38s
Step 6597: loss=3.0418, lr=0.000520, tokens/sec=1373531.08, grad_norm=0.3045, duration=0.38s
Step 6598: loss=3.0707, lr=0.000520, tokens/sec=1373976.48, grad_norm=0.2759, duration=0.38s
Step 6599: loss=3.0840, lr=0.000520, tokens/sec=1376658.44, grad_norm=0.2952, duration=0.38s
Step 6600/19073 (34.6%), Elapsed time: 2625.65s, Steps per hour: 9049.20, Estimated hours remaining: 1.38
Step 6600: loss=3.1003, lr=0.000520, tokens/sec=1377710.68, grad_norm=0.3093, duration=0.38s
Step 6601: loss=3.1247, lr=0.000520, tokens/sec=1377891.10, grad_norm=0.2889, duration=0.38s
Step 6602: loss=3.1284, lr=0.000520, tokens/sec=1374392.11, grad_norm=0.3317, duration=0.38s
Step 6603: loss=3.1228, lr=0.000520, tokens/sec=1373615.16, grad_norm=0.2928, duration=0.38s
Step 6604: loss=3.1328, lr=0.000520, tokens/sec=1376967.04, grad_norm=0.3284, duration=0.38s
Step 6605: loss=3.0630, lr=0.000520, tokens/sec=1377416.41, grad_norm=0.3210, duration=0.38s
Step 6606: loss=3.1045, lr=0.000520, tokens/sec=1374258.98, grad_norm=0.2974, duration=0.38s
Step 6607: loss=3.1491, lr=0.000520, tokens/sec=1375581.99, grad_norm=0.3451, duration=0.38s
Step 6608: loss=3.2172, lr=0.000520, tokens/sec=1378719.58, grad_norm=0.3072, duration=0.38s
Step 6609: loss=3.1543, lr=0.000520, tokens/sec=1375834.16, grad_norm=0.3238, duration=0.38s
Step 6610: loss=3.1148, lr=0.000520, tokens/sec=1376833.41, grad_norm=0.3113, duration=0.38s
Step 6611: loss=3.0799, lr=0.000520, tokens/sec=1375606.95, grad_norm=0.3007, duration=0.38s
Step 6612: loss=3.1534, lr=0.000520, tokens/sec=1375039.24, grad_norm=0.2962, duration=0.38s
Step 6613: loss=3.1256, lr=0.000520, tokens/sec=1376456.80, grad_norm=0.3116, duration=0.38s
Step 6614: loss=3.0778, lr=0.000520, tokens/sec=1374819.17, grad_norm=0.3026, duration=0.38s
Step 6615: loss=3.1493, lr=0.000520, tokens/sec=1379000.57, grad_norm=0.3074, duration=0.38s
Step 6616: loss=3.0826, lr=0.000520, tokens/sec=1376778.24, grad_norm=0.3075, duration=0.38s
Step 6617: loss=3.1577, lr=0.000520, tokens/sec=1371277.59, grad_norm=0.2921, duration=0.38s
Step 6618: loss=3.2038, lr=0.000520, tokens/sec=1377896.28, grad_norm=0.3039, duration=0.38s
Step 6619: loss=3.1034, lr=0.000520, tokens/sec=1372336.18, grad_norm=0.2900, duration=0.38s
Step 6620: loss=3.0802, lr=0.000520, tokens/sec=1373189.71, grad_norm=0.2953, duration=0.38s
Step 6621: loss=3.1357, lr=0.000520, tokens/sec=1375830.72, grad_norm=0.2827, duration=0.38s
Step 6622: loss=3.1763, lr=0.000520, tokens/sec=1372855.37, grad_norm=0.3016, duration=0.38s
Step 6623: loss=3.1511, lr=0.000520, tokens/sec=1371669.34, grad_norm=0.2867, duration=0.38s
Step 6624: loss=3.0540, lr=0.000520, tokens/sec=1375909.05, grad_norm=0.2929, duration=0.38s
Step 6625: loss=3.1147, lr=0.000520, tokens/sec=1372023.65, grad_norm=0.2840, duration=0.38s
Step 6626: loss=3.1403, lr=0.000520, tokens/sec=1373707.83, grad_norm=0.2937, duration=0.38s
Step 6627: loss=3.0993, lr=0.000520, tokens/sec=1375714.52, grad_norm=0.2751, duration=0.38s
Step 6628: loss=3.1774, lr=0.000520, tokens/sec=1375377.23, grad_norm=0.3149, duration=0.38s
Step 6629: loss=3.0873, lr=0.000520, tokens/sec=1376486.96, grad_norm=0.2650, duration=0.38s
Step 6630: loss=3.1260, lr=0.000520, tokens/sec=1371948.32, grad_norm=0.3269, duration=0.38s
Step 6631: loss=3.0830, lr=0.000520, tokens/sec=1369487.62, grad_norm=0.2894, duration=0.38s
Step 6632: loss=3.0539, lr=0.000520, tokens/sec=1373615.16, grad_norm=0.3087, duration=0.38s
Step 6633: loss=3.0644, lr=0.000520, tokens/sec=1374004.81, grad_norm=0.2975, duration=0.38s
Step 6634: loss=3.0639, lr=0.000519, tokens/sec=1374734.08, grad_norm=0.3079, duration=0.38s
Step 6635: loss=3.0539, lr=0.000519, tokens/sec=1377595.89, grad_norm=0.2838, duration=0.38s
Step 6636: loss=3.0394, lr=0.000519, tokens/sec=1373389.54, grad_norm=0.3458, duration=0.38s
Step 6637: loss=3.1150, lr=0.000519, tokens/sec=1376638.62, grad_norm=0.2780, duration=0.38s
Step 6638: loss=3.0446, lr=0.000519, tokens/sec=1373493.33, grad_norm=0.3013, duration=0.38s
Step 6639: loss=3.0293, lr=0.000519, tokens/sec=1379182.19, grad_norm=0.2903, duration=0.38s
Step 6640: loss=2.9865, lr=0.000519, tokens/sec=1378124.25, grad_norm=0.2705, duration=0.38s
Step 6641: loss=3.0301, lr=0.000519, tokens/sec=1373830.56, grad_norm=0.3013, duration=0.38s
Step 6642: loss=3.0913, lr=0.000519, tokens/sec=1380289.41, grad_norm=0.2747, duration=0.38s
Step 6643: loss=3.0417, lr=0.000519, tokens/sec=1378167.44, grad_norm=0.2791, duration=0.38s
Step 6644: loss=3.0564, lr=0.000519, tokens/sec=1378348.84, grad_norm=0.2835, duration=0.38s
Step 6645: loss=3.1140, lr=0.000519, tokens/sec=1376723.08, grad_norm=0.2955, duration=0.38s
Step 6646: loss=3.1262, lr=0.000519, tokens/sec=1377567.41, grad_norm=0.2667, duration=0.38s
Step 6647: loss=3.1550, lr=0.000519, tokens/sec=1374949.83, grad_norm=0.3173, duration=0.38s
Step 6648: loss=3.0876, lr=0.000519, tokens/sec=1378664.26, grad_norm=0.2637, duration=0.38s
Step 6649: loss=3.1289, lr=0.000519, tokens/sec=1377946.36, grad_norm=0.3064, duration=0.38s
Step 6650: loss=3.0730, lr=0.000519, tokens/sec=1380167.26, grad_norm=0.2755, duration=0.38s
Step 6651: loss=3.1332, lr=0.000519, tokens/sec=1376842.03, grad_norm=0.2987, duration=0.38s
Step 6652: loss=3.1098, lr=0.000519, tokens/sec=1375893.56, grad_norm=0.3005, duration=0.38s
Step 6653: loss=3.2106, lr=0.000519, tokens/sec=1375554.46, grad_norm=0.2849, duration=0.38s
Step 6654: loss=3.1389, lr=0.000519, tokens/sec=1373533.65, grad_norm=0.3145, duration=0.38s
Step 6655: loss=3.1362, lr=0.000519, tokens/sec=1373965.32, grad_norm=0.2927, duration=0.38s
Step 6656: loss=3.2275, lr=0.000519, tokens/sec=1375135.55, grad_norm=0.3427, duration=0.38s
Step 6657: loss=3.1443, lr=0.000519, tokens/sec=1374083.80, grad_norm=0.3068, duration=0.38s
Step 6658: loss=3.0991, lr=0.000519, tokens/sec=1376328.44, grad_norm=0.3257, duration=0.38s
Step 6659: loss=3.1226, lr=0.000519, tokens/sec=1377518.22, grad_norm=0.2963, duration=0.38s
Step 6660: loss=3.1417, lr=0.000519, tokens/sec=1375182.84, grad_norm=0.2962, duration=0.38s
Step 6661: loss=3.1412, lr=0.000519, tokens/sec=1374990.23, grad_norm=0.2946, duration=0.38s
Step 6662: loss=3.1460, lr=0.000519, tokens/sec=1375816.08, grad_norm=0.2831, duration=0.38s
Step 6663: loss=3.0879, lr=0.000519, tokens/sec=1378093.16, grad_norm=0.3001, duration=0.38s
Step 6664: loss=3.1302, lr=0.000519, tokens/sec=1378684.14, grad_norm=0.3068, duration=0.38s
Step 6665: loss=3.1298, lr=0.000519, tokens/sec=1376182.01, grad_norm=0.3120, duration=0.38s
Step 6666: loss=3.1276, lr=0.000519, tokens/sec=1375788.54, grad_norm=0.3476, duration=0.38s
Step 6667: loss=3.1828, lr=0.000519, tokens/sec=1374415.31, grad_norm=0.3306, duration=0.38s
Step 6668: loss=3.0856, lr=0.000519, tokens/sec=1372653.13, grad_norm=0.3215, duration=0.38s
Step 6669: loss=3.0521, lr=0.000519, tokens/sec=1376739.46, grad_norm=0.4045, duration=0.38s
Step 6670: loss=3.0906, lr=0.000519, tokens/sec=1372206.01, grad_norm=0.2965, duration=0.38s
Step 6671: loss=3.1303, lr=0.000519, tokens/sec=1375484.76, grad_norm=0.3329, duration=0.38s
Step 6672: loss=3.2139, lr=0.000518, tokens/sec=1372194.88, grad_norm=0.2859, duration=0.38s
Step 6673: loss=3.0404, lr=0.000518, tokens/sec=1370988.63, grad_norm=0.3264, duration=0.38s
Step 6674: loss=3.1529, lr=0.000518, tokens/sec=1375183.70, grad_norm=0.2832, duration=0.38s
Step 6675: loss=3.1342, lr=0.000518, tokens/sec=1378051.71, grad_norm=0.3303, duration=0.38s
Step 6676: loss=3.0784, lr=0.000518, tokens/sec=1376157.90, grad_norm=0.2783, duration=0.38s
Step 6677: loss=3.1383, lr=0.000518, tokens/sec=1377308.57, grad_norm=0.3300, duration=0.38s
Step 6678: loss=3.1314, lr=0.000518, tokens/sec=1377635.59, grad_norm=0.2997, duration=0.38s
Step 6679: loss=3.1186, lr=0.000518, tokens/sec=1375972.76, grad_norm=0.3162, duration=0.38s
Step 6680: loss=3.1172, lr=0.000518, tokens/sec=1377451.78, grad_norm=0.3259, duration=0.38s
Step 6681: loss=3.1038, lr=0.000518, tokens/sec=1373103.97, grad_norm=0.2999, duration=0.38s
Step 6682: loss=3.1527, lr=0.000518, tokens/sec=1372766.24, grad_norm=0.3575, duration=0.38s
Step 6683: loss=3.1668, lr=0.000518, tokens/sec=1376437.85, grad_norm=0.2798, duration=0.38s
Step 6684: loss=3.0833, lr=0.000518, tokens/sec=1375563.06, grad_norm=0.3198, duration=0.38s
Step 6685: loss=3.0394, lr=0.000518, tokens/sec=1378254.68, grad_norm=0.2639, duration=0.38s
Step 6686: loss=3.0847, lr=0.000518, tokens/sec=1377721.04, grad_norm=0.2929, duration=0.38s
Step 6687: loss=3.0409, lr=0.000518, tokens/sec=1371442.65, grad_norm=0.2631, duration=0.38s
Step 6688: loss=3.0429, lr=0.000518, tokens/sec=1375640.51, grad_norm=0.3057, duration=0.38s
Step 6689: loss=3.0469, lr=0.000518, tokens/sec=1372347.31, grad_norm=0.2789, duration=0.38s
Step 6690: loss=3.0438, lr=0.000518, tokens/sec=1376199.24, grad_norm=0.2767, duration=0.38s
Step 6691: loss=3.0595, lr=0.000518, tokens/sec=1376982.57, grad_norm=0.3151, duration=0.38s
Step 6692: loss=3.0883, lr=0.000518, tokens/sec=1374354.32, grad_norm=0.2860, duration=0.38s
Step 6693: loss=3.0679, lr=0.000518, tokens/sec=1380988.94, grad_norm=0.2920, duration=0.38s
Step 6694: loss=3.0442, lr=0.000518, tokens/sec=1379807.87, grad_norm=0.2944, duration=0.38s
Step 6695: loss=3.0378, lr=0.000518, tokens/sec=1375269.71, grad_norm=0.2861, duration=0.38s
Step 6696: loss=3.0610, lr=0.000518, tokens/sec=1376284.51, grad_norm=0.2990, duration=0.38s
Step 6697: loss=3.1405, lr=0.000518, tokens/sec=1375100.29, grad_norm=0.2868, duration=0.38s
Step 6698: loss=3.1522, lr=0.000518, tokens/sec=1375326.48, grad_norm=0.3189, duration=0.38s
Step 6699: loss=3.1216, lr=0.000518, tokens/sec=1378271.95, grad_norm=0.2919, duration=0.38s
Step 6700/19073 (35.1%), Elapsed time: 2663.85s, Steps per hour: 9054.58, Estimated hours remaining: 1.37
Step 6700: loss=3.1154, lr=0.000518, tokens/sec=1378780.95, grad_norm=0.3326, duration=0.38s
Step 6701: loss=3.1280, lr=0.000518, tokens/sec=1377866.93, grad_norm=0.2720, duration=0.38s
Step 6702: loss=3.1361, lr=0.000518, tokens/sec=1374985.93, grad_norm=0.3114, duration=0.38s
Step 6703: loss=3.1328, lr=0.000518, tokens/sec=1374750.41, grad_norm=0.2922, duration=0.38s
Step 6704: loss=3.0317, lr=0.000518, tokens/sec=1375889.25, grad_norm=0.2999, duration=0.38s
Step 6705: loss=3.0876, lr=0.000518, tokens/sec=1369332.42, grad_norm=0.2951, duration=0.38s
Step 6706: loss=3.1074, lr=0.000518, tokens/sec=1375219.82, grad_norm=0.2864, duration=0.38s
Step 6707: loss=3.1321, lr=0.000518, tokens/sec=1371112.58, grad_norm=0.2937, duration=0.38s
Step 6708: loss=3.1171, lr=0.000518, tokens/sec=1371757.48, grad_norm=0.2896, duration=0.38s
Step 6709: loss=3.1160, lr=0.000518, tokens/sec=1376319.82, grad_norm=0.2938, duration=0.38s
Step 6710: loss=3.1085, lr=0.000517, tokens/sec=1375872.04, grad_norm=0.2665, duration=0.38s
Step 6711: loss=3.0371, lr=0.000517, tokens/sec=1379255.72, grad_norm=0.2901, duration=0.38s
Step 6712: loss=3.0707, lr=0.000517, tokens/sec=1377469.04, grad_norm=0.2656, duration=0.38s
Step 6713: loss=3.0891, lr=0.000517, tokens/sec=1374475.44, grad_norm=0.2807, duration=0.38s
Step 6714: loss=3.1274, lr=0.000517, tokens/sec=1371958.60, grad_norm=0.2814, duration=0.38s
Step 6715: loss=3.1738, lr=0.000517, tokens/sec=1377308.57, grad_norm=0.2953, duration=0.38s
Step 6716: loss=3.0937, lr=0.000517, tokens/sec=1377763.33, grad_norm=0.2691, duration=0.38s
Step 6717: loss=3.2081, lr=0.000517, tokens/sec=1379152.78, grad_norm=0.3050, duration=0.38s
Step 6718: loss=3.1314, lr=0.000517, tokens/sec=1381256.10, grad_norm=0.2764, duration=0.38s
Step 6719: loss=3.1339, lr=0.000517, tokens/sec=1373515.64, grad_norm=0.2974, duration=0.38s
Step 6720: loss=3.1262, lr=0.000517, tokens/sec=1372751.67, grad_norm=0.2831, duration=0.38s
Step 6721: loss=3.1200, lr=0.000517, tokens/sec=1374270.15, grad_norm=0.2835, duration=0.38s
Step 6722: loss=3.1165, lr=0.000517, tokens/sec=1373721.56, grad_norm=0.2909, duration=0.38s
Step 6723: loss=3.0769, lr=0.000517, tokens/sec=1376524.87, grad_norm=0.2701, duration=0.38s
Step 6724: loss=3.1051, lr=0.000517, tokens/sec=1374396.41, grad_norm=0.2672, duration=0.38s
Step 6725: loss=3.0791, lr=0.000517, tokens/sec=1378446.48, grad_norm=0.2881, duration=0.38s
Step 6726: loss=3.1117, lr=0.000517, tokens/sec=1374581.98, grad_norm=0.2808, duration=0.38s
Step 6727: loss=3.0676, lr=0.000517, tokens/sec=1374038.30, grad_norm=0.2832, duration=0.38s
Step 6728: loss=3.0762, lr=0.000517, tokens/sec=1377523.40, grad_norm=0.2650, duration=0.38s
Step 6729: loss=3.0903, lr=0.000517, tokens/sec=1378938.31, grad_norm=0.2738, duration=0.38s
Step 6730: loss=3.0893, lr=0.000517, tokens/sec=1374691.11, grad_norm=0.2674, duration=0.38s
Step 6731: loss=3.0333, lr=0.000517, tokens/sec=1372989.09, grad_norm=0.2762, duration=0.38s
Step 6732: loss=3.1117, lr=0.000517, tokens/sec=1376158.76, grad_norm=0.2765, duration=0.38s
Step 6733: loss=3.0548, lr=0.000517, tokens/sec=1375411.64, grad_norm=0.2899, duration=0.38s
Step 6734: loss=3.0499, lr=0.000517, tokens/sec=1375256.81, grad_norm=0.2563, duration=0.38s
Step 6735: loss=3.0305, lr=0.000517, tokens/sec=1377955.86, grad_norm=0.3250, duration=0.38s
Step 6736: loss=3.0194, lr=0.000517, tokens/sec=1375528.64, grad_norm=0.2799, duration=0.38s
Step 6737: loss=3.0490, lr=0.000517, tokens/sec=1376589.50, grad_norm=0.3045, duration=0.38s
Step 6738: loss=3.0093, lr=0.000517, tokens/sec=1375927.13, grad_norm=0.3038, duration=0.38s
Step 6739: loss=3.0316, lr=0.000517, tokens/sec=1379337.91, grad_norm=0.2808, duration=0.38s
Step 6740: loss=2.9956, lr=0.000517, tokens/sec=1375539.83, grad_norm=0.3334, duration=0.38s
Step 6741: loss=3.0615, lr=0.000517, tokens/sec=1374185.98, grad_norm=0.2473, duration=0.38s
Step 6742: loss=3.0616, lr=0.000517, tokens/sec=1373514.78, grad_norm=0.3178, duration=0.38s
Step 6743: loss=3.0599, lr=0.000517, tokens/sec=1373437.57, grad_norm=0.2827, duration=0.38s
Step 6744: loss=3.0966, lr=0.000517, tokens/sec=1376594.67, grad_norm=0.2939, duration=0.38s
Step 6745: loss=3.1220, lr=0.000517, tokens/sec=1376687.74, grad_norm=0.3099, duration=0.38s
Step 6746: loss=3.1195, lr=0.000517, tokens/sec=1375687.84, grad_norm=0.2780, duration=0.38s
Step 6747: loss=3.1898, lr=0.000517, tokens/sec=1375725.71, grad_norm=0.3135, duration=0.38s
Step 6748: loss=3.1083, lr=0.000516, tokens/sec=1378572.64, grad_norm=0.2941, duration=0.38s
Step 6749: loss=3.1178, lr=0.000516, tokens/sec=1377599.34, grad_norm=0.3220, duration=0.38s
Validation loss at step 6750: 3.724693536758423
Step 6750: loss=3.0930, lr=0.000516, tokens/sec=152955.79, grad_norm=0.3043, duration=3.43s
Step 6751: loss=3.1191, lr=0.000516, tokens/sec=1375867.73, grad_norm=0.3215, duration=0.38s
Step 6752: loss=3.1476, lr=0.000516, tokens/sec=1377618.33, grad_norm=0.2704, duration=0.38s
Step 6753: loss=3.1502, lr=0.000516, tokens/sec=1380616.98, grad_norm=0.3187, duration=0.38s
Step 6754: loss=3.1506, lr=0.000516, tokens/sec=1372912.80, grad_norm=0.3042, duration=0.38s
Step 6755: loss=3.1467, lr=0.000516, tokens/sec=1375213.80, grad_norm=0.2951, duration=0.38s
Step 6756: loss=3.1421, lr=0.000516, tokens/sec=1376898.07, grad_norm=0.3116, duration=0.38s
Step 6757: loss=3.1019, lr=0.000516, tokens/sec=1375723.99, grad_norm=0.2844, duration=0.38s
Step 6758: loss=3.1546, lr=0.000516, tokens/sec=1377238.70, grad_norm=0.3025, duration=0.38s
Step 6759: loss=3.0682, lr=0.000516, tokens/sec=1374289.04, grad_norm=0.2976, duration=0.38s
Step 6760: loss=3.1118, lr=0.000516, tokens/sec=1377818.58, grad_norm=0.2898, duration=0.38s
Step 6761: loss=3.1249, lr=0.000516, tokens/sec=1377329.27, grad_norm=0.2847, duration=0.38s
Step 6762: loss=3.1187, lr=0.000516, tokens/sec=1371087.78, grad_norm=0.2992, duration=0.38s
Step 6763: loss=3.0567, lr=0.000516, tokens/sec=1371490.55, grad_norm=0.2883, duration=0.38s
Step 6764: loss=3.1139, lr=0.000516, tokens/sec=1373900.08, grad_norm=0.3076, duration=0.38s
Step 6765: loss=3.0574, lr=0.000516, tokens/sec=1377001.53, grad_norm=0.3091, duration=0.38s
Step 6766: loss=3.1280, lr=0.000516, tokens/sec=1374344.01, grad_norm=0.2928, duration=0.38s
Step 6767: loss=3.0940, lr=0.000516, tokens/sec=1375644.81, grad_norm=0.2725, duration=0.38s
Step 6768: loss=3.1380, lr=0.000516, tokens/sec=1375095.99, grad_norm=0.3078, duration=0.38s
Step 6769: loss=3.0884, lr=0.000516, tokens/sec=1374524.41, grad_norm=0.2886, duration=0.38s
Step 6770: loss=3.0904, lr=0.000516, tokens/sec=1376737.73, grad_norm=0.2917, duration=0.38s
Step 6771: loss=3.1003, lr=0.000516, tokens/sec=1373823.69, grad_norm=0.3026, duration=0.38s
Step 6772: loss=3.1051, lr=0.000516, tokens/sec=1373681.23, grad_norm=0.3007, duration=0.38s
Step 6773: loss=3.1039, lr=0.000516, tokens/sec=1373463.31, grad_norm=0.3180, duration=0.38s
Step 6774: loss=3.1121, lr=0.000516, tokens/sec=1377935.13, grad_norm=0.3021, duration=0.38s
Step 6775: loss=3.1372, lr=0.000516, tokens/sec=1373251.45, grad_norm=0.2819, duration=0.38s
Step 6776: loss=3.1065, lr=0.000516, tokens/sec=1377818.58, grad_norm=0.3166, duration=0.38s
Step 6777: loss=3.0860, lr=0.000516, tokens/sec=1375303.25, grad_norm=0.2814, duration=0.38s
Step 6778: loss=3.0855, lr=0.000516, tokens/sec=1378430.92, grad_norm=0.2997, duration=0.38s
Step 6779: loss=3.0317, lr=0.000516, tokens/sec=1374681.65, grad_norm=0.2859, duration=0.38s
Step 6780: loss=3.0521, lr=0.000516, tokens/sec=1372308.77, grad_norm=0.2971, duration=0.38s
Step 6781: loss=2.9797, lr=0.000516, tokens/sec=1378390.31, grad_norm=0.2728, duration=0.38s
Step 6782: loss=3.0299, lr=0.000516, tokens/sec=1375386.69, grad_norm=0.2957, duration=0.38s
Step 6783: loss=3.0235, lr=0.000516, tokens/sec=1375146.72, grad_norm=0.2915, duration=0.38s
Step 6784: loss=3.0785, lr=0.000516, tokens/sec=1376424.92, grad_norm=0.2856, duration=0.38s
Step 6785: loss=3.0443, lr=0.000516, tokens/sec=1377370.68, grad_norm=0.2922, duration=0.38s
Step 6786: loss=3.0479, lr=0.000515, tokens/sec=1377578.63, grad_norm=0.3075, duration=0.38s
Step 6787: loss=3.0323, lr=0.000515, tokens/sec=1377041.20, grad_norm=0.2856, duration=0.38s
Step 6788: loss=3.0368, lr=0.000515, tokens/sec=1375488.21, grad_norm=0.3001, duration=0.38s
Step 6789: loss=3.0667, lr=0.000515, tokens/sec=1377455.23, grad_norm=0.2810, duration=0.38s
Step 6790: loss=3.1042, lr=0.000515, tokens/sec=1376572.26, grad_norm=0.3233, duration=0.38s
Step 6791: loss=3.1083, lr=0.000515, tokens/sec=1374440.22, grad_norm=0.2833, duration=0.38s
Step 6792: loss=3.1366, lr=0.000515, tokens/sec=1380069.38, grad_norm=0.3298, duration=0.38s
Step 6793: loss=3.0924, lr=0.000515, tokens/sec=1374204.02, grad_norm=0.2808, duration=0.38s
Step 6794: loss=3.1042, lr=0.000515, tokens/sec=1373197.43, grad_norm=0.3031, duration=0.38s
Step 6795: loss=3.0702, lr=0.000515, tokens/sec=1372550.32, grad_norm=0.3198, duration=0.38s
Step 6796: loss=3.0923, lr=0.000515, tokens/sec=1373564.54, grad_norm=0.3114, duration=0.38s
Step 6797: loss=3.1453, lr=0.000515, tokens/sec=1377652.85, grad_norm=0.2930, duration=0.38s
Step 6798: loss=3.2113, lr=0.000515, tokens/sec=1376648.10, grad_norm=0.3338, duration=0.38s
Step 6799: loss=3.1169, lr=0.000515, tokens/sec=1377082.59, grad_norm=0.3058, duration=0.38s
Step 6800/19073 (35.7%), Elapsed time: 2705.09s, Steps per hour: 9049.61, Estimated hours remaining: 1.36
Step 6800: loss=3.1248, lr=0.000515, tokens/sec=1374376.65, grad_norm=0.3172, duration=0.38s
Step 6801: loss=3.1143, lr=0.000515, tokens/sec=1376354.28, grad_norm=0.3209, duration=0.38s
Step 6802: loss=3.1552, lr=0.000515, tokens/sec=1378081.07, grad_norm=0.3236, duration=0.38s
Step 6803: loss=3.1065, lr=0.000515, tokens/sec=1373085.10, grad_norm=0.3266, duration=0.38s
Step 6804: loss=3.0504, lr=0.000515, tokens/sec=1373252.31, grad_norm=0.3074, duration=0.38s
Step 6805: loss=3.1612, lr=0.000515, tokens/sec=1374746.97, grad_norm=0.3333, duration=0.38s
Step 6806: loss=3.0604, lr=0.000515, tokens/sec=1378166.57, grad_norm=0.3100, duration=0.38s
Step 6807: loss=3.1731, lr=0.000515, tokens/sec=1374701.42, grad_norm=0.3091, duration=0.38s
Step 6808: loss=3.1517, lr=0.000515, tokens/sec=1376272.45, grad_norm=0.3154, duration=0.38s
Step 6809: loss=3.1236, lr=0.000515, tokens/sec=1377837.57, grad_norm=0.2914, duration=0.38s
Step 6810: loss=3.0636, lr=0.000515, tokens/sec=1374953.27, grad_norm=0.2968, duration=0.38s
Step 6811: loss=3.1495, lr=0.000515, tokens/sec=1373815.97, grad_norm=0.2978, duration=0.38s
Step 6812: loss=3.1757, lr=0.000515, tokens/sec=1378341.07, grad_norm=0.2813, duration=0.38s
Step 6813: loss=3.1204, lr=0.000515, tokens/sec=1373468.45, grad_norm=0.2965, duration=0.38s
Step 6814: loss=3.0409, lr=0.000515, tokens/sec=1374745.25, grad_norm=0.2543, duration=0.38s
Step 6815: loss=3.1324, lr=0.000515, tokens/sec=1374866.44, grad_norm=0.2979, duration=0.38s
Step 6816: loss=3.0661, lr=0.000515, tokens/sec=1375892.70, grad_norm=0.2798, duration=0.38s
Step 6817: loss=3.1392, lr=0.000515, tokens/sec=1376838.59, grad_norm=0.2898, duration=0.38s
Step 6818: loss=3.1617, lr=0.000515, tokens/sec=1374198.01, grad_norm=0.2865, duration=0.38s
Step 6819: loss=3.1017, lr=0.000515, tokens/sec=1374459.98, grad_norm=0.2672, duration=0.38s
Step 6820: loss=3.0711, lr=0.000515, tokens/sec=1376896.35, grad_norm=0.2944, duration=0.38s
Step 6821: loss=3.0868, lr=0.000515, tokens/sec=1375368.63, grad_norm=0.2680, duration=0.38s
Step 6822: loss=3.0111, lr=0.000515, tokens/sec=1378914.10, grad_norm=0.3283, duration=0.38s
Step 6823: loss=3.0920, lr=0.000515, tokens/sec=1377594.16, grad_norm=0.2760, duration=0.38s
Step 6824: loss=3.0593, lr=0.000514, tokens/sec=1372979.66, grad_norm=0.3127, duration=0.38s
Step 6825: loss=3.0262, lr=0.000514, tokens/sec=1374762.44, grad_norm=0.2706, duration=0.38s
Step 6826: loss=3.0440, lr=0.000514, tokens/sec=1377395.70, grad_norm=0.3109, duration=0.38s
Step 6827: loss=3.0854, lr=0.000514, tokens/sec=1377956.72, grad_norm=0.2801, duration=0.38s
Step 6828: loss=3.0390, lr=0.000514, tokens/sec=1375241.32, grad_norm=0.2792, duration=0.38s
Step 6829: loss=3.0103, lr=0.000514, tokens/sec=1370119.04, grad_norm=0.2749, duration=0.38s
Step 6830: loss=2.9866, lr=0.000514, tokens/sec=1376445.60, grad_norm=0.2632, duration=0.38s
Step 6831: loss=3.0482, lr=0.000514, tokens/sec=1374777.91, grad_norm=0.2806, duration=0.38s
Step 6832: loss=3.0470, lr=0.000514, tokens/sec=1374419.60, grad_norm=0.2685, duration=0.38s
Step 6833: loss=3.0273, lr=0.000514, tokens/sec=1377766.79, grad_norm=0.2731, duration=0.38s
Step 6834: loss=3.0568, lr=0.000514, tokens/sec=1376587.77, grad_norm=0.2615, duration=0.38s
Step 6835: loss=3.1176, lr=0.000514, tokens/sec=1375575.11, grad_norm=0.2747, duration=0.38s
Step 6836: loss=3.1373, lr=0.000514, tokens/sec=1374248.67, grad_norm=0.2805, duration=0.38s
Step 6837: loss=3.1114, lr=0.000514, tokens/sec=1378869.14, grad_norm=0.2895, duration=0.38s
Step 6838: loss=3.0864, lr=0.000514, tokens/sec=1377137.79, grad_norm=0.2889, duration=0.38s
Step 6839: loss=3.0886, lr=0.000514, tokens/sec=1376409.42, grad_norm=0.2886, duration=0.38s
Step 6840: loss=3.0885, lr=0.000514, tokens/sec=1376580.88, grad_norm=0.2886, duration=0.38s
Step 6841: loss=3.0917, lr=0.000514, tokens/sec=1377055.86, grad_norm=0.2562, duration=0.38s
Step 6842: loss=3.1338, lr=0.000514, tokens/sec=1377071.38, grad_norm=0.2950, duration=0.38s
Step 6843: loss=3.1886, lr=0.000514, tokens/sec=1375946.93, grad_norm=0.2718, duration=0.38s
Step 6844: loss=3.1059, lr=0.000514, tokens/sec=1374261.56, grad_norm=0.2876, duration=0.38s
Step 6845: loss=3.1681, lr=0.000514, tokens/sec=1375760.14, grad_norm=0.3046, duration=0.38s
Step 6846: loss=3.1979, lr=0.000514, tokens/sec=1376263.83, grad_norm=0.2949, duration=0.38s
Step 6847: loss=3.0694, lr=0.000514, tokens/sec=1378500.05, grad_norm=0.2928, duration=0.38s
Step 6848: loss=3.1388, lr=0.000514, tokens/sec=1374003.95, grad_norm=0.3036, duration=0.38s
Step 6849: loss=3.1158, lr=0.000514, tokens/sec=1373610.87, grad_norm=0.2840, duration=0.38s
Step 6850: loss=3.1656, lr=0.000514, tokens/sec=1376218.18, grad_norm=0.2925, duration=0.38s
Step 6851: loss=3.1188, lr=0.000514, tokens/sec=1373043.09, grad_norm=0.2869, duration=0.38s
Step 6852: loss=3.1412, lr=0.000514, tokens/sec=1377442.29, grad_norm=0.2943, duration=0.38s
Step 6853: loss=3.0549, lr=0.000514, tokens/sec=1373670.08, grad_norm=0.2826, duration=0.38s
Step 6854: loss=3.1313, lr=0.000514, tokens/sec=1373185.42, grad_norm=0.3347, duration=0.38s
Step 6855: loss=3.1562, lr=0.000514, tokens/sec=1378532.89, grad_norm=0.2963, duration=0.38s
Step 6856: loss=3.1033, lr=0.000514, tokens/sec=1372828.80, grad_norm=0.3427, duration=0.38s
Step 6857: loss=3.1543, lr=0.000514, tokens/sec=1373285.76, grad_norm=0.2985, duration=0.38s
Step 6858: loss=3.0806, lr=0.000514, tokens/sec=1377124.85, grad_norm=0.3424, duration=0.38s
Step 6859: loss=3.0266, lr=0.000514, tokens/sec=1377407.78, grad_norm=0.3588, duration=0.38s
Step 6860: loss=3.0902, lr=0.000514, tokens/sec=1371905.53, grad_norm=0.3168, duration=0.38s
Step 6861: loss=3.1257, lr=0.000514, tokens/sec=1374114.71, grad_norm=0.2913, duration=0.38s
Step 6862: loss=3.1767, lr=0.000513, tokens/sec=1375660.30, grad_norm=0.2895, duration=0.38s
Step 6863: loss=3.0387, lr=0.000513, tokens/sec=1375988.26, grad_norm=0.3127, duration=0.38s
Step 6864: loss=3.1394, lr=0.000513, tokens/sec=1376962.73, grad_norm=0.2692, duration=0.38s
Step 6865: loss=3.1233, lr=0.000513, tokens/sec=1375674.93, grad_norm=0.3190, duration=0.38s
Step 6866: loss=3.0959, lr=0.000513, tokens/sec=1375384.97, grad_norm=0.2617, duration=0.38s
Step 6867: loss=3.0999, lr=0.000513, tokens/sec=1378888.16, grad_norm=0.2884, duration=0.38s
Step 6868: loss=3.1326, lr=0.000513, tokens/sec=1375315.29, grad_norm=0.3115, duration=0.38s
Step 6869: loss=3.1287, lr=0.000513, tokens/sec=1376159.62, grad_norm=0.2827, duration=0.38s
Step 6870: loss=3.0640, lr=0.000513, tokens/sec=1377917.87, grad_norm=0.3172, duration=0.38s
Step 6871: loss=3.1224, lr=0.000513, tokens/sec=1371817.38, grad_norm=0.2966, duration=0.38s
Step 6872: loss=3.1301, lr=0.000513, tokens/sec=1376852.38, grad_norm=0.3397, duration=0.38s
Step 6873: loss=3.1382, lr=0.000513, tokens/sec=1376623.11, grad_norm=0.2807, duration=0.38s
Step 6874: loss=3.0898, lr=0.000513, tokens/sec=1375774.77, grad_norm=0.3025, duration=0.38s
Step 6875: loss=3.0261, lr=0.000513, tokens/sec=1376045.95, grad_norm=0.2853, duration=0.38s
Step 6876: loss=3.0860, lr=0.000513, tokens/sec=1377312.02, grad_norm=0.2943, duration=0.38s
Step 6877: loss=3.0334, lr=0.000513, tokens/sec=1375606.95, grad_norm=0.2918, duration=0.38s
Step 6878: loss=3.0205, lr=0.000513, tokens/sec=1373921.54, grad_norm=0.2894, duration=0.38s
Step 6879: loss=3.0286, lr=0.000513, tokens/sec=1374075.22, grad_norm=0.2949, duration=0.38s
Step 6880: loss=3.0449, lr=0.000513, tokens/sec=1375507.99, grad_norm=0.2680, duration=0.38s
Step 6881: loss=3.0741, lr=0.000513, tokens/sec=1379247.07, grad_norm=0.3077, duration=0.38s
Step 6882: loss=3.0399, lr=0.000513, tokens/sec=1377691.69, grad_norm=0.2872, duration=0.38s
Step 6883: loss=3.0611, lr=0.000513, tokens/sec=1379150.19, grad_norm=0.2844, duration=0.38s
Step 6884: loss=3.0316, lr=0.000513, tokens/sec=1376429.23, grad_norm=0.3007, duration=0.38s
Step 6885: loss=3.0382, lr=0.000513, tokens/sec=1376944.63, grad_norm=0.2790, duration=0.38s
Step 6886: loss=3.0521, lr=0.000513, tokens/sec=1371467.45, grad_norm=0.3102, duration=0.38s
Step 6887: loss=3.0995, lr=0.000513, tokens/sec=1373062.81, grad_norm=0.3022, duration=0.38s
Step 6888: loss=3.1588, lr=0.000513, tokens/sec=1373201.72, grad_norm=0.3042, duration=0.38s
Step 6889: loss=3.1057, lr=0.000513, tokens/sec=1372442.38, grad_norm=0.2944, duration=0.38s
Step 6890: loss=3.1008, lr=0.000513, tokens/sec=1373114.25, grad_norm=0.3208, duration=0.38s
Step 6891: loss=3.1270, lr=0.000513, tokens/sec=1377952.40, grad_norm=0.2764, duration=0.38s
Step 6892: loss=3.1310, lr=0.000513, tokens/sec=1375560.48, grad_norm=0.3277, duration=0.38s
Step 6893: loss=3.0600, lr=0.000513, tokens/sec=1377025.68, grad_norm=0.2895, duration=0.38s
Step 6894: loss=3.0773, lr=0.000513, tokens/sec=1374102.69, grad_norm=0.3146, duration=0.38s
Step 6895: loss=3.0747, lr=0.000513, tokens/sec=1374392.11, grad_norm=0.2894, duration=0.38s
Step 6896: loss=3.1412, lr=0.000513, tokens/sec=1373174.28, grad_norm=0.3132, duration=0.38s
Step 6897: loss=3.1195, lr=0.000513, tokens/sec=1372570.88, grad_norm=0.2960, duration=0.38s
Step 6898: loss=3.1074, lr=0.000513, tokens/sec=1374138.75, grad_norm=0.2984, duration=0.38s
Step 6899: loss=3.0943, lr=0.000512, tokens/sec=1374948.11, grad_norm=0.2960, duration=0.38s
Step 6900/19073 (36.2%), Elapsed time: 2743.29s, Steps per hour: 9054.82, Estimated hours remaining: 1.34
Step 6900: loss=3.1023, lr=0.000512, tokens/sec=1373931.84, grad_norm=0.2778, duration=0.38s
Step 6901: loss=3.0115, lr=0.000512, tokens/sec=1373646.05, grad_norm=0.3159, duration=0.38s
Step 6902: loss=3.0366, lr=0.000512, tokens/sec=1375603.50, grad_norm=0.2803, duration=0.38s
Step 6903: loss=3.1269, lr=0.000512, tokens/sec=1374910.28, grad_norm=0.3073, duration=0.38s
Step 6904: loss=3.1193, lr=0.000512, tokens/sec=1375615.55, grad_norm=0.2863, duration=0.38s
Step 6905: loss=3.1551, lr=0.000512, tokens/sec=1377379.31, grad_norm=0.3097, duration=0.38s
Step 6906: loss=3.1144, lr=0.000512, tokens/sec=1374826.04, grad_norm=0.2684, duration=0.38s
Step 6907: loss=3.1641, lr=0.000512, tokens/sec=1377091.22, grad_norm=0.3453, duration=0.38s
Step 6908: loss=3.1400, lr=0.000512, tokens/sec=1374005.67, grad_norm=0.2818, duration=0.38s
Step 6909: loss=3.1160, lr=0.000512, tokens/sec=1371984.27, grad_norm=0.3105, duration=0.38s
Step 6910: loss=3.1083, lr=0.000512, tokens/sec=1374108.70, grad_norm=0.3022, duration=0.38s
Step 6911: loss=3.1286, lr=0.000512, tokens/sec=1375389.27, grad_norm=0.2786, duration=0.38s
Step 6912: loss=3.0535, lr=0.000512, tokens/sec=1374042.59, grad_norm=0.2898, duration=0.38s
Step 6913: loss=3.0992, lr=0.000512, tokens/sec=1370796.33, grad_norm=0.2895, duration=0.38s
Step 6914: loss=3.0682, lr=0.000512, tokens/sec=1374961.86, grad_norm=0.2670, duration=0.38s
Step 6915: loss=3.1011, lr=0.000512, tokens/sec=1376011.51, grad_norm=0.2899, duration=0.38s
Step 6916: loss=3.0957, lr=0.000512, tokens/sec=1372820.23, grad_norm=0.2769, duration=0.38s
Step 6917: loss=3.0311, lr=0.000512, tokens/sec=1376436.12, grad_norm=0.2874, duration=0.38s
Step 6918: loss=3.0915, lr=0.000512, tokens/sec=1375184.56, grad_norm=0.2780, duration=0.38s
Step 6919: loss=3.0703, lr=0.000512, tokens/sec=1373821.97, grad_norm=0.2639, duration=0.38s
Step 6920: loss=3.0965, lr=0.000512, tokens/sec=1374875.90, grad_norm=0.2828, duration=0.38s
Step 6921: loss=3.0346, lr=0.000512, tokens/sec=1372355.87, grad_norm=0.2763, duration=0.38s
Step 6922: loss=3.1019, lr=0.000512, tokens/sec=1374803.69, grad_norm=0.2647, duration=0.38s
Step 6923: loss=3.0360, lr=0.000512, tokens/sec=1375929.72, grad_norm=0.2853, duration=0.38s
Step 6924: loss=3.0679, lr=0.000512, tokens/sec=1380986.33, grad_norm=0.2695, duration=0.38s
Step 6925: loss=2.9750, lr=0.000512, tokens/sec=1374597.44, grad_norm=0.3111, duration=0.38s
Step 6926: loss=3.0163, lr=0.000512, tokens/sec=1376663.61, grad_norm=0.2918, duration=0.38s
Step 6927: loss=3.0463, lr=0.000512, tokens/sec=1375502.83, grad_norm=0.2911, duration=0.38s
Step 6928: loss=3.0003, lr=0.000512, tokens/sec=1372427.82, grad_norm=0.3119, duration=0.38s
Step 6929: loss=3.0018, lr=0.000512, tokens/sec=1374674.78, grad_norm=0.2664, duration=0.38s
Step 6930: loss=3.0032, lr=0.000512, tokens/sec=1376507.64, grad_norm=0.3285, duration=0.38s
Step 6931: loss=3.0687, lr=0.000512, tokens/sec=1376283.65, grad_norm=0.2719, duration=0.38s
Step 6932: loss=3.0199, lr=0.000512, tokens/sec=1377244.74, grad_norm=0.2981, duration=0.38s
Step 6933: loss=3.0454, lr=0.000512, tokens/sec=1376357.73, grad_norm=0.2997, duration=0.38s
Step 6934: loss=3.1049, lr=0.000512, tokens/sec=1380277.28, grad_norm=0.2972, duration=0.38s
Step 6935: loss=3.1061, lr=0.000512, tokens/sec=1378679.81, grad_norm=0.3084, duration=0.38s
Step 6936: loss=3.1817, lr=0.000511, tokens/sec=1373652.91, grad_norm=0.2860, duration=0.38s
Step 6937: loss=3.1636, lr=0.000511, tokens/sec=1376521.42, grad_norm=0.3172, duration=0.38s
Step 6938: loss=3.0616, lr=0.000511, tokens/sec=1373761.04, grad_norm=0.3228, duration=0.38s
Step 6939: loss=3.1132, lr=0.000511, tokens/sec=1375991.71, grad_norm=0.3232, duration=0.38s
Step 6940: loss=3.0508, lr=0.000511, tokens/sec=1372159.78, grad_norm=0.3309, duration=0.38s
Step 6941: loss=3.1537, lr=0.000511, tokens/sec=1376967.04, grad_norm=0.3257, duration=0.38s
Step 6942: loss=3.1537, lr=0.000511, tokens/sec=1382422.27, grad_norm=0.3122, duration=0.38s
Step 6943: loss=3.1332, lr=0.000511, tokens/sec=1374899.97, grad_norm=0.3061, duration=0.38s
Step 6944: loss=3.1738, lr=0.000511, tokens/sec=1375582.85, grad_norm=0.3352, duration=0.38s
Step 6945: loss=3.1215, lr=0.000511, tokens/sec=1376818.76, grad_norm=0.3090, duration=0.38s
Step 6946: loss=3.1015, lr=0.000511, tokens/sec=1375590.60, grad_norm=0.3108, duration=0.38s
Step 6947: loss=3.1298, lr=0.000511, tokens/sec=1373891.50, grad_norm=0.3140, duration=0.38s
Step 6948: loss=3.1112, lr=0.000511, tokens/sec=1379184.79, grad_norm=0.3029, duration=0.38s
Step 6949: loss=3.0539, lr=0.000511, tokens/sec=1376998.95, grad_norm=0.3148, duration=0.38s
Step 6950: loss=3.1359, lr=0.000511, tokens/sec=1375940.05, grad_norm=0.3154, duration=0.38s
Step 6951: loss=3.1024, lr=0.000511, tokens/sec=1372542.61, grad_norm=0.2909, duration=0.38s
Step 6952: loss=3.1059, lr=0.000511, tokens/sec=1377431.07, grad_norm=0.3047, duration=0.38s
Step 6953: loss=3.0343, lr=0.000511, tokens/sec=1374199.72, grad_norm=0.2934, duration=0.38s
Step 6954: loss=3.1022, lr=0.000511, tokens/sec=1374504.65, grad_norm=0.3157, duration=0.38s
Step 6955: loss=3.0674, lr=0.000511, tokens/sec=1373007.95, grad_norm=0.2800, duration=0.38s
Step 6956: loss=3.1199, lr=0.000511, tokens/sec=1374662.75, grad_norm=0.3141, duration=0.38s
Step 6957: loss=3.1080, lr=0.000511, tokens/sec=1375865.15, grad_norm=0.2862, duration=0.38s
Step 6958: loss=3.1010, lr=0.000511, tokens/sec=1376618.80, grad_norm=0.2790, duration=0.38s
Step 6959: loss=3.0757, lr=0.000511, tokens/sec=1374925.76, grad_norm=0.2923, duration=0.38s
Step 6960: loss=3.0955, lr=0.000511, tokens/sec=1378555.36, grad_norm=0.2876, duration=0.38s
Step 6961: loss=3.0819, lr=0.000511, tokens/sec=1374126.73, grad_norm=0.2903, duration=0.38s
Step 6962: loss=3.1121, lr=0.000511, tokens/sec=1375251.65, grad_norm=0.3121, duration=0.38s
Step 6963: loss=3.0893, lr=0.000511, tokens/sec=1376066.62, grad_norm=0.3120, duration=0.38s
Step 6964: loss=3.1269, lr=0.000511, tokens/sec=1374027.14, grad_norm=0.3075, duration=0.38s
Step 6965: loss=3.1074, lr=0.000511, tokens/sec=1373025.09, grad_norm=0.2809, duration=0.38s
Step 6966: loss=3.0880, lr=0.000511, tokens/sec=1376503.33, grad_norm=0.2955, duration=0.38s
Step 6967: loss=3.0784, lr=0.000511, tokens/sec=1376155.31, grad_norm=0.2980, duration=0.38s
Step 6968: loss=3.0827, lr=0.000511, tokens/sec=1374459.98, grad_norm=0.3001, duration=0.38s
Step 6969: loss=3.0060, lr=0.000511, tokens/sec=1374890.51, grad_norm=0.2747, duration=0.38s
Step 6970: loss=3.0488, lr=0.000511, tokens/sec=1375337.66, grad_norm=0.3189, duration=0.38s
Step 6971: loss=2.9781, lr=0.000511, tokens/sec=1374450.53, grad_norm=0.2986, duration=0.38s
Step 6972: loss=3.0229, lr=0.000511, tokens/sec=1370287.24, grad_norm=0.3160, duration=0.38s
Step 6973: loss=3.0041, lr=0.000510, tokens/sec=1375504.55, grad_norm=0.3159, duration=0.38s
Step 6974: loss=3.0617, lr=0.000510, tokens/sec=1374178.25, grad_norm=0.3026, duration=0.38s
Step 6975: loss=3.0654, lr=0.000510, tokens/sec=1379346.56, grad_norm=0.2886, duration=0.38s
Step 6976: loss=3.0419, lr=0.000510, tokens/sec=1375403.90, grad_norm=0.3266, duration=0.38s
Step 6977: loss=2.9966, lr=0.000510, tokens/sec=1375133.83, grad_norm=0.2770, duration=0.38s
Step 6978: loss=3.0191, lr=0.000510, tokens/sec=1372603.43, grad_norm=0.3086, duration=0.38s
Step 6979: loss=3.0689, lr=0.000510, tokens/sec=1373755.89, grad_norm=0.2901, duration=0.38s
Step 6980: loss=3.0837, lr=0.000510, tokens/sec=1374554.48, grad_norm=0.3108, duration=0.38s
Step 6981: loss=3.1208, lr=0.000510, tokens/sec=1371628.28, grad_norm=0.3058, duration=0.38s
Step 6982: loss=3.1083, lr=0.000510, tokens/sec=1376119.14, grad_norm=0.3061, duration=0.38s
Step 6983: loss=3.0683, lr=0.000510, tokens/sec=1371132.24, grad_norm=0.3076, duration=0.38s
Step 6984: loss=3.1124, lr=0.000510, tokens/sec=1372036.49, grad_norm=0.2965, duration=0.38s
Step 6985: loss=3.0581, lr=0.000510, tokens/sec=1372188.89, grad_norm=0.3072, duration=0.38s
Step 6986: loss=3.0900, lr=0.000510, tokens/sec=1373278.90, grad_norm=0.3066, duration=0.38s
Step 6987: loss=3.1354, lr=0.000510, tokens/sec=1375872.04, grad_norm=0.2982, duration=0.38s
Step 6988: loss=3.1725, lr=0.000510, tokens/sec=1372948.80, grad_norm=0.3102, duration=0.38s
Step 6989: loss=3.1275, lr=0.000510, tokens/sec=1375263.69, grad_norm=0.3233, duration=0.38s
Step 6990: loss=3.1553, lr=0.000510, tokens/sec=1372024.51, grad_norm=0.2817, duration=0.38s
Step 6991: loss=3.1095, lr=0.000510, tokens/sec=1377893.69, grad_norm=0.3169, duration=0.38s
Step 6992: loss=3.1292, lr=0.000510, tokens/sec=1372170.91, grad_norm=0.2794, duration=0.38s
Step 6993: loss=3.0768, lr=0.000510, tokens/sec=1372075.02, grad_norm=0.3162, duration=0.38s
Step 6994: loss=3.0588, lr=0.000510, tokens/sec=1376029.59, grad_norm=0.2981, duration=0.38s
Step 6995: loss=3.1348, lr=0.000510, tokens/sec=1372935.08, grad_norm=0.3327, duration=0.38s
Step 6996: loss=3.0741, lr=0.000510, tokens/sec=1376121.73, grad_norm=0.3237, duration=0.38s
Step 6997: loss=3.1196, lr=0.000510, tokens/sec=1374242.66, grad_norm=0.3335, duration=0.38s
Step 6998: loss=3.1761, lr=0.000510, tokens/sec=1377455.23, grad_norm=0.3334, duration=0.38s
Step 6999: loss=3.1087, lr=0.000510, tokens/sec=1374142.19, grad_norm=0.3157, duration=0.38s
Step 7000/19073 (36.7%), Elapsed time: 2781.51s, Steps per hour: 9059.82, Estimated hours remaining: 1.33
Validation loss at step 7000: 3.728217363357544
Step 7000: loss=3.0834, lr=0.000510, tokens/sec=152666.46, grad_norm=0.3030, duration=3.43s
Step 7001: loss=3.1477, lr=0.000510, tokens/sec=1376027.87, grad_norm=0.3201, duration=0.38s
Step 7002: loss=3.1468, lr=0.000510, tokens/sec=1374514.10, grad_norm=0.2939, duration=0.38s
Step 7003: loss=3.1063, lr=0.000510, tokens/sec=1375300.67, grad_norm=0.2972, duration=0.38s
Step 7004: loss=3.0617, lr=0.000510, tokens/sec=1373302.05, grad_norm=0.2777, duration=0.38s
Step 7005: loss=3.0630, lr=0.000510, tokens/sec=1375022.04, grad_norm=0.3171, duration=0.38s
Step 7006: loss=3.1058, lr=0.000510, tokens/sec=1370888.63, grad_norm=0.2824, duration=0.38s
Step 7007: loss=3.1240, lr=0.000510, tokens/sec=1371672.77, grad_norm=0.3018, duration=0.38s
Step 7008: loss=3.1735, lr=0.000510, tokens/sec=1374194.57, grad_norm=0.2838, duration=0.38s
Step 7009: loss=3.0522, lr=0.000510, tokens/sec=1377505.28, grad_norm=0.2960, duration=0.38s
Step 7010: loss=3.0746, lr=0.000509, tokens/sec=1368604.61, grad_norm=0.2650, duration=0.38s
Step 7011: loss=3.0441, lr=0.000509, tokens/sec=1373805.67, grad_norm=0.3265, duration=0.38s
Step 7012: loss=3.0375, lr=0.000509, tokens/sec=1374730.64, grad_norm=0.2734, duration=0.38s
Step 7013: loss=3.0859, lr=0.000509, tokens/sec=1373102.25, grad_norm=0.3084, duration=0.38s
Step 7014: loss=3.0283, lr=0.000509, tokens/sec=1372057.89, grad_norm=0.2640, duration=0.38s
Step 7015: loss=3.0355, lr=0.000509, tokens/sec=1373307.20, grad_norm=0.3197, duration=0.38s
Step 7016: loss=3.0143, lr=0.000509, tokens/sec=1374310.51, grad_norm=0.2631, duration=0.38s
Step 7017: loss=3.0834, lr=0.000509, tokens/sec=1371173.28, grad_norm=0.3410, duration=0.38s
Step 7018: loss=3.0182, lr=0.000509, tokens/sec=1378368.71, grad_norm=0.2667, duration=0.38s
Step 7019: loss=3.0093, lr=0.000509, tokens/sec=1378866.54, grad_norm=0.3151, duration=0.38s
Step 7020: loss=3.0085, lr=0.000509, tokens/sec=1376911.00, grad_norm=0.2996, duration=0.38s
Step 7021: loss=3.0031, lr=0.000509, tokens/sec=1373964.46, grad_norm=0.2943, duration=0.38s
Step 7022: loss=3.0317, lr=0.000509, tokens/sec=1376747.21, grad_norm=0.3169, duration=0.38s
Step 7023: loss=3.0278, lr=0.000509, tokens/sec=1373556.82, grad_norm=0.2702, duration=0.38s
Step 7024: loss=3.0629, lr=0.000509, tokens/sec=1375804.89, grad_norm=0.3050, duration=0.38s
Step 7025: loss=3.1282, lr=0.000509, tokens/sec=1373825.41, grad_norm=0.2800, duration=0.38s
Step 7026: loss=3.0929, lr=0.000509, tokens/sec=1375161.34, grad_norm=0.3189, duration=0.38s
Step 7027: loss=3.1081, lr=0.000509, tokens/sec=1373846.01, grad_norm=0.2767, duration=0.38s
Step 7028: loss=3.0482, lr=0.000509, tokens/sec=1378261.59, grad_norm=0.3094, duration=0.38s
Step 7029: loss=3.1008, lr=0.000509, tokens/sec=1375017.75, grad_norm=0.2791, duration=0.38s
Step 7030: loss=3.0488, lr=0.000509, tokens/sec=1375210.36, grad_norm=0.3023, duration=0.38s
Step 7031: loss=3.1187, lr=0.000509, tokens/sec=1375996.87, grad_norm=0.2757, duration=0.38s
Step 7032: loss=3.1152, lr=0.000509, tokens/sec=1376673.09, grad_norm=0.2934, duration=0.38s
Step 7033: loss=3.1615, lr=0.000509, tokens/sec=1375068.47, grad_norm=0.2862, duration=0.38s
Step 7034: loss=3.1360, lr=0.000509, tokens/sec=1371868.73, grad_norm=0.3062, duration=0.38s
Step 7035: loss=3.1403, lr=0.000509, tokens/sec=1372506.63, grad_norm=0.2887, duration=0.38s
Step 7036: loss=3.1281, lr=0.000509, tokens/sec=1375717.10, grad_norm=0.3367, duration=0.38s
Step 7037: loss=3.1101, lr=0.000509, tokens/sec=1376037.34, grad_norm=0.2969, duration=0.38s
Step 7038: loss=3.1345, lr=0.000509, tokens/sec=1376504.19, grad_norm=0.3052, duration=0.38s
Step 7039: loss=3.1430, lr=0.000509, tokens/sec=1377104.15, grad_norm=0.3124, duration=0.38s
Step 7040: loss=3.1472, lr=0.000509, tokens/sec=1377355.15, grad_norm=0.3181, duration=0.38s
Step 7041: loss=3.1136, lr=0.000509, tokens/sec=1378313.42, grad_norm=0.3078, duration=0.38s
Step 7042: loss=3.1109, lr=0.000509, tokens/sec=1375089.97, grad_norm=0.3181, duration=0.38s
Step 7043: loss=3.0606, lr=0.000509, tokens/sec=1374043.45, grad_norm=0.2874, duration=0.38s
Step 7044: loss=3.1583, lr=0.000509, tokens/sec=1380278.15, grad_norm=0.3094, duration=0.38s
Step 7045: loss=3.1343, lr=0.000509, tokens/sec=1375767.88, grad_norm=0.3144, duration=0.38s
Step 7046: loss=3.0777, lr=0.000509, tokens/sec=1375155.32, grad_norm=0.3019, duration=0.38s
Step 7047: loss=3.1529, lr=0.000508, tokens/sec=1377896.28, grad_norm=0.3258, duration=0.38s
Step 7048: loss=3.0602, lr=0.000508, tokens/sec=1374979.06, grad_norm=0.3005, duration=0.38s
Step 7049: loss=3.0228, lr=0.000508, tokens/sec=1371063.85, grad_norm=0.3258, duration=0.38s
Step 7050: loss=3.0861, lr=0.000508, tokens/sec=1373077.39, grad_norm=0.3101, duration=0.38s
Step 7051: loss=3.0908, lr=0.000508, tokens/sec=1374472.00, grad_norm=0.2970, duration=0.38s
Step 7052: loss=3.1782, lr=0.000508, tokens/sec=1374991.95, grad_norm=0.2703, duration=0.38s
Step 7053: loss=3.0293, lr=0.000508, tokens/sec=1374093.25, grad_norm=0.3310, duration=0.38s
Step 7054: loss=3.1285, lr=0.000508, tokens/sec=1374189.42, grad_norm=0.2871, duration=0.38s
Step 7055: loss=3.1404, lr=0.000508, tokens/sec=1373810.82, grad_norm=0.3286, duration=0.38s
Step 7056: loss=3.0606, lr=0.000508, tokens/sec=1368844.86, grad_norm=0.2952, duration=0.38s
Step 7057: loss=3.1007, lr=0.000508, tokens/sec=1380023.48, grad_norm=0.2925, duration=0.38s
Step 7058: loss=3.1453, lr=0.000508, tokens/sec=1375862.57, grad_norm=0.3252, duration=0.38s
Step 7059: loss=3.0771, lr=0.000508, tokens/sec=1377903.19, grad_norm=0.2809, duration=0.38s
Step 7060: loss=3.0837, lr=0.000508, tokens/sec=1376717.91, grad_norm=0.3351, duration=0.38s
Step 7061: loss=3.0998, lr=0.000508, tokens/sec=1376691.19, grad_norm=0.2845, duration=0.38s
Step 7062: loss=3.1095, lr=0.000508, tokens/sec=1375124.37, grad_norm=0.4320, duration=0.38s
Step 7063: loss=3.1456, lr=0.000508, tokens/sec=1376808.42, grad_norm=0.3035, duration=0.38s
Step 7064: loss=3.0735, lr=0.000508, tokens/sec=1375788.54, grad_norm=0.3009, duration=0.38s
Step 7065: loss=3.0286, lr=0.000508, tokens/sec=1373416.98, grad_norm=0.3158, duration=0.38s
Step 7066: loss=3.0757, lr=0.000508, tokens/sec=1379170.08, grad_norm=0.2986, duration=0.38s
Step 7067: loss=3.0166, lr=0.000508, tokens/sec=1372890.51, grad_norm=0.3391, duration=0.38s
Step 7068: loss=3.0017, lr=0.000508, tokens/sec=1376106.23, grad_norm=0.2883, duration=0.38s
Step 7069: loss=3.0309, lr=0.000508, tokens/sec=1373943.00, grad_norm=0.3296, duration=0.38s
Step 7070: loss=3.0613, lr=0.000508, tokens/sec=1370726.27, grad_norm=0.2895, duration=0.38s
Step 7071: loss=3.0305, lr=0.000508, tokens/sec=1372976.23, grad_norm=0.2931, duration=0.38s
Step 7072: loss=3.0367, lr=0.000508, tokens/sec=1374124.16, grad_norm=0.3041, duration=0.38s
Step 7073: loss=3.0474, lr=0.000508, tokens/sec=1372603.43, grad_norm=0.2669, duration=0.38s
Step 7074: loss=3.0356, lr=0.000508, tokens/sec=1374258.98, grad_norm=0.3050, duration=0.38s
Step 7075: loss=3.0308, lr=0.000508, tokens/sec=1374977.34, grad_norm=0.2859, duration=0.38s
Step 7076: loss=3.0139, lr=0.000508, tokens/sec=1373468.45, grad_norm=0.3006, duration=0.38s
Step 7077: loss=3.1087, lr=0.000508, tokens/sec=1378638.33, grad_norm=0.2814, duration=0.38s
Step 7078: loss=3.1441, lr=0.000508, tokens/sec=1375741.20, grad_norm=0.3181, duration=0.38s
Step 7079: loss=3.0906, lr=0.000508, tokens/sec=1377016.19, grad_norm=0.2850, duration=0.38s
Step 7080: loss=3.1013, lr=0.000508, tokens/sec=1377530.30, grad_norm=0.3182, duration=0.38s
Step 7081: loss=3.1245, lr=0.000508, tokens/sec=1375154.46, grad_norm=0.2729, duration=0.38s
Step 7082: loss=3.0583, lr=0.000508, tokens/sec=1375977.07, grad_norm=0.3279, duration=0.38s
Step 7083: loss=3.1081, lr=0.000508, tokens/sec=1377614.01, grad_norm=0.2875, duration=0.38s
Step 7084: loss=3.0701, lr=0.000507, tokens/sec=1376813.59, grad_norm=0.3409, duration=0.38s
Step 7085: loss=3.1039, lr=0.000507, tokens/sec=1373375.81, grad_norm=0.2862, duration=0.38s
Step 7086: loss=3.1254, lr=0.000507, tokens/sec=1379313.68, grad_norm=0.3221, duration=0.38s
Step 7087: loss=3.1115, lr=0.000507, tokens/sec=1378872.59, grad_norm=0.3136, duration=0.38s
Step 7088: loss=3.0874, lr=0.000507, tokens/sec=1372743.10, grad_norm=0.2960, duration=0.38s
Step 7089: loss=3.0909, lr=0.000507, tokens/sec=1375909.92, grad_norm=0.3269, duration=0.38s
Step 7090: loss=3.0774, lr=0.000507, tokens/sec=1373553.38, grad_norm=0.3144, duration=0.38s
Step 7091: loss=2.9789, lr=0.000507, tokens/sec=1374855.27, grad_norm=0.3114, duration=0.38s
Step 7092: loss=3.0762, lr=0.000507, tokens/sec=1376828.24, grad_norm=0.3273, duration=0.38s
Step 7093: loss=3.1207, lr=0.000507, tokens/sec=1375625.02, grad_norm=0.3079, duration=0.38s
Step 7094: loss=3.1011, lr=0.000507, tokens/sec=1376654.13, grad_norm=0.3076, duration=0.38s
Step 7095: loss=3.1787, lr=0.000507, tokens/sec=1378122.52, grad_norm=0.3471, duration=0.38s
Step 7096: loss=3.0728, lr=0.000507, tokens/sec=1379994.90, grad_norm=0.2969, duration=0.38s
Step 7097: loss=3.1752, lr=0.000507, tokens/sec=1373474.46, grad_norm=0.3284, duration=0.38s
Step 7098: loss=3.1269, lr=0.000507, tokens/sec=1376449.91, grad_norm=0.3144, duration=0.38s
Step 7099: loss=3.1020, lr=0.000507, tokens/sec=1373821.97, grad_norm=0.3551, duration=0.38s
Step 7100/19073 (37.2%), Elapsed time: 2822.78s, Steps per hour: 9054.89, Estimated hours remaining: 1.32
Step 7100: loss=3.1190, lr=0.000507, tokens/sec=1376778.24, grad_norm=0.3134, duration=0.38s
Step 7101: loss=3.0690, lr=0.000507, tokens/sec=1378938.31, grad_norm=0.3517, duration=0.38s
Step 7102: loss=3.0743, lr=0.000507, tokens/sec=1375861.71, grad_norm=0.2841, duration=0.38s
Step 7103: loss=3.0678, lr=0.000507, tokens/sec=1377048.96, grad_norm=0.3427, duration=0.38s
Step 7104: loss=3.0935, lr=0.000507, tokens/sec=1380905.68, grad_norm=0.2923, duration=0.38s
Step 7105: loss=3.0894, lr=0.000507, tokens/sec=1372369.58, grad_norm=0.2970, duration=0.38s
Step 7106: loss=3.0598, lr=0.000507, tokens/sec=1374498.64, grad_norm=0.3096, duration=0.38s
Step 7107: loss=3.0451, lr=0.000507, tokens/sec=1377798.73, grad_norm=0.2916, duration=0.38s
Step 7108: loss=3.0721, lr=0.000507, tokens/sec=1372194.02, grad_norm=0.2999, duration=0.38s
Step 7109: loss=3.0800, lr=0.000507, tokens/sec=1373272.89, grad_norm=0.2802, duration=0.38s
Step 7110: loss=3.1015, lr=0.000507, tokens/sec=1372534.90, grad_norm=0.2863, duration=0.38s
Step 7111: loss=3.0276, lr=0.000507, tokens/sec=1375114.05, grad_norm=0.2882, duration=0.38s
Step 7112: loss=3.0794, lr=0.000507, tokens/sec=1377541.52, grad_norm=0.2718, duration=0.38s
Step 7113: loss=3.0538, lr=0.000507, tokens/sec=1376733.42, grad_norm=0.2669, duration=0.38s
Step 7114: loss=3.0166, lr=0.000507, tokens/sec=1379859.82, grad_norm=0.2758, duration=0.38s
Step 7115: loss=2.9716, lr=0.000507, tokens/sec=1374093.25, grad_norm=0.2565, duration=0.38s
Step 7116: loss=3.0186, lr=0.000507, tokens/sec=1376793.76, grad_norm=0.3207, duration=0.38s
Step 7117: loss=3.0351, lr=0.000507, tokens/sec=1377834.98, grad_norm=0.2707, duration=0.38s
Step 7118: loss=2.9722, lr=0.000507, tokens/sec=1374526.99, grad_norm=0.3227, duration=0.38s
Step 7119: loss=3.0120, lr=0.000507, tokens/sec=1376749.80, grad_norm=0.2995, duration=0.38s
Step 7120: loss=3.0057, lr=0.000507, tokens/sec=1375710.22, grad_norm=0.2980, duration=0.38s
Step 7121: loss=3.0315, lr=0.000506, tokens/sec=1376071.78, grad_norm=0.3104, duration=0.38s
Step 7122: loss=3.0054, lr=0.000506, tokens/sec=1379783.63, grad_norm=0.2834, duration=0.38s
Step 7123: loss=3.0528, lr=0.000506, tokens/sec=1375351.42, grad_norm=0.3207, duration=0.38s
Step 7124: loss=3.0890, lr=0.000506, tokens/sec=1372989.09, grad_norm=0.2985, duration=0.38s
Step 7125: loss=3.1687, lr=0.000506, tokens/sec=1376979.12, grad_norm=0.3241, duration=0.38s
Step 7126: loss=3.1550, lr=0.000506, tokens/sec=1378930.53, grad_norm=0.3368, duration=0.38s
Step 7127: loss=3.1153, lr=0.000506, tokens/sec=1377440.57, grad_norm=0.3155, duration=0.38s
Step 7128: loss=3.0618, lr=0.000506, tokens/sec=1374128.45, grad_norm=0.3525, duration=0.38s
Step 7129: loss=3.0745, lr=0.000506, tokens/sec=1374862.14, grad_norm=0.3773, duration=0.38s
Step 7130: loss=3.0868, lr=0.000506, tokens/sec=1379502.32, grad_norm=0.3451, duration=0.38s
Step 7131: loss=3.1589, lr=0.000506, tokens/sec=1378366.98, grad_norm=0.3471, duration=0.38s
Step 7132: loss=3.1413, lr=0.000506, tokens/sec=1375111.47, grad_norm=0.3363, duration=0.38s
Step 7133: loss=3.1554, lr=0.000506, tokens/sec=1376019.26, grad_norm=0.3001, duration=0.38s
Step 7134: loss=3.1510, lr=0.000506, tokens/sec=1376526.59, grad_norm=0.3232, duration=0.38s
Step 7135: loss=3.0814, lr=0.000506, tokens/sec=1377020.50, grad_norm=0.3080, duration=0.38s
Step 7136: loss=3.1286, lr=0.000506, tokens/sec=1374366.34, grad_norm=0.3193, duration=0.38s
Step 7137: loss=3.0885, lr=0.000506, tokens/sec=1373817.68, grad_norm=0.3072, duration=0.38s
Step 7138: loss=3.0992, lr=0.000506, tokens/sec=1377125.71, grad_norm=0.3400, duration=0.38s
Step 7139: loss=3.0749, lr=0.000506, tokens/sec=1375576.83, grad_norm=0.3001, duration=0.38s
Step 7140: loss=3.1168, lr=0.000506, tokens/sec=1373670.93, grad_norm=0.3606, duration=0.38s
Step 7141: loss=3.0909, lr=0.000506, tokens/sec=1380240.89, grad_norm=0.3082, duration=0.38s
Step 7142: loss=3.0818, lr=0.000506, tokens/sec=1374802.83, grad_norm=0.3253, duration=0.38s
Step 7143: loss=3.0217, lr=0.000506, tokens/sec=1373795.37, grad_norm=0.2974, duration=0.38s
Step 7144: loss=3.1146, lr=0.000506, tokens/sec=1374169.67, grad_norm=0.3247, duration=0.38s
Step 7145: loss=3.0604, lr=0.000506, tokens/sec=1378342.79, grad_norm=0.3113, duration=0.38s
Step 7146: loss=3.1318, lr=0.000506, tokens/sec=1374949.83, grad_norm=0.3043, duration=0.38s
Step 7147: loss=3.0719, lr=0.000506, tokens/sec=1378724.76, grad_norm=0.3182, duration=0.38s
Step 7148: loss=3.0875, lr=0.000506, tokens/sec=1375479.60, grad_norm=0.2902, duration=0.38s
Step 7149: loss=3.0812, lr=0.000506, tokens/sec=1376752.39, grad_norm=0.3062, duration=0.38s
Step 7150: loss=3.0782, lr=0.000506, tokens/sec=1376719.63, grad_norm=0.2948, duration=0.38s
Step 7151: loss=3.0913, lr=0.000506, tokens/sec=1377268.89, grad_norm=0.2952, duration=0.38s
Step 7152: loss=3.0996, lr=0.000506, tokens/sec=1371365.67, grad_norm=0.3152, duration=0.38s
Step 7153: loss=3.1007, lr=0.000506, tokens/sec=1376524.87, grad_norm=0.2944, duration=0.38s
Step 7154: loss=3.0978, lr=0.000506, tokens/sec=1375814.36, grad_norm=0.2912, duration=0.38s
Step 7155: loss=3.0908, lr=0.000506, tokens/sec=1376043.37, grad_norm=0.2862, duration=0.38s
Step 7156: loss=3.0799, lr=0.000506, tokens/sec=1375895.28, grad_norm=0.2966, duration=0.38s
Step 7157: loss=3.0747, lr=0.000505, tokens/sec=1375327.34, grad_norm=0.2837, duration=0.38s
Step 7158: loss=3.0584, lr=0.000505, tokens/sec=1379210.74, grad_norm=0.3172, duration=0.38s
Step 7159: loss=3.0002, lr=0.000505, tokens/sec=1374696.26, grad_norm=0.2812, duration=0.38s
Step 7160: loss=3.0458, lr=0.000505, tokens/sec=1378455.12, grad_norm=0.3084, duration=0.38s
Step 7161: loss=2.9694, lr=0.000505, tokens/sec=1379253.13, grad_norm=0.3156, duration=0.38s
Step 7162: loss=3.0002, lr=0.000505, tokens/sec=1374985.93, grad_norm=0.3101, duration=0.38s
Step 7163: loss=2.9896, lr=0.000505, tokens/sec=1375234.44, grad_norm=0.3071, duration=0.38s
Step 7164: loss=3.0831, lr=0.000505, tokens/sec=1377417.27, grad_norm=0.3157, duration=0.38s
Step 7165: loss=3.0533, lr=0.000505, tokens/sec=1377800.45, grad_norm=0.2760, duration=0.38s
Step 7166: loss=3.0042, lr=0.000505, tokens/sec=1375200.90, grad_norm=0.3199, duration=0.38s
Step 7167: loss=2.9825, lr=0.000505, tokens/sec=1375148.44, grad_norm=0.3002, duration=0.38s
Step 7168: loss=3.0217, lr=0.000505, tokens/sec=1376561.06, grad_norm=0.2861, duration=0.38s
Step 7169: loss=3.0523, lr=0.000505, tokens/sec=1376484.37, grad_norm=0.3009, duration=0.38s
Step 7170: loss=3.0928, lr=0.000505, tokens/sec=1375700.75, grad_norm=0.2750, duration=0.38s
Step 7171: loss=3.0914, lr=0.000505, tokens/sec=1374019.41, grad_norm=0.3139, duration=0.38s
Step 7172: loss=3.0797, lr=0.000505, tokens/sec=1379182.19, grad_norm=0.2732, duration=0.38s
Step 7173: loss=3.0755, lr=0.000505, tokens/sec=1376519.70, grad_norm=0.3038, duration=0.38s
Step 7174: loss=3.0996, lr=0.000505, tokens/sec=1374962.72, grad_norm=0.2707, duration=0.38s
Step 7175: loss=3.0534, lr=0.000505, tokens/sec=1377359.47, grad_norm=0.2921, duration=0.38s
Step 7176: loss=3.0798, lr=0.000505, tokens/sec=1377173.15, grad_norm=0.2927, duration=0.38s
Step 7177: loss=3.0953, lr=0.000505, tokens/sec=1376611.04, grad_norm=0.2830, duration=0.38s
Step 7178: loss=3.1806, lr=0.000505, tokens/sec=1372176.90, grad_norm=0.3103, duration=0.38s
Step 7179: loss=3.1599, lr=0.000505, tokens/sec=1375674.93, grad_norm=0.3026, duration=0.38s
Step 7180: loss=3.1533, lr=0.000505, tokens/sec=1374787.36, grad_norm=0.2985, duration=0.38s
Step 7181: loss=3.0849, lr=0.000505, tokens/sec=1379637.33, grad_norm=0.2946, duration=0.38s
Step 7182: loss=3.1000, lr=0.000505, tokens/sec=1375641.37, grad_norm=0.2863, duration=0.38s
Step 7183: loss=3.0840, lr=0.000505, tokens/sec=1373506.20, grad_norm=0.2955, duration=0.38s
Step 7184: loss=3.0322, lr=0.000505, tokens/sec=1374402.42, grad_norm=0.2946, duration=0.38s
Step 7185: loss=3.1465, lr=0.000505, tokens/sec=1376250.05, grad_norm=0.2992, duration=0.38s
Step 7186: loss=3.0191, lr=0.000505, tokens/sec=1370899.74, grad_norm=0.3088, duration=0.38s
Step 7187: loss=3.1395, lr=0.000505, tokens/sec=1378391.18, grad_norm=0.3078, duration=0.38s
Step 7188: loss=3.1551, lr=0.000505, tokens/sec=1376911.00, grad_norm=0.3126, duration=0.38s
Step 7189: loss=3.1239, lr=0.000505, tokens/sec=1375164.78, grad_norm=0.3230, duration=0.38s
Step 7190: loss=3.0799, lr=0.000505, tokens/sec=1376910.14, grad_norm=0.3062, duration=0.38s
Step 7191: loss=3.1158, lr=0.000505, tokens/sec=1376773.94, grad_norm=0.3273, duration=0.38s
Step 7192: loss=3.1322, lr=0.000505, tokens/sec=1376550.72, grad_norm=0.3075, duration=0.38s
Step 7193: loss=3.1234, lr=0.000504, tokens/sec=1377920.46, grad_norm=0.2927, duration=0.38s
Step 7194: loss=2.9881, lr=0.000504, tokens/sec=1376786.00, grad_norm=0.2975, duration=0.38s
Step 7195: loss=3.0990, lr=0.000504, tokens/sec=1371641.96, grad_norm=0.3008, duration=0.38s
Step 7196: loss=3.0980, lr=0.000504, tokens/sec=1376463.69, grad_norm=0.3157, duration=0.38s
Step 7197: loss=3.1343, lr=0.000504, tokens/sec=1374645.56, grad_norm=0.2940, duration=0.38s
Step 7198: loss=3.1234, lr=0.000504, tokens/sec=1374182.55, grad_norm=0.3135, duration=0.38s
Step 7199: loss=3.0576, lr=0.000504, tokens/sec=1375111.47, grad_norm=0.2878, duration=0.38s
Step 7200/19073 (37.7%), Elapsed time: 2860.97s, Steps per hour: 9059.86, Estimated hours remaining: 1.31
Step 7200: loss=3.0333, lr=0.000504, tokens/sec=1375464.98, grad_norm=0.3156, duration=0.38s
Step 7201: loss=3.0730, lr=0.000504, tokens/sec=1372426.11, grad_norm=0.3245, duration=0.38s
Step 7202: loss=3.0334, lr=0.000504, tokens/sec=1371288.71, grad_norm=0.3112, duration=0.38s
Step 7203: loss=3.0586, lr=0.000504, tokens/sec=1371684.74, grad_norm=0.3030, duration=0.38s
Step 7204: loss=3.0347, lr=0.000504, tokens/sec=1374066.63, grad_norm=0.2922, duration=0.38s
Step 7205: loss=3.0028, lr=0.000504, tokens/sec=1377544.97, grad_norm=0.3062, duration=0.38s
Step 7206: loss=3.0083, lr=0.000504, tokens/sec=1369682.11, grad_norm=0.2841, duration=0.38s
Step 7207: loss=3.0625, lr=0.000504, tokens/sec=1373146.84, grad_norm=0.3230, duration=0.38s
Step 7208: loss=3.0205, lr=0.000504, tokens/sec=1373991.94, grad_norm=0.2866, duration=0.38s
Step 7209: loss=3.0303, lr=0.000504, tokens/sec=1373022.52, grad_norm=0.2904, duration=0.38s
Step 7210: loss=2.9622, lr=0.000504, tokens/sec=1374307.94, grad_norm=0.3031, duration=0.38s
Step 7211: loss=2.9904, lr=0.000504, tokens/sec=1373401.54, grad_norm=0.2906, duration=0.38s
Step 7212: loss=3.0348, lr=0.000504, tokens/sec=1376696.36, grad_norm=0.2909, duration=0.38s
Step 7213: loss=3.0345, lr=0.000504, tokens/sec=1374432.49, grad_norm=0.2945, duration=0.38s
Step 7214: loss=3.0700, lr=0.000504, tokens/sec=1376111.39, grad_norm=0.2769, duration=0.38s
Step 7215: loss=3.0843, lr=0.000504, tokens/sec=1378445.61, grad_norm=0.2891, duration=0.38s
Step 7216: loss=3.0924, lr=0.000504, tokens/sec=1374265.85, grad_norm=0.3012, duration=0.38s
Step 7217: loss=3.0685, lr=0.000504, tokens/sec=1375512.30, grad_norm=0.2915, duration=0.38s
Step 7218: loss=3.0630, lr=0.000504, tokens/sec=1378094.02, grad_norm=0.3203, duration=0.38s
Step 7219: loss=3.0607, lr=0.000504, tokens/sec=1374702.28, grad_norm=0.2786, duration=0.38s
Step 7220: loss=3.0761, lr=0.000504, tokens/sec=1374082.94, grad_norm=0.3160, duration=0.38s
Step 7221: loss=3.0990, lr=0.000504, tokens/sec=1376073.50, grad_norm=0.2853, duration=0.38s
Step 7222: loss=3.0851, lr=0.000504, tokens/sec=1374301.06, grad_norm=0.3004, duration=0.38s
Step 7223: loss=3.1921, lr=0.000504, tokens/sec=1371885.84, grad_norm=0.2979, duration=0.38s
Step 7224: loss=3.1134, lr=0.000504, tokens/sec=1374787.36, grad_norm=0.3219, duration=0.38s
Step 7225: loss=3.0691, lr=0.000504, tokens/sec=1376766.18, grad_norm=0.2901, duration=0.38s
Step 7226: loss=3.1670, lr=0.000504, tokens/sec=1372827.09, grad_norm=0.3266, duration=0.38s
Step 7227: loss=3.1042, lr=0.000504, tokens/sec=1375723.99, grad_norm=0.3305, duration=0.38s
Step 7228: loss=3.1568, lr=0.000504, tokens/sec=1376217.32, grad_norm=0.3113, duration=0.38s
Step 7229: loss=3.1229, lr=0.000504, tokens/sec=1373570.54, grad_norm=0.3170, duration=0.38s
Step 7230: loss=3.1415, lr=0.000503, tokens/sec=1372468.08, grad_norm=0.3088, duration=0.38s
Step 7231: loss=3.0837, lr=0.000503, tokens/sec=1376523.15, grad_norm=0.3014, duration=0.38s
Step 7232: loss=3.1114, lr=0.000503, tokens/sec=1375258.53, grad_norm=0.3111, duration=0.38s
Step 7233: loss=3.0841, lr=0.000503, tokens/sec=1371202.35, grad_norm=0.2923, duration=0.38s
Step 7234: loss=3.1384, lr=0.000503, tokens/sec=1375886.67, grad_norm=0.3008, duration=0.38s
Step 7235: loss=3.1074, lr=0.000503, tokens/sec=1377713.27, grad_norm=0.3138, duration=0.38s
Step 7236: loss=3.0729, lr=0.000503, tokens/sec=1374535.58, grad_norm=0.3297, duration=0.38s
Step 7237: loss=3.1316, lr=0.000503, tokens/sec=1374161.94, grad_norm=0.3062, duration=0.38s
Step 7238: loss=3.0595, lr=0.000503, tokens/sec=1375200.04, grad_norm=0.3336, duration=0.38s
Step 7239: loss=3.0257, lr=0.000503, tokens/sec=1370574.20, grad_norm=0.3680, duration=0.38s
Step 7240: loss=3.0502, lr=0.000503, tokens/sec=1375495.95, grad_norm=0.3240, duration=0.38s
Step 7241: loss=3.0958, lr=0.000503, tokens/sec=1377737.44, grad_norm=0.3025, duration=0.38s
Step 7242: loss=3.1734, lr=0.000503, tokens/sec=1372071.59, grad_norm=0.3053, duration=0.38s
Step 7243: loss=3.0177, lr=0.000503, tokens/sec=1373193.14, grad_norm=0.3235, duration=0.38s
Step 7244: loss=3.1488, lr=0.000503, tokens/sec=1378948.68, grad_norm=0.3009, duration=0.38s
Step 7245: loss=3.1051, lr=0.000503, tokens/sec=1373338.07, grad_norm=0.3293, duration=0.38s
Step 7246: loss=3.0649, lr=0.000503, tokens/sec=1375333.36, grad_norm=0.3106, duration=0.38s
Step 7247: loss=3.1146, lr=0.000503, tokens/sec=1373112.54, grad_norm=0.2934, duration=0.38s
Step 7248: loss=3.0953, lr=0.000503, tokens/sec=1373334.64, grad_norm=0.3118, duration=0.38s
Step 7249: loss=3.1041, lr=0.000503, tokens/sec=1376492.13, grad_norm=0.3084, duration=0.38s
Validation loss at step 7250: 3.7211015224456787
Step 7250: loss=3.0615, lr=0.000503, tokens/sec=152950.38, grad_norm=0.2932, duration=3.43s
Step 7251: loss=3.0744, lr=0.000503, tokens/sec=1379132.89, grad_norm=0.2965, duration=0.38s
Step 7252: loss=3.1192, lr=0.000503, tokens/sec=1376027.01, grad_norm=0.4062, duration=0.38s
Step 7253: loss=3.1299, lr=0.000503, tokens/sec=1371919.22, grad_norm=0.3050, duration=0.38s
Step 7254: loss=3.0753, lr=0.000503, tokens/sec=1377500.97, grad_norm=0.2862, duration=0.38s
Step 7255: loss=3.0207, lr=0.000503, tokens/sec=1377397.43, grad_norm=0.2977, duration=0.38s
Step 7256: loss=3.0590, lr=0.000503, tokens/sec=1376674.81, grad_norm=0.2882, duration=0.38s
Step 7257: loss=2.9971, lr=0.000503, tokens/sec=1376193.21, grad_norm=0.3099, duration=0.38s
Step 7258: loss=3.0004, lr=0.000503, tokens/sec=1374210.89, grad_norm=0.2856, duration=0.38s
Step 7259: loss=3.0487, lr=0.000503, tokens/sec=1367788.25, grad_norm=0.3003, duration=0.38s
Step 7260: loss=3.0185, lr=0.000503, tokens/sec=1374047.74, grad_norm=0.3133, duration=0.38s
Step 7261: loss=3.0214, lr=0.000503, tokens/sec=1374585.41, grad_norm=0.2873, duration=0.38s
Step 7262: loss=3.0246, lr=0.000503, tokens/sec=1374995.39, grad_norm=0.2990, duration=0.38s
Step 7263: loss=3.0500, lr=0.000503, tokens/sec=1376320.68, grad_norm=0.2751, duration=0.38s
Step 7264: loss=3.0260, lr=0.000503, tokens/sec=1376149.29, grad_norm=0.2923, duration=0.38s
Step 7265: loss=2.9913, lr=0.000503, tokens/sec=1376390.46, grad_norm=0.2782, duration=0.38s
Step 7266: loss=3.0230, lr=0.000502, tokens/sec=1375105.45, grad_norm=0.3026, duration=0.38s
Step 7267: loss=3.0934, lr=0.000502, tokens/sec=1377618.33, grad_norm=0.2747, duration=0.38s
Step 7268: loss=3.1285, lr=0.000502, tokens/sec=1376822.21, grad_norm=0.2927, duration=0.38s
Step 7269: loss=3.0929, lr=0.000502, tokens/sec=1377024.82, grad_norm=0.2844, duration=0.38s
Step 7270: loss=3.0967, lr=0.000502, tokens/sec=1375348.84, grad_norm=0.3125, duration=0.38s
Step 7271: loss=3.0506, lr=0.000502, tokens/sec=1374061.48, grad_norm=0.2776, duration=0.38s
Step 7272: loss=3.1063, lr=0.000502, tokens/sec=1374239.23, grad_norm=0.3214, duration=0.38s
Step 7273: loss=3.0964, lr=0.000502, tokens/sec=1375577.69, grad_norm=0.2836, duration=0.38s
Step 7274: loss=3.0993, lr=0.000502, tokens/sec=1378404.14, grad_norm=0.3149, duration=0.38s
Step 7275: loss=3.0902, lr=0.000502, tokens/sec=1374837.22, grad_norm=0.2959, duration=0.38s
Step 7276: loss=3.1172, lr=0.000502, tokens/sec=1376304.32, grad_norm=0.3042, duration=0.38s
Step 7277: loss=3.0929, lr=0.000502, tokens/sec=1375447.77, grad_norm=0.2994, duration=0.38s
Step 7278: loss=3.0827, lr=0.000502, tokens/sec=1376574.85, grad_norm=0.2933, duration=0.38s
Step 7279: loss=3.0631, lr=0.000502, tokens/sec=1376727.39, grad_norm=0.2974, duration=0.38s
Step 7280: loss=3.0442, lr=0.000502, tokens/sec=1372581.16, grad_norm=0.2985, duration=0.38s
Step 7281: loss=3.0157, lr=0.000502, tokens/sec=1376556.75, grad_norm=0.3023, duration=0.38s
Step 7282: loss=3.0716, lr=0.000502, tokens/sec=1377422.45, grad_norm=0.3024, duration=0.38s
Step 7283: loss=3.0998, lr=0.000502, tokens/sec=1372923.94, grad_norm=0.2982, duration=0.38s
Step 7284: loss=3.1211, lr=0.000502, tokens/sec=1374283.89, grad_norm=0.3054, duration=0.38s
Step 7285: loss=3.1321, lr=0.000502, tokens/sec=1376559.34, grad_norm=0.3260, duration=0.38s
Step 7286: loss=3.0852, lr=0.000502, tokens/sec=1376892.04, grad_norm=0.3177, duration=0.38s
Step 7287: loss=3.1557, lr=0.000502, tokens/sec=1377397.43, grad_norm=0.3300, duration=0.38s
Step 7288: loss=3.1079, lr=0.000502, tokens/sec=1374106.13, grad_norm=0.3145, duration=0.38s
Step 7289: loss=3.1116, lr=0.000502, tokens/sec=1375697.31, grad_norm=0.3690, duration=0.38s
Step 7290: loss=3.0562, lr=0.000502, tokens/sec=1377702.91, grad_norm=0.2924, duration=0.38s
Step 7291: loss=3.0877, lr=0.000502, tokens/sec=1377063.62, grad_norm=0.3468, duration=0.38s
Step 7292: loss=3.0407, lr=0.000502, tokens/sec=1373614.30, grad_norm=0.3223, duration=0.38s
Step 7293: loss=3.0886, lr=0.000502, tokens/sec=1378568.32, grad_norm=0.3059, duration=0.38s
Step 7294: loss=3.0823, lr=0.000502, tokens/sec=1373502.77, grad_norm=0.3094, duration=0.38s
Step 7295: loss=3.0513, lr=0.000502, tokens/sec=1374410.15, grad_norm=0.2848, duration=0.38s
Step 7296: loss=3.0752, lr=0.000502, tokens/sec=1377854.84, grad_norm=0.3085, duration=0.38s
Step 7297: loss=3.0232, lr=0.000502, tokens/sec=1376821.35, grad_norm=0.2872, duration=0.38s
Step 7298: loss=3.0795, lr=0.000502, tokens/sec=1374003.10, grad_norm=0.2872, duration=0.38s
Step 7299: loss=3.0853, lr=0.000502, tokens/sec=1375949.52, grad_norm=0.2898, duration=0.38s
Step 7300/19073 (38.3%), Elapsed time: 2902.23s, Steps per hour: 9055.10, Estimated hours remaining: 1.30
Step 7300: loss=3.0921, lr=0.000502, tokens/sec=1375021.19, grad_norm=0.2581, duration=0.38s
Step 7301: loss=3.0080, lr=0.000502, tokens/sec=1375840.19, grad_norm=0.2829, duration=0.38s
Step 7302: loss=3.0983, lr=0.000501, tokens/sec=1380252.16, grad_norm=0.2804, duration=0.38s
Step 7303: loss=3.0003, lr=0.000501, tokens/sec=1377189.53, grad_norm=0.2530, duration=0.38s
Step 7304: loss=3.0179, lr=0.000501, tokens/sec=1377664.07, grad_norm=0.2949, duration=0.38s
Step 7305: loss=2.9720, lr=0.000501, tokens/sec=1372662.55, grad_norm=0.2586, duration=0.38s
Step 7306: loss=3.0055, lr=0.000501, tokens/sec=1375863.43, grad_norm=0.2981, duration=0.38s
Step 7307: loss=3.0117, lr=0.000501, tokens/sec=1376846.34, grad_norm=0.2932, duration=0.38s
Step 7308: loss=2.9804, lr=0.000501, tokens/sec=1376122.59, grad_norm=0.2961, duration=0.38s
Step 7309: loss=3.0157, lr=0.000501, tokens/sec=1377671.84, grad_norm=0.3137, duration=0.38s
Step 7310: loss=2.9683, lr=0.000501, tokens/sec=1378494.00, grad_norm=0.2889, duration=0.38s
Step 7311: loss=3.0164, lr=0.000501, tokens/sec=1378337.61, grad_norm=0.3031, duration=0.38s
Step 7312: loss=3.0141, lr=0.000501, tokens/sec=1378523.38, grad_norm=0.2914, duration=0.38s
Step 7313: loss=3.0384, lr=0.000501, tokens/sec=1375714.52, grad_norm=0.3022, duration=0.38s
Step 7314: loss=3.1529, lr=0.000501, tokens/sec=1374778.77, grad_norm=0.3064, duration=0.38s
Step 7315: loss=3.1387, lr=0.000501, tokens/sec=1378505.23, grad_norm=0.3185, duration=0.38s
Step 7316: loss=3.1082, lr=0.000501, tokens/sec=1376410.28, grad_norm=0.3181, duration=0.38s
Step 7317: loss=3.1146, lr=0.000501, tokens/sec=1374649.00, grad_norm=0.3222, duration=0.38s
Step 7318: loss=3.0199, lr=0.000501, tokens/sec=1377217.13, grad_norm=0.3163, duration=0.38s
Step 7319: loss=3.1090, lr=0.000501, tokens/sec=1377280.10, grad_norm=0.3615, duration=0.38s
Step 7320: loss=3.0877, lr=0.000501, tokens/sec=1378467.21, grad_norm=0.3447, duration=0.38s
Step 7321: loss=3.1424, lr=0.000501, tokens/sec=1371524.76, grad_norm=0.3315, duration=0.38s
Step 7322: loss=3.1635, lr=0.000501, tokens/sec=1376103.64, grad_norm=0.3479, duration=0.38s
Step 7323: loss=3.1306, lr=0.000501, tokens/sec=1379189.11, grad_norm=0.2806, duration=0.38s
Step 7324: loss=3.1081, lr=0.000501, tokens/sec=1380729.67, grad_norm=0.3324, duration=0.38s
Step 7325: loss=3.1086, lr=0.000501, tokens/sec=1379397.61, grad_norm=0.2947, duration=0.38s
Step 7326: loss=3.0861, lr=0.000501, tokens/sec=1372218.00, grad_norm=0.3107, duration=0.38s
Step 7327: loss=3.0740, lr=0.000501, tokens/sec=1374082.94, grad_norm=0.3055, duration=0.38s
Step 7328: loss=3.1149, lr=0.000501, tokens/sec=1375247.35, grad_norm=0.3149, duration=0.38s
Step 7329: loss=3.0486, lr=0.000501, tokens/sec=1376652.41, grad_norm=0.2868, duration=0.38s
Step 7330: loss=3.0963, lr=0.000501, tokens/sec=1376152.73, grad_norm=0.3035, duration=0.38s
Step 7331: loss=3.0650, lr=0.000501, tokens/sec=1375802.31, grad_norm=0.3141, duration=0.38s
Step 7332: loss=3.0675, lr=0.000501, tokens/sec=1377838.44, grad_norm=0.2885, duration=0.38s
Step 7333: loss=3.0301, lr=0.000501, tokens/sec=1376986.01, grad_norm=0.2873, duration=0.38s
Step 7334: loss=3.1026, lr=0.000501, tokens/sec=1376552.44, grad_norm=0.3103, duration=0.38s
Step 7335: loss=3.0742, lr=0.000501, tokens/sec=1379815.66, grad_norm=0.2992, duration=0.38s
Step 7336: loss=3.0939, lr=0.000501, tokens/sec=1373677.80, grad_norm=0.2929, duration=0.38s
Step 7337: loss=3.0565, lr=0.000501, tokens/sec=1375552.74, grad_norm=0.3076, duration=0.38s
Step 7338: loss=3.0925, lr=0.000500, tokens/sec=1375182.84, grad_norm=0.3065, duration=0.38s
Step 7339: loss=3.0591, lr=0.000500, tokens/sec=1377237.83, grad_norm=0.2864, duration=0.38s
Step 7340: loss=3.0873, lr=0.000500, tokens/sec=1378261.59, grad_norm=0.2983, duration=0.38s
Step 7341: loss=3.0792, lr=0.000500, tokens/sec=1373706.97, grad_norm=0.2760, duration=0.38s
Step 7342: loss=3.1136, lr=0.000500, tokens/sec=1382019.14, grad_norm=0.2880, duration=0.38s
Step 7343: loss=3.0725, lr=0.000500, tokens/sec=1374537.30, grad_norm=0.2855, duration=0.38s
Step 7344: loss=3.0817, lr=0.000500, tokens/sec=1378759.34, grad_norm=0.2743, duration=0.38s
Step 7345: loss=3.0835, lr=0.000500, tokens/sec=1376704.12, grad_norm=0.2635, duration=0.38s
Step 7346: loss=3.0753, lr=0.000500, tokens/sec=1378356.62, grad_norm=0.2778, duration=0.38s
Step 7347: loss=3.0462, lr=0.000500, tokens/sec=1372466.37, grad_norm=0.2588, duration=0.38s
Step 7348: loss=3.0514, lr=0.000500, tokens/sec=1376230.24, grad_norm=0.2969, duration=0.38s
Step 7349: loss=2.9984, lr=0.000500, tokens/sec=1378204.58, grad_norm=0.2800, duration=0.38s
Step 7350: loss=3.0361, lr=0.000500, tokens/sec=1378138.93, grad_norm=0.2696, duration=0.38s
Step 7351: loss=2.9476, lr=0.000500, tokens/sec=1375495.09, grad_norm=0.2946, duration=0.38s
Step 7352: loss=2.9843, lr=0.000500, tokens/sec=1374991.09, grad_norm=0.3111, duration=0.38s
Step 7353: loss=3.0080, lr=0.000500, tokens/sec=1373726.71, grad_norm=0.2977, duration=0.38s
Step 7354: loss=3.0723, lr=0.000500, tokens/sec=1375847.07, grad_norm=0.3156, duration=0.38s
Step 7355: loss=3.0177, lr=0.000500, tokens/sec=1378311.69, grad_norm=0.2899, duration=0.38s
Step 7356: loss=2.9866, lr=0.000500, tokens/sec=1376441.29, grad_norm=0.2933, duration=0.38s
Step 7357: loss=2.9836, lr=0.000500, tokens/sec=1373913.82, grad_norm=0.3013, duration=0.38s
Step 7358: loss=3.0030, lr=0.000500, tokens/sec=1373603.15, grad_norm=0.2682, duration=0.38s
Step 7359: loss=3.0623, lr=0.000500, tokens/sec=1379169.22, grad_norm=0.3292, duration=0.38s
Step 7360: loss=3.0650, lr=0.000500, tokens/sec=1378852.71, grad_norm=0.2743, duration=0.38s
Step 7361: loss=3.0619, lr=0.000500, tokens/sec=1377473.35, grad_norm=0.3087, duration=0.38s
Step 7362: loss=3.0866, lr=0.000500, tokens/sec=1377081.73, grad_norm=0.2823, duration=0.38s
Step 7363: loss=3.0651, lr=0.000500, tokens/sec=1375544.99, grad_norm=0.2935, duration=0.38s
Step 7364: loss=3.0973, lr=0.000500, tokens/sec=1372944.51, grad_norm=0.2856, duration=0.38s
Step 7365: loss=3.0430, lr=0.000500, tokens/sec=1377775.42, grad_norm=0.2689, duration=0.38s
Step 7366: loss=3.0399, lr=0.000500, tokens/sec=1379068.89, grad_norm=0.2837, duration=0.38s
Step 7367: loss=3.1083, lr=0.000500, tokens/sec=1374204.88, grad_norm=0.2922, duration=0.38s
Step 7368: loss=3.2126, lr=0.000500, tokens/sec=1376868.76, grad_norm=0.3031, duration=0.38s
Step 7369: loss=3.1556, lr=0.000500, tokens/sec=1378616.72, grad_norm=0.3039, duration=0.38s
Step 7370: loss=3.1314, lr=0.000500, tokens/sec=1378062.07, grad_norm=0.3041, duration=0.38s
Step 7371: loss=3.0577, lr=0.000500, tokens/sec=1374965.30, grad_norm=0.3022, duration=0.38s
Step 7372: loss=3.1090, lr=0.000500, tokens/sec=1373680.37, grad_norm=0.3052, duration=0.38s
Step 7373: loss=3.0599, lr=0.000499, tokens/sec=1377017.06, grad_norm=0.3056, duration=0.38s
Step 7374: loss=3.0452, lr=0.000499, tokens/sec=1377301.67, grad_norm=0.3056, duration=0.38s
Step 7375: loss=3.0925, lr=0.000499, tokens/sec=1379476.35, grad_norm=0.2992, duration=0.38s
Step 7376: loss=3.0394, lr=0.000499, tokens/sec=1376636.89, grad_norm=0.3024, duration=0.38s
Step 7377: loss=3.1235, lr=0.000499, tokens/sec=1374869.88, grad_norm=0.3208, duration=0.38s
Step 7378: loss=3.1721, lr=0.000499, tokens/sec=1377467.31, grad_norm=0.2955, duration=0.38s
Step 7379: loss=3.1216, lr=0.000499, tokens/sec=1375217.24, grad_norm=0.3256, duration=0.38s
Step 7380: loss=3.0467, lr=0.000499, tokens/sec=1374939.51, grad_norm=0.2942, duration=0.38s
Step 7381: loss=3.1039, lr=0.000499, tokens/sec=1375024.62, grad_norm=0.3207, duration=0.38s
Step 7382: loss=3.1514, lr=0.000499, tokens/sec=1378714.39, grad_norm=0.3161, duration=0.38s
Step 7383: loss=3.0503, lr=0.000499, tokens/sec=1377033.44, grad_norm=0.2738, duration=0.38s
Step 7384: loss=3.0289, lr=0.000499, tokens/sec=1378904.59, grad_norm=0.3228, duration=0.38s
Step 7385: loss=3.0858, lr=0.000499, tokens/sec=1379811.33, grad_norm=0.2868, duration=0.38s
Step 7386: loss=3.1065, lr=0.000499, tokens/sec=1378253.81, grad_norm=0.3250, duration=0.38s
Step 7387: loss=3.0874, lr=0.000499, tokens/sec=1372271.95, grad_norm=0.2903, duration=0.38s
Step 7388: loss=3.1282, lr=0.000499, tokens/sec=1377244.74, grad_norm=0.2908, duration=0.38s
Step 7389: loss=3.0149, lr=0.000499, tokens/sec=1374815.73, grad_norm=0.3391, duration=0.38s
Step 7390: loss=3.0656, lr=0.000499, tokens/sec=1375807.48, grad_norm=0.3196, duration=0.38s
Step 7391: loss=3.0660, lr=0.000499, tokens/sec=1376560.20, grad_norm=0.2925, duration=0.38s
Step 7392: loss=3.0084, lr=0.000499, tokens/sec=1374555.34, grad_norm=0.3172, duration=0.38s
Step 7393: loss=3.0657, lr=0.000499, tokens/sec=1373447.86, grad_norm=0.2811, duration=0.38s
Step 7394: loss=3.0054, lr=0.000499, tokens/sec=1377255.09, grad_norm=0.2909, duration=0.38s
Step 7395: loss=2.9971, lr=0.000499, tokens/sec=1373797.94, grad_norm=0.2905, duration=0.38s
Step 7396: loss=2.9877, lr=0.000499, tokens/sec=1372865.65, grad_norm=0.2777, duration=0.38s
Step 7397: loss=3.0646, lr=0.000499, tokens/sec=1376464.56, grad_norm=0.2910, duration=0.38s
Step 7398: loss=3.0425, lr=0.000499, tokens/sec=1371787.43, grad_norm=0.2968, duration=0.38s
Step 7399: loss=2.9845, lr=0.000499, tokens/sec=1371412.71, grad_norm=0.2898, duration=0.38s
Step 7400/19073 (38.8%), Elapsed time: 2940.41s, Steps per hour: 9059.97, Estimated hours remaining: 1.29
Step 7400: loss=2.9495, lr=0.000499, tokens/sec=1373143.41, grad_norm=0.2965, duration=0.38s
Step 7401: loss=2.9954, lr=0.000499, tokens/sec=1373213.72, grad_norm=0.3059, duration=0.38s
Step 7402: loss=3.0396, lr=0.000499, tokens/sec=1375421.10, grad_norm=0.2627, duration=0.38s
Step 7403: loss=3.0427, lr=0.000499, tokens/sec=1367465.88, grad_norm=0.2920, duration=0.38s
Step 7404: loss=3.0268, lr=0.000499, tokens/sec=1376517.98, grad_norm=0.2736, duration=0.38s
Step 7405: loss=3.0855, lr=0.000499, tokens/sec=1377400.88, grad_norm=0.3036, duration=0.38s
Step 7406: loss=3.0546, lr=0.000499, tokens/sec=1376316.38, grad_norm=0.2979, duration=0.38s
Step 7407: loss=3.0862, lr=0.000499, tokens/sec=1373071.39, grad_norm=0.3044, duration=0.38s
Step 7408: loss=3.0236, lr=0.000499, tokens/sec=1372694.26, grad_norm=0.3057, duration=0.38s
Step 7409: loss=3.0864, lr=0.000498, tokens/sec=1375525.20, grad_norm=0.2891, duration=0.38s
Step 7410: loss=3.0574, lr=0.000498, tokens/sec=1376339.64, grad_norm=0.3094, duration=0.38s
Step 7411: loss=3.0689, lr=0.000498, tokens/sec=1374370.64, grad_norm=0.2766, duration=0.38s
Step 7412: loss=3.1149, lr=0.000498, tokens/sec=1379452.12, grad_norm=0.3063, duration=0.38s
Step 7413: loss=3.1684, lr=0.000498, tokens/sec=1377877.29, grad_norm=0.3041, duration=0.38s
Step 7414: loss=3.0383, lr=0.000498, tokens/sec=1376592.08, grad_norm=0.3098, duration=0.38s
Step 7415: loss=3.1089, lr=0.000498, tokens/sec=1374274.44, grad_norm=0.3055, duration=0.38s
Step 7416: loss=3.1613, lr=0.000498, tokens/sec=1375460.67, grad_norm=0.2994, duration=0.38s
Step 7417: loss=3.1289, lr=0.000498, tokens/sec=1375605.23, grad_norm=0.3337, duration=0.38s
Step 7418: loss=3.1378, lr=0.000498, tokens/sec=1374182.55, grad_norm=0.3052, duration=0.38s
Step 7419: loss=3.1185, lr=0.000498, tokens/sec=1372120.39, grad_norm=0.3241, duration=0.38s
Step 7420: loss=3.1109, lr=0.000498, tokens/sec=1372361.87, grad_norm=0.3115, duration=0.38s
Step 7421: loss=3.0856, lr=0.000498, tokens/sec=1376186.32, grad_norm=0.2921, duration=0.38s
Step 7422: loss=3.1382, lr=0.000498, tokens/sec=1380020.88, grad_norm=0.3279, duration=0.38s
Step 7423: loss=3.0600, lr=0.000498, tokens/sec=1376300.01, grad_norm=0.2899, duration=0.38s
Step 7424: loss=3.1128, lr=0.000498, tokens/sec=1378608.94, grad_norm=0.3223, duration=0.38s
Step 7425: loss=3.1069, lr=0.000498, tokens/sec=1379174.41, grad_norm=0.3200, duration=0.38s
Step 7426: loss=3.0537, lr=0.000498, tokens/sec=1377466.45, grad_norm=0.3261, duration=0.38s
Step 7427: loss=3.1312, lr=0.000498, tokens/sec=1378075.89, grad_norm=0.3177, duration=0.38s
Step 7428: loss=3.0569, lr=0.000498, tokens/sec=1371470.02, grad_norm=0.3193, duration=0.38s
Step 7429: loss=2.9898, lr=0.000498, tokens/sec=1373670.93, grad_norm=0.3457, duration=0.38s
Step 7430: loss=3.0538, lr=0.000498, tokens/sec=1375388.41, grad_norm=0.3389, duration=0.38s
Step 7431: loss=3.0840, lr=0.000498, tokens/sec=1376647.24, grad_norm=0.2938, duration=0.38s
Step 7432: loss=3.1610, lr=0.000498, tokens/sec=1374691.97, grad_norm=0.3106, duration=0.38s
Step 7433: loss=3.0366, lr=0.000498, tokens/sec=1371555.56, grad_norm=0.3085, duration=0.38s
Step 7434: loss=3.1156, lr=0.000498, tokens/sec=1375237.88, grad_norm=0.3165, duration=0.38s
Step 7435: loss=3.1061, lr=0.000498, tokens/sec=1369756.33, grad_norm=0.3080, duration=0.38s
Step 7436: loss=3.0766, lr=0.000498, tokens/sec=1380169.86, grad_norm=0.3003, duration=0.38s
Step 7437: loss=3.0662, lr=0.000498, tokens/sec=1378009.39, grad_norm=0.3217, duration=0.38s
Step 7438: loss=3.1166, lr=0.000498, tokens/sec=1378183.85, grad_norm=0.2921, duration=0.38s
Step 7439: loss=3.0785, lr=0.000498, tokens/sec=1374728.92, grad_norm=0.2913, duration=0.38s
Step 7440: loss=3.0359, lr=0.000498, tokens/sec=1372709.68, grad_norm=0.2935, duration=0.38s
Step 7441: loss=3.0822, lr=0.000498, tokens/sec=1377835.85, grad_norm=0.3116, duration=0.38s
Step 7442: loss=3.0882, lr=0.000498, tokens/sec=1376988.60, grad_norm=0.3087, duration=0.38s
Step 7443: loss=3.1335, lr=0.000498, tokens/sec=1375657.72, grad_norm=0.2950, duration=0.38s
Step 7444: loss=3.0696, lr=0.000498, tokens/sec=1378683.27, grad_norm=0.3127, duration=0.38s
Step 7445: loss=3.0029, lr=0.000497, tokens/sec=1379340.51, grad_norm=0.2835, duration=0.38s
Step 7446: loss=3.0397, lr=0.000497, tokens/sec=1379293.79, grad_norm=0.2966, duration=0.38s
Step 7447: loss=2.9927, lr=0.000497, tokens/sec=1374304.50, grad_norm=0.3067, duration=0.38s
Step 7448: loss=3.0170, lr=0.000497, tokens/sec=1376118.28, grad_norm=0.3023, duration=0.38s
Step 7449: loss=2.9996, lr=0.000497, tokens/sec=1377130.89, grad_norm=0.2792, duration=0.38s
Step 7450: loss=3.0130, lr=0.000497, tokens/sec=1377092.94, grad_norm=0.3147, duration=0.38s
Step 7451: loss=3.0093, lr=0.000497, tokens/sec=1377625.23, grad_norm=0.2728, duration=0.38s
Step 7452: loss=3.0236, lr=0.000497, tokens/sec=1377965.35, grad_norm=0.2991, duration=0.38s
Step 7453: loss=3.0416, lr=0.000497, tokens/sec=1376441.29, grad_norm=0.2857, duration=0.38s
Step 7454: loss=2.9892, lr=0.000497, tokens/sec=1374486.61, grad_norm=0.3022, duration=0.38s
Step 7455: loss=2.9970, lr=0.000497, tokens/sec=1372170.91, grad_norm=0.2780, duration=0.38s
Step 7456: loss=3.0076, lr=0.000497, tokens/sec=1371122.83, grad_norm=0.3132, duration=0.38s
Step 7457: loss=3.0785, lr=0.000497, tokens/sec=1373043.09, grad_norm=0.2700, duration=0.38s
Step 7458: loss=3.1305, lr=0.000497, tokens/sec=1377186.95, grad_norm=0.3091, duration=0.38s
Step 7459: loss=3.0886, lr=0.000497, tokens/sec=1374822.60, grad_norm=0.2797, duration=0.38s
Step 7460: loss=3.0220, lr=0.000497, tokens/sec=1375029.78, grad_norm=0.3024, duration=0.38s
Step 7461: loss=3.1001, lr=0.000497, tokens/sec=1376418.03, grad_norm=0.3158, duration=0.38s
Step 7462: loss=3.0946, lr=0.000497, tokens/sec=1380904.82, grad_norm=0.3030, duration=0.38s
Step 7463: loss=3.1287, lr=0.000497, tokens/sec=1375301.53, grad_norm=0.3134, duration=0.38s
Step 7464: loss=3.0837, lr=0.000497, tokens/sec=1374971.32, grad_norm=0.3131, duration=0.38s
Step 7465: loss=3.0812, lr=0.000497, tokens/sec=1375303.25, grad_norm=0.2887, duration=0.38s
Step 7466: loss=3.0975, lr=0.000497, tokens/sec=1374926.62, grad_norm=0.2986, duration=0.38s
Step 7467: loss=3.0855, lr=0.000497, tokens/sec=1373435.86, grad_norm=0.2863, duration=0.38s
Step 7468: loss=3.0598, lr=0.000497, tokens/sec=1376329.30, grad_norm=0.3151, duration=0.38s
Step 7469: loss=3.0281, lr=0.000497, tokens/sec=1378104.39, grad_norm=0.2771, duration=0.38s
Step 7470: loss=3.0822, lr=0.000497, tokens/sec=1377624.37, grad_norm=0.3195, duration=0.38s
Step 7471: loss=3.0063, lr=0.000497, tokens/sec=1375493.37, grad_norm=0.2878, duration=0.38s
Step 7472: loss=3.0518, lr=0.000497, tokens/sec=1380043.40, grad_norm=0.3117, duration=0.38s
Step 7473: loss=3.1194, lr=0.000497, tokens/sec=1373109.97, grad_norm=0.3025, duration=0.38s
Step 7474: loss=3.0765, lr=0.000497, tokens/sec=1373997.09, grad_norm=0.3035, duration=0.38s
Step 7475: loss=3.1438, lr=0.000497, tokens/sec=1376593.81, grad_norm=0.3255, duration=0.38s
Step 7476: loss=3.0641, lr=0.000497, tokens/sec=1377551.01, grad_norm=0.3105, duration=0.38s
Step 7477: loss=3.1371, lr=0.000497, tokens/sec=1374376.65, grad_norm=0.3309, duration=0.38s
Step 7478: loss=3.1129, lr=0.000497, tokens/sec=1375979.65, grad_norm=0.3104, duration=0.38s
Step 7479: loss=3.0443, lr=0.000497, tokens/sec=1378827.64, grad_norm=0.3452, duration=0.38s
Step 7480: loss=3.0795, lr=0.000496, tokens/sec=1374919.74, grad_norm=0.3143, duration=0.38s
Step 7481: loss=3.0511, lr=0.000496, tokens/sec=1377664.93, grad_norm=0.3105, duration=0.38s
Step 7482: loss=3.0638, lr=0.000496, tokens/sec=1374336.28, grad_norm=0.3238, duration=0.38s
Step 7483: loss=3.0742, lr=0.000496, tokens/sec=1372280.51, grad_norm=0.2887, duration=0.38s
Step 7484: loss=3.0461, lr=0.000496, tokens/sec=1376077.81, grad_norm=0.3257, duration=0.38s
Step 7485: loss=3.0660, lr=0.000496, tokens/sec=1374234.93, grad_norm=0.2937, duration=0.38s
Step 7486: loss=3.0551, lr=0.000496, tokens/sec=1374460.83, grad_norm=0.3063, duration=0.38s
Step 7487: loss=3.0335, lr=0.000496, tokens/sec=1378239.13, grad_norm=0.2894, duration=0.38s
Step 7488: loss=3.0854, lr=0.000496, tokens/sec=1379339.64, grad_norm=0.3005, duration=0.38s
Step 7489: loss=3.0760, lr=0.000496, tokens/sec=1375311.85, grad_norm=0.2841, duration=0.38s
Step 7490: loss=3.0723, lr=0.000496, tokens/sec=1376583.46, grad_norm=0.2751, duration=0.38s
Step 7491: loss=3.0221, lr=0.000496, tokens/sec=1373401.54, grad_norm=0.2763, duration=0.38s
Step 7492: loss=3.0454, lr=0.000496, tokens/sec=1376203.54, grad_norm=0.2820, duration=0.38s
Step 7493: loss=2.9973, lr=0.000496, tokens/sec=1378972.03, grad_norm=0.2702, duration=0.38s
Step 7494: loss=3.0146, lr=0.000496, tokens/sec=1375510.58, grad_norm=0.2673, duration=0.38s
Step 7495: loss=2.9614, lr=0.000496, tokens/sec=1379270.43, grad_norm=0.2744, duration=0.38s
Step 7496: loss=2.9782, lr=0.000496, tokens/sec=1371328.90, grad_norm=0.2815, duration=0.38s
Step 7497: loss=3.0180, lr=0.000496, tokens/sec=1376455.08, grad_norm=0.2866, duration=0.38s
Step 7498: loss=2.9848, lr=0.000496, tokens/sec=1372506.63, grad_norm=0.2819, duration=0.38s
Step 7499: loss=2.9757, lr=0.000496, tokens/sec=1371457.19, grad_norm=0.2874, duration=0.38s
Step 7500/19073 (39.3%), Elapsed time: 2978.62s, Steps per hour: 9064.61, Estimated hours remaining: 1.28
Validation loss at step 7500: 3.7427453994750977
Step 7500: loss=2.9530, lr=0.000496, tokens/sec=154944.22, grad_norm=0.2996, duration=3.38s
Step 7501: loss=3.0224, lr=0.000496, tokens/sec=1379382.90, grad_norm=0.2771, duration=0.38s
Step 7502: loss=2.9985, lr=0.000496, tokens/sec=1378334.15, grad_norm=0.2821, duration=0.38s
Step 7503: loss=3.0998, lr=0.000496, tokens/sec=1377691.69, grad_norm=0.2900, duration=0.38s
Step 7504: loss=3.1242, lr=0.000496, tokens/sec=1375867.73, grad_norm=0.2919, duration=0.38s
Step 7505: loss=3.0939, lr=0.000496, tokens/sec=1377355.15, grad_norm=0.3199, duration=0.38s
Step 7506: loss=3.1062, lr=0.000496, tokens/sec=1376059.73, grad_norm=0.2845, duration=0.38s
Step 7507: loss=3.0721, lr=0.000496, tokens/sec=1377012.74, grad_norm=0.3112, duration=0.38s
Step 7508: loss=3.0538, lr=0.000496, tokens/sec=1371750.63, grad_norm=0.3193, duration=0.38s
Step 7509: loss=3.1108, lr=0.000496, tokens/sec=1377779.73, grad_norm=0.3242, duration=0.38s
Step 7510: loss=3.0716, lr=0.000496, tokens/sec=1377690.83, grad_norm=0.3733, duration=0.38s
Step 7511: loss=3.1618, lr=0.000496, tokens/sec=1376607.59, grad_norm=0.3259, duration=0.38s
Step 7512: loss=3.1372, lr=0.000496, tokens/sec=1375754.11, grad_norm=0.3746, duration=0.38s
Step 7513: loss=3.0888, lr=0.000496, tokens/sec=1376299.15, grad_norm=0.3062, duration=0.38s
Step 7514: loss=3.1333, lr=0.000496, tokens/sec=1375865.15, grad_norm=0.3469, duration=0.38s
Step 7515: loss=3.0652, lr=0.000495, tokens/sec=1376422.34, grad_norm=0.3151, duration=0.38s
Step 7516: loss=3.0699, lr=0.000495, tokens/sec=1374580.26, grad_norm=0.3141, duration=0.38s
Step 7517: loss=3.0942, lr=0.000495, tokens/sec=1376583.46, grad_norm=0.3064, duration=0.38s
Step 7518: loss=3.0903, lr=0.000495, tokens/sec=1374750.41, grad_norm=0.3184, duration=0.38s
Step 7519: loss=3.0334, lr=0.000495, tokens/sec=1373108.25, grad_norm=0.2913, duration=0.38s
Step 7520: loss=3.0716, lr=0.000495, tokens/sec=1373667.50, grad_norm=0.3151, duration=0.38s
Step 7521: loss=3.0549, lr=0.000495, tokens/sec=1372152.07, grad_norm=0.3056, duration=0.38s
Step 7522: loss=3.0818, lr=0.000495, tokens/sec=1374264.99, grad_norm=0.2835, duration=0.38s
Step 7523: loss=3.0201, lr=0.000495, tokens/sec=1376648.96, grad_norm=0.2818, duration=0.38s
Step 7524: loss=3.1146, lr=0.000495, tokens/sec=1375430.56, grad_norm=0.3042, duration=0.38s
Step 7525: loss=3.0337, lr=0.000495, tokens/sec=1375300.67, grad_norm=0.2875, duration=0.38s
Step 7526: loss=3.0802, lr=0.000495, tokens/sec=1372590.58, grad_norm=0.2954, duration=0.38s
Step 7527: loss=3.0650, lr=0.000495, tokens/sec=1374950.69, grad_norm=0.2978, duration=0.38s
Step 7528: loss=3.0721, lr=0.000495, tokens/sec=1373855.45, grad_norm=0.2950, duration=0.38s
Step 7529: loss=3.0715, lr=0.000495, tokens/sec=1375071.91, grad_norm=0.2882, duration=0.38s
Step 7530: loss=3.0730, lr=0.000495, tokens/sec=1373757.61, grad_norm=0.2922, duration=0.38s
Step 7531: loss=3.0945, lr=0.000495, tokens/sec=1373655.49, grad_norm=0.2869, duration=0.38s
Step 7532: loss=3.0832, lr=0.000495, tokens/sec=1377572.59, grad_norm=0.2848, duration=0.38s
Step 7533: loss=3.0612, lr=0.000495, tokens/sec=1377261.12, grad_norm=0.3267, duration=0.38s
Step 7534: loss=3.0740, lr=0.000495, tokens/sec=1376561.06, grad_norm=0.3006, duration=0.38s
Step 7535: loss=3.0796, lr=0.000495, tokens/sec=1374228.06, grad_norm=0.2697, duration=0.38s
Step 7536: loss=3.0503, lr=0.000495, tokens/sec=1373677.80, grad_norm=0.2865, duration=0.38s
Step 7537: loss=3.0476, lr=0.000495, tokens/sec=1377102.43, grad_norm=0.2875, duration=0.38s
Step 7538: loss=3.0522, lr=0.000495, tokens/sec=1375220.68, grad_norm=0.2935, duration=0.38s
Step 7539: loss=2.9924, lr=0.000495, tokens/sec=1378614.99, grad_norm=0.3016, duration=0.38s
Step 7540: loss=3.0175, lr=0.000495, tokens/sec=1372770.52, grad_norm=0.2855, duration=0.38s
Step 7541: loss=2.9329, lr=0.000495, tokens/sec=1378989.33, grad_norm=0.3156, duration=0.38s
Step 7542: loss=3.0056, lr=0.000495, tokens/sec=1375473.58, grad_norm=0.3196, duration=0.38s
Step 7543: loss=3.0012, lr=0.000495, tokens/sec=1376673.95, grad_norm=0.3144, duration=0.38s
Step 7544: loss=3.0387, lr=0.000495, tokens/sec=1373494.19, grad_norm=0.3304, duration=0.38s
Step 7545: loss=3.0036, lr=0.000495, tokens/sec=1376471.45, grad_norm=0.3069, duration=0.38s
Step 7546: loss=2.9905, lr=0.000495, tokens/sec=1379093.97, grad_norm=0.3042, duration=0.38s
Step 7547: loss=2.9687, lr=0.000495, tokens/sec=1378328.11, grad_norm=0.3307, duration=0.38s
Step 7548: loss=3.0155, lr=0.000495, tokens/sec=1375541.55, grad_norm=0.2814, duration=0.38s
Step 7549: loss=3.0351, lr=0.000495, tokens/sec=1378726.49, grad_norm=0.2959, duration=0.38s
Step 7550: loss=3.0424, lr=0.000495, tokens/sec=1378316.01, grad_norm=0.3104, duration=0.38s
Step 7551: loss=3.0720, lr=0.000494, tokens/sec=1375482.18, grad_norm=0.2911, duration=0.38s
Step 7552: loss=3.0770, lr=0.000494, tokens/sec=1374423.90, grad_norm=0.2962, duration=0.38s
Step 7553: loss=3.0620, lr=0.000494, tokens/sec=1376639.48, grad_norm=0.2824, duration=0.38s
Step 7554: loss=3.0898, lr=0.000494, tokens/sec=1377673.56, grad_norm=0.2886, duration=0.38s
Step 7555: loss=3.0085, lr=0.000494, tokens/sec=1375428.84, grad_norm=0.2987, duration=0.38s
Step 7556: loss=3.0515, lr=0.000494, tokens/sec=1376716.19, grad_norm=0.2870, duration=0.38s
Step 7557: loss=3.1410, lr=0.000494, tokens/sec=1377412.96, grad_norm=0.3127, duration=0.38s
Step 7558: loss=3.2085, lr=0.000494, tokens/sec=1378462.89, grad_norm=0.3003, duration=0.38s
Step 7559: loss=3.1368, lr=0.000494, tokens/sec=1377368.09, grad_norm=0.3086, duration=0.38s
Step 7560: loss=3.1026, lr=0.000494, tokens/sec=1375278.31, grad_norm=0.3004, duration=0.38s
Step 7561: loss=3.0659, lr=0.000494, tokens/sec=1375343.68, grad_norm=0.2864, duration=0.38s
Step 7562: loss=3.0866, lr=0.000494, tokens/sec=1378066.39, grad_norm=0.3108, duration=0.38s
Step 7563: loss=3.0743, lr=0.000494, tokens/sec=1379937.75, grad_norm=0.2885, duration=0.38s
Step 7564: loss=2.9955, lr=0.000494, tokens/sec=1373133.12, grad_norm=0.3027, duration=0.38s
Step 7565: loss=3.1141, lr=0.000494, tokens/sec=1379458.18, grad_norm=0.3035, duration=0.38s
Step 7566: loss=3.0227, lr=0.000494, tokens/sec=1377314.61, grad_norm=0.3140, duration=0.38s
Step 7567: loss=3.1414, lr=0.000494, tokens/sec=1372892.22, grad_norm=0.3032, duration=0.38s
Step 7568: loss=3.1747, lr=0.000494, tokens/sec=1375512.30, grad_norm=0.3245, duration=0.38s
Step 7569: loss=3.0917, lr=0.000494, tokens/sec=1376820.48, grad_norm=0.3092, duration=0.38s
Step 7570: loss=3.0366, lr=0.000494, tokens/sec=1377111.91, grad_norm=0.3178, duration=0.38s
Step 7571: loss=3.1216, lr=0.000494, tokens/sec=1376332.74, grad_norm=0.3287, duration=0.38s
Step 7572: loss=3.0789, lr=0.000494, tokens/sec=1377869.52, grad_norm=0.2946, duration=0.38s
Step 7573: loss=3.0897, lr=0.000494, tokens/sec=1379721.30, grad_norm=0.3137, duration=0.38s
Step 7574: loss=3.0151, lr=0.000494, tokens/sec=1374983.36, grad_norm=0.2907, duration=0.38s
Step 7575: loss=3.0970, lr=0.000494, tokens/sec=1373796.23, grad_norm=0.3082, duration=0.38s
Step 7576: loss=3.0572, lr=0.000494, tokens/sec=1369678.70, grad_norm=0.3186, duration=0.38s
Step 7577: loss=3.0917, lr=0.000494, tokens/sec=1372432.96, grad_norm=0.3056, duration=0.38s
Step 7578: loss=3.0850, lr=0.000494, tokens/sec=1376048.53, grad_norm=0.3148, duration=0.38s
Step 7579: loss=3.0439, lr=0.000494, tokens/sec=1372500.63, grad_norm=0.3020, duration=0.38s
Step 7580: loss=3.0559, lr=0.000494, tokens/sec=1372574.31, grad_norm=0.2948, duration=0.38s
Step 7581: loss=3.0402, lr=0.000494, tokens/sec=1374032.29, grad_norm=0.2823, duration=0.38s
Step 7582: loss=3.0140, lr=0.000494, tokens/sec=1377297.35, grad_norm=0.3040, duration=0.38s
Step 7583: loss=3.0369, lr=0.000494, tokens/sec=1375704.19, grad_norm=0.2791, duration=0.38s
Step 7584: loss=3.0011, lr=0.000494, tokens/sec=1374066.63, grad_norm=0.2816, duration=0.38s
Step 7585: loss=2.9781, lr=0.000494, tokens/sec=1374447.95, grad_norm=0.2724, duration=0.38s
Step 7586: loss=2.9907, lr=0.000493, tokens/sec=1376474.89, grad_norm=0.2792, duration=0.38s
Step 7587: loss=3.0834, lr=0.000493, tokens/sec=1375614.69, grad_norm=0.2625, duration=0.38s
Step 7588: loss=2.9969, lr=0.000493, tokens/sec=1375040.96, grad_norm=0.2829, duration=0.38s
Step 7589: loss=2.9693, lr=0.000493, tokens/sec=1377784.05, grad_norm=0.2651, duration=0.38s
Step 7590: loss=2.9521, lr=0.000493, tokens/sec=1375983.10, grad_norm=0.2870, duration=0.38s
Step 7591: loss=2.9986, lr=0.000493, tokens/sec=1377584.67, grad_norm=0.2806, duration=0.38s
Step 7592: loss=3.0495, lr=0.000493, tokens/sec=1373789.36, grad_norm=0.2688, duration=0.38s
Step 7593: loss=3.0009, lr=0.000493, tokens/sec=1376575.71, grad_norm=0.2793, duration=0.38s
Step 7594: loss=3.0287, lr=0.000493, tokens/sec=1376508.50, grad_norm=0.2841, duration=0.38s
Step 7595: loss=3.0455, lr=0.000493, tokens/sec=1372807.37, grad_norm=0.2831, duration=0.38s
Step 7596: loss=3.0705, lr=0.000493, tokens/sec=1375131.25, grad_norm=0.2887, duration=0.38s
Step 7597: loss=3.0483, lr=0.000493, tokens/sec=1370990.34, grad_norm=0.2960, duration=0.38s
Step 7598: loss=3.0484, lr=0.000493, tokens/sec=1376959.28, grad_norm=0.2920, duration=0.38s
Step 7599: loss=3.0707, lr=0.000493, tokens/sec=1373902.66, grad_norm=0.2875, duration=0.38s
Step 7600/19073 (39.8%), Elapsed time: 3019.81s, Steps per hour: 9060.16, Estimated hours remaining: 1.27
Step 7600: loss=3.0255, lr=0.000493, tokens/sec=1377457.82, grad_norm=0.3004, duration=0.38s
Step 7601: loss=3.1019, lr=0.000493, tokens/sec=1378383.40, grad_norm=0.3130, duration=0.38s
Step 7602: loss=3.0897, lr=0.000493, tokens/sec=1377857.43, grad_norm=0.3078, duration=0.38s
Step 7603: loss=3.0929, lr=0.000493, tokens/sec=1375683.54, grad_norm=0.3262, duration=0.38s
Step 7604: loss=3.0806, lr=0.000493, tokens/sec=1375744.64, grad_norm=0.3154, duration=0.38s
Step 7605: loss=3.1028, lr=0.000493, tokens/sec=1377041.20, grad_norm=0.3228, duration=0.38s
Step 7606: loss=3.1848, lr=0.000493, tokens/sec=1372754.24, grad_norm=0.3160, duration=0.38s
Step 7607: loss=3.1083, lr=0.000493, tokens/sec=1373045.67, grad_norm=0.3254, duration=0.38s
Step 7608: loss=3.1338, lr=0.000493, tokens/sec=1374867.30, grad_norm=0.3289, duration=0.38s
Step 7609: loss=3.0855, lr=0.000493, tokens/sec=1375288.63, grad_norm=0.3054, duration=0.38s
Step 7610: loss=3.1145, lr=0.000493, tokens/sec=1376845.48, grad_norm=0.3269, duration=0.38s
Step 7611: loss=3.1124, lr=0.000493, tokens/sec=1374502.93, grad_norm=0.2969, duration=0.38s
Step 7612: loss=3.1173, lr=0.000493, tokens/sec=1374464.27, grad_norm=0.3247, duration=0.38s
Step 7613: loss=3.0379, lr=0.000493, tokens/sec=1376688.60, grad_norm=0.3028, duration=0.38s
Step 7614: loss=3.1115, lr=0.000493, tokens/sec=1374725.48, grad_norm=0.3291, duration=0.38s
Step 7615: loss=3.0868, lr=0.000493, tokens/sec=1371680.47, grad_norm=0.3236, duration=0.38s
Step 7616: loss=3.0537, lr=0.000493, tokens/sec=1379331.85, grad_norm=0.3684, duration=0.38s
Step 7617: loss=3.1259, lr=0.000493, tokens/sec=1373504.48, grad_norm=0.2971, duration=0.38s
Step 7618: loss=3.0262, lr=0.000493, tokens/sec=1372145.22, grad_norm=0.3689, duration=0.38s
Step 7619: loss=2.9900, lr=0.000493, tokens/sec=1375217.24, grad_norm=0.3327, duration=0.38s
Step 7620: loss=3.0464, lr=0.000493, tokens/sec=1372915.37, grad_norm=0.3354, duration=0.38s
Step 7621: loss=3.0764, lr=0.000492, tokens/sec=1374233.22, grad_norm=0.3290, duration=0.38s
Step 7622: loss=3.1798, lr=0.000492, tokens/sec=1377435.39, grad_norm=0.3013, duration=0.38s
Step 7623: loss=3.0032, lr=0.000492, tokens/sec=1376834.28, grad_norm=0.3082, duration=0.38s
Step 7624: loss=3.1166, lr=0.000492, tokens/sec=1372229.13, grad_norm=0.3284, duration=0.38s
Step 7625: loss=3.1198, lr=0.000492, tokens/sec=1376879.97, grad_norm=0.3148, duration=0.38s
Step 7626: loss=3.0276, lr=0.000492, tokens/sec=1375473.58, grad_norm=0.3113, duration=0.38s
Step 7627: loss=3.0865, lr=0.000492, tokens/sec=1373462.45, grad_norm=0.3025, duration=0.38s
Step 7628: loss=3.0936, lr=0.000492, tokens/sec=1376126.03, grad_norm=0.3072, duration=0.38s
Step 7629: loss=3.0543, lr=0.000492, tokens/sec=1372395.27, grad_norm=0.2968, duration=0.38s
Step 7630: loss=3.0440, lr=0.000492, tokens/sec=1371768.60, grad_norm=0.3310, duration=0.38s
Step 7631: loss=3.0670, lr=0.000492, tokens/sec=1371538.45, grad_norm=0.3049, duration=0.38s
Step 7632: loss=3.0836, lr=0.000492, tokens/sec=1371949.18, grad_norm=0.3372, duration=0.38s
Step 7633: loss=3.1243, lr=0.000492, tokens/sec=1375037.52, grad_norm=0.2994, duration=0.38s
Step 7634: loss=3.0511, lr=0.000492, tokens/sec=1378686.73, grad_norm=0.3206, duration=0.38s
Step 7635: loss=2.9854, lr=0.000492, tokens/sec=1377581.22, grad_norm=0.3147, duration=0.38s
Step 7636: loss=3.0402, lr=0.000492, tokens/sec=1373685.52, grad_norm=0.3116, duration=0.38s
Step 7637: loss=3.0108, lr=0.000492, tokens/sec=1376994.64, grad_norm=0.2973, duration=0.38s
Step 7638: loss=2.9717, lr=0.000492, tokens/sec=1376955.84, grad_norm=0.3121, duration=0.38s
Step 7639: loss=2.9943, lr=0.000492, tokens/sec=1374525.27, grad_norm=0.2821, duration=0.38s
Step 7640: loss=3.0022, lr=0.000492, tokens/sec=1375045.26, grad_norm=0.2974, duration=0.38s
Step 7641: loss=3.0131, lr=0.000492, tokens/sec=1375286.05, grad_norm=0.2915, duration=0.38s
Step 7642: loss=3.0151, lr=0.000492, tokens/sec=1375872.90, grad_norm=0.2907, duration=0.38s
Step 7643: loss=3.0036, lr=0.000492, tokens/sec=1375102.87, grad_norm=0.2783, duration=0.38s
Step 7644: loss=2.9952, lr=0.000492, tokens/sec=1374636.11, grad_norm=0.2909, duration=0.38s
Step 7645: loss=2.9837, lr=0.000492, tokens/sec=1373821.97, grad_norm=0.2859, duration=0.38s
Step 7646: loss=2.9924, lr=0.000492, tokens/sec=1373249.74, grad_norm=0.2945, duration=0.38s
Step 7647: loss=3.0798, lr=0.000492, tokens/sec=1377739.16, grad_norm=0.2924, duration=0.38s
Step 7648: loss=3.1292, lr=0.000492, tokens/sec=1373787.64, grad_norm=0.3068, duration=0.38s
Step 7649: loss=3.0157, lr=0.000492, tokens/sec=1374945.53, grad_norm=0.3113, duration=0.38s
Step 7650: loss=3.0698, lr=0.000492, tokens/sec=1377017.06, grad_norm=0.3120, duration=0.38s
Step 7651: loss=3.0902, lr=0.000492, tokens/sec=1373864.89, grad_norm=0.3094, duration=0.38s
Step 7652: loss=3.1243, lr=0.000492, tokens/sec=1372741.39, grad_norm=0.3336, duration=0.38s
Step 7653: loss=3.1144, lr=0.000492, tokens/sec=1374419.60, grad_norm=0.2931, duration=0.38s
Step 7654: loss=3.0762, lr=0.000492, tokens/sec=1374905.12, grad_norm=0.3177, duration=0.38s
Step 7655: loss=3.0625, lr=0.000492, tokens/sec=1375946.07, grad_norm=0.3031, duration=0.38s
Step 7656: loss=3.0904, lr=0.000491, tokens/sec=1379732.55, grad_norm=0.3037, duration=0.38s
Step 7657: loss=3.0593, lr=0.000491, tokens/sec=1374966.16, grad_norm=0.2982, duration=0.38s
Step 7658: loss=3.0248, lr=0.000491, tokens/sec=1380455.78, grad_norm=0.2978, duration=0.38s
Step 7659: loss=3.0690, lr=0.000491, tokens/sec=1373668.36, grad_norm=0.3129, duration=0.38s
Step 7660: loss=3.0717, lr=0.000491, tokens/sec=1372529.76, grad_norm=0.3042, duration=0.38s
Step 7661: loss=2.9879, lr=0.000491, tokens/sec=1374363.77, grad_norm=0.3097, duration=0.38s
Step 7662: loss=3.0730, lr=0.000491, tokens/sec=1374475.44, grad_norm=0.3199, duration=0.38s
Step 7663: loss=3.0783, lr=0.000491, tokens/sec=1375228.42, grad_norm=0.3059, duration=0.38s
Step 7664: loss=3.0873, lr=0.000491, tokens/sec=1376757.56, grad_norm=0.3320, duration=0.38s
Step 7665: loss=3.1239, lr=0.000491, tokens/sec=1372355.87, grad_norm=0.3285, duration=0.38s
Step 7666: loss=3.0476, lr=0.000491, tokens/sec=1376322.41, grad_norm=0.3266, duration=0.38s
Step 7667: loss=3.1443, lr=0.000491, tokens/sec=1374252.11, grad_norm=0.3256, duration=0.38s
Step 7668: loss=3.0536, lr=0.000491, tokens/sec=1373735.29, grad_norm=0.3146, duration=0.38s
Step 7669: loss=3.0661, lr=0.000491, tokens/sec=1377778.01, grad_norm=0.3323, duration=0.38s
Step 7670: loss=3.0398, lr=0.000491, tokens/sec=1372327.61, grad_norm=0.3169, duration=0.38s
Step 7671: loss=3.0769, lr=0.000491, tokens/sec=1374404.14, grad_norm=0.3247, duration=0.38s
Step 7672: loss=3.0477, lr=0.000491, tokens/sec=1376276.75, grad_norm=0.3352, duration=0.38s
Step 7673: loss=3.0368, lr=0.000491, tokens/sec=1378227.04, grad_norm=0.3142, duration=0.38s
Step 7674: loss=3.0607, lr=0.000491, tokens/sec=1374314.81, grad_norm=0.3253, duration=0.38s
Step 7675: loss=3.0466, lr=0.000491, tokens/sec=1372340.46, grad_norm=0.3086, duration=0.38s
Step 7676: loss=3.0640, lr=0.000491, tokens/sec=1377291.32, grad_norm=0.3142, duration=0.38s
Step 7677: loss=3.0370, lr=0.000491, tokens/sec=1379329.26, grad_norm=0.3184, duration=0.38s
Step 7678: loss=3.0747, lr=0.000491, tokens/sec=1375702.47, grad_norm=0.3019, duration=0.38s
Step 7679: loss=3.0546, lr=0.000491, tokens/sec=1373870.04, grad_norm=0.3152, duration=0.38s
Step 7680: loss=3.0899, lr=0.000491, tokens/sec=1371632.55, grad_norm=0.2946, duration=0.38s
Step 7681: loss=2.9721, lr=0.000491, tokens/sec=1376469.72, grad_norm=0.2923, duration=0.38s
Step 7682: loss=3.0437, lr=0.000491, tokens/sec=1376292.26, grad_norm=0.3061, duration=0.38s
Step 7683: loss=2.9987, lr=0.000491, tokens/sec=1378576.96, grad_norm=0.2871, duration=0.38s
Step 7684: loss=3.0065, lr=0.000491, tokens/sec=1375179.40, grad_norm=0.2821, duration=0.38s
Step 7685: loss=2.9366, lr=0.000491, tokens/sec=1374335.42, grad_norm=0.3078, duration=0.38s
Step 7686: loss=2.9870, lr=0.000491, tokens/sec=1374353.46, grad_norm=0.2792, duration=0.38s
Step 7687: loss=3.0222, lr=0.000491, tokens/sec=1377969.67, grad_norm=0.3003, duration=0.38s
Step 7688: loss=2.9461, lr=0.000491, tokens/sec=1370859.57, grad_norm=0.3126, duration=0.38s
Step 7689: loss=2.9604, lr=0.000491, tokens/sec=1377620.05, grad_norm=0.2832, duration=0.38s
Step 7690: loss=2.9603, lr=0.000491, tokens/sec=1376568.82, grad_norm=0.3177, duration=0.38s
Step 7691: loss=3.0081, lr=0.000490, tokens/sec=1373362.09, grad_norm=0.3208, duration=0.38s
Step 7692: loss=3.0609, lr=0.000490, tokens/sec=1371749.77, grad_norm=0.2908, duration=0.38s
Step 7693: loss=3.0742, lr=0.000490, tokens/sec=1377767.65, grad_norm=0.3335, duration=0.38s
Step 7694: loss=3.0770, lr=0.000490, tokens/sec=1376949.80, grad_norm=0.3029, duration=0.38s
Step 7695: loss=3.0930, lr=0.000490, tokens/sec=1378801.70, grad_norm=0.3179, duration=0.38s
Step 7696: loss=3.0651, lr=0.000490, tokens/sec=1372470.65, grad_norm=0.3172, duration=0.38s
Step 7697: loss=3.1037, lr=0.000490, tokens/sec=1374506.37, grad_norm=0.2893, duration=0.38s
Step 7698: loss=3.0594, lr=0.000490, tokens/sec=1377507.87, grad_norm=0.3337, duration=0.38s
Step 7699: loss=3.0948, lr=0.000490, tokens/sec=1375470.14, grad_norm=0.3160, duration=0.38s
Step 7700/19073 (40.4%), Elapsed time: 3058.03s, Steps per hour: 9064.66, Estimated hours remaining: 1.25
Step 7700: loss=3.0917, lr=0.000490, tokens/sec=1375485.62, grad_norm=0.3537, duration=0.38s
Step 7701: loss=3.1361, lr=0.000490, tokens/sec=1373408.41, grad_norm=0.3345, duration=0.38s
Step 7702: loss=3.0956, lr=0.000490, tokens/sec=1375115.77, grad_norm=0.3342, duration=0.38s
Step 7703: loss=3.1174, lr=0.000490, tokens/sec=1374520.11, grad_norm=0.3226, duration=0.38s
Step 7704: loss=3.0892, lr=0.000490, tokens/sec=1378272.82, grad_norm=0.2984, duration=0.38s
Step 7705: loss=3.0507, lr=0.000490, tokens/sec=1377265.44, grad_norm=0.3434, duration=0.38s
Step 7706: loss=3.0883, lr=0.000490, tokens/sec=1375656.00, grad_norm=0.2947, duration=0.38s
Step 7707: loss=3.0721, lr=0.000490, tokens/sec=1368972.68, grad_norm=0.3289, duration=0.38s
Step 7708: loss=3.0755, lr=0.000490, tokens/sec=1375936.60, grad_norm=0.2994, duration=0.38s
Step 7709: loss=3.0093, lr=0.000490, tokens/sec=1375206.06, grad_norm=0.3343, duration=0.38s
Step 7710: loss=3.0569, lr=0.000490, tokens/sec=1375103.73, grad_norm=0.2938, duration=0.38s
Step 7711: loss=3.0685, lr=0.000490, tokens/sec=1380803.37, grad_norm=0.3132, duration=0.38s
Step 7712: loss=3.0719, lr=0.000490, tokens/sec=1373622.88, grad_norm=0.2864, duration=0.38s
Step 7713: loss=3.0347, lr=0.000490, tokens/sec=1377297.35, grad_norm=0.3047, duration=0.38s
Step 7714: loss=3.0775, lr=0.000490, tokens/sec=1371062.14, grad_norm=0.2806, duration=0.38s
Step 7715: loss=3.0202, lr=0.000490, tokens/sec=1377342.21, grad_norm=0.3034, duration=0.38s
Step 7716: loss=3.0850, lr=0.000490, tokens/sec=1376740.32, grad_norm=0.3035, duration=0.38s
Step 7717: loss=3.0427, lr=0.000490, tokens/sec=1376106.23, grad_norm=0.2942, duration=0.38s
Step 7718: loss=3.0810, lr=0.000490, tokens/sec=1378778.36, grad_norm=0.3121, duration=0.38s
Step 7719: loss=3.0568, lr=0.000490, tokens/sec=1372536.61, grad_norm=0.2934, duration=0.38s
Step 7720: loss=3.0909, lr=0.000490, tokens/sec=1375861.71, grad_norm=0.3005, duration=0.38s
Step 7721: loss=3.0690, lr=0.000490, tokens/sec=1376884.28, grad_norm=0.3096, duration=0.38s
Step 7722: loss=3.0673, lr=0.000490, tokens/sec=1375985.68, grad_norm=0.2942, duration=0.38s
Step 7723: loss=3.0500, lr=0.000490, tokens/sec=1375700.75, grad_norm=0.2945, duration=0.38s
Step 7724: loss=3.0697, lr=0.000490, tokens/sec=1377362.05, grad_norm=0.3014, duration=0.38s
Step 7725: loss=3.0554, lr=0.000489, tokens/sec=1376973.94, grad_norm=0.2894, duration=0.38s
Step 7726: loss=3.0462, lr=0.000489, tokens/sec=1377573.45, grad_norm=0.2881, duration=0.38s
Step 7727: loss=3.0415, lr=0.000489, tokens/sec=1376786.00, grad_norm=0.2926, duration=0.38s
Step 7728: loss=3.0413, lr=0.000489, tokens/sec=1374005.67, grad_norm=0.3062, duration=0.38s
Step 7729: loss=2.9713, lr=0.000489, tokens/sec=1374481.45, grad_norm=0.2688, duration=0.38s
Step 7730: loss=3.0013, lr=0.000489, tokens/sec=1375447.77, grad_norm=0.3121, duration=0.38s
Step 7731: loss=2.9533, lr=0.000489, tokens/sec=1375141.57, grad_norm=0.2955, duration=0.38s
Step 7732: loss=2.9956, lr=0.000489, tokens/sec=1377827.21, grad_norm=0.3262, duration=0.38s
Step 7733: loss=2.9663, lr=0.000489, tokens/sec=1376087.28, grad_norm=0.3305, duration=0.38s
Step 7734: loss=3.0215, lr=0.000489, tokens/sec=1377127.44, grad_norm=0.2826, duration=0.38s
Step 7735: loss=3.0041, lr=0.000489, tokens/sec=1378332.43, grad_norm=0.3503, duration=0.38s
Step 7736: loss=2.9713, lr=0.000489, tokens/sec=1373033.66, grad_norm=0.2868, duration=0.38s
Step 7737: loss=2.9768, lr=0.000489, tokens/sec=1370998.88, grad_norm=0.3086, duration=0.38s
Step 7738: loss=2.9895, lr=0.000489, tokens/sec=1374384.38, grad_norm=0.3137, duration=0.38s
Step 7739: loss=3.0095, lr=0.000489, tokens/sec=1376035.62, grad_norm=0.2762, duration=0.38s
Step 7740: loss=3.0477, lr=0.000489, tokens/sec=1375616.41, grad_norm=0.3087, duration=0.38s
Step 7741: loss=3.0636, lr=0.000489, tokens/sec=1378530.30, grad_norm=0.3056, duration=0.38s
Step 7742: loss=3.0750, lr=0.000489, tokens/sec=1375508.86, grad_norm=0.3056, duration=0.38s
Step 7743: loss=3.0531, lr=0.000489, tokens/sec=1377499.24, grad_norm=0.3044, duration=0.38s
Step 7744: loss=3.0535, lr=0.000489, tokens/sec=1374688.53, grad_norm=0.2873, duration=0.38s
Step 7745: loss=3.0226, lr=0.000489, tokens/sec=1375699.03, grad_norm=0.2994, duration=0.38s
Step 7746: loss=3.0881, lr=0.000489, tokens/sec=1374831.20, grad_norm=0.2953, duration=0.38s
Step 7747: loss=3.1348, lr=0.000489, tokens/sec=1374084.66, grad_norm=0.2831, duration=0.38s
Step 7748: loss=3.1882, lr=0.000489, tokens/sec=1376349.11, grad_norm=0.3088, duration=0.38s
Step 7749: loss=3.1063, lr=0.000489, tokens/sec=1377854.84, grad_norm=0.2950, duration=0.38s
Validation loss at step 7750: 3.7336950302124023
Step 7750: loss=3.1143, lr=0.000489, tokens/sec=153510.38, grad_norm=0.3112, duration=3.42s
Step 7751: loss=3.0414, lr=0.000489, tokens/sec=1380894.41, grad_norm=0.2784, duration=0.38s
Step 7752: loss=3.0995, lr=0.000489, tokens/sec=1372919.65, grad_norm=0.2901, duration=0.38s
Step 7753: loss=3.0219, lr=0.000489, tokens/sec=1377134.34, grad_norm=0.2946, duration=0.38s
Step 7754: loss=3.0164, lr=0.000489, tokens/sec=1376847.21, grad_norm=0.3082, duration=0.38s
Step 7755: loss=3.0982, lr=0.000489, tokens/sec=1375705.05, grad_norm=0.2958, duration=0.38s
Step 7756: loss=3.0397, lr=0.000489, tokens/sec=1371267.33, grad_norm=0.3230, duration=0.38s
Step 7757: loss=3.1385, lr=0.000489, tokens/sec=1372971.94, grad_norm=0.3008, duration=0.38s
Step 7758: loss=3.1422, lr=0.000489, tokens/sec=1372295.93, grad_norm=0.3184, duration=0.38s
Step 7759: loss=3.0799, lr=0.000489, tokens/sec=1374551.04, grad_norm=0.2958, duration=0.38s
Step 7760: loss=3.0550, lr=0.000488, tokens/sec=1375429.70, grad_norm=0.2951, duration=0.38s
Step 7761: loss=3.0502, lr=0.000488, tokens/sec=1373685.52, grad_norm=0.3317, duration=0.38s
Step 7762: loss=3.1173, lr=0.000488, tokens/sec=1374918.02, grad_norm=0.2810, duration=0.38s
Step 7763: loss=3.0794, lr=0.000488, tokens/sec=1371482.85, grad_norm=0.3199, duration=0.38s
Step 7764: loss=3.0292, lr=0.000488, tokens/sec=1376691.19, grad_norm=0.3008, duration=0.38s
Step 7765: loss=3.0460, lr=0.000488, tokens/sec=1374143.91, grad_norm=0.2864, duration=0.38s
Step 7766: loss=3.0645, lr=0.000488, tokens/sec=1374457.40, grad_norm=0.3182, duration=0.38s
Step 7767: loss=3.0499, lr=0.000488, tokens/sec=1371926.07, grad_norm=0.3241, duration=0.38s
Step 7768: loss=3.1169, lr=0.000488, tokens/sec=1376051.98, grad_norm=0.3136, duration=0.38s
Step 7769: loss=3.0395, lr=0.000488, tokens/sec=1372983.94, grad_norm=0.3109, duration=0.38s
Step 7770: loss=3.0296, lr=0.000488, tokens/sec=1377375.00, grad_norm=0.2877, duration=0.38s
Step 7771: loss=3.0484, lr=0.000488, tokens/sec=1376165.65, grad_norm=0.2967, duration=0.38s
Step 7772: loss=2.9836, lr=0.000488, tokens/sec=1375706.77, grad_norm=0.2962, duration=0.38s
Step 7773: loss=3.0323, lr=0.000488, tokens/sec=1374974.76, grad_norm=0.2925, duration=0.38s
Step 7774: loss=2.9821, lr=0.000488, tokens/sec=1374186.84, grad_norm=0.2995, duration=0.38s
Step 7775: loss=2.9804, lr=0.000488, tokens/sec=1373847.72, grad_norm=0.2791, duration=0.38s
Step 7776: loss=3.0134, lr=0.000488, tokens/sec=1376774.80, grad_norm=0.2894, duration=0.38s
Step 7777: loss=3.0407, lr=0.000488, tokens/sec=1376679.12, grad_norm=0.2878, duration=0.38s
Step 7778: loss=2.9807, lr=0.000488, tokens/sec=1376286.23, grad_norm=0.2700, duration=0.38s
Step 7779: loss=2.9736, lr=0.000488, tokens/sec=1376095.89, grad_norm=0.2717, duration=0.38s
Step 7780: loss=2.9582, lr=0.000488, tokens/sec=1376507.64, grad_norm=0.3092, duration=0.38s
Step 7781: loss=3.0067, lr=0.000488, tokens/sec=1375533.81, grad_norm=0.2718, duration=0.38s
Step 7782: loss=3.0085, lr=0.000488, tokens/sec=1373522.50, grad_norm=0.2987, duration=0.38s
Step 7783: loss=3.0018, lr=0.000488, tokens/sec=1376032.17, grad_norm=0.2667, duration=0.38s
Step 7784: loss=2.9905, lr=0.000488, tokens/sec=1378043.07, grad_norm=0.2846, duration=0.38s
Step 7785: loss=3.0607, lr=0.000488, tokens/sec=1379017.00, grad_norm=0.2764, duration=0.38s
Step 7786: loss=3.0319, lr=0.000488, tokens/sec=1376942.04, grad_norm=0.2825, duration=0.38s
Step 7787: loss=3.0732, lr=0.000488, tokens/sec=1375576.83, grad_norm=0.3016, duration=0.38s
Step 7788: loss=3.0306, lr=0.000488, tokens/sec=1376861.00, grad_norm=0.2822, duration=0.38s
Step 7789: loss=3.0408, lr=0.000488, tokens/sec=1375440.03, grad_norm=0.2967, duration=0.38s
Step 7790: loss=3.0599, lr=0.000488, tokens/sec=1381324.65, grad_norm=0.2948, duration=0.38s
Step 7791: loss=3.0766, lr=0.000488, tokens/sec=1374120.72, grad_norm=0.2967, duration=0.38s
Step 7792: loss=3.0157, lr=0.000488, tokens/sec=1374695.40, grad_norm=0.2970, duration=0.38s
Step 7793: loss=3.1366, lr=0.000488, tokens/sec=1376113.98, grad_norm=0.3112, duration=0.38s
Step 7794: loss=3.0732, lr=0.000488, tokens/sec=1377210.23, grad_norm=0.2944, duration=0.38s
Step 7795: loss=3.1300, lr=0.000487, tokens/sec=1376379.26, grad_norm=0.3229, duration=0.38s
Step 7796: loss=3.1653, lr=0.000487, tokens/sec=1373434.14, grad_norm=0.3038, duration=0.38s
Step 7797: loss=3.1016, lr=0.000487, tokens/sec=1377984.35, grad_norm=0.3138, duration=0.38s
Step 7798: loss=3.1017, lr=0.000487, tokens/sec=1374556.20, grad_norm=0.3113, duration=0.38s
Step 7799: loss=3.0896, lr=0.000487, tokens/sec=1374137.04, grad_norm=0.2956, duration=0.38s
Step 7800/19073 (40.9%), Elapsed time: 3099.27s, Steps per hour: 9060.21, Estimated hours remaining: 1.24
Step 7800: loss=3.1401, lr=0.000487, tokens/sec=1370212.95, grad_norm=0.3145, duration=0.38s
Step 7801: loss=3.0915, lr=0.000487, tokens/sec=1375048.70, grad_norm=0.2895, duration=0.38s
Step 7802: loss=3.0915, lr=0.000487, tokens/sec=1374173.96, grad_norm=0.3100, duration=0.38s
Step 7803: loss=3.0332, lr=0.000487, tokens/sec=1373356.08, grad_norm=0.2916, duration=0.38s
Step 7804: loss=3.0911, lr=0.000487, tokens/sec=1374161.08, grad_norm=0.3067, duration=0.38s
Step 7805: loss=3.0855, lr=0.000487, tokens/sec=1375100.29, grad_norm=0.3071, duration=0.38s
Step 7806: loss=3.0524, lr=0.000487, tokens/sec=1374010.82, grad_norm=0.3344, duration=0.38s
Step 7807: loss=3.0942, lr=0.000487, tokens/sec=1375563.06, grad_norm=0.2960, duration=0.38s
Step 7808: loss=3.0248, lr=0.000487, tokens/sec=1377709.82, grad_norm=0.3259, duration=0.38s
Step 7809: loss=2.9829, lr=0.000487, tokens/sec=1376693.78, grad_norm=0.3586, duration=0.38s
Step 7810: loss=3.0324, lr=0.000487, tokens/sec=1379982.78, grad_norm=0.3163, duration=0.38s
Step 7811: loss=3.0956, lr=0.000487, tokens/sec=1372432.96, grad_norm=0.3105, duration=0.38s
Step 7812: loss=3.1443, lr=0.000487, tokens/sec=1374486.61, grad_norm=0.3033, duration=0.38s
Step 7813: loss=3.0043, lr=0.000487, tokens/sec=1377442.29, grad_norm=0.3003, duration=0.38s
Step 7814: loss=3.1306, lr=0.000487, tokens/sec=1375821.25, grad_norm=0.3415, duration=0.38s
Step 7815: loss=3.0688, lr=0.000487, tokens/sec=1373846.87, grad_norm=0.3074, duration=0.38s
Step 7816: loss=3.0497, lr=0.000487, tokens/sec=1377437.98, grad_norm=0.3450, duration=0.38s
Step 7817: loss=3.0663, lr=0.000487, tokens/sec=1375717.10, grad_norm=0.3073, duration=0.38s
Step 7818: loss=3.0728, lr=0.000487, tokens/sec=1378175.21, grad_norm=0.3371, duration=0.38s
Step 7819: loss=3.0629, lr=0.000487, tokens/sec=1375865.15, grad_norm=0.2989, duration=0.38s
Step 7820: loss=3.0277, lr=0.000487, tokens/sec=1374452.24, grad_norm=0.3672, duration=0.38s
Step 7821: loss=3.0688, lr=0.000487, tokens/sec=1379997.50, grad_norm=0.3268, duration=0.38s
Step 7822: loss=3.0887, lr=0.000487, tokens/sec=1376564.51, grad_norm=0.3481, duration=0.38s
Step 7823: loss=3.1068, lr=0.000487, tokens/sec=1377033.44, grad_norm=0.3389, duration=0.38s
Step 7824: loss=3.0318, lr=0.000487, tokens/sec=1379072.35, grad_norm=0.2969, duration=0.38s
Step 7825: loss=2.9858, lr=0.000487, tokens/sec=1375952.96, grad_norm=0.3195, duration=0.38s
Step 7826: loss=3.0561, lr=0.000487, tokens/sec=1374081.23, grad_norm=0.3169, duration=0.38s
Step 7827: loss=2.9657, lr=0.000487, tokens/sec=1378935.71, grad_norm=0.2848, duration=0.38s
Step 7828: loss=2.9661, lr=0.000487, tokens/sec=1378479.31, grad_norm=0.3206, duration=0.38s
Step 7829: loss=2.9836, lr=0.000486, tokens/sec=1375600.92, grad_norm=0.3063, duration=0.38s
Step 7830: loss=3.0040, lr=0.000486, tokens/sec=1373827.98, grad_norm=0.2959, duration=0.38s
Step 7831: loss=3.0087, lr=0.000486, tokens/sec=1375095.99, grad_norm=0.3216, duration=0.38s
Step 7832: loss=2.9783, lr=0.000486, tokens/sec=1376936.87, grad_norm=0.2840, duration=0.38s
Step 7833: loss=3.0146, lr=0.000486, tokens/sec=1374477.16, grad_norm=0.3074, duration=0.38s
Step 7834: loss=2.9819, lr=0.000486, tokens/sec=1373040.52, grad_norm=0.2998, duration=0.38s
Step 7835: loss=2.9715, lr=0.000486, tokens/sec=1378544.99, grad_norm=0.2902, duration=0.38s
Step 7836: loss=2.9926, lr=0.000486, tokens/sec=1375449.49, grad_norm=0.3135, duration=0.38s
Step 7837: loss=3.0792, lr=0.000486, tokens/sec=1376527.45, grad_norm=0.3266, duration=0.38s
Step 7838: loss=3.0550, lr=0.000486, tokens/sec=1378015.44, grad_norm=0.3154, duration=0.38s
Step 7839: loss=3.0665, lr=0.000486, tokens/sec=1376948.94, grad_norm=0.3094, duration=0.38s
Step 7840: loss=3.0609, lr=0.000486, tokens/sec=1376512.81, grad_norm=0.3058, duration=0.38s
Step 7841: loss=3.1204, lr=0.000486, tokens/sec=1380223.57, grad_norm=0.2981, duration=0.38s
Step 7842: loss=3.1111, lr=0.000486, tokens/sec=1379794.88, grad_norm=0.3058, duration=0.38s
Step 7843: loss=3.1067, lr=0.000486, tokens/sec=1376349.11, grad_norm=0.3037, duration=0.38s
Step 7844: loss=3.0564, lr=0.000486, tokens/sec=1376757.56, grad_norm=0.2775, duration=0.38s
Step 7845: loss=3.0565, lr=0.000486, tokens/sec=1374740.95, grad_norm=0.3071, duration=0.38s
Step 7846: loss=3.0650, lr=0.000486, tokens/sec=1375164.78, grad_norm=0.2910, duration=0.38s
Step 7847: loss=3.0294, lr=0.000486, tokens/sec=1375556.18, grad_norm=0.3115, duration=0.38s
Step 7848: loss=3.0646, lr=0.000486, tokens/sec=1379619.15, grad_norm=0.2994, duration=0.38s
Step 7849: loss=3.0617, lr=0.000486, tokens/sec=1374287.32, grad_norm=0.2924, duration=0.38s
Step 7850: loss=3.0543, lr=0.000486, tokens/sec=1375270.57, grad_norm=0.2903, duration=0.38s
Step 7851: loss=3.0089, lr=0.000486, tokens/sec=1379095.70, grad_norm=0.3010, duration=0.38s
Step 7852: loss=3.0317, lr=0.000486, tokens/sec=1374762.44, grad_norm=0.3030, duration=0.38s
Step 7853: loss=3.0905, lr=0.000486, tokens/sec=1378571.78, grad_norm=0.2922, duration=0.38s
Step 7854: loss=3.0692, lr=0.000486, tokens/sec=1375836.74, grad_norm=0.3051, duration=0.38s
Step 7855: loss=3.1064, lr=0.000486, tokens/sec=1377847.07, grad_norm=0.3056, duration=0.38s
Step 7856: loss=3.0582, lr=0.000486, tokens/sec=1380243.49, grad_norm=0.3153, duration=0.38s
Step 7857: loss=3.0814, lr=0.000486, tokens/sec=1377284.41, grad_norm=0.3194, duration=0.38s
Step 7858: loss=3.0719, lr=0.000486, tokens/sec=1375999.45, grad_norm=0.3379, duration=0.38s
Step 7859: loss=3.0298, lr=0.000486, tokens/sec=1372408.12, grad_norm=0.3227, duration=0.38s
Step 7860: loss=3.0666, lr=0.000486, tokens/sec=1378674.63, grad_norm=0.3478, duration=0.38s
Step 7861: loss=3.0618, lr=0.000486, tokens/sec=1374875.04, grad_norm=0.3074, duration=0.38s
Step 7862: loss=3.0108, lr=0.000486, tokens/sec=1379713.50, grad_norm=0.3498, duration=0.38s
Step 7863: loss=3.0566, lr=0.000486, tokens/sec=1377477.67, grad_norm=0.3289, duration=0.38s
Step 7864: loss=3.0430, lr=0.000485, tokens/sec=1374889.65, grad_norm=0.3270, duration=0.38s
Step 7865: loss=3.0559, lr=0.000485, tokens/sec=1377029.99, grad_norm=0.3262, duration=0.38s
Step 7866: loss=3.0697, lr=0.000485, tokens/sec=1375817.81, grad_norm=0.2977, duration=0.38s
Step 7867: loss=3.0307, lr=0.000485, tokens/sec=1378023.21, grad_norm=0.3288, duration=0.38s
Step 7868: loss=3.0565, lr=0.000485, tokens/sec=1375518.32, grad_norm=0.3118, duration=0.38s
Step 7869: loss=3.0733, lr=0.000485, tokens/sec=1375784.24, grad_norm=0.3077, duration=0.38s
Step 7870: loss=3.0385, lr=0.000485, tokens/sec=1379618.29, grad_norm=0.3031, duration=0.38s
Step 7871: loss=2.9717, lr=0.000485, tokens/sec=1374169.67, grad_norm=0.3072, duration=0.38s
Step 7872: loss=3.0442, lr=0.000485, tokens/sec=1373806.53, grad_norm=0.3099, duration=0.38s
Step 7873: loss=2.9911, lr=0.000485, tokens/sec=1374520.97, grad_norm=0.3089, duration=0.38s
Step 7874: loss=2.9796, lr=0.000485, tokens/sec=1377293.04, grad_norm=0.2918, duration=0.38s
Step 7875: loss=2.9477, lr=0.000485, tokens/sec=1376169.09, grad_norm=0.3173, duration=0.38s
Step 7876: loss=2.9918, lr=0.000485, tokens/sec=1376563.64, grad_norm=0.2941, duration=0.38s
Step 7877: loss=2.9868, lr=0.000485, tokens/sec=1380222.70, grad_norm=0.2887, duration=0.38s
Step 7878: loss=2.9320, lr=0.000485, tokens/sec=1373331.21, grad_norm=0.3091, duration=0.38s
Step 7879: loss=2.9694, lr=0.000485, tokens/sec=1372092.14, grad_norm=0.2969, duration=0.38s
Step 7880: loss=2.9450, lr=0.000485, tokens/sec=1376810.14, grad_norm=0.2820, duration=0.38s
Step 7881: loss=3.0720, lr=0.000485, tokens/sec=1375615.55, grad_norm=0.3602, duration=0.38s
Step 7882: loss=3.0365, lr=0.000485, tokens/sec=1372265.95, grad_norm=0.2853, duration=0.38s
Step 7883: loss=3.0277, lr=0.000485, tokens/sec=1374885.35, grad_norm=0.3241, duration=0.38s
Step 7884: loss=3.0749, lr=0.000485, tokens/sec=1374909.42, grad_norm=0.3344, duration=0.38s
Step 7885: loss=3.0480, lr=0.000485, tokens/sec=1372749.96, grad_norm=0.2964, duration=0.38s
Step 7886: loss=3.0989, lr=0.000485, tokens/sec=1375681.82, grad_norm=0.3575, duration=0.38s
Step 7887: loss=3.1063, lr=0.000485, tokens/sec=1372200.02, grad_norm=0.2899, duration=0.38s
Step 7888: loss=3.0415, lr=0.000485, tokens/sec=1373080.82, grad_norm=0.3693, duration=0.38s
Step 7889: loss=3.1173, lr=0.000485, tokens/sec=1371533.32, grad_norm=0.3284, duration=0.38s
Step 7890: loss=3.0694, lr=0.000485, tokens/sec=1378969.44, grad_norm=0.3209, duration=0.38s
Step 7891: loss=3.0956, lr=0.000485, tokens/sec=1376402.52, grad_norm=0.3396, duration=0.38s
Step 7892: loss=3.1232, lr=0.000485, tokens/sec=1376114.84, grad_norm=0.3099, duration=0.38s
Step 7893: loss=3.0748, lr=0.000485, tokens/sec=1373501.91, grad_norm=0.3302, duration=0.38s
Step 7894: loss=3.0735, lr=0.000485, tokens/sec=1374714.31, grad_norm=0.3052, duration=0.38s
Step 7895: loss=3.0705, lr=0.000485, tokens/sec=1372828.80, grad_norm=0.3294, duration=0.38s
Step 7896: loss=3.0657, lr=0.000485, tokens/sec=1372893.08, grad_norm=0.3238, duration=0.38s
Step 7897: loss=3.0558, lr=0.000485, tokens/sec=1376586.91, grad_norm=0.2945, duration=0.38s
Step 7898: loss=3.0527, lr=0.000484, tokens/sec=1373250.60, grad_norm=0.3284, duration=0.38s
Step 7899: loss=2.9982, lr=0.000484, tokens/sec=1370800.61, grad_norm=0.3059, duration=0.38s
Step 7900/19073 (41.4%), Elapsed time: 3137.46s, Steps per hour: 9064.67, Estimated hours remaining: 1.23
Step 7900: loss=3.0715, lr=0.000484, tokens/sec=1375872.90, grad_norm=0.3265, duration=0.38s
Step 7901: loss=3.0586, lr=0.000484, tokens/sec=1375940.91, grad_norm=0.3311, duration=0.38s
Step 7902: loss=3.0854, lr=0.000484, tokens/sec=1371031.36, grad_norm=0.2902, duration=0.38s
Step 7903: loss=2.9980, lr=0.000484, tokens/sec=1376549.00, grad_norm=0.3280, duration=0.38s
Step 7904: loss=3.0638, lr=0.000484, tokens/sec=1375772.19, grad_norm=0.2870, duration=0.38s
Step 7905: loss=3.0265, lr=0.000484, tokens/sec=1374335.42, grad_norm=0.3238, duration=0.38s
Step 7906: loss=3.0698, lr=0.000484, tokens/sec=1374368.06, grad_norm=0.3650, duration=0.38s
Step 7907: loss=3.0546, lr=0.000484, tokens/sec=1375835.02, grad_norm=0.2952, duration=0.38s
Step 7908: loss=3.0715, lr=0.000484, tokens/sec=1375724.85, grad_norm=0.3299, duration=0.38s
Step 7909: loss=3.0745, lr=0.000484, tokens/sec=1371395.61, grad_norm=0.3084, duration=0.38s
Step 7910: loss=3.0611, lr=0.000484, tokens/sec=1375120.07, grad_norm=0.3017, duration=0.38s
Step 7911: loss=3.0530, lr=0.000484, tokens/sec=1373096.25, grad_norm=0.3230, duration=0.38s
Step 7912: loss=3.0617, lr=0.000484, tokens/sec=1371961.16, grad_norm=0.3210, duration=0.38s
Step 7913: loss=3.0473, lr=0.000484, tokens/sec=1375102.01, grad_norm=0.3056, duration=0.38s
Step 7914: loss=3.0471, lr=0.000484, tokens/sec=1375095.99, grad_norm=0.3113, duration=0.38s
Step 7915: loss=3.0525, lr=0.000484, tokens/sec=1370022.59, grad_norm=0.3143, duration=0.38s
Step 7916: loss=3.0455, lr=0.000484, tokens/sec=1374943.81, grad_norm=0.3043, duration=0.38s
Step 7917: loss=3.0348, lr=0.000484, tokens/sec=1370376.05, grad_norm=0.2823, duration=0.38s
Step 7918: loss=3.0242, lr=0.000484, tokens/sec=1372263.38, grad_norm=0.3361, duration=0.38s
Step 7919: loss=2.9581, lr=0.000484, tokens/sec=1375071.05, grad_norm=0.2851, duration=0.38s
Step 7920: loss=3.0244, lr=0.000484, tokens/sec=1374807.13, grad_norm=0.2917, duration=0.38s
Step 7921: loss=2.9461, lr=0.000484, tokens/sec=1376835.14, grad_norm=0.3312, duration=0.38s
Step 7922: loss=2.9599, lr=0.000484, tokens/sec=1375577.69, grad_norm=0.2804, duration=0.38s
Step 7923: loss=2.9529, lr=0.000484, tokens/sec=1371434.95, grad_norm=0.3525, duration=0.38s
Step 7924: loss=3.0266, lr=0.000484, tokens/sec=1375068.47, grad_norm=0.3030, duration=0.38s
Step 7925: loss=2.9904, lr=0.000484, tokens/sec=1372480.07, grad_norm=0.3213, duration=0.38s
Step 7926: loss=2.9871, lr=0.000484, tokens/sec=1376074.37, grad_norm=0.3495, duration=0.38s
Step 7927: loss=2.9516, lr=0.000484, tokens/sec=1376004.62, grad_norm=0.2973, duration=0.38s
Step 7928: loss=2.9662, lr=0.000484, tokens/sec=1375201.76, grad_norm=0.3334, duration=0.38s
Step 7929: loss=3.0195, lr=0.000484, tokens/sec=1377458.68, grad_norm=0.3303, duration=0.38s
Step 7930: loss=3.0392, lr=0.000484, tokens/sec=1375421.96, grad_norm=0.2983, duration=0.38s
Step 7931: loss=3.0624, lr=0.000484, tokens/sec=1375536.39, grad_norm=0.3549, duration=0.38s
Step 7932: loss=3.0692, lr=0.000483, tokens/sec=1372397.84, grad_norm=0.3256, duration=0.38s
Step 7933: loss=3.0187, lr=0.000483, tokens/sec=1376462.83, grad_norm=0.3273, duration=0.38s
Step 7934: loss=3.0676, lr=0.000483, tokens/sec=1382127.72, grad_norm=0.3414, duration=0.38s
Step 7935: loss=3.0556, lr=0.000483, tokens/sec=1376275.03, grad_norm=0.3064, duration=0.38s
Step 7936: loss=3.0862, lr=0.000483, tokens/sec=1373139.98, grad_norm=0.3304, duration=0.38s
Step 7937: loss=3.1167, lr=0.000483, tokens/sec=1377653.71, grad_norm=0.2914, duration=0.38s
Step 7938: loss=3.1613, lr=0.000483, tokens/sec=1375952.10, grad_norm=0.3139, duration=0.38s
Step 7939: loss=3.1212, lr=0.000483, tokens/sec=1375465.84, grad_norm=0.3106, duration=0.38s
Step 7940: loss=3.0933, lr=0.000483, tokens/sec=1375501.11, grad_norm=0.2992, duration=0.38s
Step 7941: loss=3.0585, lr=0.000483, tokens/sec=1374412.73, grad_norm=0.2956, duration=0.38s
Step 7942: loss=3.0476, lr=0.000483, tokens/sec=1377735.71, grad_norm=0.2968, duration=0.38s
Step 7943: loss=3.0437, lr=0.000483, tokens/sec=1375654.28, grad_norm=0.2887, duration=0.38s
Step 7944: loss=3.0034, lr=0.000483, tokens/sec=1378037.03, grad_norm=0.3091, duration=0.38s
Step 7945: loss=3.1193, lr=0.000483, tokens/sec=1379268.70, grad_norm=0.2875, duration=0.38s
Step 7946: loss=3.0397, lr=0.000483, tokens/sec=1374252.11, grad_norm=0.3063, duration=0.38s
Step 7947: loss=3.1114, lr=0.000483, tokens/sec=1375727.43, grad_norm=0.3054, duration=0.38s
Step 7948: loss=3.1311, lr=0.000483, tokens/sec=1373737.87, grad_norm=0.3195, duration=0.38s
Step 7949: loss=3.1004, lr=0.000483, tokens/sec=1372271.09, grad_norm=0.3038, duration=0.38s
Step 7950: loss=2.9833, lr=0.000483, tokens/sec=1374667.90, grad_norm=0.2972, duration=0.38s
Step 7951: loss=3.0878, lr=0.000483, tokens/sec=1375535.53, grad_norm=0.3113, duration=0.38s
Step 7952: loss=3.1063, lr=0.000483, tokens/sec=1376884.28, grad_norm=0.3046, duration=0.38s
Step 7953: loss=3.0896, lr=0.000483, tokens/sec=1377769.38, grad_norm=0.2988, duration=0.38s
Step 7954: loss=2.9780, lr=0.000483, tokens/sec=1376458.52, grad_norm=0.3237, duration=0.38s
Step 7955: loss=3.0526, lr=0.000483, tokens/sec=1379504.05, grad_norm=0.2962, duration=0.38s
Step 7956: loss=3.0151, lr=0.000483, tokens/sec=1375862.57, grad_norm=0.3319, duration=0.38s
Step 7957: loss=3.0817, lr=0.000483, tokens/sec=1374061.48, grad_norm=0.2979, duration=0.38s
Step 7958: loss=3.1133, lr=0.000483, tokens/sec=1373885.49, grad_norm=0.2942, duration=0.38s
Step 7959: loss=3.0144, lr=0.000483, tokens/sec=1379356.08, grad_norm=0.2982, duration=0.38s
Step 7960: loss=3.0374, lr=0.000483, tokens/sec=1373661.49, grad_norm=0.2787, duration=0.38s
Step 7961: loss=3.0170, lr=0.000483, tokens/sec=1376540.38, grad_norm=0.2931, duration=0.38s
Step 7962: loss=2.9794, lr=0.000483, tokens/sec=1375466.70, grad_norm=0.2861, duration=0.38s
Step 7963: loss=3.0117, lr=0.000483, tokens/sec=1376240.58, grad_norm=0.3000, duration=0.38s
Step 7964: loss=2.9859, lr=0.000483, tokens/sec=1373643.47, grad_norm=0.2784, duration=0.38s
Step 7965: loss=3.0023, lr=0.000483, tokens/sec=1375779.07, grad_norm=0.2813, duration=0.38s
Step 7966: loss=2.9668, lr=0.000483, tokens/sec=1375975.35, grad_norm=0.2753, duration=0.38s
Step 7967: loss=3.0274, lr=0.000482, tokens/sec=1374132.74, grad_norm=0.2767, duration=0.38s
Step 7968: loss=2.9868, lr=0.000482, tokens/sec=1376907.55, grad_norm=0.2813, duration=0.38s
Step 7969: loss=2.9793, lr=0.000482, tokens/sec=1376183.73, grad_norm=0.2705, duration=0.38s
Step 7970: loss=2.9652, lr=0.000482, tokens/sec=1375137.27, grad_norm=0.3059, duration=0.38s
Step 7971: loss=2.9646, lr=0.000482, tokens/sec=1379017.00, grad_norm=0.2685, duration=0.38s
Step 7972: loss=3.0094, lr=0.000482, tokens/sec=1376942.90, grad_norm=0.2979, duration=0.38s
Step 7973: loss=2.9648, lr=0.000482, tokens/sec=1373971.33, grad_norm=0.2664, duration=0.38s
Step 7974: loss=3.0073, lr=0.000482, tokens/sec=1374713.45, grad_norm=0.3031, duration=0.38s
Step 7975: loss=3.0238, lr=0.000482, tokens/sec=1373826.27, grad_norm=0.2728, duration=0.38s
Step 7976: loss=3.0556, lr=0.000482, tokens/sec=1375188.00, grad_norm=0.2851, duration=0.38s
Step 7977: loss=3.0549, lr=0.000482, tokens/sec=1373628.89, grad_norm=0.3000, duration=0.38s
Step 7978: loss=3.0032, lr=0.000482, tokens/sec=1379838.17, grad_norm=0.2979, duration=0.38s
Step 7979: loss=3.0715, lr=0.000482, tokens/sec=1375886.67, grad_norm=0.2942, duration=0.38s
Step 7980: loss=3.0339, lr=0.000482, tokens/sec=1375727.43, grad_norm=0.3166, duration=0.38s
Step 7981: loss=3.0045, lr=0.000482, tokens/sec=1375419.38, grad_norm=0.3068, duration=0.38s
Step 7982: loss=3.0598, lr=0.000482, tokens/sec=1376232.83, grad_norm=0.3037, duration=0.38s
Step 7983: loss=3.1313, lr=0.000482, tokens/sec=1377465.59, grad_norm=0.3091, duration=0.38s
Step 7984: loss=3.0985, lr=0.000482, tokens/sec=1377321.51, grad_norm=0.3015, duration=0.38s
Step 7985: loss=3.1097, lr=0.000482, tokens/sec=1373941.29, grad_norm=0.3190, duration=0.38s
Step 7986: loss=3.1614, lr=0.000482, tokens/sec=1378568.32, grad_norm=0.3045, duration=0.38s
Step 7987: loss=3.0726, lr=0.000482, tokens/sec=1377206.78, grad_norm=0.3053, duration=0.38s
Step 7988: loss=3.1052, lr=0.000482, tokens/sec=1378882.10, grad_norm=0.3106, duration=0.38s
Step 7989: loss=3.1156, lr=0.000482, tokens/sec=1375728.29, grad_norm=0.2877, duration=0.38s
Step 7990: loss=3.1196, lr=0.000482, tokens/sec=1376010.65, grad_norm=0.2997, duration=0.38s
Step 7991: loss=3.0680, lr=0.000482, tokens/sec=1376439.57, grad_norm=0.2731, duration=0.38s
Step 7992: loss=3.0882, lr=0.000482, tokens/sec=1374824.32, grad_norm=0.2954, duration=0.38s
Step 7993: loss=3.0148, lr=0.000482, tokens/sec=1376963.60, grad_norm=0.2828, duration=0.38s
Step 7994: loss=3.0897, lr=0.000482, tokens/sec=1375736.90, grad_norm=0.2946, duration=0.38s
Step 7995: loss=3.0809, lr=0.000482, tokens/sec=1377107.60, grad_norm=0.3073, duration=0.38s
Step 7996: loss=3.0164, lr=0.000482, tokens/sec=1376729.11, grad_norm=0.3267, duration=0.38s
Step 7997: loss=3.0946, lr=0.000482, tokens/sec=1377286.14, grad_norm=0.3056, duration=0.38s
Step 7998: loss=3.0160, lr=0.000482, tokens/sec=1376142.40, grad_norm=0.3730, duration=0.38s
Step 7999: loss=2.9703, lr=0.000482, tokens/sec=1373831.42, grad_norm=0.3382, duration=0.38s
Step 8000/19073 (41.9%), Elapsed time: 3175.66s, Steps per hour: 9068.97, Estimated hours remaining: 1.22
Validation loss at step 8000: 3.739332437515259
Step 8000: loss=3.0537, lr=0.000482, tokens/sec=153545.14, grad_norm=0.3215, duration=3.41s
Step 8001: loss=3.0595, lr=0.000481, tokens/sec=1377123.12, grad_norm=0.3131, duration=0.38s
Step 8002: loss=3.1468, lr=0.000481, tokens/sec=1377727.94, grad_norm=0.3286, duration=0.38s
Step 8003: loss=3.0127, lr=0.000481, tokens/sec=1375720.54, grad_norm=0.2952, duration=0.38s
Step 8004: loss=3.0798, lr=0.000481, tokens/sec=1376293.98, grad_norm=0.3472, duration=0.38s
Step 8005: loss=3.0911, lr=0.000481, tokens/sec=1378704.02, grad_norm=0.2993, duration=0.38s
Step 8006: loss=3.0299, lr=0.000481, tokens/sec=1374129.31, grad_norm=0.3363, duration=0.38s
Step 8007: loss=3.0438, lr=0.000481, tokens/sec=1377626.96, grad_norm=0.3065, duration=0.38s
Step 8008: loss=3.0797, lr=0.000481, tokens/sec=1378028.39, grad_norm=0.3033, duration=0.38s
Step 8009: loss=3.0478, lr=0.000481, tokens/sec=1376852.38, grad_norm=0.3195, duration=0.38s
Step 8010: loss=3.0298, lr=0.000481, tokens/sec=1369450.95, grad_norm=0.3008, duration=0.38s
Step 8011: loss=3.0622, lr=0.000481, tokens/sec=1375020.33, grad_norm=0.3559, duration=0.38s
Step 8012: loss=3.0654, lr=0.000481, tokens/sec=1377594.16, grad_norm=0.3183, duration=0.38s
Step 8013: loss=3.0888, lr=0.000481, tokens/sec=1375747.23, grad_norm=0.3300, duration=0.38s
Step 8014: loss=3.0315, lr=0.000481, tokens/sec=1379134.62, grad_norm=0.3081, duration=0.38s
Step 8015: loss=3.0042, lr=0.000481, tokens/sec=1376131.20, grad_norm=0.3046, duration=0.38s
Step 8016: loss=3.0142, lr=0.000481, tokens/sec=1375780.79, grad_norm=0.3208, duration=0.38s
Step 8017: loss=2.9622, lr=0.000481, tokens/sec=1375489.07, grad_norm=0.3050, duration=0.38s
Step 8018: loss=2.9557, lr=0.000481, tokens/sec=1376065.75, grad_norm=0.3267, duration=0.38s
Step 8019: loss=2.9851, lr=0.000481, tokens/sec=1378831.96, grad_norm=0.3167, duration=0.38s
Step 8020: loss=2.9986, lr=0.000481, tokens/sec=1372990.80, grad_norm=0.3036, duration=0.38s
Step 8021: loss=2.9703, lr=0.000481, tokens/sec=1373041.38, grad_norm=0.3325, duration=0.38s
Step 8022: loss=2.9882, lr=0.000481, tokens/sec=1376322.41, grad_norm=0.3080, duration=0.38s
Step 8023: loss=3.0024, lr=0.000481, tokens/sec=1377913.55, grad_norm=0.2952, duration=0.38s
Step 8024: loss=2.9716, lr=0.000481, tokens/sec=1377783.19, grad_norm=0.3219, duration=0.38s
Step 8025: loss=2.9737, lr=0.000481, tokens/sec=1376622.24, grad_norm=0.3007, duration=0.38s
Step 8026: loss=2.9900, lr=0.000481, tokens/sec=1376935.14, grad_norm=0.2819, duration=0.38s
Step 8027: loss=3.0065, lr=0.000481, tokens/sec=1373813.39, grad_norm=0.3194, duration=0.38s
Step 8028: loss=3.1065, lr=0.000481, tokens/sec=1375779.07, grad_norm=0.3126, duration=0.38s
Step 8029: loss=3.0570, lr=0.000481, tokens/sec=1377639.04, grad_norm=0.2917, duration=0.38s
Step 8030: loss=3.0916, lr=0.000481, tokens/sec=1374622.36, grad_norm=0.3116, duration=0.38s
Step 8031: loss=3.1068, lr=0.000481, tokens/sec=1378090.57, grad_norm=0.3053, duration=0.38s
Step 8032: loss=3.1004, lr=0.000481, tokens/sec=1375520.04, grad_norm=0.2928, duration=0.38s
Step 8033: loss=3.0907, lr=0.000481, tokens/sec=1375434.00, grad_norm=0.3322, duration=0.38s
Step 8034: loss=3.0521, lr=0.000481, tokens/sec=1378132.89, grad_norm=0.2631, duration=0.38s
Step 8035: loss=3.0336, lr=0.000480, tokens/sec=1377142.96, grad_norm=0.3219, duration=0.38s
Step 8036: loss=3.0315, lr=0.000480, tokens/sec=1376349.11, grad_norm=0.2954, duration=0.38s
Step 8037: loss=3.0668, lr=0.000480, tokens/sec=1377699.46, grad_norm=0.3183, duration=0.38s
Step 8038: loss=3.0592, lr=0.000480, tokens/sec=1374151.63, grad_norm=0.2987, duration=0.38s
Step 8039: loss=3.0437, lr=0.000480, tokens/sec=1374441.94, grad_norm=0.2938, duration=0.38s
Step 8040: loss=3.0756, lr=0.000480, tokens/sec=1376966.18, grad_norm=0.2923, duration=0.38s
Step 8041: loss=2.9662, lr=0.000480, tokens/sec=1378546.71, grad_norm=0.2859, duration=0.38s
Step 8042: loss=3.0393, lr=0.000480, tokens/sec=1378649.56, grad_norm=0.2824, duration=0.38s
Step 8043: loss=3.0708, lr=0.000480, tokens/sec=1377998.17, grad_norm=0.2934, duration=0.38s
Step 8044: loss=3.0528, lr=0.000480, tokens/sec=1377984.35, grad_norm=0.2766, duration=0.38s
Step 8045: loss=3.1148, lr=0.000480, tokens/sec=1375238.74, grad_norm=0.3206, duration=0.38s
Step 8046: loss=2.9955, lr=0.000480, tokens/sec=1374849.25, grad_norm=0.2890, duration=0.38s
Step 8047: loss=3.1038, lr=0.000480, tokens/sec=1375043.54, grad_norm=0.3012, duration=0.38s
Step 8048: loss=3.0392, lr=0.000480, tokens/sec=1374652.44, grad_norm=0.3168, duration=0.38s
Step 8049: loss=3.0537, lr=0.000480, tokens/sec=1371127.11, grad_norm=0.2873, duration=0.38s
Step 8050: loss=3.0539, lr=0.000480, tokens/sec=1378529.43, grad_norm=0.3101, duration=0.38s
Step 8051: loss=3.0256, lr=0.000480, tokens/sec=1375515.74, grad_norm=0.2916, duration=0.38s
Step 8052: loss=3.0268, lr=0.000480, tokens/sec=1376713.60, grad_norm=0.2941, duration=0.38s
Step 8053: loss=3.0353, lr=0.000480, tokens/sec=1370716.87, grad_norm=0.3147, duration=0.38s
Step 8054: loss=3.0517, lr=0.000480, tokens/sec=1371715.55, grad_norm=0.2863, duration=0.38s
Step 8055: loss=3.0617, lr=0.000480, tokens/sec=1374325.97, grad_norm=0.3242, duration=0.38s
Step 8056: loss=3.0629, lr=0.000480, tokens/sec=1376972.22, grad_norm=0.3014, duration=0.38s
Step 8057: loss=3.0094, lr=0.000480, tokens/sec=1377410.37, grad_norm=0.3262, duration=0.38s
Step 8058: loss=3.0754, lr=0.000480, tokens/sec=1377030.85, grad_norm=0.2985, duration=0.38s
Step 8059: loss=3.0221, lr=0.000480, tokens/sec=1376079.53, grad_norm=0.3133, duration=0.38s
Step 8060: loss=3.0410, lr=0.000480, tokens/sec=1375416.80, grad_norm=0.3087, duration=0.38s
Step 8061: loss=2.9722, lr=0.000480, tokens/sec=1375494.23, grad_norm=0.2976, duration=0.38s
Step 8062: loss=3.0351, lr=0.000480, tokens/sec=1377170.56, grad_norm=0.3105, duration=0.38s
Step 8063: loss=2.9629, lr=0.000480, tokens/sec=1375893.56, grad_norm=0.3063, duration=0.38s
Step 8064: loss=2.9901, lr=0.000480, tokens/sec=1378587.33, grad_norm=0.2980, duration=0.38s
Step 8065: loss=2.9528, lr=0.000480, tokens/sec=1372719.11, grad_norm=0.3076, duration=0.38s
Step 8066: loss=2.9566, lr=0.000480, tokens/sec=1376461.11, grad_norm=0.2939, duration=0.38s
Step 8067: loss=2.9709, lr=0.000480, tokens/sec=1374019.41, grad_norm=0.3047, duration=0.38s
Step 8068: loss=2.9408, lr=0.000480, tokens/sec=1377792.68, grad_norm=0.2987, duration=0.38s
Step 8069: loss=2.9547, lr=0.000479, tokens/sec=1376890.31, grad_norm=0.3182, duration=0.38s
Step 8070: loss=3.0073, lr=0.000479, tokens/sec=1378340.20, grad_norm=0.2858, duration=0.38s
Step 8071: loss=3.0463, lr=0.000479, tokens/sec=1371679.61, grad_norm=0.3265, duration=0.38s
Step 8072: loss=2.9900, lr=0.000479, tokens/sec=1374052.03, grad_norm=0.3238, duration=0.38s
Step 8073: loss=3.0274, lr=0.000479, tokens/sec=1379159.70, grad_norm=0.2931, duration=0.38s
Step 8074: loss=3.0360, lr=0.000479, tokens/sec=1374530.42, grad_norm=0.3356, duration=0.38s
Step 8075: loss=3.0826, lr=0.000479, tokens/sec=1375968.46, grad_norm=0.3000, duration=0.38s
Step 8076: loss=3.1015, lr=0.000479, tokens/sec=1378405.00, grad_norm=0.3138, duration=0.38s
Step 8077: loss=3.0924, lr=0.000479, tokens/sec=1378889.02, grad_norm=0.3410, duration=0.38s
Step 8078: loss=3.0603, lr=0.000479, tokens/sec=1376082.98, grad_norm=0.3084, duration=0.38s
Step 8079: loss=3.0933, lr=0.000479, tokens/sec=1373227.44, grad_norm=0.3557, duration=0.38s
Step 8080: loss=3.0257, lr=0.000479, tokens/sec=1378002.48, grad_norm=0.2834, duration=0.38s
Step 8081: loss=3.1244, lr=0.000479, tokens/sec=1375514.02, grad_norm=0.3607, duration=0.38s
Step 8082: loss=3.0801, lr=0.000479, tokens/sec=1374200.58, grad_norm=0.3000, duration=0.38s
Step 8083: loss=3.0586, lr=0.000479, tokens/sec=1376568.82, grad_norm=0.3005, duration=0.38s
Step 8084: loss=3.0942, lr=0.000479, tokens/sec=1374348.30, grad_norm=0.3220, duration=0.38s
Step 8085: loss=3.0449, lr=0.000479, tokens/sec=1377913.55, grad_norm=0.2975, duration=0.38s
Step 8086: loss=3.0531, lr=0.000479, tokens/sec=1373193.14, grad_norm=0.3222, duration=0.38s
Step 8087: loss=3.0323, lr=0.000479, tokens/sec=1371312.65, grad_norm=0.3113, duration=0.38s
Step 8088: loss=3.0404, lr=0.000479, tokens/sec=1376291.40, grad_norm=0.3060, duration=0.38s
Step 8089: loss=3.0121, lr=0.000479, tokens/sec=1379082.73, grad_norm=0.3207, duration=0.38s
Step 8090: loss=3.0596, lr=0.000479, tokens/sec=1376779.11, grad_norm=0.3041, duration=0.38s
Step 8091: loss=3.0705, lr=0.000479, tokens/sec=1373888.07, grad_norm=0.3347, duration=0.38s
Step 8092: loss=3.0493, lr=0.000479, tokens/sec=1373501.05, grad_norm=0.2938, duration=0.38s
Step 8093: loss=2.9868, lr=0.000479, tokens/sec=1373178.56, grad_norm=0.3211, duration=0.38s
Step 8094: loss=3.0684, lr=0.000479, tokens/sec=1371939.76, grad_norm=0.3088, duration=0.38s
Step 8095: loss=3.0058, lr=0.000479, tokens/sec=1374617.21, grad_norm=0.2927, duration=0.38s
Step 8096: loss=3.0782, lr=0.000479, tokens/sec=1376125.17, grad_norm=0.3574, duration=0.38s
Step 8097: loss=3.0409, lr=0.000479, tokens/sec=1375824.69, grad_norm=0.3028, duration=0.38s
Step 8098: loss=3.0878, lr=0.000479, tokens/sec=1376024.42, grad_norm=0.3248, duration=0.38s
Step 8099: loss=3.0444, lr=0.000479, tokens/sec=1372754.24, grad_norm=0.3078, duration=0.38s
Step 8100/19073 (42.5%), Elapsed time: 3216.89s, Steps per hour: 9064.67, Estimated hours remaining: 1.21
Step 8100: loss=3.0482, lr=0.000479, tokens/sec=1373424.70, grad_norm=0.3057, duration=0.38s
Step 8101: loss=3.0437, lr=0.000479, tokens/sec=1377415.54, grad_norm=0.3038, duration=0.38s
Step 8102: loss=3.0599, lr=0.000479, tokens/sec=1376302.60, grad_norm=0.3249, duration=0.38s
Step 8103: loss=3.0249, lr=0.000478, tokens/sec=1372774.81, grad_norm=0.3070, duration=0.38s
Step 8104: loss=3.0444, lr=0.000478, tokens/sec=1374184.27, grad_norm=0.2994, duration=0.38s
Step 8105: loss=3.0511, lr=0.000478, tokens/sec=1372849.37, grad_norm=0.3204, duration=0.38s
Step 8106: loss=3.0407, lr=0.000478, tokens/sec=1377523.40, grad_norm=0.3077, duration=0.38s
Step 8107: loss=3.0126, lr=0.000478, tokens/sec=1378491.41, grad_norm=0.2831, duration=0.38s
Step 8108: loss=3.0075, lr=0.000478, tokens/sec=1374838.08, grad_norm=0.3212, duration=0.38s
Step 8109: loss=2.9796, lr=0.000478, tokens/sec=1375877.20, grad_norm=0.2928, duration=0.38s
Step 8110: loss=3.0143, lr=0.000478, tokens/sec=1376730.84, grad_norm=0.2783, duration=0.38s
Step 8111: loss=2.9115, lr=0.000478, tokens/sec=1377866.93, grad_norm=0.3478, duration=0.38s
Step 8112: loss=2.9458, lr=0.000478, tokens/sec=1374520.97, grad_norm=0.2694, duration=0.38s
Step 8113: loss=2.9552, lr=0.000478, tokens/sec=1374514.96, grad_norm=0.3441, duration=0.38s
Step 8114: loss=3.0127, lr=0.000478, tokens/sec=1372816.80, grad_norm=0.3162, duration=0.38s
Step 8115: loss=3.0008, lr=0.000478, tokens/sec=1374706.58, grad_norm=0.3089, duration=0.38s
Step 8116: loss=2.9602, lr=0.000478, tokens/sec=1372977.08, grad_norm=0.3241, duration=0.38s
Step 8117: loss=2.9276, lr=0.000478, tokens/sec=1377185.22, grad_norm=0.3122, duration=0.38s
Step 8118: loss=2.9744, lr=0.000478, tokens/sec=1376731.70, grad_norm=0.3183, duration=0.38s
Step 8119: loss=3.0125, lr=0.000478, tokens/sec=1371191.23, grad_norm=0.3128, duration=0.38s
Step 8120: loss=3.0397, lr=0.000478, tokens/sec=1376707.57, grad_norm=0.3265, duration=0.38s
Step 8121: loss=3.0531, lr=0.000478, tokens/sec=1373213.72, grad_norm=0.3065, duration=0.38s
Step 8122: loss=3.0346, lr=0.000478, tokens/sec=1376633.45, grad_norm=0.3367, duration=0.38s
Step 8123: loss=3.0322, lr=0.000478, tokens/sec=1375390.13, grad_norm=0.3241, duration=0.38s
Step 8124: loss=3.1007, lr=0.000478, tokens/sec=1374815.73, grad_norm=0.3227, duration=0.38s
Step 8125: loss=3.0542, lr=0.000478, tokens/sec=1376706.70, grad_norm=0.3079, duration=0.38s
Step 8126: loss=3.0645, lr=0.000478, tokens/sec=1376099.34, grad_norm=0.3124, duration=0.38s
Step 8127: loss=3.0913, lr=0.000478, tokens/sec=1379434.82, grad_norm=0.3065, duration=0.38s
Step 8128: loss=3.1705, lr=0.000478, tokens/sec=1376298.29, grad_norm=0.3219, duration=0.38s
Step 8129: loss=3.0960, lr=0.000478, tokens/sec=1375635.34, grad_norm=0.3149, duration=0.38s
Step 8130: loss=3.1070, lr=0.000478, tokens/sec=1376094.17, grad_norm=0.3041, duration=0.38s
Step 8131: loss=3.0058, lr=0.000478, tokens/sec=1374958.42, grad_norm=0.2921, duration=0.38s
Step 8132: loss=3.0728, lr=0.000478, tokens/sec=1378495.73, grad_norm=0.2983, duration=0.38s
Step 8133: loss=3.0291, lr=0.000478, tokens/sec=1378995.38, grad_norm=0.2806, duration=0.38s
Step 8134: loss=3.0204, lr=0.000478, tokens/sec=1375630.18, grad_norm=0.3039, duration=0.38s
Step 8135: loss=3.1197, lr=0.000478, tokens/sec=1375424.54, grad_norm=0.2930, duration=0.38s
Step 8136: loss=3.0091, lr=0.000478, tokens/sec=1372337.03, grad_norm=0.2921, duration=0.38s
Step 8137: loss=3.0966, lr=0.000477, tokens/sec=1375894.42, grad_norm=0.3022, duration=0.38s
Step 8138: loss=3.1509, lr=0.000477, tokens/sec=1376272.45, grad_norm=0.3191, duration=0.38s
Step 8139: loss=3.0305, lr=0.000477, tokens/sec=1372595.72, grad_norm=0.3138, duration=0.38s
Step 8140: loss=3.0241, lr=0.000477, tokens/sec=1377272.34, grad_norm=0.3099, duration=0.38s
Step 8141: loss=3.0813, lr=0.000477, tokens/sec=1375243.04, grad_norm=0.3208, duration=0.38s
Step 8142: loss=3.1208, lr=0.000477, tokens/sec=1379074.08, grad_norm=0.3072, duration=0.38s
Step 8143: loss=3.0414, lr=0.000477, tokens/sec=1379609.63, grad_norm=0.2860, duration=0.38s
Step 8144: loss=2.9851, lr=0.000477, tokens/sec=1373660.64, grad_norm=0.2965, duration=0.38s
Step 8145: loss=3.0106, lr=0.000477, tokens/sec=1374535.58, grad_norm=0.3508, duration=0.38s
Step 8146: loss=3.0535, lr=0.000477, tokens/sec=1377402.60, grad_norm=0.3004, duration=0.38s
Step 8147: loss=3.0801, lr=0.000477, tokens/sec=1378062.07, grad_norm=0.3159, duration=0.38s
Step 8148: loss=3.0877, lr=0.000477, tokens/sec=1373219.72, grad_norm=0.2850, duration=0.38s
Step 8149: loss=3.0224, lr=0.000477, tokens/sec=1375157.04, grad_norm=0.2946, duration=0.38s
Step 8150: loss=3.0111, lr=0.000477, tokens/sec=1377713.27, grad_norm=0.2886, duration=0.38s
Step 8151: loss=3.0152, lr=0.000477, tokens/sec=1376108.81, grad_norm=0.2822, duration=0.38s
Step 8152: loss=2.9635, lr=0.000477, tokens/sec=1372075.87, grad_norm=0.2856, duration=0.38s
Step 8153: loss=3.0170, lr=0.000477, tokens/sec=1373437.57, grad_norm=0.2914, duration=0.38s
Step 8154: loss=3.0105, lr=0.000477, tokens/sec=1370535.76, grad_norm=0.2805, duration=0.38s
Step 8155: loss=2.9586, lr=0.000477, tokens/sec=1375040.10, grad_norm=0.2869, duration=0.38s
Step 8156: loss=2.9564, lr=0.000477, tokens/sec=1373579.12, grad_norm=0.2671, duration=0.38s
Step 8157: loss=3.0327, lr=0.000477, tokens/sec=1374133.60, grad_norm=0.2759, duration=0.38s
Step 8158: loss=2.9953, lr=0.000477, tokens/sec=1376327.58, grad_norm=0.2740, duration=0.38s
Step 8159: loss=2.9898, lr=0.000477, tokens/sec=1372736.25, grad_norm=0.2605, duration=0.38s
Step 8160: loss=2.9269, lr=0.000477, tokens/sec=1372012.52, grad_norm=0.2833, duration=0.38s
Step 8161: loss=2.9675, lr=0.000477, tokens/sec=1373688.95, grad_norm=0.2803, duration=0.38s
Step 8162: loss=2.9697, lr=0.000477, tokens/sec=1376854.10, grad_norm=0.2855, duration=0.38s
Step 8163: loss=2.9815, lr=0.000477, tokens/sec=1372323.33, grad_norm=0.2773, duration=0.38s
Step 8164: loss=2.9711, lr=0.000477, tokens/sec=1374441.08, grad_norm=0.2861, duration=0.38s
Step 8165: loss=3.0490, lr=0.000477, tokens/sec=1372600.86, grad_norm=0.2799, duration=0.38s
Step 8166: loss=3.0405, lr=0.000477, tokens/sec=1374507.23, grad_norm=0.2835, duration=0.38s
Step 8167: loss=3.0276, lr=0.000477, tokens/sec=1376401.66, grad_norm=0.2830, duration=0.38s
Step 8168: loss=3.0350, lr=0.000477, tokens/sec=1374829.48, grad_norm=0.3184, duration=0.38s
Step 8169: loss=3.0498, lr=0.000477, tokens/sec=1372420.11, grad_norm=0.2800, duration=0.38s
Step 8170: loss=2.9635, lr=0.000476, tokens/sec=1376531.76, grad_norm=0.3277, duration=0.38s
Step 8171: loss=3.0488, lr=0.000476, tokens/sec=1376745.49, grad_norm=0.2903, duration=0.38s
Step 8172: loss=3.0560, lr=0.000476, tokens/sec=1373993.65, grad_norm=0.3102, duration=0.38s
Step 8173: loss=3.1551, lr=0.000476, tokens/sec=1372493.78, grad_norm=0.3168, duration=0.38s
Step 8174: loss=3.0807, lr=0.000476, tokens/sec=1375741.20, grad_norm=0.2954, duration=0.38s
Step 8175: loss=3.1076, lr=0.000476, tokens/sec=1374655.01, grad_norm=0.3265, duration=0.38s
Step 8176: loss=3.1295, lr=0.000476, tokens/sec=1377063.62, grad_norm=0.2890, duration=0.38s
Step 8177: loss=3.0762, lr=0.000476, tokens/sec=1373043.09, grad_norm=0.3013, duration=0.38s
Step 8178: loss=3.1337, lr=0.000476, tokens/sec=1371399.88, grad_norm=0.3233, duration=0.38s
Step 8179: loss=3.0957, lr=0.000476, tokens/sec=1374533.86, grad_norm=0.2973, duration=0.38s
Step 8180: loss=3.0960, lr=0.000476, tokens/sec=1374496.06, grad_norm=0.3085, duration=0.38s
Step 8181: loss=3.0656, lr=0.000476, tokens/sec=1374453.96, grad_norm=0.2971, duration=0.38s
Step 8182: loss=3.0693, lr=0.000476, tokens/sec=1377238.70, grad_norm=0.2797, duration=0.38s
Step 8183: loss=3.0159, lr=0.000476, tokens/sec=1373200.86, grad_norm=0.3017, duration=0.38s
Step 8184: loss=3.0888, lr=0.000476, tokens/sec=1373061.10, grad_norm=0.2761, duration=0.38s
Step 8185: loss=3.0506, lr=0.000476, tokens/sec=1373249.74, grad_norm=0.2875, duration=0.38s
Step 8186: loss=3.0202, lr=0.000476, tokens/sec=1373113.40, grad_norm=0.2914, duration=0.38s
Step 8187: loss=3.0874, lr=0.000476, tokens/sec=1371564.11, grad_norm=0.2978, duration=0.38s
Step 8188: loss=3.0013, lr=0.000476, tokens/sec=1375839.33, grad_norm=0.3064, duration=0.38s
Step 8189: loss=2.9896, lr=0.000476, tokens/sec=1374630.95, grad_norm=0.3141, duration=0.38s
Step 8190: loss=3.0162, lr=0.000476, tokens/sec=1378999.70, grad_norm=0.2807, duration=0.38s
Step 8191: loss=3.0628, lr=0.000476, tokens/sec=1371777.16, grad_norm=0.2930, duration=0.38s
Step 8192: loss=3.1598, lr=0.000476, tokens/sec=1376650.68, grad_norm=0.3001, duration=0.38s
Step 8193: loss=2.9631, lr=0.000476, tokens/sec=1371912.38, grad_norm=0.2959, duration=0.38s
Step 8194: loss=3.1016, lr=0.000476, tokens/sec=1375943.49, grad_norm=0.3129, duration=0.38s
Step 8195: loss=3.0698, lr=0.000476, tokens/sec=1377455.23, grad_norm=0.2872, duration=0.38s
Step 8196: loss=3.0049, lr=0.000476, tokens/sec=1377751.25, grad_norm=0.2892, duration=0.38s
Step 8197: loss=3.0523, lr=0.000476, tokens/sec=1374551.04, grad_norm=0.2927, duration=0.38s
Step 8198: loss=3.0646, lr=0.000476, tokens/sec=1374348.30, grad_norm=0.2862, duration=0.38s
Step 8199: loss=3.0512, lr=0.000476, tokens/sec=1374945.53, grad_norm=0.3091, duration=0.38s
Step 8200/19073 (43.0%), Elapsed time: 3255.10s, Steps per hour: 9068.85, Estimated hours remaining: 1.20
Step 8200: loss=3.0204, lr=0.000476, tokens/sec=1375955.54, grad_norm=0.2821, duration=0.38s
Step 8201: loss=3.0443, lr=0.000476, tokens/sec=1370259.06, grad_norm=0.3096, duration=0.38s
Step 8202: loss=3.0448, lr=0.000476, tokens/sec=1374301.06, grad_norm=0.3111, duration=0.38s
Step 8203: loss=3.0876, lr=0.000476, tokens/sec=1374374.93, grad_norm=0.3087, duration=0.38s
Step 8204: loss=3.0495, lr=0.000475, tokens/sec=1374392.11, grad_norm=0.2950, duration=0.38s
Step 8205: loss=2.9600, lr=0.000475, tokens/sec=1372320.76, grad_norm=0.2972, duration=0.38s
Step 8206: loss=3.0086, lr=0.000475, tokens/sec=1374272.72, grad_norm=0.2948, duration=0.38s
Step 8207: loss=2.9501, lr=0.000475, tokens/sec=1374482.31, grad_norm=0.2784, duration=0.38s
Step 8208: loss=2.9557, lr=0.000475, tokens/sec=1375239.60, grad_norm=0.3003, duration=0.38s
Step 8209: loss=2.9761, lr=0.000475, tokens/sec=1375524.34, grad_norm=0.3010, duration=0.38s
Step 8210: loss=2.9598, lr=0.000475, tokens/sec=1376666.20, grad_norm=0.2905, duration=0.38s
Step 8211: loss=2.9810, lr=0.000475, tokens/sec=1372262.53, grad_norm=0.3256, duration=0.38s
Step 8212: loss=2.9738, lr=0.000475, tokens/sec=1375177.68, grad_norm=0.3100, duration=0.38s
Step 8213: loss=2.9870, lr=0.000475, tokens/sec=1378505.23, grad_norm=0.2799, duration=0.38s
Step 8214: loss=2.9738, lr=0.000475, tokens/sec=1371612.02, grad_norm=0.3219, duration=0.38s
Step 8215: loss=2.9733, lr=0.000475, tokens/sec=1376561.92, grad_norm=0.3081, duration=0.38s
Step 8216: loss=2.9193, lr=0.000475, tokens/sec=1375549.29, grad_norm=0.2964, duration=0.38s
Step 8217: loss=3.0544, lr=0.000475, tokens/sec=1374954.98, grad_norm=0.3317, duration=0.38s
Step 8218: loss=3.0972, lr=0.000475, tokens/sec=1376695.50, grad_norm=0.3262, duration=0.38s
Step 8219: loss=3.0881, lr=0.000475, tokens/sec=1375949.52, grad_norm=0.3034, duration=0.38s
Step 8220: loss=3.0775, lr=0.000475, tokens/sec=1378324.65, grad_norm=0.3107, duration=0.38s
Step 8221: loss=3.1001, lr=0.000475, tokens/sec=1373579.12, grad_norm=0.3133, duration=0.38s
Step 8222: loss=3.0852, lr=0.000475, tokens/sec=1375623.30, grad_norm=0.2855, duration=0.38s
Step 8223: loss=3.0868, lr=0.000475, tokens/sec=1373313.20, grad_norm=0.3223, duration=0.38s
Step 8224: loss=3.0275, lr=0.000475, tokens/sec=1377240.42, grad_norm=0.2646, duration=0.38s
Step 8225: loss=3.0003, lr=0.000475, tokens/sec=1373276.32, grad_norm=0.3047, duration=0.38s
Step 8226: loss=3.0737, lr=0.000475, tokens/sec=1373958.46, grad_norm=0.3116, duration=0.38s
Step 8227: loss=3.0587, lr=0.000475, tokens/sec=1376533.49, grad_norm=0.2969, duration=0.38s
Step 8228: loss=3.0420, lr=0.000475, tokens/sec=1376173.40, grad_norm=0.3067, duration=0.38s
Step 8229: loss=3.0681, lr=0.000475, tokens/sec=1375727.43, grad_norm=0.2915, duration=0.38s
Step 8230: loss=3.0319, lr=0.000475, tokens/sec=1375353.14, grad_norm=0.2836, duration=0.38s
Step 8231: loss=2.9781, lr=0.000475, tokens/sec=1378296.14, grad_norm=0.2952, duration=0.38s
Step 8232: loss=3.0210, lr=0.000475, tokens/sec=1374856.13, grad_norm=0.2769, duration=0.38s
Step 8233: loss=3.0550, lr=0.000475, tokens/sec=1377117.09, grad_norm=0.3070, duration=0.38s
Step 8234: loss=3.0607, lr=0.000475, tokens/sec=1376415.45, grad_norm=0.2827, duration=0.38s
Step 8235: loss=3.0526, lr=0.000475, tokens/sec=1374149.06, grad_norm=0.3032, duration=0.38s
Step 8236: loss=3.0177, lr=0.000475, tokens/sec=1374172.24, grad_norm=0.2969, duration=0.38s
Step 8237: loss=3.0659, lr=0.000475, tokens/sec=1377032.58, grad_norm=0.2884, duration=0.38s
Step 8238: loss=3.0616, lr=0.000474, tokens/sec=1375717.96, grad_norm=0.3046, duration=0.38s
Step 8239: loss=3.0389, lr=0.000474, tokens/sec=1375464.98, grad_norm=0.2803, duration=0.38s
Step 8240: loss=3.0156, lr=0.000474, tokens/sec=1375901.31, grad_norm=0.3050, duration=0.38s
Step 8241: loss=3.0422, lr=0.000474, tokens/sec=1377217.13, grad_norm=0.2890, duration=0.38s
Step 8242: loss=3.0049, lr=0.000474, tokens/sec=1376486.96, grad_norm=0.2750, duration=0.38s
Step 8243: loss=3.0465, lr=0.000474, tokens/sec=1379056.78, grad_norm=0.2997, duration=0.38s
Step 8244: loss=3.0571, lr=0.000474, tokens/sec=1377847.93, grad_norm=0.2719, duration=0.38s
Step 8245: loss=3.0538, lr=0.000474, tokens/sec=1375814.36, grad_norm=0.3022, duration=0.38s
Step 8246: loss=3.0405, lr=0.000474, tokens/sec=1377681.33, grad_norm=0.2757, duration=0.38s
Step 8247: loss=3.0267, lr=0.000474, tokens/sec=1376498.16, grad_norm=0.3083, duration=0.38s
Step 8248: loss=3.0212, lr=0.000474, tokens/sec=1376988.60, grad_norm=0.2890, duration=0.38s
Step 8249: loss=3.0206, lr=0.000474, tokens/sec=1371299.82, grad_norm=0.2789, duration=0.38s
Validation loss at step 8250: 3.744910717010498
Step 8250: loss=3.0413, lr=0.000474, tokens/sec=153702.35, grad_norm=0.3070, duration=3.41s
Step 8251: loss=2.9641, lr=0.000474, tokens/sec=1370935.63, grad_norm=0.2905, duration=0.38s
Step 8252: loss=3.0075, lr=0.000474, tokens/sec=1364489.31, grad_norm=0.2935, duration=0.38s
Step 8253: loss=2.9723, lr=0.000474, tokens/sec=1374489.19, grad_norm=0.3041, duration=0.38s
Step 8254: loss=2.9929, lr=0.000474, tokens/sec=1373901.80, grad_norm=0.2922, duration=0.38s
Step 8255: loss=2.9180, lr=0.000474, tokens/sec=1376600.70, grad_norm=0.2988, duration=0.38s
Step 8256: loss=2.9437, lr=0.000474, tokens/sec=1373543.95, grad_norm=0.2874, duration=0.38s
Step 8257: loss=2.9775, lr=0.000474, tokens/sec=1371411.00, grad_norm=0.2912, duration=0.38s
Step 8258: loss=2.9263, lr=0.000474, tokens/sec=1375398.73, grad_norm=0.2867, duration=0.38s
Step 8259: loss=3.0168, lr=0.000474, tokens/sec=1375207.78, grad_norm=0.3207, duration=0.38s
Step 8260: loss=2.9786, lr=0.000474, tokens/sec=1376389.60, grad_norm=0.2664, duration=0.38s
Step 8261: loss=2.9966, lr=0.000474, tokens/sec=1373144.26, grad_norm=0.3145, duration=0.38s
Step 8262: loss=2.9873, lr=0.000474, tokens/sec=1374307.08, grad_norm=0.2904, duration=0.38s
Step 8263: loss=2.9837, lr=0.000474, tokens/sec=1376959.28, grad_norm=0.2851, duration=0.38s
Step 8264: loss=3.0668, lr=0.000474, tokens/sec=1372480.07, grad_norm=0.3017, duration=0.38s
Step 8265: loss=3.0869, lr=0.000474, tokens/sec=1371549.57, grad_norm=0.2992, duration=0.38s
Step 8266: loss=3.0851, lr=0.000474, tokens/sec=1372752.53, grad_norm=0.3163, duration=0.38s
Step 8267: loss=3.1155, lr=0.000474, tokens/sec=1369626.66, grad_norm=0.3043, duration=0.38s
Step 8268: loss=3.0341, lr=0.000474, tokens/sec=1374901.69, grad_norm=0.3063, duration=0.38s
Step 8269: loss=3.0472, lr=0.000474, tokens/sec=1371230.56, grad_norm=0.3062, duration=0.38s
Step 8270: loss=3.0533, lr=0.000474, tokens/sec=1373084.25, grad_norm=0.3001, duration=0.38s
Step 8271: loss=3.0818, lr=0.000473, tokens/sec=1372888.80, grad_norm=0.3277, duration=0.38s
Step 8272: loss=3.0666, lr=0.000473, tokens/sec=1372502.34, grad_norm=0.2946, duration=0.38s
Step 8273: loss=3.0782, lr=0.000473, tokens/sec=1370322.25, grad_norm=0.3084, duration=0.38s
Step 8274: loss=3.0670, lr=0.000473, tokens/sec=1375647.39, grad_norm=0.2973, duration=0.38s
Step 8275: loss=3.0288, lr=0.000473, tokens/sec=1375718.82, grad_norm=0.2883, duration=0.38s
Step 8276: loss=3.0276, lr=0.000473, tokens/sec=1374406.71, grad_norm=0.3088, duration=0.38s
Step 8277: loss=3.0202, lr=0.000473, tokens/sec=1374985.08, grad_norm=0.2775, duration=0.38s
Step 8278: loss=3.0533, lr=0.000473, tokens/sec=1373746.45, grad_norm=0.2852, duration=0.38s
Step 8279: loss=2.9981, lr=0.000473, tokens/sec=1373030.23, grad_norm=0.2901, duration=0.38s
Step 8280: loss=3.0691, lr=0.000473, tokens/sec=1371934.63, grad_norm=0.2982, duration=0.38s
Step 8281: loss=3.0331, lr=0.000473, tokens/sec=1374488.33, grad_norm=0.3117, duration=0.38s
Step 8282: loss=3.0366, lr=0.000473, tokens/sec=1374372.36, grad_norm=0.2705, duration=0.38s
Step 8283: loss=2.9904, lr=0.000473, tokens/sec=1368523.70, grad_norm=0.3032, duration=0.38s
Step 8284: loss=3.0512, lr=0.000473, tokens/sec=1376113.98, grad_norm=0.3039, duration=0.38s
Step 8285: loss=3.0143, lr=0.000473, tokens/sec=1374225.49, grad_norm=0.2813, duration=0.38s
Step 8286: loss=3.0643, lr=0.000473, tokens/sec=1375274.01, grad_norm=0.3220, duration=0.38s
Step 8287: loss=3.0545, lr=0.000473, tokens/sec=1374239.23, grad_norm=0.3134, duration=0.38s
Step 8288: loss=3.0566, lr=0.000473, tokens/sec=1377063.62, grad_norm=0.3034, duration=0.38s
Step 8289: loss=3.0279, lr=0.000473, tokens/sec=1377109.33, grad_norm=0.3062, duration=0.38s
Step 8290: loss=3.0374, lr=0.000473, tokens/sec=1379147.59, grad_norm=0.2871, duration=0.38s
Step 8291: loss=3.0412, lr=0.000473, tokens/sec=1376504.19, grad_norm=0.2979, duration=0.38s
Step 8292: loss=3.0334, lr=0.000473, tokens/sec=1377246.46, grad_norm=0.3025, duration=0.38s
Step 8293: loss=3.0184, lr=0.000473, tokens/sec=1377210.23, grad_norm=0.2977, duration=0.38s
Step 8294: loss=3.0415, lr=0.000473, tokens/sec=1376772.21, grad_norm=0.3070, duration=0.38s
Step 8295: loss=3.0436, lr=0.000473, tokens/sec=1378085.39, grad_norm=0.2883, duration=0.38s
Step 8296: loss=3.0203, lr=0.000473, tokens/sec=1377841.03, grad_norm=0.3072, duration=0.38s
Step 8297: loss=2.9988, lr=0.000473, tokens/sec=1377620.05, grad_norm=0.2803, duration=0.38s
Step 8298: loss=3.0277, lr=0.000473, tokens/sec=1375740.34, grad_norm=0.3151, duration=0.38s
Step 8299: loss=2.9739, lr=0.000473, tokens/sec=1377121.40, grad_norm=0.2995, duration=0.38s
Step 8300/19073 (43.5%), Elapsed time: 3296.36s, Steps per hour: 9064.55, Estimated hours remaining: 1.19
Step 8300: loss=2.9787, lr=0.000473, tokens/sec=1377137.79, grad_norm=0.2734, duration=0.38s
Step 8301: loss=2.8955, lr=0.000473, tokens/sec=1373998.80, grad_norm=0.3103, duration=0.38s
Step 8302: loss=2.9487, lr=0.000473, tokens/sec=1377953.27, grad_norm=0.2727, duration=0.38s
Step 8303: loss=2.9407, lr=0.000473, tokens/sec=1378461.16, grad_norm=0.3194, duration=0.38s
Step 8304: loss=3.0209, lr=0.000473, tokens/sec=1379359.54, grad_norm=0.2997, duration=0.38s
Step 8305: loss=2.9769, lr=0.000472, tokens/sec=1377645.08, grad_norm=0.2871, duration=0.38s
Step 8306: loss=2.9358, lr=0.000472, tokens/sec=1376817.04, grad_norm=0.3254, duration=0.38s
Step 8307: loss=2.9351, lr=0.000472, tokens/sec=1374241.80, grad_norm=0.2784, duration=0.38s
Step 8308: loss=2.9659, lr=0.000472, tokens/sec=1378904.59, grad_norm=0.3268, duration=0.38s
Step 8309: loss=3.0109, lr=0.000472, tokens/sec=1374468.57, grad_norm=0.3234, duration=0.38s
Step 8310: loss=3.0313, lr=0.000472, tokens/sec=1375096.85, grad_norm=0.3101, duration=0.38s
Step 8311: loss=3.0188, lr=0.000472, tokens/sec=1376732.56, grad_norm=0.3289, duration=0.38s
Step 8312: loss=3.0451, lr=0.000472, tokens/sec=1377716.72, grad_norm=0.3050, duration=0.38s
Step 8313: loss=3.0674, lr=0.000472, tokens/sec=1375198.32, grad_norm=0.3467, duration=0.38s
Step 8314: loss=3.0966, lr=0.000472, tokens/sec=1379200.36, grad_norm=0.3173, duration=0.38s
Step 8315: loss=3.0339, lr=0.000472, tokens/sec=1377406.05, grad_norm=0.3138, duration=0.38s
Step 8316: loss=3.0395, lr=0.000472, tokens/sec=1372764.52, grad_norm=0.3316, duration=0.38s
Step 8317: loss=3.1005, lr=0.000472, tokens/sec=1372298.49, grad_norm=0.3024, duration=0.38s
Step 8318: loss=3.1493, lr=0.000472, tokens/sec=1373542.23, grad_norm=0.3141, duration=0.38s
Step 8319: loss=3.1108, lr=0.000472, tokens/sec=1376723.94, grad_norm=0.3172, duration=0.38s
Step 8320: loss=3.0572, lr=0.000472, tokens/sec=1375272.29, grad_norm=0.3056, duration=0.38s
Step 8321: loss=3.0325, lr=0.000472, tokens/sec=1376276.75, grad_norm=0.3060, duration=0.38s
Step 8322: loss=3.0594, lr=0.000472, tokens/sec=1375318.73, grad_norm=0.3080, duration=0.38s
Step 8323: loss=3.0495, lr=0.000472, tokens/sec=1373623.74, grad_norm=0.2891, duration=0.38s
Step 8324: loss=3.0240, lr=0.000472, tokens/sec=1374999.69, grad_norm=0.3297, duration=0.38s
Step 8325: loss=3.0914, lr=0.000472, tokens/sec=1377592.44, grad_norm=0.3147, duration=0.38s
Step 8326: loss=2.9983, lr=0.000472, tokens/sec=1377548.43, grad_norm=0.2929, duration=0.38s
Step 8327: loss=3.1188, lr=0.000472, tokens/sec=1373148.55, grad_norm=0.3194, duration=0.38s
Step 8328: loss=3.0806, lr=0.000472, tokens/sec=1373929.27, grad_norm=0.3184, duration=0.38s
Step 8329: loss=3.0682, lr=0.000472, tokens/sec=1372087.86, grad_norm=0.3210, duration=0.38s
Step 8330: loss=3.0130, lr=0.000472, tokens/sec=1374594.86, grad_norm=0.3028, duration=0.38s
Step 8331: loss=3.0910, lr=0.000472, tokens/sec=1378680.68, grad_norm=0.3400, duration=0.38s
Step 8332: loss=3.0703, lr=0.000472, tokens/sec=1372912.80, grad_norm=0.3189, duration=0.38s
Step 8333: loss=3.0502, lr=0.000472, tokens/sec=1376397.35, grad_norm=0.2856, duration=0.38s
Step 8334: loss=2.9418, lr=0.000472, tokens/sec=1376510.22, grad_norm=0.3278, duration=0.38s
Step 8335: loss=3.0467, lr=0.000472, tokens/sec=1376526.59, grad_norm=0.3277, duration=0.38s
Step 8336: loss=3.0503, lr=0.000472, tokens/sec=1373162.27, grad_norm=0.3045, duration=0.38s
Step 8337: loss=3.0532, lr=0.000472, tokens/sec=1375077.07, grad_norm=0.2907, duration=0.38s
Step 8338: loss=3.1006, lr=0.000471, tokens/sec=1378434.38, grad_norm=0.3078, duration=0.38s
Step 8339: loss=2.9944, lr=0.000471, tokens/sec=1379490.20, grad_norm=0.2891, duration=0.38s
Step 8340: loss=3.0076, lr=0.000471, tokens/sec=1375761.86, grad_norm=0.2958, duration=0.38s
Step 8341: loss=2.9983, lr=0.000471, tokens/sec=1378921.01, grad_norm=0.2845, duration=0.38s
Step 8342: loss=2.9673, lr=0.000471, tokens/sec=1375435.73, grad_norm=0.2912, duration=0.38s
Step 8343: loss=3.0421, lr=0.000471, tokens/sec=1379299.84, grad_norm=0.2969, duration=0.38s
Step 8344: loss=2.9643, lr=0.000471, tokens/sec=1375943.49, grad_norm=0.3067, duration=0.38s
Step 8345: loss=2.9459, lr=0.000471, tokens/sec=1373984.21, grad_norm=0.3029, duration=0.38s
Step 8346: loss=2.9626, lr=0.000471, tokens/sec=1377367.23, grad_norm=0.3111, duration=0.38s
Step 8347: loss=3.0372, lr=0.000471, tokens/sec=1376618.80, grad_norm=0.2880, duration=0.38s
Step 8348: loss=3.0032, lr=0.000471, tokens/sec=1377636.45, grad_norm=0.3030, duration=0.38s
Step 8349: loss=2.9507, lr=0.000471, tokens/sec=1372983.09, grad_norm=0.2860, duration=0.38s
Step 8350: loss=2.9312, lr=0.000471, tokens/sec=1373394.68, grad_norm=0.3063, duration=0.38s
Step 8351: loss=2.9312, lr=0.000471, tokens/sec=1374954.98, grad_norm=0.2946, duration=0.38s
Step 8352: loss=2.9879, lr=0.000471, tokens/sec=1374734.94, grad_norm=0.2801, duration=0.38s
Step 8353: loss=2.9456, lr=0.000471, tokens/sec=1376376.68, grad_norm=0.3012, duration=0.38s
Step 8354: loss=2.9965, lr=0.000471, tokens/sec=1376554.17, grad_norm=0.2734, duration=0.38s
Step 8355: loss=3.0344, lr=0.000471, tokens/sec=1378082.80, grad_norm=0.2922, duration=0.38s
Step 8356: loss=3.0124, lr=0.000471, tokens/sec=1378189.03, grad_norm=0.2858, duration=0.38s
Step 8357: loss=3.0582, lr=0.000471, tokens/sec=1375524.34, grad_norm=0.2885, duration=0.38s
Step 8358: loss=3.0154, lr=0.000471, tokens/sec=1372484.35, grad_norm=0.3256, duration=0.38s
Step 8359: loss=2.9902, lr=0.000471, tokens/sec=1376816.17, grad_norm=0.4335, duration=0.38s
Step 8360: loss=3.0061, lr=0.000471, tokens/sec=1376741.18, grad_norm=0.2936, duration=0.38s
Step 8361: loss=3.0491, lr=0.000471, tokens/sec=1379980.18, grad_norm=0.3026, duration=0.38s
Step 8362: loss=3.0867, lr=0.000471, tokens/sec=1378687.59, grad_norm=0.3240, duration=0.38s
Step 8363: loss=3.1433, lr=0.000471, tokens/sec=1377678.74, grad_norm=0.3055, duration=0.38s
Step 8364: loss=3.0811, lr=0.000471, tokens/sec=1377841.89, grad_norm=0.3115, duration=0.38s
Step 8365: loss=3.0811, lr=0.000471, tokens/sec=1379070.62, grad_norm=0.2986, duration=0.38s
Step 8366: loss=3.1416, lr=0.000471, tokens/sec=1376012.37, grad_norm=0.3318, duration=0.38s
Step 8367: loss=3.1067, lr=0.000471, tokens/sec=1378945.23, grad_norm=0.2744, duration=0.38s
Step 8368: loss=3.1158, lr=0.000471, tokens/sec=1378842.33, grad_norm=0.3118, duration=0.38s
Step 8369: loss=3.0765, lr=0.000471, tokens/sec=1376126.89, grad_norm=0.2973, duration=0.38s
Step 8370: loss=3.0949, lr=0.000471, tokens/sec=1378257.27, grad_norm=0.2925, duration=0.38s
Step 8371: loss=3.0492, lr=0.000471, tokens/sec=1377367.23, grad_norm=0.2899, duration=0.38s
Step 8372: loss=3.0714, lr=0.000470, tokens/sec=1374359.47, grad_norm=0.2829, duration=0.38s
Step 8373: loss=3.0180, lr=0.000470, tokens/sec=1379468.57, grad_norm=0.2929, duration=0.38s
Step 8374: loss=3.0608, lr=0.000470, tokens/sec=1370504.16, grad_norm=0.2819, duration=0.38s
Step 8375: loss=3.0547, lr=0.000470, tokens/sec=1376860.14, grad_norm=0.3303, duration=0.38s
Step 8376: loss=3.0122, lr=0.000470, tokens/sec=1379214.20, grad_norm=0.2967, duration=0.38s
Step 8377: loss=3.0802, lr=0.000470, tokens/sec=1378895.94, grad_norm=0.2916, duration=0.38s
Step 8378: loss=3.0244, lr=0.000470, tokens/sec=1375370.35, grad_norm=0.2853, duration=0.38s
Step 8379: loss=2.9575, lr=0.000470, tokens/sec=1375065.04, grad_norm=0.3109, duration=0.38s
Step 8380: loss=3.0202, lr=0.000470, tokens/sec=1376436.98, grad_norm=0.2867, duration=0.38s
Step 8381: loss=3.0770, lr=0.000470, tokens/sec=1376163.06, grad_norm=0.3031, duration=0.38s
Step 8382: loss=3.1116, lr=0.000470, tokens/sec=1378098.34, grad_norm=0.2979, duration=0.38s
Step 8383: loss=2.9914, lr=0.000470, tokens/sec=1377087.77, grad_norm=0.3095, duration=0.38s
Step 8384: loss=3.0792, lr=0.000470, tokens/sec=1376080.39, grad_norm=0.2959, duration=0.38s
Step 8385: loss=3.0477, lr=0.000470, tokens/sec=1378691.05, grad_norm=0.2990, duration=0.38s
Step 8386: loss=3.0119, lr=0.000470, tokens/sec=1378570.91, grad_norm=0.2929, duration=0.38s
Step 8387: loss=3.0350, lr=0.000470, tokens/sec=1375201.76, grad_norm=0.2835, duration=0.38s
Step 8388: loss=3.0691, lr=0.000470, tokens/sec=1378167.44, grad_norm=0.2913, duration=0.38s
Step 8389: loss=3.0442, lr=0.000470, tokens/sec=1378313.42, grad_norm=0.2806, duration=0.38s
Step 8390: loss=3.0054, lr=0.000470, tokens/sec=1372851.08, grad_norm=0.3020, duration=0.38s
Step 8391: loss=3.0266, lr=0.000470, tokens/sec=1380376.05, grad_norm=0.2899, duration=0.38s
Step 8392: loss=3.0445, lr=0.000470, tokens/sec=1378573.51, grad_norm=0.3342, duration=0.38s
Step 8393: loss=3.1047, lr=0.000470, tokens/sec=1371575.23, grad_norm=0.2940, duration=0.38s
Step 8394: loss=3.0081, lr=0.000470, tokens/sec=1377884.19, grad_norm=0.2875, duration=0.38s
Step 8395: loss=2.9559, lr=0.000470, tokens/sec=1376873.07, grad_norm=0.2938, duration=0.38s
Step 8396: loss=3.0006, lr=0.000470, tokens/sec=1372622.28, grad_norm=0.2971, duration=0.38s
Step 8397: loss=2.9550, lr=0.000470, tokens/sec=1378191.62, grad_norm=0.2778, duration=0.38s
Step 8398: loss=2.9532, lr=0.000470, tokens/sec=1376317.24, grad_norm=0.3225, duration=0.38s
Step 8399: loss=2.9421, lr=0.000470, tokens/sec=1381160.67, grad_norm=0.3111, duration=0.38s
Step 8400/19073 (44.0%), Elapsed time: 3334.54s, Steps per hour: 9068.71, Estimated hours remaining: 1.18
Step 8400: loss=2.9725, lr=0.000470, tokens/sec=1374179.11, grad_norm=0.3168, duration=0.38s
Step 8401: loss=2.9675, lr=0.000470, tokens/sec=1376567.09, grad_norm=0.3388, duration=0.38s
Step 8402: loss=2.9627, lr=0.000470, tokens/sec=1380470.51, grad_norm=0.3080, duration=0.38s
Step 8403: loss=2.9902, lr=0.000470, tokens/sec=1373948.15, grad_norm=0.3002, duration=0.38s
Step 8404: loss=2.9736, lr=0.000470, tokens/sec=1372684.83, grad_norm=0.3400, duration=0.38s
Step 8405: loss=2.9026, lr=0.000469, tokens/sec=1376683.43, grad_norm=0.3343, duration=0.38s
Step 8406: loss=2.9691, lr=0.000469, tokens/sec=1378974.63, grad_norm=0.2909, duration=0.38s
Step 8407: loss=3.0427, lr=0.000469, tokens/sec=1377575.18, grad_norm=0.3286, duration=0.38s
Step 8408: loss=3.1266, lr=0.000469, tokens/sec=1373759.32, grad_norm=0.3294, duration=0.38s
Step 8409: loss=3.0782, lr=0.000469, tokens/sec=1376952.39, grad_norm=0.3189, duration=0.38s
Step 8410: loss=3.0724, lr=0.000469, tokens/sec=1376113.98, grad_norm=0.3186, duration=0.38s
Step 8411: loss=3.0831, lr=0.000469, tokens/sec=1371990.27, grad_norm=0.3339, duration=0.38s
Step 8412: loss=3.0809, lr=0.000469, tokens/sec=1377454.37, grad_norm=0.3015, duration=0.38s
Step 8413: loss=3.0636, lr=0.000469, tokens/sec=1377232.66, grad_norm=0.3199, duration=0.38s
Step 8414: loss=2.9971, lr=0.000469, tokens/sec=1376524.87, grad_norm=0.2991, duration=0.38s
Step 8415: loss=3.0399, lr=0.000469, tokens/sec=1376790.31, grad_norm=0.3133, duration=0.38s
Step 8416: loss=3.0673, lr=0.000469, tokens/sec=1378144.98, grad_norm=0.3140, duration=0.38s
Step 8417: loss=3.0427, lr=0.000469, tokens/sec=1375513.16, grad_norm=0.3049, duration=0.38s
Step 8418: loss=3.0652, lr=0.000469, tokens/sec=1376174.26, grad_norm=0.3207, duration=0.38s
Step 8419: loss=3.0259, lr=0.000469, tokens/sec=1378097.48, grad_norm=0.2934, duration=0.38s
Step 8420: loss=3.0458, lr=0.000469, tokens/sec=1378519.06, grad_norm=0.3079, duration=0.38s
Step 8421: loss=2.9608, lr=0.000469, tokens/sec=1377127.44, grad_norm=0.3080, duration=0.38s
Step 8422: loss=3.0064, lr=0.000469, tokens/sec=1377999.89, grad_norm=0.2907, duration=0.38s
Step 8423: loss=3.0695, lr=0.000469, tokens/sec=1375459.81, grad_norm=0.3237, duration=0.38s
Step 8424: loss=3.0006, lr=0.000469, tokens/sec=1377349.11, grad_norm=0.2773, duration=0.38s
Step 8425: loss=3.0718, lr=0.000469, tokens/sec=1379025.65, grad_norm=0.2962, duration=0.38s
Step 8426: loss=2.9826, lr=0.000469, tokens/sec=1372911.94, grad_norm=0.2936, duration=0.38s
Step 8427: loss=3.0953, lr=0.000469, tokens/sec=1368352.53, grad_norm=0.2900, duration=0.38s
Step 8428: loss=3.0493, lr=0.000469, tokens/sec=1378360.07, grad_norm=0.2972, duration=0.38s
Step 8429: loss=3.0034, lr=0.000469, tokens/sec=1378537.21, grad_norm=0.2766, duration=0.38s
Step 8430: loss=3.0348, lr=0.000469, tokens/sec=1377764.20, grad_norm=0.2998, duration=0.38s
Step 8431: loss=3.0226, lr=0.000469, tokens/sec=1378815.53, grad_norm=0.2935, duration=0.38s
Step 8432: loss=3.0168, lr=0.000469, tokens/sec=1377374.13, grad_norm=0.2863, duration=0.38s
Step 8433: loss=3.0498, lr=0.000469, tokens/sec=1374709.15, grad_norm=0.2868, duration=0.38s
Step 8434: loss=3.0511, lr=0.000469, tokens/sec=1375521.76, grad_norm=0.2773, duration=0.38s
Step 8435: loss=3.0349, lr=0.000469, tokens/sec=1374562.21, grad_norm=0.3027, duration=0.38s
Step 8436: loss=3.0611, lr=0.000469, tokens/sec=1376194.07, grad_norm=0.2798, duration=0.38s
Step 8437: loss=2.9761, lr=0.000469, tokens/sec=1372777.38, grad_norm=0.2934, duration=0.38s
Step 8438: loss=3.0224, lr=0.000468, tokens/sec=1376533.49, grad_norm=0.2961, duration=0.38s
Step 8439: loss=3.0216, lr=0.000468, tokens/sec=1375421.10, grad_norm=0.2757, duration=0.38s
Step 8440: loss=3.0303, lr=0.000468, tokens/sec=1375712.80, grad_norm=0.2897, duration=0.38s
Step 8441: loss=2.9350, lr=0.000468, tokens/sec=1375519.18, grad_norm=0.2709, duration=0.38s
Step 8442: loss=3.0190, lr=0.000468, tokens/sec=1375018.61, grad_norm=0.2922, duration=0.38s
Step 8443: loss=2.9778, lr=0.000468, tokens/sec=1375820.39, grad_norm=0.2965, duration=0.38s
Step 8444: loss=2.9573, lr=0.000468, tokens/sec=1371390.48, grad_norm=0.2825, duration=0.38s
Step 8445: loss=2.9047, lr=0.000468, tokens/sec=1373200.86, grad_norm=0.3098, duration=0.38s
Step 8446: loss=2.9513, lr=0.000468, tokens/sec=1373750.74, grad_norm=0.2918, duration=0.38s
Step 8447: loss=2.9653, lr=0.000468, tokens/sec=1375267.99, grad_norm=0.2835, duration=0.38s
Step 8448: loss=2.9902, lr=0.000468, tokens/sec=1375801.45, grad_norm=0.3035, duration=0.38s
Step 8449: loss=2.9936, lr=0.000468, tokens/sec=1374618.06, grad_norm=0.3049, duration=0.38s
Step 8450: loss=2.9341, lr=0.000468, tokens/sec=1376273.31, grad_norm=0.2969, duration=0.38s
Step 8451: loss=2.9969, lr=0.000468, tokens/sec=1376706.70, grad_norm=0.3052, duration=0.38s
Step 8452: loss=2.9462, lr=0.000468, tokens/sec=1374569.95, grad_norm=0.3117, duration=0.38s
Step 8453: loss=3.0195, lr=0.000468, tokens/sec=1377434.53, grad_norm=0.3055, duration=0.38s
Step 8454: loss=3.0724, lr=0.000468, tokens/sec=1373448.72, grad_norm=0.3056, duration=0.38s
Step 8455: loss=3.0715, lr=0.000468, tokens/sec=1378025.80, grad_norm=0.3174, duration=0.38s
Step 8456: loss=3.1078, lr=0.000468, tokens/sec=1374806.27, grad_norm=0.3127, duration=0.38s
Step 8457: loss=3.0918, lr=0.000468, tokens/sec=1374190.28, grad_norm=0.3113, duration=0.38s
Step 8458: loss=2.9944, lr=0.000468, tokens/sec=1372666.84, grad_norm=0.2986, duration=0.38s
Step 8459: loss=3.0782, lr=0.000468, tokens/sec=1370638.27, grad_norm=0.3351, duration=0.38s
Step 8460: loss=3.0139, lr=0.000468, tokens/sec=1377488.02, grad_norm=0.3005, duration=0.38s
Step 8461: loss=3.0699, lr=0.000468, tokens/sec=1375384.97, grad_norm=0.3223, duration=0.38s
Step 8462: loss=3.0882, lr=0.000468, tokens/sec=1373242.88, grad_norm=0.3202, duration=0.38s
Step 8463: loss=3.0574, lr=0.000468, tokens/sec=1376107.09, grad_norm=0.2949, duration=0.38s
Step 8464: loss=3.0565, lr=0.000468, tokens/sec=1377963.63, grad_norm=0.3280, duration=0.38s
Step 8465: loss=3.0066, lr=0.000468, tokens/sec=1377527.72, grad_norm=0.2988, duration=0.38s
Step 8466: loss=3.0182, lr=0.000468, tokens/sec=1375809.20, grad_norm=0.3142, duration=0.38s
Step 8467: loss=3.0346, lr=0.000468, tokens/sec=1373729.29, grad_norm=0.3140, duration=0.38s
Step 8468: loss=3.0455, lr=0.000468, tokens/sec=1375478.74, grad_norm=0.2937, duration=0.38s
Step 8469: loss=3.0134, lr=0.000468, tokens/sec=1374902.55, grad_norm=0.3191, duration=0.38s
Step 8470: loss=3.0401, lr=0.000468, tokens/sec=1375212.94, grad_norm=0.3037, duration=0.38s
Step 8471: loss=3.0217, lr=0.000468, tokens/sec=1373226.58, grad_norm=0.3014, duration=0.38s
Step 8472: loss=3.0450, lr=0.000467, tokens/sec=1372683.97, grad_norm=0.3369, duration=0.38s
Step 8473: loss=2.9744, lr=0.000467, tokens/sec=1375464.12, grad_norm=0.2894, duration=0.38s
Step 8474: loss=3.0617, lr=0.000467, tokens/sec=1375135.55, grad_norm=0.3491, duration=0.38s
Step 8475: loss=3.0027, lr=0.000467, tokens/sec=1374650.72, grad_norm=0.3140, duration=0.38s
Step 8476: loss=3.0816, lr=0.000467, tokens/sec=1374752.13, grad_norm=0.3162, duration=0.38s
Step 8477: loss=3.0292, lr=0.000467, tokens/sec=1374693.69, grad_norm=0.3169, duration=0.38s
Step 8478: loss=3.0437, lr=0.000467, tokens/sec=1377180.91, grad_norm=0.3161, duration=0.38s
Step 8479: loss=3.0231, lr=0.000467, tokens/sec=1376363.76, grad_norm=0.3098, duration=0.38s
Step 8480: loss=3.0374, lr=0.000467, tokens/sec=1379126.84, grad_norm=0.3069, duration=0.38s
Step 8481: loss=3.0174, lr=0.000467, tokens/sec=1379125.11, grad_norm=0.3165, duration=0.38s
Step 8482: loss=3.0334, lr=0.000467, tokens/sec=1373542.23, grad_norm=0.3227, duration=0.38s
Step 8483: loss=3.0181, lr=0.000467, tokens/sec=1374213.46, grad_norm=0.3177, duration=0.38s
Step 8484: loss=3.0365, lr=0.000467, tokens/sec=1376567.09, grad_norm=0.3226, duration=0.38s
Step 8485: loss=3.0259, lr=0.000467, tokens/sec=1374215.18, grad_norm=0.3119, duration=0.38s
Step 8486: loss=3.0050, lr=0.000467, tokens/sec=1372821.94, grad_norm=0.3015, duration=0.38s
Step 8487: loss=3.0229, lr=0.000467, tokens/sec=1372242.83, grad_norm=0.3230, duration=0.38s
Step 8488: loss=3.0207, lr=0.000467, tokens/sec=1373667.50, grad_norm=0.2884, duration=0.38s
Step 8489: loss=2.9410, lr=0.000467, tokens/sec=1379475.49, grad_norm=0.3285, duration=0.38s
Step 8490: loss=2.9658, lr=0.000467, tokens/sec=1370861.28, grad_norm=0.2771, duration=0.38s
Step 8491: loss=2.9018, lr=0.000467, tokens/sec=1371466.60, grad_norm=0.3356, duration=0.38s
Step 8492: loss=2.9338, lr=0.000467, tokens/sec=1375748.95, grad_norm=0.3194, duration=0.38s
Step 8493: loss=2.9519, lr=0.000467, tokens/sec=1374009.11, grad_norm=0.3199, duration=0.38s
Step 8494: loss=3.0005, lr=0.000467, tokens/sec=1377145.55, grad_norm=0.3544, duration=0.38s
Step 8495: loss=2.9496, lr=0.000467, tokens/sec=1377923.05, grad_norm=0.2827, duration=0.38s
Step 8496: loss=2.9466, lr=0.000467, tokens/sec=1378005.08, grad_norm=0.3386, duration=0.38s
Step 8497: loss=2.9294, lr=0.000467, tokens/sec=1373840.00, grad_norm=0.3143, duration=0.38s
Step 8498: loss=2.9646, lr=0.000467, tokens/sec=1374350.88, grad_norm=0.3228, duration=0.38s
Step 8499: loss=3.0052, lr=0.000467, tokens/sec=1377297.35, grad_norm=0.3391, duration=0.38s
Step 8500/19073 (44.6%), Elapsed time: 3372.76s, Steps per hour: 9072.70, Estimated hours remaining: 1.17
Validation loss at step 8500: 3.74654483795166
Step 8500: loss=2.9957, lr=0.000467, tokens/sec=153319.39, grad_norm=0.2993, duration=3.42s
Step 8501: loss=3.0349, lr=0.000467, tokens/sec=1376058.87, grad_norm=0.3471, duration=0.38s
Step 8502: loss=3.0840, lr=0.000467, tokens/sec=1378185.57, grad_norm=0.3308, duration=0.38s
Step 8503: loss=3.0607, lr=0.000467, tokens/sec=1377073.97, grad_norm=0.3185, duration=0.38s
Step 8504: loss=3.0783, lr=0.000467, tokens/sec=1377046.37, grad_norm=0.3409, duration=0.38s
Step 8505: loss=3.0090, lr=0.000466, tokens/sec=1374906.84, grad_norm=0.3054, duration=0.38s
Step 8506: loss=3.0506, lr=0.000466, tokens/sec=1378598.57, grad_norm=0.3277, duration=0.38s
Step 8507: loss=3.0813, lr=0.000466, tokens/sec=1378441.29, grad_norm=0.3222, duration=0.38s
Step 8508: loss=3.1652, lr=0.000466, tokens/sec=1374503.79, grad_norm=0.3342, duration=0.38s
Step 8509: loss=3.0614, lr=0.000466, tokens/sec=1376846.34, grad_norm=0.3205, duration=0.38s
Step 8510: loss=3.0810, lr=0.000466, tokens/sec=1377068.79, grad_norm=0.3257, duration=0.38s
Step 8511: loss=3.0173, lr=0.000466, tokens/sec=1376330.16, grad_norm=0.3103, duration=0.38s
Step 8512: loss=3.0806, lr=0.000466, tokens/sec=1375569.09, grad_norm=0.3123, duration=0.38s
Step 8513: loss=3.0523, lr=0.000466, tokens/sec=1375031.50, grad_norm=0.3206, duration=0.38s
Step 8514: loss=2.9944, lr=0.000466, tokens/sec=1375284.33, grad_norm=0.3064, duration=0.38s
Step 8515: loss=3.0798, lr=0.000466, tokens/sec=1377703.77, grad_norm=0.3231, duration=0.38s
Step 8516: loss=3.0223, lr=0.000466, tokens/sec=1373646.05, grad_norm=0.3096, duration=0.38s
Step 8517: loss=3.0461, lr=0.000466, tokens/sec=1377268.02, grad_norm=0.3276, duration=0.38s
Step 8518: loss=3.1186, lr=0.000466, tokens/sec=1370080.63, grad_norm=0.2983, duration=0.38s
Step 8519: loss=3.0577, lr=0.000466, tokens/sec=1377284.41, grad_norm=0.3107, duration=0.38s
Step 8520: loss=3.0280, lr=0.000466, tokens/sec=1373652.91, grad_norm=0.3022, duration=0.38s
Step 8521: loss=3.0387, lr=0.000466, tokens/sec=1377932.54, grad_norm=0.3026, duration=0.38s
Step 8522: loss=3.0787, lr=0.000466, tokens/sec=1372387.56, grad_norm=0.3026, duration=0.38s
Step 8523: loss=3.0076, lr=0.000466, tokens/sec=1373731.86, grad_norm=0.3547, duration=0.38s
Step 8524: loss=2.9782, lr=0.000466, tokens/sec=1369771.69, grad_norm=0.3295, duration=0.38s
Step 8525: loss=3.0367, lr=0.000466, tokens/sec=1372400.41, grad_norm=0.2969, duration=0.38s
Step 8526: loss=3.0244, lr=0.000466, tokens/sec=1372923.94, grad_norm=0.3263, duration=0.38s
Step 8527: loss=3.0601, lr=0.000466, tokens/sec=1368216.31, grad_norm=0.2866, duration=0.38s
Step 8528: loss=3.0669, lr=0.000466, tokens/sec=1372746.53, grad_norm=0.2894, duration=0.38s
Step 8529: loss=2.9903, lr=0.000466, tokens/sec=1377123.99, grad_norm=0.2857, duration=0.38s
Step 8530: loss=2.9899, lr=0.000466, tokens/sec=1376689.47, grad_norm=0.2842, duration=0.38s
Step 8531: loss=3.0008, lr=0.000466, tokens/sec=1375486.49, grad_norm=0.2822, duration=0.38s
Step 8532: loss=2.9903, lr=0.000466, tokens/sec=1375166.50, grad_norm=0.2933, duration=0.38s
Step 8533: loss=2.9955, lr=0.000466, tokens/sec=1375665.46, grad_norm=0.2945, duration=0.38s
Step 8534: loss=2.9517, lr=0.000466, tokens/sec=1376725.67, grad_norm=0.3071, duration=0.38s
Step 8535: loss=2.9517, lr=0.000466, tokens/sec=1380029.54, grad_norm=0.3017, duration=0.38s
Step 8536: loss=2.9670, lr=0.000466, tokens/sec=1375184.56, grad_norm=0.2919, duration=0.38s
Step 8537: loss=3.0477, lr=0.000466, tokens/sec=1374130.17, grad_norm=0.2977, duration=0.38s
Step 8538: loss=2.9662, lr=0.000465, tokens/sec=1374147.34, grad_norm=0.3167, duration=0.38s
Step 8539: loss=2.9512, lr=0.000465, tokens/sec=1373841.72, grad_norm=0.2923, duration=0.38s
Step 8540: loss=2.8917, lr=0.000465, tokens/sec=1375669.77, grad_norm=0.2872, duration=0.38s
Step 8541: loss=2.9488, lr=0.000465, tokens/sec=1376679.99, grad_norm=0.3223, duration=0.38s
Step 8542: loss=2.9507, lr=0.000465, tokens/sec=1376487.82, grad_norm=0.2704, duration=0.38s
Step 8543: loss=2.9707, lr=0.000465, tokens/sec=1380155.13, grad_norm=0.2830, duration=0.38s
Step 8544: loss=2.9813, lr=0.000465, tokens/sec=1372468.94, grad_norm=0.2921, duration=0.38s
Step 8545: loss=3.0064, lr=0.000465, tokens/sec=1377088.63, grad_norm=0.2857, duration=0.38s
Step 8546: loss=3.0440, lr=0.000465, tokens/sec=1377458.68, grad_norm=0.2926, duration=0.38s
Step 8547: loss=3.0393, lr=0.000465, tokens/sec=1374990.23, grad_norm=0.2821, duration=0.38s
Step 8548: loss=2.9799, lr=0.000465, tokens/sec=1377192.12, grad_norm=0.6856, duration=0.38s
Step 8549: loss=3.0272, lr=0.000465, tokens/sec=1373803.95, grad_norm=0.3178, duration=0.38s
Step 8550: loss=3.0068, lr=0.000465, tokens/sec=1380291.14, grad_norm=0.3360, duration=0.38s
Step 8551: loss=3.0802, lr=0.000465, tokens/sec=1377362.05, grad_norm=0.3167, duration=0.38s
Step 8552: loss=3.0714, lr=0.000465, tokens/sec=1376814.45, grad_norm=0.3090, duration=0.38s
Step 8553: loss=3.1445, lr=0.000465, tokens/sec=1373628.89, grad_norm=0.3111, duration=0.38s
Step 8554: loss=3.0591, lr=0.000465, tokens/sec=1379187.38, grad_norm=0.2960, duration=0.38s
Step 8555: loss=3.0901, lr=0.000465, tokens/sec=1372662.55, grad_norm=0.3076, duration=0.38s
Step 8556: loss=3.1709, lr=0.000465, tokens/sec=1378259.00, grad_norm=0.3022, duration=0.38s
Step 8557: loss=3.0942, lr=0.000465, tokens/sec=1375609.53, grad_norm=0.2895, duration=0.38s
Step 8558: loss=3.0964, lr=0.000465, tokens/sec=1376468.00, grad_norm=0.2877, duration=0.38s
Step 8559: loss=3.0777, lr=0.000465, tokens/sec=1372542.61, grad_norm=0.2989, duration=0.38s
Step 8560: loss=3.0822, lr=0.000465, tokens/sec=1376745.49, grad_norm=0.2784, duration=0.38s
Step 8561: loss=3.0546, lr=0.000465, tokens/sec=1376256.08, grad_norm=0.2977, duration=0.38s
Step 8562: loss=3.0756, lr=0.000465, tokens/sec=1371859.31, grad_norm=0.2823, duration=0.38s
Step 8563: loss=2.9878, lr=0.000465, tokens/sec=1377263.71, grad_norm=0.2884, duration=0.38s
Step 8564: loss=3.0651, lr=0.000465, tokens/sec=1381289.94, grad_norm=0.2971, duration=0.38s
Step 8565: loss=3.0514, lr=0.000465, tokens/sec=1380440.18, grad_norm=0.3219, duration=0.38s
Step 8566: loss=3.0062, lr=0.000465, tokens/sec=1372971.94, grad_norm=0.3299, duration=0.38s
Step 8567: loss=3.1063, lr=0.000465, tokens/sec=1375510.58, grad_norm=0.2972, duration=0.38s
Step 8568: loss=2.9931, lr=0.000465, tokens/sec=1375419.38, grad_norm=0.2874, duration=0.38s
Step 8569: loss=2.9660, lr=0.000465, tokens/sec=1375508.86, grad_norm=0.3121, duration=0.38s
Step 8570: loss=3.0329, lr=0.000465, tokens/sec=1376008.93, grad_norm=0.3081, duration=0.38s
Step 8571: loss=3.0293, lr=0.000464, tokens/sec=1377246.46, grad_norm=0.2944, duration=0.38s
Step 8572: loss=3.1367, lr=0.000464, tokens/sec=1377130.02, grad_norm=0.3001, duration=0.38s
Step 8573: loss=2.9715, lr=0.000464, tokens/sec=1376861.00, grad_norm=0.2949, duration=0.38s
Step 8574: loss=3.0617, lr=0.000464, tokens/sec=1377224.03, grad_norm=0.3192, duration=0.38s
Step 8575: loss=3.0573, lr=0.000464, tokens/sec=1375025.48, grad_norm=0.2780, duration=0.38s
Step 8576: loss=2.9998, lr=0.000464, tokens/sec=1381497.34, grad_norm=0.3097, duration=0.38s
Step 8577: loss=3.0426, lr=0.000464, tokens/sec=1379766.31, grad_norm=0.2884, duration=0.38s
Step 8578: loss=3.0648, lr=0.000464, tokens/sec=1374889.65, grad_norm=0.2976, duration=0.38s
Step 8579: loss=3.0310, lr=0.000464, tokens/sec=1377222.31, grad_norm=0.2908, duration=0.38s
Step 8580: loss=2.9937, lr=0.000464, tokens/sec=1376301.73, grad_norm=0.2920, duration=0.38s
Step 8581: loss=3.0283, lr=0.000464, tokens/sec=1373991.94, grad_norm=0.2998, duration=0.38s
Step 8582: loss=3.0647, lr=0.000464, tokens/sec=1378015.44, grad_norm=0.3099, duration=0.38s
Step 8583: loss=3.0628, lr=0.000464, tokens/sec=1378352.30, grad_norm=0.2940, duration=0.38s
Step 8584: loss=3.0035, lr=0.000464, tokens/sec=1377962.76, grad_norm=0.2831, duration=0.38s
Step 8585: loss=2.9492, lr=0.000464, tokens/sec=1378087.11, grad_norm=0.2799, duration=0.38s
Step 8586: loss=3.0049, lr=0.000464, tokens/sec=1377331.86, grad_norm=0.2984, duration=0.38s
Step 8587: loss=2.9496, lr=0.000464, tokens/sec=1376508.50, grad_norm=0.2765, duration=0.38s
Step 8588: loss=2.9152, lr=0.000464, tokens/sec=1376339.64, grad_norm=0.2833, duration=0.38s
Step 8589: loss=2.9518, lr=0.000464, tokens/sec=1373811.68, grad_norm=0.3079, duration=0.38s
Step 8590: loss=2.9580, lr=0.000464, tokens/sec=1372549.46, grad_norm=0.2896, duration=0.38s
Step 8591: loss=2.9560, lr=0.000464, tokens/sec=1373785.07, grad_norm=0.3038, duration=0.38s
Step 8592: loss=2.9665, lr=0.000464, tokens/sec=1377336.17, grad_norm=0.3527, duration=0.38s
Step 8593: loss=2.9864, lr=0.000464, tokens/sec=1372932.51, grad_norm=0.2974, duration=0.38s
Step 8594: loss=2.8983, lr=0.000464, tokens/sec=1375755.83, grad_norm=0.3141, duration=0.38s
Step 8595: loss=2.9551, lr=0.000464, tokens/sec=1373993.65, grad_norm=0.3690, duration=0.38s
Step 8596: loss=2.9619, lr=0.000464, tokens/sec=1380904.82, grad_norm=0.2988, duration=0.38s
Step 8597: loss=3.0787, lr=0.000464, tokens/sec=1375054.72, grad_norm=0.3403, duration=0.38s
Step 8598: loss=3.1151, lr=0.000464, tokens/sec=1374128.45, grad_norm=0.3371, duration=0.38s
Step 8599: loss=3.0720, lr=0.000464, tokens/sec=1376058.87, grad_norm=0.3243, duration=0.38s
Step 8600/19073 (45.1%), Elapsed time: 3413.99s, Steps per hour: 9068.58, Estimated hours remaining: 1.15
Step 8600: loss=3.0559, lr=0.000464, tokens/sec=1374774.47, grad_norm=0.3197, duration=0.38s
Step 8601: loss=3.0799, lr=0.000464, tokens/sec=1377936.86, grad_norm=0.3408, duration=0.38s
Step 8602: loss=3.0592, lr=0.000464, tokens/sec=1374805.41, grad_norm=0.3416, duration=0.38s
Step 8603: loss=3.0330, lr=0.000464, tokens/sec=1376465.42, grad_norm=0.3159, duration=0.38s
Step 8604: loss=3.0390, lr=0.000463, tokens/sec=1377186.95, grad_norm=0.3257, duration=0.38s
Step 8605: loss=3.0372, lr=0.000463, tokens/sec=1378067.25, grad_norm=0.3106, duration=0.38s
Step 8606: loss=3.0511, lr=0.000463, tokens/sec=1377636.45, grad_norm=0.3171, duration=0.38s
Step 8607: loss=3.0649, lr=0.000463, tokens/sec=1376357.73, grad_norm=0.3122, duration=0.38s
Step 8608: loss=3.0262, lr=0.000463, tokens/sec=1377116.22, grad_norm=0.3382, duration=0.38s
Step 8609: loss=3.0406, lr=0.000463, tokens/sec=1377383.62, grad_norm=0.2946, duration=0.38s
Step 8610: loss=3.0310, lr=0.000463, tokens/sec=1379033.43, grad_norm=0.3024, duration=0.38s
Step 8611: loss=2.9458, lr=0.000463, tokens/sec=1382378.82, grad_norm=0.3147, duration=0.38s
Step 8612: loss=3.0174, lr=0.000463, tokens/sec=1379261.78, grad_norm=0.2976, duration=0.38s
Step 8613: loss=3.0074, lr=0.000463, tokens/sec=1376988.60, grad_norm=0.3108, duration=0.38s
Step 8614: loss=3.0233, lr=0.000463, tokens/sec=1376950.66, grad_norm=0.3198, duration=0.38s
Step 8615: loss=3.0388, lr=0.000463, tokens/sec=1374100.97, grad_norm=0.2932, duration=0.38s
Step 8616: loss=3.0086, lr=0.000463, tokens/sec=1376195.79, grad_norm=0.2860, duration=0.38s
Step 8617: loss=3.0842, lr=0.000463, tokens/sec=1375155.32, grad_norm=0.3114, duration=0.38s
Step 8618: loss=3.0118, lr=0.000463, tokens/sec=1377061.03, grad_norm=0.2925, duration=0.38s
Step 8619: loss=3.0218, lr=0.000463, tokens/sec=1376665.33, grad_norm=0.2864, duration=0.38s
Step 8620: loss=3.0173, lr=0.000463, tokens/sec=1378346.25, grad_norm=0.3027, duration=0.38s
Step 8621: loss=3.0348, lr=0.000463, tokens/sec=1373610.01, grad_norm=0.3055, duration=0.38s
Step 8622: loss=3.0227, lr=0.000463, tokens/sec=1373991.08, grad_norm=0.2847, duration=0.38s
Step 8623: loss=3.0430, lr=0.000463, tokens/sec=1374709.15, grad_norm=0.2879, duration=0.38s
Step 8624: loss=3.0303, lr=0.000463, tokens/sec=1374256.40, grad_norm=0.2870, duration=0.38s
Step 8625: loss=3.0547, lr=0.000463, tokens/sec=1376017.54, grad_norm=0.3017, duration=0.38s
Step 8626: loss=3.0110, lr=0.000463, tokens/sec=1375184.56, grad_norm=0.2932, duration=0.38s
Step 8627: loss=2.9752, lr=0.000463, tokens/sec=1375015.17, grad_norm=0.2846, duration=0.38s
Step 8628: loss=3.0227, lr=0.000463, tokens/sec=1372976.23, grad_norm=0.2969, duration=0.38s
Step 8629: loss=3.0123, lr=0.000463, tokens/sec=1380884.87, grad_norm=0.2826, duration=0.38s
Step 8630: loss=3.0042, lr=0.000463, tokens/sec=1375106.31, grad_norm=0.2946, duration=0.38s
Step 8631: loss=2.9466, lr=0.000463, tokens/sec=1376244.88, grad_norm=0.2872, duration=0.38s
Step 8632: loss=3.0239, lr=0.000463, tokens/sec=1375957.27, grad_norm=0.2955, duration=0.38s
Step 8633: loss=2.9440, lr=0.000463, tokens/sec=1376657.58, grad_norm=0.3127, duration=0.38s
Step 8634: loss=2.9444, lr=0.000463, tokens/sec=1377399.15, grad_norm=0.3056, duration=0.38s
Step 8635: loss=2.9122, lr=0.000463, tokens/sec=1380403.78, grad_norm=0.3082, duration=0.38s
Step 8636: loss=2.9374, lr=0.000463, tokens/sec=1377994.71, grad_norm=0.3203, duration=0.38s
Step 8637: loss=3.0293, lr=0.000462, tokens/sec=1376489.54, grad_norm=0.3039, duration=0.38s
Step 8638: loss=2.9667, lr=0.000462, tokens/sec=1376922.21, grad_norm=0.3253, duration=0.38s
Step 8639: loss=2.9487, lr=0.000462, tokens/sec=1378065.52, grad_norm=0.3334, duration=0.38s
Step 8640: loss=2.9362, lr=0.000462, tokens/sec=1377564.82, grad_norm=0.3112, duration=0.38s
Step 8641: loss=2.9585, lr=0.000462, tokens/sec=1377229.21, grad_norm=0.3389, duration=0.38s
Step 8642: loss=2.9855, lr=0.000462, tokens/sec=1377004.12, grad_norm=0.3190, duration=0.38s
Step 8643: loss=3.0258, lr=0.000462, tokens/sec=1376226.80, grad_norm=0.3302, duration=0.38s
Step 8644: loss=3.0599, lr=0.000462, tokens/sec=1375366.90, grad_norm=0.3197, duration=0.38s
Step 8645: loss=3.0977, lr=0.000462, tokens/sec=1373172.56, grad_norm=0.3321, duration=0.38s
Step 8646: loss=3.0883, lr=0.000462, tokens/sec=1370617.77, grad_norm=0.3017, duration=0.38s
Step 8647: loss=3.0524, lr=0.000462, tokens/sec=1376053.70, grad_norm=0.3252, duration=0.38s
Step 8648: loss=3.0271, lr=0.000462, tokens/sec=1372164.91, grad_norm=0.3311, duration=0.38s
Step 8649: loss=3.0384, lr=0.000462, tokens/sec=1377815.13, grad_norm=0.3328, duration=0.38s
Step 8650: loss=3.0043, lr=0.000462, tokens/sec=1373463.31, grad_norm=0.3556, duration=0.38s
Step 8651: loss=3.0900, lr=0.000462, tokens/sec=1373416.13, grad_norm=0.3242, duration=0.38s
Step 8652: loss=3.0671, lr=0.000462, tokens/sec=1376097.62, grad_norm=0.3452, duration=0.38s
Step 8653: loss=3.0469, lr=0.000462, tokens/sec=1376834.28, grad_norm=0.3144, duration=0.38s
Step 8654: loss=3.0320, lr=0.000462, tokens/sec=1378678.95, grad_norm=0.3224, duration=0.38s
Step 8655: loss=2.9999, lr=0.000462, tokens/sec=1375042.68, grad_norm=0.3387, duration=0.38s
Step 8656: loss=3.0328, lr=0.000462, tokens/sec=1372377.29, grad_norm=0.2987, duration=0.38s
Step 8657: loss=3.0290, lr=0.000462, tokens/sec=1376510.22, grad_norm=0.3226, duration=0.38s
Step 8658: loss=3.0588, lr=0.000462, tokens/sec=1377809.08, grad_norm=0.3179, duration=0.38s
Step 8659: loss=2.9786, lr=0.000462, tokens/sec=1369951.75, grad_norm=0.2925, duration=0.38s
Step 8660: loss=3.0260, lr=0.000462, tokens/sec=1375680.09, grad_norm=0.3225, duration=0.38s
Step 8661: loss=3.0261, lr=0.000462, tokens/sec=1379382.04, grad_norm=0.3065, duration=0.38s
Step 8662: loss=3.0292, lr=0.000462, tokens/sec=1376668.78, grad_norm=0.3286, duration=0.38s
Step 8663: loss=2.9847, lr=0.000462, tokens/sec=1380908.28, grad_norm=0.3184, duration=0.38s
Step 8664: loss=3.0520, lr=0.000462, tokens/sec=1375551.02, grad_norm=0.3193, duration=0.38s
Step 8665: loss=3.0204, lr=0.000462, tokens/sec=1375477.02, grad_norm=0.3271, duration=0.38s
Step 8666: loss=3.0533, lr=0.000462, tokens/sec=1374969.60, grad_norm=0.3101, duration=0.38s
Step 8667: loss=3.0146, lr=0.000462, tokens/sec=1374112.99, grad_norm=0.3094, duration=0.38s
Step 8668: loss=3.0363, lr=0.000462, tokens/sec=1378376.49, grad_norm=0.3233, duration=0.38s
Step 8669: loss=3.0236, lr=0.000462, tokens/sec=1377994.71, grad_norm=0.3257, duration=0.38s
Step 8670: loss=3.0132, lr=0.000461, tokens/sec=1379683.21, grad_norm=0.3159, duration=0.38s
Step 8671: loss=3.0175, lr=0.000461, tokens/sec=1377149.00, grad_norm=0.3175, duration=0.38s
Step 8672: loss=3.0326, lr=0.000461, tokens/sec=1376477.48, grad_norm=0.3463, duration=0.38s
Step 8673: loss=3.0111, lr=0.000461, tokens/sec=1377717.58, grad_norm=0.3158, duration=0.38s
Step 8674: loss=3.0197, lr=0.000461, tokens/sec=1374820.88, grad_norm=0.3164, duration=0.38s
Step 8675: loss=3.0129, lr=0.000461, tokens/sec=1371947.47, grad_norm=0.3305, duration=0.38s
Step 8676: loss=3.0280, lr=0.000461, tokens/sec=1375897.00, grad_norm=0.3064, duration=0.38s
Step 8677: loss=3.0186, lr=0.000461, tokens/sec=1379191.71, grad_norm=0.3326, duration=0.38s
Step 8678: loss=2.9873, lr=0.000461, tokens/sec=1373718.99, grad_norm=0.2933, duration=0.38s
Step 8679: loss=2.9256, lr=0.000461, tokens/sec=1378831.09, grad_norm=0.3320, duration=0.38s
Step 8680: loss=2.9723, lr=0.000461, tokens/sec=1369567.80, grad_norm=0.3149, duration=0.38s
Step 8681: loss=2.8844, lr=0.000461, tokens/sec=1376817.04, grad_norm=0.3102, duration=0.38s
Step 8682: loss=2.9499, lr=0.000461, tokens/sec=1375789.40, grad_norm=0.3407, duration=0.38s
Step 8683: loss=2.9286, lr=0.000461, tokens/sec=1373165.70, grad_norm=0.2982, duration=0.38s
Step 8684: loss=2.9764, lr=0.000461, tokens/sec=1375263.69, grad_norm=0.3460, duration=0.38s
Step 8685: loss=2.9616, lr=0.000461, tokens/sec=1377773.69, grad_norm=0.3164, duration=0.38s
Step 8686: loss=2.9385, lr=0.000461, tokens/sec=1376676.54, grad_norm=0.3075, duration=0.38s
Step 8687: loss=2.9288, lr=0.000461, tokens/sec=1375335.08, grad_norm=0.3257, duration=0.38s
Step 8688: loss=2.9603, lr=0.000461, tokens/sec=1372599.15, grad_norm=0.3108, duration=0.38s
Step 8689: loss=2.9711, lr=0.000461, tokens/sec=1371960.31, grad_norm=0.3495, duration=0.38s
Step 8690: loss=3.0104, lr=0.000461, tokens/sec=1375235.30, grad_norm=0.3353, duration=0.38s
Step 8691: loss=3.0687, lr=0.000461, tokens/sec=1373767.90, grad_norm=0.3141, duration=0.38s
Step 8692: loss=3.0844, lr=0.000461, tokens/sec=1374582.84, grad_norm=0.3883, duration=0.38s
Step 8693: loss=3.0421, lr=0.000461, tokens/sec=1374860.42, grad_norm=0.3045, duration=0.38s
Step 8694: loss=3.0546, lr=0.000461, tokens/sec=1371004.87, grad_norm=0.3645, duration=0.38s
Step 8695: loss=3.0223, lr=0.000461, tokens/sec=1375215.52, grad_norm=0.3187, duration=0.38s
Step 8696: loss=3.0303, lr=0.000461, tokens/sec=1376350.83, grad_norm=0.3260, duration=0.38s
Step 8697: loss=3.0971, lr=0.000461, tokens/sec=1374677.36, grad_norm=0.3385, duration=0.38s
Step 8698: loss=3.1151, lr=0.000461, tokens/sec=1371600.04, grad_norm=0.3213, duration=0.38s
Step 8699: loss=3.0861, lr=0.000461, tokens/sec=1375085.67, grad_norm=0.3435, duration=0.38s
Step 8700/19073 (45.6%), Elapsed time: 3452.18s, Steps per hour: 9072.52, Estimated hours remaining: 1.14
Step 8700: loss=3.0684, lr=0.000461, tokens/sec=1379151.92, grad_norm=0.3441, duration=0.38s
Step 8701: loss=3.0401, lr=0.000461, tokens/sec=1376601.56, grad_norm=0.3149, duration=0.38s
Step 8702: loss=3.0824, lr=0.000461, tokens/sec=1379247.07, grad_norm=0.3549, duration=0.38s
Step 8703: loss=3.0217, lr=0.000460, tokens/sec=1374352.60, grad_norm=0.3172, duration=0.38s
Step 8704: loss=2.9866, lr=0.000460, tokens/sec=1377286.14, grad_norm=0.3205, duration=0.38s
Step 8705: loss=3.1008, lr=0.000460, tokens/sec=1377893.69, grad_norm=0.3196, duration=0.38s
Step 8706: loss=2.9507, lr=0.000460, tokens/sec=1379247.94, grad_norm=0.3066, duration=0.38s
Step 8707: loss=3.0888, lr=0.000460, tokens/sec=1373274.61, grad_norm=0.3220, duration=0.38s
Step 8708: loss=3.1091, lr=0.000460, tokens/sec=1373575.69, grad_norm=0.3527, duration=0.38s
Step 8709: loss=3.0761, lr=0.000460, tokens/sec=1378895.94, grad_norm=0.3114, duration=0.38s
Step 8710: loss=2.9810, lr=0.000460, tokens/sec=1377656.30, grad_norm=0.3324, duration=0.38s
Step 8711: loss=3.0495, lr=0.000460, tokens/sec=1375261.11, grad_norm=0.3119, duration=0.38s
Step 8712: loss=3.0355, lr=0.000460, tokens/sec=1374328.55, grad_norm=0.3450, duration=0.38s
Step 8713: loss=3.0453, lr=0.000460, tokens/sec=1373072.24, grad_norm=0.3402, duration=0.38s
Step 8714: loss=2.9737, lr=0.000460, tokens/sec=1380487.84, grad_norm=0.3340, duration=0.38s
Step 8715: loss=3.0120, lr=0.000460, tokens/sec=1375173.38, grad_norm=0.3288, duration=0.38s
Step 8716: loss=3.0322, lr=0.000460, tokens/sec=1377891.10, grad_norm=0.3159, duration=0.38s
Step 8717: loss=3.0357, lr=0.000460, tokens/sec=1375234.44, grad_norm=0.3446, duration=0.38s
Step 8718: loss=3.0658, lr=0.000460, tokens/sec=1377272.34, grad_norm=0.2766, duration=0.38s
Step 8719: loss=2.9776, lr=0.000460, tokens/sec=1379359.54, grad_norm=0.3127, duration=0.38s
Step 8720: loss=2.9940, lr=0.000460, tokens/sec=1377872.97, grad_norm=0.2987, duration=0.38s
Step 8721: loss=3.0231, lr=0.000460, tokens/sec=1374950.69, grad_norm=0.2870, duration=0.38s
Step 8722: loss=2.9478, lr=0.000460, tokens/sec=1379237.55, grad_norm=0.3049, duration=0.38s
Step 8723: loss=2.9847, lr=0.000460, tokens/sec=1371143.35, grad_norm=0.3152, duration=0.38s
Step 8724: loss=2.9593, lr=0.000460, tokens/sec=1372221.42, grad_norm=0.3001, duration=0.38s
Step 8725: loss=2.9605, lr=0.000460, tokens/sec=1377745.21, grad_norm=0.3123, duration=0.38s
Step 8726: loss=2.9797, lr=0.000460, tokens/sec=1372083.58, grad_norm=0.3083, duration=0.38s
Step 8727: loss=3.0086, lr=0.000460, tokens/sec=1375551.88, grad_norm=0.2868, duration=0.38s
Step 8728: loss=2.9685, lr=0.000460, tokens/sec=1377021.37, grad_norm=0.2831, duration=0.38s
Step 8729: loss=2.9166, lr=0.000460, tokens/sec=1375341.10, grad_norm=0.3350, duration=0.38s
Step 8730: loss=2.9100, lr=0.000460, tokens/sec=1379558.57, grad_norm=0.2798, duration=0.38s
Step 8731: loss=2.9120, lr=0.000460, tokens/sec=1371995.40, grad_norm=0.2971, duration=0.38s
Step 8732: loss=2.9812, lr=0.000460, tokens/sec=1374460.83, grad_norm=0.3292, duration=0.38s
Step 8733: loss=2.9568, lr=0.000460, tokens/sec=1377790.96, grad_norm=0.2918, duration=0.38s
Step 8734: loss=2.9550, lr=0.000460, tokens/sec=1377679.61, grad_norm=0.3182, duration=0.38s
Step 8735: loss=3.0410, lr=0.000460, tokens/sec=1375149.30, grad_norm=0.3122, duration=0.38s
Step 8736: loss=3.0236, lr=0.000459, tokens/sec=1377586.40, grad_norm=0.3145, duration=0.38s
Step 8737: loss=2.9930, lr=0.000459, tokens/sec=1375194.02, grad_norm=0.3693, duration=0.38s
Step 8738: loss=2.9898, lr=0.000459, tokens/sec=1375842.77, grad_norm=0.3636, duration=0.38s
Step 8739: loss=3.0199, lr=0.000459, tokens/sec=1373154.55, grad_norm=0.3019, duration=0.38s
Step 8740: loss=3.0316, lr=0.000459, tokens/sec=1380020.88, grad_norm=0.3205, duration=0.38s
Step 8741: loss=3.0560, lr=0.000459, tokens/sec=1377286.14, grad_norm=0.3264, duration=0.38s
Step 8742: loss=3.0641, lr=0.000459, tokens/sec=1378011.12, grad_norm=0.3060, duration=0.38s
Step 8743: loss=3.1115, lr=0.000459, tokens/sec=1377660.62, grad_norm=0.3491, duration=0.38s
Step 8744: loss=3.0599, lr=0.000459, tokens/sec=1376392.19, grad_norm=0.3029, duration=0.38s
Step 8745: loss=3.1126, lr=0.000459, tokens/sec=1378520.79, grad_norm=0.3372, duration=0.38s
Step 8746: loss=3.1472, lr=0.000459, tokens/sec=1370318.83, grad_norm=0.3129, duration=0.38s
Step 8747: loss=3.0663, lr=0.000459, tokens/sec=1374215.18, grad_norm=0.2963, duration=0.38s
Step 8748: loss=3.0886, lr=0.000459, tokens/sec=1373590.28, grad_norm=0.2736, duration=0.38s
Step 8749: loss=3.0564, lr=0.000459, tokens/sec=1375925.41, grad_norm=0.3068, duration=0.38s
Validation loss at step 8750: 3.7402989864349365
Step 8750: loss=3.0761, lr=0.000459, tokens/sec=153019.20, grad_norm=0.2933, duration=3.43s
Step 8751: loss=3.0497, lr=0.000459, tokens/sec=1370685.26, grad_norm=0.3021, duration=0.38s
Step 8752: loss=3.0425, lr=0.000459, tokens/sec=1371951.75, grad_norm=0.3229, duration=0.38s
Step 8753: loss=2.9847, lr=0.000459, tokens/sec=1376543.83, grad_norm=0.2744, duration=0.38s
Step 8754: loss=3.0542, lr=0.000459, tokens/sec=1378296.14, grad_norm=0.3102, duration=0.38s
Step 8755: loss=3.0358, lr=0.000459, tokens/sec=1375086.53, grad_norm=0.3120, duration=0.38s
Step 8756: loss=3.0235, lr=0.000459, tokens/sec=1374779.63, grad_norm=0.3086, duration=0.38s
Step 8757: loss=3.0686, lr=0.000459, tokens/sec=1374590.57, grad_norm=0.3037, duration=0.38s
Step 8758: loss=2.9940, lr=0.000459, tokens/sec=1376355.14, grad_norm=0.2881, duration=0.38s
Step 8759: loss=2.9752, lr=0.000459, tokens/sec=1376573.12, grad_norm=0.3548, duration=0.38s
Step 8760: loss=2.9839, lr=0.000459, tokens/sec=1373955.02, grad_norm=0.2997, duration=0.38s
Step 8761: loss=3.0496, lr=0.000459, tokens/sec=1371796.84, grad_norm=0.2761, duration=0.38s
Step 8762: loss=3.1138, lr=0.000459, tokens/sec=1373192.28, grad_norm=0.3100, duration=0.38s
Step 8763: loss=2.9445, lr=0.000459, tokens/sec=1373456.44, grad_norm=0.2685, duration=0.38s
Step 8764: loss=3.0650, lr=0.000459, tokens/sec=1373156.27, grad_norm=0.3122, duration=0.38s
Step 8765: loss=3.0396, lr=0.000459, tokens/sec=1374612.05, grad_norm=0.2893, duration=0.38s
Step 8766: loss=3.0003, lr=0.000459, tokens/sec=1371682.18, grad_norm=0.2890, duration=0.38s
Step 8767: loss=3.0316, lr=0.000459, tokens/sec=1373888.92, grad_norm=0.3026, duration=0.38s
Step 8768: loss=3.0472, lr=0.000459, tokens/sec=1374901.69, grad_norm=0.2931, duration=0.38s
Step 8769: loss=3.0129, lr=0.000458, tokens/sec=1376258.67, grad_norm=0.3045, duration=0.38s
Step 8770: loss=2.9900, lr=0.000458, tokens/sec=1376369.79, grad_norm=0.2974, duration=0.38s
Step 8771: loss=3.0450, lr=0.000458, tokens/sec=1370559.68, grad_norm=0.2967, duration=0.38s
Step 8772: loss=3.0323, lr=0.000458, tokens/sec=1375686.98, grad_norm=0.3719, duration=0.38s
Step 8773: loss=3.0571, lr=0.000458, tokens/sec=1368609.72, grad_norm=0.3133, duration=0.38s
Step 8774: loss=2.9927, lr=0.000458, tokens/sec=1376463.69, grad_norm=0.3023, duration=0.38s
Step 8775: loss=2.9475, lr=0.000458, tokens/sec=1372570.02, grad_norm=0.2796, duration=0.38s
Step 8776: loss=2.9974, lr=0.000458, tokens/sec=1377932.54, grad_norm=0.2908, duration=0.38s
Step 8777: loss=2.9091, lr=0.000458, tokens/sec=1370928.80, grad_norm=0.3032, duration=0.38s
Step 8778: loss=2.9236, lr=0.000458, tokens/sec=1379407.99, grad_norm=0.2868, duration=0.38s
Step 8779: loss=2.9353, lr=0.000458, tokens/sec=1377058.45, grad_norm=0.2976, duration=0.38s
Step 8780: loss=2.9436, lr=0.000458, tokens/sec=1374693.69, grad_norm=0.2986, duration=0.38s
Step 8781: loss=2.9563, lr=0.000458, tokens/sec=1374745.25, grad_norm=0.2932, duration=0.38s
Step 8782: loss=2.9620, lr=0.000458, tokens/sec=1375366.90, grad_norm=0.2945, duration=0.38s
Step 8783: loss=2.9146, lr=0.000458, tokens/sec=1376142.40, grad_norm=0.3028, duration=0.38s
Step 8784: loss=2.9469, lr=0.000458, tokens/sec=1372608.58, grad_norm=0.2695, duration=0.38s
Step 8785: loss=2.9437, lr=0.000458, tokens/sec=1375538.11, grad_norm=0.3348, duration=0.38s
Step 8786: loss=2.9938, lr=0.000458, tokens/sec=1379457.32, grad_norm=0.2992, duration=0.38s
Step 8787: loss=3.0602, lr=0.000458, tokens/sec=1374371.50, grad_norm=0.2982, duration=0.38s
Step 8788: loss=3.1052, lr=0.000458, tokens/sec=1375981.37, grad_norm=0.3438, duration=0.38s
Step 8789: loss=3.0524, lr=0.000458, tokens/sec=1371932.06, grad_norm=0.3207, duration=0.38s
Step 8790: loss=3.0504, lr=0.000458, tokens/sec=1374863.00, grad_norm=0.3191, duration=0.38s
Step 8791: loss=3.0537, lr=0.000458, tokens/sec=1376986.01, grad_norm=0.3290, duration=0.38s
Step 8792: loss=3.0228, lr=0.000458, tokens/sec=1376272.45, grad_norm=0.3236, duration=0.38s
Step 8793: loss=3.0693, lr=0.000458, tokens/sec=1378631.41, grad_norm=0.3144, duration=0.38s
Step 8794: loss=3.0319, lr=0.000458, tokens/sec=1375509.72, grad_norm=0.3237, duration=0.38s
Step 8795: loss=3.0177, lr=0.000458, tokens/sec=1376952.39, grad_norm=0.3159, duration=0.38s
Step 8796: loss=3.0701, lr=0.000458, tokens/sec=1376118.28, grad_norm=0.3280, duration=0.38s
Step 8797: loss=3.0221, lr=0.000458, tokens/sec=1373161.41, grad_norm=0.2973, duration=0.38s
Step 8798: loss=3.0370, lr=0.000458, tokens/sec=1371094.62, grad_norm=0.3249, duration=0.38s
Step 8799: loss=3.0232, lr=0.000458, tokens/sec=1372221.42, grad_norm=0.3089, duration=0.38s
Step 8800/19073 (46.1%), Elapsed time: 3493.43s, Steps per hour: 9068.46, Estimated hours remaining: 1.13
Step 8800: loss=3.0122, lr=0.000458, tokens/sec=1376784.28, grad_norm=0.2980, duration=0.38s
Step 8801: loss=2.9538, lr=0.000457, tokens/sec=1372364.44, grad_norm=0.3056, duration=0.38s
Step 8802: loss=2.9562, lr=0.000457, tokens/sec=1372950.51, grad_norm=0.3029, duration=0.38s
Step 8803: loss=3.0271, lr=0.000457, tokens/sec=1376629.14, grad_norm=0.2968, duration=0.38s
Step 8804: loss=2.9883, lr=0.000457, tokens/sec=1374025.42, grad_norm=0.2878, duration=0.38s
Step 8805: loss=3.0643, lr=0.000457, tokens/sec=1372606.86, grad_norm=0.3055, duration=0.38s
Step 8806: loss=2.9958, lr=0.000457, tokens/sec=1377698.59, grad_norm=0.2852, duration=0.38s
Step 8807: loss=3.0471, lr=0.000457, tokens/sec=1376492.13, grad_norm=0.3100, duration=0.38s
Step 8808: loss=3.0304, lr=0.000457, tokens/sec=1371651.38, grad_norm=0.3006, duration=0.38s
Step 8809: loss=3.0050, lr=0.000457, tokens/sec=1377196.43, grad_norm=0.2952, duration=0.38s
Step 8810: loss=3.0256, lr=0.000457, tokens/sec=1374708.30, grad_norm=0.2826, duration=0.38s
Step 8811: loss=3.0419, lr=0.000457, tokens/sec=1376362.03, grad_norm=0.3065, duration=0.38s
Step 8812: loss=3.0157, lr=0.000457, tokens/sec=1375571.67, grad_norm=0.2940, duration=0.38s
Step 8813: loss=3.0230, lr=0.000457, tokens/sec=1375461.54, grad_norm=0.2862, duration=0.38s
Step 8814: loss=3.0493, lr=0.000457, tokens/sec=1374717.75, grad_norm=0.2929, duration=0.38s
Step 8815: loss=3.0045, lr=0.000457, tokens/sec=1375646.53, grad_norm=0.2879, duration=0.38s
Step 8816: loss=3.0113, lr=0.000457, tokens/sec=1375273.15, grad_norm=0.3081, duration=0.38s
Step 8817: loss=2.9759, lr=0.000457, tokens/sec=1375200.04, grad_norm=0.2773, duration=0.38s
Step 8818: loss=3.0141, lr=0.000457, tokens/sec=1376188.90, grad_norm=0.3081, duration=0.38s
Step 8819: loss=2.9871, lr=0.000457, tokens/sec=1374704.00, grad_norm=0.2770, duration=0.38s
Step 8820: loss=3.0155, lr=0.000457, tokens/sec=1375200.90, grad_norm=0.2956, duration=0.38s
Step 8821: loss=2.9551, lr=0.000457, tokens/sec=1376214.74, grad_norm=0.2923, duration=0.38s
Step 8822: loss=2.9875, lr=0.000457, tokens/sec=1377369.82, grad_norm=0.2919, duration=0.38s
Step 8823: loss=2.9320, lr=0.000457, tokens/sec=1379363.00, grad_norm=0.3272, duration=0.38s
Step 8824: loss=2.9552, lr=0.000457, tokens/sec=1373749.88, grad_norm=0.3001, duration=0.38s
Step 8825: loss=2.8970, lr=0.000457, tokens/sec=1378249.49, grad_norm=0.3054, duration=0.38s
Step 8826: loss=3.0008, lr=0.000457, tokens/sec=1374522.69, grad_norm=0.3262, duration=0.38s
Step 8827: loss=3.0061, lr=0.000457, tokens/sec=1374582.84, grad_norm=0.3253, duration=0.38s
Step 8828: loss=2.9212, lr=0.000457, tokens/sec=1379693.59, grad_norm=0.3030, duration=0.38s
Step 8829: loss=2.9515, lr=0.000457, tokens/sec=1379474.62, grad_norm=0.3294, duration=0.38s
Step 8830: loss=2.8969, lr=0.000457, tokens/sec=1376643.79, grad_norm=0.3096, duration=0.38s
Step 8831: loss=2.9933, lr=0.000457, tokens/sec=1379200.36, grad_norm=0.3248, duration=0.38s
Step 8832: loss=2.9888, lr=0.000457, tokens/sec=1377211.96, grad_norm=0.3288, duration=0.38s
Step 8833: loss=3.0113, lr=0.000457, tokens/sec=1377853.11, grad_norm=0.3263, duration=0.38s
Step 8834: loss=3.0849, lr=0.000456, tokens/sec=1380003.56, grad_norm=0.3431, duration=0.38s
Step 8835: loss=3.0738, lr=0.000456, tokens/sec=1373671.79, grad_norm=0.3323, duration=0.38s
Step 8836: loss=3.0490, lr=0.000456, tokens/sec=1372480.07, grad_norm=0.3336, duration=0.38s
Step 8837: loss=3.0824, lr=0.000456, tokens/sec=1380066.78, grad_norm=0.3180, duration=0.38s
Step 8838: loss=2.9876, lr=0.000456, tokens/sec=1375513.16, grad_norm=0.3410, duration=0.38s
Step 8839: loss=3.0242, lr=0.000456, tokens/sec=1372374.72, grad_norm=0.3275, duration=0.38s
Step 8840: loss=3.0203, lr=0.000456, tokens/sec=1377645.95, grad_norm=0.3476, duration=0.38s
Step 8841: loss=3.0687, lr=0.000456, tokens/sec=1372691.69, grad_norm=0.3294, duration=0.38s
Step 8842: loss=3.0562, lr=0.000456, tokens/sec=1375086.53, grad_norm=0.3487, duration=0.38s
Step 8843: loss=3.0237, lr=0.000456, tokens/sec=1378487.95, grad_norm=0.3401, duration=0.38s
Step 8844: loss=3.0259, lr=0.000456, tokens/sec=1375267.13, grad_norm=0.3074, duration=0.38s
Step 8845: loss=3.0120, lr=0.000456, tokens/sec=1377855.70, grad_norm=0.3252, duration=0.38s
Step 8846: loss=3.0206, lr=0.000456, tokens/sec=1374442.79, grad_norm=0.3064, duration=0.38s
Step 8847: loss=3.0418, lr=0.000456, tokens/sec=1374901.69, grad_norm=0.2895, duration=0.38s
Step 8848: loss=3.0239, lr=0.000456, tokens/sec=1373438.43, grad_norm=0.3202, duration=0.38s
Step 8849: loss=2.9638, lr=0.000456, tokens/sec=1377440.57, grad_norm=0.2960, duration=0.38s
Step 8850: loss=3.0267, lr=0.000456, tokens/sec=1373465.88, grad_norm=0.2991, duration=0.38s
Step 8851: loss=3.0087, lr=0.000456, tokens/sec=1373513.06, grad_norm=0.2967, duration=0.38s
Step 8852: loss=3.0377, lr=0.000456, tokens/sec=1373043.95, grad_norm=0.3167, duration=0.38s
Step 8853: loss=2.9741, lr=0.000456, tokens/sec=1377786.64, grad_norm=0.2965, duration=0.38s
Step 8854: loss=3.0675, lr=0.000456, tokens/sec=1375534.67, grad_norm=0.3194, duration=0.38s
Step 8855: loss=2.9916, lr=0.000456, tokens/sec=1375514.02, grad_norm=0.3040, duration=0.38s
Step 8856: loss=3.0400, lr=0.000456, tokens/sec=1379383.77, grad_norm=0.3070, duration=0.38s
Step 8857: loss=3.0080, lr=0.000456, tokens/sec=1376749.80, grad_norm=0.3085, duration=0.38s
Step 8858: loss=3.0355, lr=0.000456, tokens/sec=1376307.76, grad_norm=0.3135, duration=0.38s
Step 8859: loss=2.9960, lr=0.000456, tokens/sec=1369938.09, grad_norm=0.2886, duration=0.38s
Step 8860: loss=3.0115, lr=0.000456, tokens/sec=1375829.86, grad_norm=0.3253, duration=0.38s
Step 8861: loss=3.0155, lr=0.000456, tokens/sec=1376197.51, grad_norm=0.2932, duration=0.38s
Step 8862: loss=3.0241, lr=0.000456, tokens/sec=1372929.94, grad_norm=0.3249, duration=0.38s
Step 8863: loss=2.9946, lr=0.000456, tokens/sec=1378551.04, grad_norm=0.3128, duration=0.38s
Step 8864: loss=3.0012, lr=0.000456, tokens/sec=1375075.35, grad_norm=0.3115, duration=0.38s
Step 8865: loss=3.0347, lr=0.000456, tokens/sec=1377319.78, grad_norm=0.3049, duration=0.38s
Step 8866: loss=3.0212, lr=0.000456, tokens/sec=1373422.13, grad_norm=0.3191, duration=0.38s
Step 8867: loss=2.9838, lr=0.000455, tokens/sec=1375568.22, grad_norm=0.3200, duration=0.38s
Step 8868: loss=2.9719, lr=0.000455, tokens/sec=1376269.86, grad_norm=0.2983, duration=0.38s
Step 8869: loss=2.9291, lr=0.000455, tokens/sec=1376898.93, grad_norm=0.3281, duration=0.38s
Step 8870: loss=2.9565, lr=0.000455, tokens/sec=1374446.23, grad_norm=0.2945, duration=0.38s
Step 8871: loss=2.8976, lr=0.000455, tokens/sec=1377213.68, grad_norm=0.3148, duration=0.38s
Step 8872: loss=2.9224, lr=0.000455, tokens/sec=1373516.49, grad_norm=0.3284, duration=0.38s
Step 8873: loss=2.9012, lr=0.000455, tokens/sec=1374350.02, grad_norm=0.2910, duration=0.38s
Step 8874: loss=2.9839, lr=0.000455, tokens/sec=1376400.80, grad_norm=0.3233, duration=0.38s
Step 8875: loss=2.9528, lr=0.000455, tokens/sec=1376925.66, grad_norm=0.3192, duration=0.38s
Step 8876: loss=2.9340, lr=0.000455, tokens/sec=1374307.08, grad_norm=0.3048, duration=0.38s
Step 8877: loss=2.9230, lr=0.000455, tokens/sec=1377035.16, grad_norm=0.3258, duration=0.38s
Step 8878: loss=2.9236, lr=0.000455, tokens/sec=1376979.12, grad_norm=0.3182, duration=0.38s
Step 8879: loss=2.9830, lr=0.000455, tokens/sec=1374832.92, grad_norm=0.3250, duration=0.38s
Step 8880: loss=3.0446, lr=0.000455, tokens/sec=1377036.89, grad_norm=0.3368, duration=0.38s
Step 8881: loss=3.0641, lr=0.000455, tokens/sec=1376062.31, grad_norm=0.3225, duration=0.38s
Step 8882: loss=3.0596, lr=0.000455, tokens/sec=1374153.35, grad_norm=0.3685, duration=0.38s
Step 8883: loss=3.0172, lr=0.000455, tokens/sec=1374342.29, grad_norm=0.3212, duration=0.38s
Step 8884: loss=3.0644, lr=0.000455, tokens/sec=1376779.11, grad_norm=0.3540, duration=0.38s
Step 8885: loss=2.9952, lr=0.000455, tokens/sec=1372221.42, grad_norm=0.3110, duration=0.38s
Step 8886: loss=3.0446, lr=0.000455, tokens/sec=1380779.09, grad_norm=0.3145, duration=0.38s
Step 8887: loss=3.0483, lr=0.000455, tokens/sec=1374611.19, grad_norm=0.3289, duration=0.38s
Step 8888: loss=3.1370, lr=0.000455, tokens/sec=1376790.31, grad_norm=0.3136, duration=0.38s
Step 8889: loss=3.0687, lr=0.000455, tokens/sec=1378044.80, grad_norm=0.3277, duration=0.38s
Step 8890: loss=3.0858, lr=0.000455, tokens/sec=1377528.58, grad_norm=0.3286, duration=0.38s
Step 8891: loss=3.0392, lr=0.000455, tokens/sec=1377115.36, grad_norm=0.3269, duration=0.38s
Step 8892: loss=3.0503, lr=0.000455, tokens/sec=1377375.00, grad_norm=0.3223, duration=0.38s
Step 8893: loss=3.0126, lr=0.000455, tokens/sec=1380421.98, grad_norm=0.3339, duration=0.38s
Step 8894: loss=3.0049, lr=0.000455, tokens/sec=1375681.82, grad_norm=0.3050, duration=0.38s
Step 8895: loss=3.0313, lr=0.000455, tokens/sec=1376624.83, grad_norm=0.2993, duration=0.38s
Step 8896: loss=2.9894, lr=0.000455, tokens/sec=1377545.84, grad_norm=0.3286, duration=0.38s
Step 8897: loss=3.0765, lr=0.000455, tokens/sec=1377026.54, grad_norm=0.3099, duration=0.38s
Step 8898: loss=3.1188, lr=0.000455, tokens/sec=1377086.04, grad_norm=0.3266, duration=0.38s
Step 8899: loss=3.0266, lr=0.000454, tokens/sec=1378732.54, grad_norm=0.3366, duration=0.38s
Step 8900/19073 (46.7%), Elapsed time: 3531.63s, Steps per hour: 9072.31, Estimated hours remaining: 1.12
Step 8900: loss=2.9900, lr=0.000454, tokens/sec=1373430.71, grad_norm=0.3442, duration=0.38s
Step 8901: loss=3.0038, lr=0.000454, tokens/sec=1375435.73, grad_norm=0.4089, duration=0.38s
Step 8902: loss=3.0743, lr=0.000454, tokens/sec=1380952.51, grad_norm=0.3870, duration=0.38s
Step 8903: loss=3.0379, lr=0.000454, tokens/sec=1373634.04, grad_norm=0.3331, duration=0.38s
Step 8904: loss=2.9477, lr=0.000454, tokens/sec=1377641.63, grad_norm=0.3257, duration=0.38s
Step 8905: loss=3.0280, lr=0.000454, tokens/sec=1378050.84, grad_norm=0.3828, duration=0.38s
Step 8906: loss=3.0032, lr=0.000454, tokens/sec=1377817.72, grad_norm=0.2885, duration=0.38s
Step 8907: loss=3.0318, lr=0.000454, tokens/sec=1374922.32, grad_norm=0.3379, duration=0.38s
Step 8908: loss=3.0509, lr=0.000454, tokens/sec=1377968.81, grad_norm=0.3186, duration=0.38s
Step 8909: loss=2.9807, lr=0.000454, tokens/sec=1374952.41, grad_norm=0.2899, duration=0.38s
Step 8910: loss=3.0152, lr=0.000454, tokens/sec=1378555.36, grad_norm=0.3075, duration=0.38s
Step 8911: loss=2.9797, lr=0.000454, tokens/sec=1375278.31, grad_norm=0.3087, duration=0.38s
Step 8912: loss=2.9319, lr=0.000454, tokens/sec=1379287.73, grad_norm=0.2863, duration=0.38s
Step 8913: loss=2.9906, lr=0.000454, tokens/sec=1372913.65, grad_norm=0.3108, duration=0.38s
Step 8914: loss=2.9663, lr=0.000454, tokens/sec=1378969.44, grad_norm=0.3016, duration=0.38s
Step 8915: loss=2.9666, lr=0.000454, tokens/sec=1376237.13, grad_norm=0.2859, duration=0.38s
Step 8916: loss=2.9391, lr=0.000454, tokens/sec=1381107.76, grad_norm=0.2929, duration=0.38s
Step 8917: loss=3.0115, lr=0.000454, tokens/sec=1374269.29, grad_norm=0.3095, duration=0.38s
Step 8918: loss=2.9299, lr=0.000454, tokens/sec=1374037.44, grad_norm=0.2842, duration=0.38s
Step 8919: loss=2.9360, lr=0.000454, tokens/sec=1376640.34, grad_norm=0.3222, duration=0.38s
Step 8920: loss=2.8761, lr=0.000454, tokens/sec=1379912.64, grad_norm=0.3087, duration=0.38s
Step 8921: loss=2.9390, lr=0.000454, tokens/sec=1378998.84, grad_norm=0.3082, duration=0.38s
Step 8922: loss=2.9656, lr=0.000454, tokens/sec=1372156.35, grad_norm=0.3089, duration=0.38s
Step 8923: loss=2.9305, lr=0.000454, tokens/sec=1373777.35, grad_norm=0.3207, duration=0.38s
Step 8924: loss=2.9872, lr=0.000454, tokens/sec=1373670.08, grad_norm=0.3058, duration=0.38s
Step 8925: loss=3.0173, lr=0.000454, tokens/sec=1375039.24, grad_norm=0.2969, duration=0.38s
Step 8926: loss=2.9644, lr=0.000454, tokens/sec=1370460.60, grad_norm=0.3799, duration=0.38s
Step 8927: loss=3.0112, lr=0.000454, tokens/sec=1378062.93, grad_norm=0.3002, duration=0.38s
Step 8928: loss=2.9806, lr=0.000454, tokens/sec=1378314.28, grad_norm=0.3069, duration=0.38s
Step 8929: loss=3.0477, lr=0.000454, tokens/sec=1376818.76, grad_norm=0.3329, duration=0.38s
Step 8930: loss=3.0116, lr=0.000454, tokens/sec=1376411.14, grad_norm=0.3027, duration=0.38s
Step 8931: loss=3.0511, lr=0.000454, tokens/sec=1376431.82, grad_norm=0.3111, duration=0.38s
Step 8932: loss=3.0339, lr=0.000453, tokens/sec=1377847.07, grad_norm=0.3183, duration=0.38s
Step 8933: loss=3.1148, lr=0.000453, tokens/sec=1376867.90, grad_norm=0.3272, duration=0.38s
Step 8934: loss=3.0845, lr=0.000453, tokens/sec=1377524.26, grad_norm=0.2986, duration=0.38s
Step 8935: loss=3.0936, lr=0.000453, tokens/sec=1378396.36, grad_norm=0.3385, duration=0.38s
Step 8936: loss=3.1226, lr=0.000453, tokens/sec=1376048.53, grad_norm=0.3019, duration=0.38s
Step 8937: loss=3.0628, lr=0.000453, tokens/sec=1375977.93, grad_norm=0.3180, duration=0.38s
Step 8938: loss=3.0704, lr=0.000453, tokens/sec=1378640.05, grad_norm=0.2938, duration=0.38s
Step 8939: loss=3.0565, lr=0.000453, tokens/sec=1377775.42, grad_norm=0.3317, duration=0.38s
Step 8940: loss=3.0746, lr=0.000453, tokens/sec=1379896.18, grad_norm=0.2984, duration=0.38s
Step 8941: loss=3.0179, lr=0.000453, tokens/sec=1373994.51, grad_norm=0.3117, duration=0.38s
Step 8942: loss=3.0412, lr=0.000453, tokens/sec=1375175.96, grad_norm=0.3083, duration=0.38s
Step 8943: loss=2.9781, lr=0.000453, tokens/sec=1378129.43, grad_norm=0.2869, duration=0.38s
Step 8944: loss=3.0428, lr=0.000453, tokens/sec=1376068.34, grad_norm=0.2981, duration=0.38s
Step 8945: loss=3.0576, lr=0.000453, tokens/sec=1376764.45, grad_norm=0.3196, duration=0.38s
Step 8946: loss=2.9893, lr=0.000453, tokens/sec=1376555.89, grad_norm=0.2917, duration=0.38s
Step 8947: loss=3.0680, lr=0.000453, tokens/sec=1371047.61, grad_norm=0.2817, duration=0.38s
Step 8948: loss=3.0058, lr=0.000453, tokens/sec=1373833.99, grad_norm=0.2992, duration=0.38s
Step 8949: loss=2.9273, lr=0.000453, tokens/sec=1374927.48, grad_norm=0.3672, duration=0.38s
Step 8950: loss=3.0047, lr=0.000453, tokens/sec=1370481.09, grad_norm=0.2802, duration=0.38s
Step 8951: loss=3.0282, lr=0.000453, tokens/sec=1373257.46, grad_norm=0.2823, duration=0.38s
Step 8952: loss=3.0931, lr=0.000453, tokens/sec=1376323.27, grad_norm=0.3071, duration=0.38s
Step 8953: loss=2.9561, lr=0.000453, tokens/sec=1374369.78, grad_norm=0.2956, duration=0.38s
Step 8954: loss=3.0472, lr=0.000453, tokens/sec=1374222.91, grad_norm=0.2882, duration=0.38s
Step 8955: loss=3.0441, lr=0.000453, tokens/sec=1378970.30, grad_norm=0.2920, duration=0.38s
Step 8956: loss=2.9923, lr=0.000453, tokens/sec=1376405.11, grad_norm=0.2880, duration=0.38s
Step 8957: loss=3.0152, lr=0.000453, tokens/sec=1370044.78, grad_norm=0.2929, duration=0.38s
Step 8958: loss=3.0284, lr=0.000453, tokens/sec=1374411.87, grad_norm=0.2849, duration=0.38s
Step 8959: loss=3.0124, lr=0.000453, tokens/sec=1378718.71, grad_norm=0.3025, duration=0.38s
Step 8960: loss=3.0096, lr=0.000453, tokens/sec=1376169.95, grad_norm=0.2893, duration=0.38s
Step 8961: loss=3.0009, lr=0.000453, tokens/sec=1375853.10, grad_norm=0.3066, duration=0.38s
Step 8962: loss=3.0121, lr=0.000453, tokens/sec=1377265.44, grad_norm=0.2979, duration=0.38s
Step 8963: loss=3.0443, lr=0.000453, tokens/sec=1377877.29, grad_norm=0.2981, duration=0.38s
Step 8964: loss=2.9928, lr=0.000453, tokens/sec=1378796.51, grad_norm=0.3194, duration=0.38s
Step 8965: loss=2.9413, lr=0.000452, tokens/sec=1375682.68, grad_norm=0.2716, duration=0.38s
Step 8966: loss=2.9604, lr=0.000452, tokens/sec=1374176.54, grad_norm=0.3081, duration=0.38s
Step 8967: loss=2.9206, lr=0.000452, tokens/sec=1380014.82, grad_norm=0.3109, duration=0.38s
Step 8968: loss=2.9101, lr=0.000452, tokens/sec=1373580.84, grad_norm=0.3177, duration=0.38s
Step 8969: loss=2.9247, lr=0.000452, tokens/sec=1375808.34, grad_norm=0.2888, duration=0.38s
Step 8970: loss=2.9464, lr=0.000452, tokens/sec=1377047.24, grad_norm=0.3159, duration=0.38s
Step 8971: loss=2.9539, lr=0.000452, tokens/sec=1374369.78, grad_norm=0.3046, duration=0.38s
Step 8972: loss=2.8880, lr=0.000452, tokens/sec=1374524.41, grad_norm=0.3038, duration=0.38s
Step 8973: loss=2.9672, lr=0.000452, tokens/sec=1375286.05, grad_norm=0.3355, duration=0.38s
Step 8974: loss=2.9384, lr=0.000452, tokens/sec=1378753.29, grad_norm=0.2781, duration=0.38s
Step 8975: loss=2.9767, lr=0.000452, tokens/sec=1375292.07, grad_norm=0.3171, duration=0.38s
Step 8976: loss=2.9848, lr=0.000452, tokens/sec=1373018.23, grad_norm=0.3347, duration=0.38s
Step 8977: loss=3.0531, lr=0.000452, tokens/sec=1375568.22, grad_norm=0.3024, duration=0.38s
Step 8978: loss=3.0879, lr=0.000452, tokens/sec=1377063.62, grad_norm=0.3301, duration=0.38s
Step 8979: loss=3.0449, lr=0.000452, tokens/sec=1379091.37, grad_norm=0.3223, duration=0.38s
Step 8980: loss=3.0245, lr=0.000452, tokens/sec=1376381.85, grad_norm=0.3207, duration=0.38s
Step 8981: loss=3.0217, lr=0.000452, tokens/sec=1377606.25, grad_norm=0.3104, duration=0.38s
Step 8982: loss=3.0629, lr=0.000452, tokens/sec=1378072.43, grad_norm=0.3392, duration=0.38s
Step 8983: loss=3.0627, lr=0.000452, tokens/sec=1374777.91, grad_norm=0.3184, duration=0.38s
Step 8984: loss=3.0147, lr=0.000452, tokens/sec=1374021.13, grad_norm=0.3078, duration=0.38s
Step 8985: loss=3.0383, lr=0.000452, tokens/sec=1376399.94, grad_norm=0.3173, duration=0.38s
Step 8986: loss=3.0274, lr=0.000452, tokens/sec=1377111.91, grad_norm=0.3224, duration=0.38s
Step 8987: loss=3.0331, lr=0.000452, tokens/sec=1372202.59, grad_norm=0.3064, duration=0.38s
Step 8988: loss=3.0163, lr=0.000452, tokens/sec=1377011.02, grad_norm=0.3211, duration=0.38s
Step 8989: loss=3.0052, lr=0.000452, tokens/sec=1374935.21, grad_norm=0.3236, duration=0.38s
Step 8990: loss=3.0216, lr=0.000452, tokens/sec=1379392.42, grad_norm=0.3121, duration=0.38s
Step 8991: loss=2.8933, lr=0.000452, tokens/sec=1377333.59, grad_norm=0.3056, duration=0.38s
Step 8992: loss=2.9794, lr=0.000452, tokens/sec=1372343.88, grad_norm=0.3279, duration=0.38s
Step 8993: loss=2.9911, lr=0.000452, tokens/sec=1371505.94, grad_norm=0.3096, duration=0.38s
Step 8994: loss=3.0142, lr=0.000452, tokens/sec=1376151.87, grad_norm=0.2924, duration=0.38s
Step 8995: loss=3.0543, lr=0.000452, tokens/sec=1374244.38, grad_norm=0.3147, duration=0.38s
Step 8996: loss=2.9607, lr=0.000452, tokens/sec=1373291.76, grad_norm=0.3051, duration=0.38s
Step 8997: loss=3.0624, lr=0.000451, tokens/sec=1371726.67, grad_norm=0.3051, duration=0.38s
Step 8998: loss=3.0139, lr=0.000451, tokens/sec=1373447.86, grad_norm=0.3005, duration=0.38s
Step 8999: loss=3.0154, lr=0.000451, tokens/sec=1373655.49, grad_norm=0.2816, duration=0.38s
Step 9000/19073 (47.2%), Elapsed time: 3569.82s, Steps per hour: 9076.08, Estimated hours remaining: 1.11
Validation loss at step 9000: 3.7504451274871826
Step 9000: loss=3.0320, lr=0.000451, tokens/sec=153482.70, grad_norm=0.3101, duration=3.42s
Step 9001: loss=3.0326, lr=0.000451, tokens/sec=1380824.18, grad_norm=0.2834, duration=0.38s
Step 9002: loss=2.9963, lr=0.000451, tokens/sec=1378712.66, grad_norm=0.2949, duration=0.38s
Step 9003: loss=3.0435, lr=0.000451, tokens/sec=1376715.32, grad_norm=0.2961, duration=0.38s
Step 9004: loss=3.0001, lr=0.000451, tokens/sec=1375289.49, grad_norm=0.2872, duration=0.38s
Step 9005: loss=3.0050, lr=0.000451, tokens/sec=1377479.39, grad_norm=0.2991, duration=0.38s
Step 9006: loss=3.0149, lr=0.000451, tokens/sec=1375507.13, grad_norm=0.3048, duration=0.38s
Step 9007: loss=2.9681, lr=0.000451, tokens/sec=1374082.94, grad_norm=0.2991, duration=0.38s
Step 9008: loss=2.9879, lr=0.000451, tokens/sec=1375280.03, grad_norm=0.2924, duration=0.38s
Step 9009: loss=2.9971, lr=0.000451, tokens/sec=1377979.17, grad_norm=0.2887, duration=0.38s
Step 9010: loss=3.0224, lr=0.000451, tokens/sec=1375215.52, grad_norm=0.3013, duration=0.38s
Step 9011: loss=2.9195, lr=0.000451, tokens/sec=1375541.55, grad_norm=0.2966, duration=0.38s
Step 9012: loss=2.9745, lr=0.000451, tokens/sec=1373213.72, grad_norm=0.2764, duration=0.38s
Step 9013: loss=2.9384, lr=0.000451, tokens/sec=1377576.04, grad_norm=0.3120, duration=0.38s
Step 9014: loss=2.9406, lr=0.000451, tokens/sec=1376024.42, grad_norm=0.2985, duration=0.38s
Step 9015: loss=2.9638, lr=0.000451, tokens/sec=1373426.42, grad_norm=0.3046, duration=0.38s
Step 9016: loss=2.9785, lr=0.000451, tokens/sec=1375200.90, grad_norm=0.3393, duration=0.38s
Step 9017: loss=2.9574, lr=0.000451, tokens/sec=1374769.31, grad_norm=0.3106, duration=0.38s
Step 9018: loss=2.9226, lr=0.000451, tokens/sec=1374736.66, grad_norm=0.3145, duration=0.38s
Step 9019: loss=2.9130, lr=0.000451, tokens/sec=1375223.26, grad_norm=0.3186, duration=0.38s
Step 9020: loss=2.9311, lr=0.000451, tokens/sec=1372214.57, grad_norm=0.2990, duration=0.38s
Step 9021: loss=2.9992, lr=0.000451, tokens/sec=1373377.53, grad_norm=0.3242, duration=0.38s
Step 9022: loss=2.9740, lr=0.000451, tokens/sec=1377438.84, grad_norm=0.3074, duration=0.38s
Step 9023: loss=3.0360, lr=0.000451, tokens/sec=1372485.21, grad_norm=0.3411, duration=0.38s
Step 9024: loss=3.0630, lr=0.000451, tokens/sec=1373555.96, grad_norm=0.3278, duration=0.38s
Step 9025: loss=3.0329, lr=0.000451, tokens/sec=1377511.32, grad_norm=0.3022, duration=0.38s
Step 9026: loss=3.0805, lr=0.000451, tokens/sec=1373474.46, grad_norm=0.3830, duration=0.38s
Step 9027: loss=3.0397, lr=0.000451, tokens/sec=1378348.84, grad_norm=0.2835, duration=0.38s
Step 9028: loss=2.9748, lr=0.000451, tokens/sec=1376294.84, grad_norm=0.3477, duration=0.38s
Step 9029: loss=3.0445, lr=0.000451, tokens/sec=1375883.23, grad_norm=0.3174, duration=0.38s
Step 9030: loss=2.9964, lr=0.000450, tokens/sec=1375256.81, grad_norm=0.3218, duration=0.38s
Step 9031: loss=3.0541, lr=0.000450, tokens/sec=1372613.72, grad_norm=0.3224, duration=0.38s
Step 9032: loss=3.0290, lr=0.000450, tokens/sec=1373990.22, grad_norm=0.3122, duration=0.38s
Step 9033: loss=3.0170, lr=0.000450, tokens/sec=1369623.25, grad_norm=0.3196, duration=0.38s
Step 9034: loss=3.0396, lr=0.000450, tokens/sec=1376902.38, grad_norm=0.3125, duration=0.38s
Step 9035: loss=3.0024, lr=0.000450, tokens/sec=1377896.28, grad_norm=0.3012, duration=0.38s
Step 9036: loss=3.0369, lr=0.000450, tokens/sec=1376744.63, grad_norm=0.3168, duration=0.38s
Step 9037: loss=3.0050, lr=0.000450, tokens/sec=1372164.06, grad_norm=0.3077, duration=0.38s
Step 9038: loss=3.0117, lr=0.000450, tokens/sec=1378161.39, grad_norm=0.2955, duration=0.38s
Step 9039: loss=2.9726, lr=0.000450, tokens/sec=1377249.91, grad_norm=0.3211, duration=0.38s
Step 9040: loss=3.0092, lr=0.000450, tokens/sec=1376829.10, grad_norm=0.2828, duration=0.38s
Step 9041: loss=3.0201, lr=0.000450, tokens/sec=1378825.91, grad_norm=0.2900, duration=0.38s
Step 9042: loss=3.0270, lr=0.000450, tokens/sec=1373364.66, grad_norm=0.3110, duration=0.38s
Step 9043: loss=2.9885, lr=0.000450, tokens/sec=1374990.23, grad_norm=0.2784, duration=0.38s
Step 9044: loss=3.0374, lr=0.000450, tokens/sec=1376379.26, grad_norm=0.2932, duration=0.38s
Step 9045: loss=2.9787, lr=0.000450, tokens/sec=1377964.49, grad_norm=0.3158, duration=0.38s
Step 9046: loss=3.0311, lr=0.000450, tokens/sec=1374783.93, grad_norm=0.2809, duration=0.38s
Step 9047: loss=3.0058, lr=0.000450, tokens/sec=1375634.48, grad_norm=0.2985, duration=0.38s
Step 9048: loss=3.0083, lr=0.000450, tokens/sec=1373962.75, grad_norm=0.3160, duration=0.38s
Step 9049: loss=2.9958, lr=0.000450, tokens/sec=1374484.03, grad_norm=0.2863, duration=0.38s
Step 9050: loss=3.0089, lr=0.000450, tokens/sec=1376525.73, grad_norm=0.2970, duration=0.38s
Step 9051: loss=3.0082, lr=0.000450, tokens/sec=1380246.09, grad_norm=0.3095, duration=0.38s
Step 9052: loss=3.0081, lr=0.000450, tokens/sec=1371326.33, grad_norm=0.2966, duration=0.38s
Step 9053: loss=2.9761, lr=0.000450, tokens/sec=1376683.43, grad_norm=0.3026, duration=0.38s
Step 9054: loss=3.0246, lr=0.000450, tokens/sec=1377506.14, grad_norm=0.2995, duration=0.38s
Step 9055: loss=3.0255, lr=0.000450, tokens/sec=1378954.74, grad_norm=0.2960, duration=0.38s
Step 9056: loss=2.9855, lr=0.000450, tokens/sec=1376310.35, grad_norm=0.2907, duration=0.38s
Step 9057: loss=2.9674, lr=0.000450, tokens/sec=1376275.89, grad_norm=0.3260, duration=0.38s
Step 9058: loss=2.9732, lr=0.000450, tokens/sec=1377509.59, grad_norm=0.2731, duration=0.38s
Step 9059: loss=2.9129, lr=0.000450, tokens/sec=1371578.66, grad_norm=0.3071, duration=0.38s
Step 9060: loss=2.9675, lr=0.000450, tokens/sec=1378678.95, grad_norm=0.3133, duration=0.38s
Step 9061: loss=2.8698, lr=0.000450, tokens/sec=1376390.46, grad_norm=0.2842, duration=0.38s
Step 9062: loss=2.9007, lr=0.000449, tokens/sec=1377456.96, grad_norm=0.3098, duration=0.38s
Step 9063: loss=2.9120, lr=0.000449, tokens/sec=1376024.42, grad_norm=0.3015, duration=0.38s
Step 9064: loss=2.9742, lr=0.000449, tokens/sec=1373389.54, grad_norm=0.2949, duration=0.38s
Step 9065: loss=2.9509, lr=0.000449, tokens/sec=1379823.45, grad_norm=0.3177, duration=0.38s
Step 9066: loss=2.9296, lr=0.000449, tokens/sec=1379201.22, grad_norm=0.3075, duration=0.38s
Step 9067: loss=2.8870, lr=0.000449, tokens/sec=1374225.49, grad_norm=0.2910, duration=0.38s
Step 9068: loss=2.9378, lr=0.000449, tokens/sec=1373917.25, grad_norm=0.3142, duration=0.38s
Step 9069: loss=3.0184, lr=0.000449, tokens/sec=1375538.97, grad_norm=0.3238, duration=0.38s
Step 9070: loss=3.0414, lr=0.000449, tokens/sec=1375362.60, grad_norm=0.3119, duration=0.38s
Step 9071: loss=3.0444, lr=0.000449, tokens/sec=1375940.91, grad_norm=0.3298, duration=0.38s
Step 9072: loss=3.0331, lr=0.000449, tokens/sec=1376797.21, grad_norm=0.3632, duration=0.38s
Step 9073: loss=3.0286, lr=0.000449, tokens/sec=1376062.31, grad_norm=0.3284, duration=0.38s
Step 9074: loss=3.0406, lr=0.000449, tokens/sec=1377602.79, grad_norm=0.3343, duration=0.38s
Step 9075: loss=3.0110, lr=0.000449, tokens/sec=1373635.75, grad_norm=0.3431, duration=0.38s
Step 9076: loss=2.9930, lr=0.000449, tokens/sec=1375510.58, grad_norm=0.3093, duration=0.38s
Step 9077: loss=3.0674, lr=0.000449, tokens/sec=1377399.15, grad_norm=0.3295, duration=0.38s
Step 9078: loss=3.1221, lr=0.000449, tokens/sec=1373718.13, grad_norm=0.3310, duration=0.38s
Step 9079: loss=3.0890, lr=0.000449, tokens/sec=1375588.02, grad_norm=0.3362, duration=0.38s
Step 9080: loss=3.0878, lr=0.000449, tokens/sec=1375767.88, grad_norm=0.3201, duration=0.38s
Step 9081: loss=3.0116, lr=0.000449, tokens/sec=1376121.73, grad_norm=0.3258, duration=0.38s
Step 9082: loss=3.0386, lr=0.000449, tokens/sec=1376617.94, grad_norm=0.3116, duration=0.38s
Step 9083: loss=3.0304, lr=0.000449, tokens/sec=1376972.22, grad_norm=0.2963, duration=0.38s
Step 9084: loss=2.9339, lr=0.000449, tokens/sec=1374736.66, grad_norm=0.3178, duration=0.38s
Step 9085: loss=3.0680, lr=0.000449, tokens/sec=1373833.13, grad_norm=0.2972, duration=0.38s
Step 9086: loss=2.9778, lr=0.000449, tokens/sec=1372293.36, grad_norm=0.3018, duration=0.38s
Step 9087: loss=3.0911, lr=0.000449, tokens/sec=1372867.37, grad_norm=0.3054, duration=0.38s
Step 9088: loss=3.0687, lr=0.000449, tokens/sec=1373356.94, grad_norm=0.3087, duration=0.38s
Step 9089: loss=3.0363, lr=0.000449, tokens/sec=1370992.90, grad_norm=0.3064, duration=0.38s
Step 9090: loss=2.9410, lr=0.000449, tokens/sec=1375018.61, grad_norm=0.3384, duration=0.38s
Step 9091: loss=3.0395, lr=0.000449, tokens/sec=1371528.18, grad_norm=0.3389, duration=0.38s
Step 9092: loss=3.0637, lr=0.000449, tokens/sec=1372313.91, grad_norm=0.3152, duration=0.38s
Step 9093: loss=3.0122, lr=0.000449, tokens/sec=1375589.74, grad_norm=0.3266, duration=0.38s
Step 9094: loss=2.9564, lr=0.000449, tokens/sec=1374973.90, grad_norm=0.2878, duration=0.38s
Step 9095: loss=2.9975, lr=0.000448, tokens/sec=1375443.47, grad_norm=0.3430, duration=0.38s
Step 9096: loss=2.9988, lr=0.000448, tokens/sec=1379938.61, grad_norm=0.2984, duration=0.38s
Step 9097: loss=3.0140, lr=0.000448, tokens/sec=1373243.73, grad_norm=0.3147, duration=0.38s
Step 9098: loss=3.0539, lr=0.000448, tokens/sec=1373582.55, grad_norm=0.3303, duration=0.38s
Step 9099: loss=3.0000, lr=0.000448, tokens/sec=1371275.88, grad_norm=0.2915, duration=0.38s
Step 9100/19073 (47.7%), Elapsed time: 3611.07s, Steps per hour: 9072.09, Estimated hours remaining: 1.10
Step 9100: loss=2.9741, lr=0.000448, tokens/sec=1372409.83, grad_norm=0.3129, duration=0.38s
Step 9101: loss=2.9684, lr=0.000448, tokens/sec=1373187.14, grad_norm=0.2901, duration=0.38s
Step 9102: loss=2.9411, lr=0.000448, tokens/sec=1372761.95, grad_norm=0.3036, duration=0.38s
Step 9103: loss=2.9959, lr=0.000448, tokens/sec=1372456.09, grad_norm=0.2944, duration=0.38s
Step 9104: loss=2.9754, lr=0.000448, tokens/sec=1378278.87, grad_norm=0.3073, duration=0.38s
Step 9105: loss=2.9277, lr=0.000448, tokens/sec=1374492.62, grad_norm=0.2874, duration=0.38s
Step 9106: loss=2.9387, lr=0.000448, tokens/sec=1371576.94, grad_norm=0.2762, duration=0.38s
Step 9107: loss=2.9730, lr=0.000448, tokens/sec=1376640.34, grad_norm=0.2984, duration=0.38s
Step 9108: loss=2.9482, lr=0.000448, tokens/sec=1373562.82, grad_norm=0.2959, duration=0.38s
Step 9109: loss=2.8991, lr=0.000448, tokens/sec=1374786.50, grad_norm=0.2797, duration=0.38s
Step 9110: loss=2.9022, lr=0.000448, tokens/sec=1374939.51, grad_norm=0.3131, duration=0.38s
Step 9111: loss=2.9220, lr=0.000448, tokens/sec=1377525.13, grad_norm=0.2944, duration=0.38s
Step 9112: loss=2.9385, lr=0.000448, tokens/sec=1375761.86, grad_norm=0.2979, duration=0.38s
Step 9113: loss=2.9642, lr=0.000448, tokens/sec=1376926.52, grad_norm=0.3122, duration=0.38s
Step 9114: loss=2.9670, lr=0.000448, tokens/sec=1380204.51, grad_norm=0.3182, duration=0.38s
Step 9115: loss=2.9543, lr=0.000448, tokens/sec=1373565.40, grad_norm=0.3177, duration=0.38s
Step 9116: loss=2.9981, lr=0.000448, tokens/sec=1372705.40, grad_norm=0.3304, duration=0.38s
Step 9117: loss=3.0051, lr=0.000448, tokens/sec=1374984.22, grad_norm=0.3190, duration=0.38s
Step 9118: loss=3.0085, lr=0.000448, tokens/sec=1376292.26, grad_norm=0.3119, duration=0.38s
Step 9119: loss=3.0290, lr=0.000448, tokens/sec=1376960.15, grad_norm=0.3288, duration=0.38s
Step 9120: loss=3.0087, lr=0.000448, tokens/sec=1375279.17, grad_norm=0.3115, duration=0.38s
Step 9121: loss=3.0203, lr=0.000448, tokens/sec=1372553.74, grad_norm=0.3195, duration=0.38s
Step 9122: loss=3.0372, lr=0.000448, tokens/sec=1373783.35, grad_norm=0.3072, duration=0.38s
Step 9123: loss=3.1435, lr=0.000448, tokens/sec=1375645.67, grad_norm=0.3585, duration=0.38s
Step 9124: loss=3.0632, lr=0.000448, tokens/sec=1376293.12, grad_norm=0.3123, duration=0.38s
Step 9125: loss=3.0683, lr=0.000448, tokens/sec=1378962.52, grad_norm=0.3252, duration=0.38s
Step 9126: loss=3.1188, lr=0.000448, tokens/sec=1377832.39, grad_norm=0.3197, duration=0.38s
Step 9127: loss=3.0447, lr=0.000447, tokens/sec=1379076.67, grad_norm=0.2981, duration=0.38s
Step 9128: loss=3.0699, lr=0.000447, tokens/sec=1376070.06, grad_norm=0.3029, duration=0.38s
Step 9129: loss=3.0552, lr=0.000447, tokens/sec=1375220.68, grad_norm=0.3112, duration=0.38s
Step 9130: loss=3.0447, lr=0.000447, tokens/sec=1375812.64, grad_norm=0.2874, duration=0.38s
Step 9131: loss=3.0194, lr=0.000447, tokens/sec=1375121.79, grad_norm=0.3072, duration=0.38s
Step 9132: loss=3.0339, lr=0.000447, tokens/sec=1374770.17, grad_norm=0.3156, duration=0.38s
Step 9133: loss=2.9661, lr=0.000447, tokens/sec=1375783.38, grad_norm=0.2909, duration=0.38s
Step 9134: loss=3.0659, lr=0.000447, tokens/sec=1377985.21, grad_norm=0.3128, duration=0.38s
Step 9135: loss=3.0219, lr=0.000447, tokens/sec=1376991.19, grad_norm=0.3259, duration=0.38s
Step 9136: loss=2.9923, lr=0.000447, tokens/sec=1379755.92, grad_norm=0.3436, duration=0.38s
Step 9137: loss=3.0783, lr=0.000447, tokens/sec=1375665.46, grad_norm=0.3151, duration=0.38s
Step 9138: loss=2.9575, lr=0.000447, tokens/sec=1374629.24, grad_norm=0.3088, duration=0.38s
Step 9139: loss=2.9491, lr=0.000447, tokens/sec=1374729.78, grad_norm=0.3492, duration=0.38s
Step 9140: loss=2.9857, lr=0.000447, tokens/sec=1376949.80, grad_norm=0.3088, duration=0.38s
Step 9141: loss=3.0047, lr=0.000447, tokens/sec=1373126.26, grad_norm=0.2835, duration=0.38s
Step 9142: loss=3.0978, lr=0.000447, tokens/sec=1376813.59, grad_norm=0.3079, duration=0.38s
Step 9143: loss=2.9368, lr=0.000447, tokens/sec=1373619.45, grad_norm=0.2929, duration=0.38s
Step 9144: loss=3.0543, lr=0.000447, tokens/sec=1376362.03, grad_norm=0.2991, duration=0.38s
Step 9145: loss=3.0389, lr=0.000447, tokens/sec=1374841.51, grad_norm=0.2891, duration=0.38s
Step 9146: loss=2.9762, lr=0.000447, tokens/sec=1373658.92, grad_norm=0.2806, duration=0.38s
Step 9147: loss=2.9978, lr=0.000447, tokens/sec=1378589.06, grad_norm=0.2959, duration=0.38s
Step 9148: loss=3.0262, lr=0.000447, tokens/sec=1377094.67, grad_norm=0.2838, duration=0.38s
Step 9149: loss=3.0302, lr=0.000447, tokens/sec=1373120.26, grad_norm=0.2778, duration=0.38s
Step 9150: loss=2.9646, lr=0.000447, tokens/sec=1370092.58, grad_norm=0.2841, duration=0.38s
Step 9151: loss=2.9981, lr=0.000447, tokens/sec=1374083.80, grad_norm=0.2844, duration=0.38s
Step 9152: loss=3.0150, lr=0.000447, tokens/sec=1378405.00, grad_norm=0.3173, duration=0.38s
Step 9153: loss=3.0451, lr=0.000447, tokens/sec=1376467.14, grad_norm=0.2929, duration=0.38s
Step 9154: loss=2.9884, lr=0.000447, tokens/sec=1374876.76, grad_norm=0.3092, duration=0.38s
Step 9155: loss=2.9059, lr=0.000447, tokens/sec=1375763.58, grad_norm=0.2926, duration=0.38s
Step 9156: loss=2.9697, lr=0.000447, tokens/sec=1372039.06, grad_norm=0.2844, duration=0.38s
Step 9157: loss=2.9095, lr=0.000447, tokens/sec=1371929.49, grad_norm=0.3257, duration=0.38s
Step 9158: loss=2.8970, lr=0.000447, tokens/sec=1373073.10, grad_norm=0.2947, duration=0.38s
Step 9159: loss=2.9270, lr=0.000447, tokens/sec=1374731.50, grad_norm=0.2905, duration=0.38s
Step 9160: loss=2.9448, lr=0.000446, tokens/sec=1378377.35, grad_norm=0.3098, duration=0.38s
Step 9161: loss=2.8836, lr=0.000446, tokens/sec=1374736.66, grad_norm=0.3094, duration=0.38s
Step 9162: loss=2.9425, lr=0.000446, tokens/sec=1376271.59, grad_norm=0.2827, duration=0.38s
Step 9163: loss=2.9563, lr=0.000446, tokens/sec=1376127.76, grad_norm=0.3159, duration=0.38s
Step 9164: loss=2.9697, lr=0.000446, tokens/sec=1373140.83, grad_norm=0.3008, duration=0.38s
Step 9165: loss=2.9629, lr=0.000446, tokens/sec=1376378.40, grad_norm=0.2943, duration=0.38s
Step 9166: loss=2.9740, lr=0.000446, tokens/sec=1373160.56, grad_norm=0.3242, duration=0.38s
Step 9167: loss=3.0368, lr=0.000446, tokens/sec=1377714.99, grad_norm=0.3112, duration=0.38s
Step 9168: loss=3.0829, lr=0.000446, tokens/sec=1374844.95, grad_norm=0.3139, duration=0.38s
Step 9169: loss=3.0227, lr=0.000446, tokens/sec=1373834.85, grad_norm=0.3343, duration=0.38s
Step 9170: loss=2.9947, lr=0.000446, tokens/sec=1375079.65, grad_norm=0.3215, duration=0.38s
Step 9171: loss=3.0621, lr=0.000446, tokens/sec=1373581.70, grad_norm=0.3142, duration=0.38s
Step 9172: loss=3.0546, lr=0.000446, tokens/sec=1373626.31, grad_norm=0.3137, duration=0.38s
Step 9173: loss=3.0439, lr=0.000446, tokens/sec=1373959.31, grad_norm=0.3114, duration=0.38s
Step 9174: loss=3.0371, lr=0.000446, tokens/sec=1373262.60, grad_norm=0.3204, duration=0.38s
Step 9175: loss=2.9962, lr=0.000446, tokens/sec=1375484.76, grad_norm=0.3108, duration=0.38s
Step 9176: loss=3.0425, lr=0.000446, tokens/sec=1373790.22, grad_norm=0.3361, duration=0.38s
Step 9177: loss=3.0142, lr=0.000446, tokens/sec=1375530.37, grad_norm=0.2972, duration=0.38s
Step 9178: loss=2.9998, lr=0.000446, tokens/sec=1376811.00, grad_norm=0.3063, duration=0.38s
Step 9179: loss=3.0168, lr=0.000446, tokens/sec=1378163.12, grad_norm=0.3240, duration=0.38s
Step 9180: loss=2.9604, lr=0.000446, tokens/sec=1373429.85, grad_norm=0.3017, duration=0.38s
Step 9181: loss=2.9143, lr=0.000446, tokens/sec=1371066.41, grad_norm=0.2984, duration=0.38s
Step 9182: loss=2.9399, lr=0.000446, tokens/sec=1376658.44, grad_norm=0.3083, duration=0.38s
Step 9183: loss=3.0171, lr=0.000446, tokens/sec=1376642.07, grad_norm=0.3002, duration=0.38s
Step 9184: loss=3.0012, lr=0.000446, tokens/sec=1370134.41, grad_norm=0.2912, duration=0.38s
Step 9185: loss=3.0143, lr=0.000446, tokens/sec=1374302.78, grad_norm=0.2975, duration=0.38s
Step 9186: loss=2.9779, lr=0.000446, tokens/sec=1374893.09, grad_norm=0.2827, duration=0.38s
Step 9187: loss=3.0428, lr=0.000446, tokens/sec=1379465.10, grad_norm=0.3103, duration=0.38s
Step 9188: loss=3.0245, lr=0.000446, tokens/sec=1375699.89, grad_norm=0.3156, duration=0.38s
Step 9189: loss=3.0217, lr=0.000446, tokens/sec=1377039.48, grad_norm=0.2901, duration=0.38s
Step 9190: loss=3.0235, lr=0.000446, tokens/sec=1373004.52, grad_norm=0.3132, duration=0.38s
Step 9191: loss=3.0122, lr=0.000446, tokens/sec=1371932.06, grad_norm=0.2926, duration=0.38s
Step 9192: loss=3.0151, lr=0.000445, tokens/sec=1377845.34, grad_norm=0.3017, duration=0.38s
Step 9193: loss=2.9914, lr=0.000445, tokens/sec=1373700.11, grad_norm=0.3064, duration=0.38s
Step 9194: loss=2.9989, lr=0.000445, tokens/sec=1373372.38, grad_norm=0.2776, duration=0.38s
Step 9195: loss=3.0077, lr=0.000445, tokens/sec=1375742.06, grad_norm=0.2985, duration=0.38s
Step 9196: loss=3.0057, lr=0.000445, tokens/sec=1380072.85, grad_norm=0.2980, duration=0.38s
Step 9197: loss=2.9431, lr=0.000445, tokens/sec=1373170.85, grad_norm=0.2949, duration=0.38s
Step 9198: loss=2.9984, lr=0.000445, tokens/sec=1376911.87, grad_norm=0.2985, duration=0.38s
Step 9199: loss=3.0012, lr=0.000445, tokens/sec=1376629.14, grad_norm=0.2825, duration=0.38s
Step 9200/19073 (48.2%), Elapsed time: 3649.29s, Steps per hour: 9075.73, Estimated hours remaining: 1.09
Step 9200: loss=2.9888, lr=0.000445, tokens/sec=1379317.15, grad_norm=0.2866, duration=0.38s
Step 9201: loss=2.9084, lr=0.000445, tokens/sec=1377434.53, grad_norm=0.3005, duration=0.38s
Step 9202: loss=2.9846, lr=0.000445, tokens/sec=1377352.56, grad_norm=0.2734, duration=0.38s
Step 9203: loss=2.9243, lr=0.000445, tokens/sec=1375531.23, grad_norm=0.2996, duration=0.38s
Step 9204: loss=3.0034, lr=0.000445, tokens/sec=1376037.34, grad_norm=0.3052, duration=0.38s
Step 9205: loss=2.9398, lr=0.000445, tokens/sec=1372779.09, grad_norm=0.2921, duration=0.38s
Step 9206: loss=2.9305, lr=0.000445, tokens/sec=1377085.18, grad_norm=0.3071, duration=0.38s
Step 9207: loss=2.9587, lr=0.000445, tokens/sec=1379224.58, grad_norm=0.3009, duration=0.38s
Step 9208: loss=2.8808, lr=0.000445, tokens/sec=1376340.50, grad_norm=0.3034, duration=0.38s
Step 9209: loss=2.9467, lr=0.000445, tokens/sec=1377503.55, grad_norm=0.3049, duration=0.38s
Step 9210: loss=2.9378, lr=0.000445, tokens/sec=1378623.63, grad_norm=0.2816, duration=0.38s
Step 9211: loss=2.9845, lr=0.000445, tokens/sec=1376708.43, grad_norm=0.3170, duration=0.38s
Step 9212: loss=2.9977, lr=0.000445, tokens/sec=1367744.86, grad_norm=0.3020, duration=0.38s
Step 9213: loss=3.0121, lr=0.000445, tokens/sec=1376745.49, grad_norm=0.3045, duration=0.38s
Step 9214: loss=3.0236, lr=0.000445, tokens/sec=1378991.92, grad_norm=0.3275, duration=0.38s
Step 9215: loss=3.0619, lr=0.000445, tokens/sec=1375367.77, grad_norm=0.2788, duration=0.38s
Step 9216: loss=3.0364, lr=0.000445, tokens/sec=1378469.81, grad_norm=0.3390, duration=0.38s
Step 9217: loss=3.0296, lr=0.000445, tokens/sec=1377106.74, grad_norm=0.2828, duration=0.38s
Step 9218: loss=2.9920, lr=0.000445, tokens/sec=1374171.39, grad_norm=0.3085, duration=0.38s
Step 9219: loss=3.0216, lr=0.000445, tokens/sec=1375463.26, grad_norm=0.3126, duration=0.38s
Step 9220: loss=2.9842, lr=0.000445, tokens/sec=1377254.22, grad_norm=0.3097, duration=0.38s
Step 9221: loss=3.0308, lr=0.000445, tokens/sec=1373838.28, grad_norm=0.3226, duration=0.38s
Step 9222: loss=3.0198, lr=0.000445, tokens/sec=1374491.76, grad_norm=0.2978, duration=0.38s
Step 9223: loss=3.0291, lr=0.000445, tokens/sec=1373666.64, grad_norm=0.3083, duration=0.38s
Step 9224: loss=3.0317, lr=0.000444, tokens/sec=1371397.32, grad_norm=0.3082, duration=0.38s
Step 9225: loss=3.0170, lr=0.000444, tokens/sec=1374380.94, grad_norm=0.2845, duration=0.38s
Step 9226: loss=2.9973, lr=0.000444, tokens/sec=1374963.58, grad_norm=0.3030, duration=0.38s
Step 9227: loss=2.9916, lr=0.000444, tokens/sec=1374003.10, grad_norm=0.2861, duration=0.38s
Step 9228: loss=3.0152, lr=0.000444, tokens/sec=1374563.07, grad_norm=0.2836, duration=0.38s
Step 9229: loss=2.9546, lr=0.000444, tokens/sec=1375789.40, grad_norm=0.3103, duration=0.38s
Step 9230: loss=3.0229, lr=0.000444, tokens/sec=1374994.53, grad_norm=0.2793, duration=0.38s
Step 9231: loss=3.0057, lr=0.000444, tokens/sec=1374770.17, grad_norm=0.2826, duration=0.38s
Step 9232: loss=3.0414, lr=0.000444, tokens/sec=1377729.67, grad_norm=0.2916, duration=0.38s
Step 9233: loss=2.9625, lr=0.000444, tokens/sec=1372238.55, grad_norm=0.2825, duration=0.38s
Step 9234: loss=3.0268, lr=0.000444, tokens/sec=1374246.10, grad_norm=0.3019, duration=0.38s
Step 9235: loss=2.9696, lr=0.000444, tokens/sec=1376931.69, grad_norm=0.2886, duration=0.38s
Step 9236: loss=3.0311, lr=0.000444, tokens/sec=1377596.75, grad_norm=0.2811, duration=0.38s
Step 9237: loss=2.9798, lr=0.000444, tokens/sec=1373978.20, grad_norm=0.2747, duration=0.38s
Step 9238: loss=3.0065, lr=0.000444, tokens/sec=1375244.77, grad_norm=0.3082, duration=0.38s
Step 9239: loss=2.9924, lr=0.000444, tokens/sec=1375136.41, grad_norm=0.2776, duration=0.38s
Step 9240: loss=3.0024, lr=0.000444, tokens/sec=1377559.64, grad_norm=0.2860, duration=0.38s
Step 9241: loss=2.9892, lr=0.000444, tokens/sec=1374203.16, grad_norm=0.2784, duration=0.38s
Step 9242: loss=2.9918, lr=0.000444, tokens/sec=1377841.89, grad_norm=0.2944, duration=0.38s
Step 9243: loss=2.9984, lr=0.000444, tokens/sec=1374047.74, grad_norm=0.2920, duration=0.38s
Step 9244: loss=3.0167, lr=0.000444, tokens/sec=1378233.08, grad_norm=0.2902, duration=0.38s
Step 9245: loss=2.9920, lr=0.000444, tokens/sec=1373275.46, grad_norm=0.2859, duration=0.38s
Step 9246: loss=2.9721, lr=0.000444, tokens/sec=1379779.30, grad_norm=0.2723, duration=0.38s
Step 9247: loss=2.9706, lr=0.000444, tokens/sec=1373882.92, grad_norm=0.3146, duration=0.38s
Step 9248: loss=2.9583, lr=0.000444, tokens/sec=1378405.86, grad_norm=0.2625, duration=0.38s
Step 9249: loss=2.9225, lr=0.000444, tokens/sec=1375523.48, grad_norm=0.2877, duration=0.38s
Validation loss at step 9250: 3.7654640674591064
Step 9250: loss=2.9419, lr=0.000444, tokens/sec=154035.76, grad_norm=0.2857, duration=3.40s
Step 9251: loss=2.8473, lr=0.000444, tokens/sec=1379538.66, grad_norm=0.2694, duration=0.38s
Step 9252: loss=2.9067, lr=0.000444, tokens/sec=1380796.43, grad_norm=0.2912, duration=0.38s
Step 9253: loss=2.9010, lr=0.000444, tokens/sec=1371673.62, grad_norm=0.2752, duration=0.38s
Step 9254: loss=2.9720, lr=0.000444, tokens/sec=1376679.12, grad_norm=0.2872, duration=0.38s
Step 9255: loss=2.9438, lr=0.000444, tokens/sec=1375668.05, grad_norm=0.2887, duration=0.38s
Step 9256: loss=2.8935, lr=0.000444, tokens/sec=1376380.12, grad_norm=0.2796, duration=0.38s
Step 9257: loss=2.9019, lr=0.000443, tokens/sec=1375069.33, grad_norm=0.2877, duration=0.38s
Step 9258: loss=2.9719, lr=0.000443, tokens/sec=1378161.39, grad_norm=0.2878, duration=0.38s
Step 9259: loss=3.0140, lr=0.000443, tokens/sec=1377895.42, grad_norm=0.2979, duration=0.38s
Step 9260: loss=3.0197, lr=0.000443, tokens/sec=1374917.16, grad_norm=0.3193, duration=0.38s
Step 9261: loss=3.0189, lr=0.000443, tokens/sec=1375157.04, grad_norm=0.3088, duration=0.38s
Step 9262: loss=3.0444, lr=0.000443, tokens/sec=1375118.35, grad_norm=0.3600, duration=0.38s
Step 9263: loss=3.0041, lr=0.000443, tokens/sec=1376245.75, grad_norm=0.3004, duration=0.38s
Step 9264: loss=3.0543, lr=0.000443, tokens/sec=1376075.23, grad_norm=0.3213, duration=0.38s
Step 9265: loss=2.9611, lr=0.000443, tokens/sec=1376961.87, grad_norm=0.2996, duration=0.38s
Step 9266: loss=3.0145, lr=0.000443, tokens/sec=1375150.16, grad_norm=0.3096, duration=0.38s
Step 9267: loss=3.0540, lr=0.000443, tokens/sec=1376962.73, grad_norm=0.3019, duration=0.38s
Step 9268: loss=3.1417, lr=0.000443, tokens/sec=1377149.00, grad_norm=0.3147, duration=0.38s
Step 9269: loss=3.0897, lr=0.000443, tokens/sec=1374450.53, grad_norm=0.3063, duration=0.38s
Step 9270: loss=3.0579, lr=0.000443, tokens/sec=1378349.71, grad_norm=0.3088, duration=0.38s
Step 9271: loss=2.9997, lr=0.000443, tokens/sec=1378774.90, grad_norm=0.2890, duration=0.38s
Step 9272: loss=3.0603, lr=0.000443, tokens/sec=1371079.24, grad_norm=0.3159, duration=0.38s
Step 9273: loss=2.9571, lr=0.000443, tokens/sec=1376422.34, grad_norm=0.2964, duration=0.38s
Step 9274: loss=2.9717, lr=0.000443, tokens/sec=1375334.22, grad_norm=0.3065, duration=0.38s
Step 9275: loss=3.0570, lr=0.000443, tokens/sec=1377739.16, grad_norm=0.2839, duration=0.38s
Step 9276: loss=2.9916, lr=0.000443, tokens/sec=1373874.33, grad_norm=0.3066, duration=0.38s
Step 9277: loss=3.0382, lr=0.000443, tokens/sec=1375535.53, grad_norm=0.2982, duration=0.38s
Step 9278: loss=3.0768, lr=0.000443, tokens/sec=1373493.33, grad_norm=0.2901, duration=0.38s
Step 9279: loss=2.9839, lr=0.000443, tokens/sec=1376225.94, grad_norm=0.3387, duration=0.38s
Step 9280: loss=2.9782, lr=0.000443, tokens/sec=1378551.90, grad_norm=0.3058, duration=0.38s
Step 9281: loss=3.0340, lr=0.000443, tokens/sec=1375453.79, grad_norm=0.3041, duration=0.38s
Step 9282: loss=3.0391, lr=0.000443, tokens/sec=1368243.56, grad_norm=0.3008, duration=0.38s
Step 9283: loss=3.0213, lr=0.000443, tokens/sec=1378106.11, grad_norm=0.2884, duration=0.38s
Step 9284: loss=2.9279, lr=0.000443, tokens/sec=1378440.43, grad_norm=0.2814, duration=0.38s
Step 9285: loss=2.9916, lr=0.000443, tokens/sec=1378406.73, grad_norm=0.2979, duration=0.38s
Step 9286: loss=2.9835, lr=0.000443, tokens/sec=1373682.95, grad_norm=0.2832, duration=0.38s
Step 9287: loss=3.0167, lr=0.000443, tokens/sec=1375927.99, grad_norm=0.3045, duration=0.38s
Step 9288: loss=3.0759, lr=0.000443, tokens/sec=1377035.16, grad_norm=0.2990, duration=0.38s
Step 9289: loss=2.9580, lr=0.000442, tokens/sec=1377801.32, grad_norm=0.2829, duration=0.38s
Step 9290: loss=2.9628, lr=0.000442, tokens/sec=1374544.17, grad_norm=0.3064, duration=0.38s
Step 9291: loss=2.9738, lr=0.000442, tokens/sec=1376185.46, grad_norm=0.2865, duration=0.38s
Step 9292: loss=2.9455, lr=0.000442, tokens/sec=1378024.94, grad_norm=0.2872, duration=0.38s
Step 9293: loss=3.0063, lr=0.000442, tokens/sec=1375664.60, grad_norm=0.3013, duration=0.38s
Step 9294: loss=2.9376, lr=0.000442, tokens/sec=1377717.58, grad_norm=0.2929, duration=0.38s
Step 9295: loss=2.9320, lr=0.000442, tokens/sec=1374357.75, grad_norm=0.2931, duration=0.38s
Step 9296: loss=2.9024, lr=0.000442, tokens/sec=1379549.91, grad_norm=0.2878, duration=0.38s
Step 9297: loss=2.9900, lr=0.000442, tokens/sec=1374006.53, grad_norm=0.2841, duration=0.38s
Step 9298: loss=2.9134, lr=0.000442, tokens/sec=1378302.19, grad_norm=0.2876, duration=0.38s
Step 9299: loss=2.9225, lr=0.000442, tokens/sec=1373640.04, grad_norm=0.2783, duration=0.38s
Step 9300/19073 (48.8%), Elapsed time: 3690.51s, Steps per hour: 9071.92, Estimated hours remaining: 1.08
Step 9300: loss=2.8858, lr=0.000442, tokens/sec=1379411.45, grad_norm=0.2876, duration=0.38s
Step 9301: loss=2.8963, lr=0.000442, tokens/sec=1381532.06, grad_norm=0.2999, duration=0.38s
Step 9302: loss=2.9706, lr=0.000442, tokens/sec=1376300.87, grad_norm=0.2895, duration=0.38s
Step 9303: loss=2.9440, lr=0.000442, tokens/sec=1377800.45, grad_norm=0.3204, duration=0.38s
Step 9304: loss=2.8988, lr=0.000442, tokens/sec=1379118.19, grad_norm=0.2900, duration=0.38s
Step 9305: loss=2.9933, lr=0.000442, tokens/sec=1378309.10, grad_norm=0.3203, duration=0.38s
Step 9306: loss=2.9902, lr=0.000442, tokens/sec=1377841.03, grad_norm=0.3249, duration=0.38s
Step 9307: loss=3.0328, lr=0.000442, tokens/sec=1377423.31, grad_norm=0.2899, duration=0.38s
Step 9308: loss=2.9910, lr=0.000442, tokens/sec=1375274.87, grad_norm=0.3331, duration=0.38s
Step 9309: loss=3.0243, lr=0.000442, tokens/sec=1376784.28, grad_norm=0.3041, duration=0.38s
Step 9310: loss=2.9798, lr=0.000442, tokens/sec=1377086.90, grad_norm=0.3126, duration=0.38s
Step 9311: loss=3.0280, lr=0.000442, tokens/sec=1380270.35, grad_norm=0.3387, duration=0.38s
Step 9312: loss=3.0650, lr=0.000442, tokens/sec=1375746.36, grad_norm=0.3069, duration=0.38s
Step 9313: loss=3.1212, lr=0.000442, tokens/sec=1377747.79, grad_norm=0.3363, duration=0.38s
Step 9314: loss=3.0404, lr=0.000442, tokens/sec=1376614.49, grad_norm=0.3336, duration=0.38s
Step 9315: loss=3.0644, lr=0.000442, tokens/sec=1375765.30, grad_norm=0.2998, duration=0.38s
Step 9316: loss=3.1026, lr=0.000442, tokens/sec=1372224.85, grad_norm=0.3403, duration=0.38s
Step 9317: loss=3.0425, lr=0.000442, tokens/sec=1375790.26, grad_norm=0.3017, duration=0.38s
Step 9318: loss=3.0697, lr=0.000442, tokens/sec=1374301.06, grad_norm=0.3060, duration=0.38s
Step 9319: loss=3.0247, lr=0.000442, tokens/sec=1376314.65, grad_norm=0.3171, duration=0.38s
Step 9320: loss=3.0467, lr=0.000442, tokens/sec=1377014.47, grad_norm=0.3038, duration=0.38s
Step 9321: loss=3.0086, lr=0.000441, tokens/sec=1378872.59, grad_norm=0.2846, duration=0.38s
Step 9322: loss=3.0241, lr=0.000441, tokens/sec=1378707.47, grad_norm=0.3155, duration=0.38s
Step 9323: loss=2.9894, lr=0.000441, tokens/sec=1374983.36, grad_norm=0.2817, duration=0.38s
Step 9324: loss=3.0322, lr=0.000441, tokens/sec=1379594.05, grad_norm=0.3064, duration=0.38s
Step 9325: loss=3.0257, lr=0.000441, tokens/sec=1376372.37, grad_norm=0.2986, duration=0.38s
Step 9326: loss=3.0028, lr=0.000441, tokens/sec=1376727.39, grad_norm=0.3218, duration=0.38s
Step 9327: loss=3.0284, lr=0.000441, tokens/sec=1375583.71, grad_norm=0.2882, duration=0.38s
Step 9328: loss=2.9806, lr=0.000441, tokens/sec=1376070.06, grad_norm=0.3129, duration=0.38s
Step 9329: loss=2.9316, lr=0.000441, tokens/sec=1378847.52, grad_norm=0.3256, duration=0.38s
Step 9330: loss=2.9640, lr=0.000441, tokens/sec=1373121.11, grad_norm=0.2840, duration=0.38s
Step 9331: loss=3.0142, lr=0.000441, tokens/sec=1378359.21, grad_norm=0.2788, duration=0.38s
Step 9332: loss=3.0818, lr=0.000441, tokens/sec=1375458.95, grad_norm=0.2906, duration=0.38s
Step 9333: loss=2.9423, lr=0.000441, tokens/sec=1374385.24, grad_norm=0.2990, duration=0.38s
Step 9334: loss=3.0478, lr=0.000441, tokens/sec=1373902.66, grad_norm=0.2848, duration=0.38s
Step 9335: loss=3.0224, lr=0.000441, tokens/sec=1374193.71, grad_norm=0.2784, duration=0.38s
Step 9336: loss=2.9585, lr=0.000441, tokens/sec=1375136.41, grad_norm=0.2871, duration=0.38s
Step 9337: loss=2.9974, lr=0.000441, tokens/sec=1375445.19, grad_norm=0.2852, duration=0.38s
Step 9338: loss=3.0491, lr=0.000441, tokens/sec=1376818.76, grad_norm=0.2842, duration=0.38s
Step 9339: loss=2.9869, lr=0.000441, tokens/sec=1376367.20, grad_norm=0.2643, duration=0.38s
Step 9340: loss=2.9613, lr=0.000441, tokens/sec=1374359.47, grad_norm=0.2836, duration=0.38s
Step 9341: loss=2.9882, lr=0.000441, tokens/sec=1375360.02, grad_norm=0.2797, duration=0.38s
Step 9342: loss=3.0083, lr=0.000441, tokens/sec=1378984.14, grad_norm=0.2937, duration=0.38s
Step 9343: loss=3.0400, lr=0.000441, tokens/sec=1373982.49, grad_norm=0.2758, duration=0.38s
Step 9344: loss=2.9500, lr=0.000441, tokens/sec=1373349.22, grad_norm=0.2785, duration=0.38s
Step 9345: loss=2.9161, lr=0.000441, tokens/sec=1376989.46, grad_norm=0.2714, duration=0.38s
Step 9346: loss=2.9582, lr=0.000441, tokens/sec=1375965.01, grad_norm=0.2885, duration=0.38s
Step 9347: loss=2.8975, lr=0.000441, tokens/sec=1376508.50, grad_norm=0.2912, duration=0.38s
Step 9348: loss=2.9018, lr=0.000441, tokens/sec=1374399.84, grad_norm=0.2919, duration=0.38s
Step 9349: loss=2.9246, lr=0.000441, tokens/sec=1371942.33, grad_norm=0.3011, duration=0.38s
Step 9350: loss=2.8714, lr=0.000441, tokens/sec=1376356.00, grad_norm=0.3010, duration=0.38s
Step 9351: loss=2.9361, lr=0.000441, tokens/sec=1377453.51, grad_norm=0.3026, duration=0.38s
Step 9352: loss=2.9352, lr=0.000441, tokens/sec=1373753.32, grad_norm=0.3091, duration=0.38s
Step 9353: loss=2.9872, lr=0.000441, tokens/sec=1376039.92, grad_norm=0.3024, duration=0.38s
Step 9354: loss=2.9595, lr=0.000440, tokens/sec=1375471.86, grad_norm=0.3030, duration=0.38s
Step 9355: loss=2.9563, lr=0.000440, tokens/sec=1377663.21, grad_norm=0.2962, duration=0.38s
Step 9356: loss=2.9581, lr=0.000440, tokens/sec=1375943.49, grad_norm=0.3070, duration=0.38s
Step 9357: loss=3.0324, lr=0.000440, tokens/sec=1373387.82, grad_norm=0.2975, duration=0.38s
Step 9358: loss=3.0583, lr=0.000440, tokens/sec=1378876.92, grad_norm=0.3174, duration=0.38s
Step 9359: loss=2.9904, lr=0.000440, tokens/sec=1376973.08, grad_norm=0.3102, duration=0.38s
Step 9360: loss=3.0346, lr=0.000440, tokens/sec=1375833.30, grad_norm=0.3242, duration=0.38s
Step 9361: loss=3.0558, lr=0.000440, tokens/sec=1377023.09, grad_norm=0.3253, duration=0.38s
Step 9362: loss=3.0382, lr=0.000440, tokens/sec=1377939.45, grad_norm=0.3156, duration=0.38s
Step 9363: loss=3.0647, lr=0.000440, tokens/sec=1374899.97, grad_norm=0.3273, duration=0.38s
Step 9364: loss=2.9944, lr=0.000440, tokens/sec=1377916.14, grad_norm=0.3099, duration=0.38s
Step 9365: loss=3.0104, lr=0.000440, tokens/sec=1377523.40, grad_norm=0.3096, duration=0.38s
Step 9366: loss=3.0243, lr=0.000440, tokens/sec=1374616.35, grad_norm=0.3273, duration=0.38s
Step 9367: loss=2.9966, lr=0.000440, tokens/sec=1375377.23, grad_norm=0.2959, duration=0.38s
Step 9368: loss=3.0101, lr=0.000440, tokens/sec=1380413.31, grad_norm=0.2893, duration=0.38s
Step 9369: loss=2.9553, lr=0.000440, tokens/sec=1376823.93, grad_norm=0.3149, duration=0.38s
Step 9370: loss=2.9825, lr=0.000440, tokens/sec=1376109.67, grad_norm=0.2854, duration=0.38s
Step 9371: loss=2.8756, lr=0.000440, tokens/sec=1375854.82, grad_norm=0.2901, duration=0.38s
Step 9372: loss=2.9663, lr=0.000440, tokens/sec=1377030.85, grad_norm=0.3022, duration=0.38s
Step 9373: loss=3.0056, lr=0.000440, tokens/sec=1370722.00, grad_norm=0.2996, duration=0.38s
Step 9374: loss=2.9664, lr=0.000440, tokens/sec=1377456.96, grad_norm=0.3082, duration=0.38s
Step 9375: loss=3.0346, lr=0.000440, tokens/sec=1377305.12, grad_norm=0.3130, duration=0.38s
Step 9376: loss=2.9589, lr=0.000440, tokens/sec=1374737.52, grad_norm=0.3006, duration=0.38s
Step 9377: loss=3.0537, lr=0.000440, tokens/sec=1377198.16, grad_norm=0.3024, duration=0.38s
Step 9378: loss=3.0316, lr=0.000440, tokens/sec=1374585.41, grad_norm=0.2930, duration=0.38s
Step 9379: loss=3.0141, lr=0.000440, tokens/sec=1376415.45, grad_norm=0.2881, duration=0.38s
Step 9380: loss=3.0050, lr=0.000440, tokens/sec=1371057.01, grad_norm=0.3185, duration=0.38s
Step 9381: loss=3.0324, lr=0.000440, tokens/sec=1372680.55, grad_norm=0.2870, duration=0.38s
Step 9382: loss=2.9662, lr=0.000440, tokens/sec=1379338.77, grad_norm=0.2814, duration=0.38s
Step 9383: loss=2.9938, lr=0.000440, tokens/sec=1372091.28, grad_norm=0.3203, duration=0.38s
Step 9384: loss=3.0032, lr=0.000440, tokens/sec=1373594.57, grad_norm=0.2760, duration=0.38s
Step 9385: loss=2.9988, lr=0.000440, tokens/sec=1375631.04, grad_norm=0.2915, duration=0.38s
Step 9386: loss=2.9823, lr=0.000439, tokens/sec=1374162.80, grad_norm=0.3123, duration=0.38s
Step 9387: loss=2.9538, lr=0.000439, tokens/sec=1372038.20, grad_norm=0.2916, duration=0.38s
Step 9388: loss=3.0047, lr=0.000439, tokens/sec=1378504.37, grad_norm=0.3050, duration=0.38s
Step 9389: loss=2.9682, lr=0.000439, tokens/sec=1376394.77, grad_norm=0.2879, duration=0.38s
Step 9390: loss=2.9740, lr=0.000439, tokens/sec=1374895.67, grad_norm=0.2712, duration=0.38s
Step 9391: loss=2.9165, lr=0.000439, tokens/sec=1375305.83, grad_norm=0.3071, duration=0.38s
Step 9392: loss=2.9698, lr=0.000439, tokens/sec=1373536.23, grad_norm=0.2745, duration=0.38s
Step 9393: loss=2.9866, lr=0.000439, tokens/sec=1373797.94, grad_norm=0.2948, duration=0.38s
Step 9394: loss=2.9830, lr=0.000439, tokens/sec=1374661.03, grad_norm=0.3030, duration=0.38s
Step 9395: loss=2.8933, lr=0.000439, tokens/sec=1378503.51, grad_norm=0.2836, duration=0.38s
Step 9396: loss=2.9323, lr=0.000439, tokens/sec=1377913.55, grad_norm=0.3005, duration=0.38s
Step 9397: loss=2.9199, lr=0.000439, tokens/sec=1374162.80, grad_norm=0.2981, duration=0.38s
Step 9398: loss=2.9152, lr=0.000439, tokens/sec=1375028.06, grad_norm=0.2922, duration=0.38s
Step 9399: loss=2.9530, lr=0.000439, tokens/sec=1372416.69, grad_norm=0.3108, duration=0.38s
Step 9400/19073 (49.3%), Elapsed time: 3728.70s, Steps per hour: 9075.56, Estimated hours remaining: 1.07
Step 9400: loss=2.9244, lr=0.000439, tokens/sec=1378210.62, grad_norm=0.2887, duration=0.38s
Step 9401: loss=3.0084, lr=0.000439, tokens/sec=1373935.28, grad_norm=0.3001, duration=0.38s
Step 9402: loss=2.9742, lr=0.000439, tokens/sec=1374040.87, grad_norm=0.2914, duration=0.38s
Step 9403: loss=2.9709, lr=0.000439, tokens/sec=1376825.66, grad_norm=0.2904, duration=0.38s
Step 9404: loss=3.0517, lr=0.000439, tokens/sec=1371736.08, grad_norm=0.3175, duration=0.38s
Step 9405: loss=3.0210, lr=0.000439, tokens/sec=1374563.07, grad_norm=0.2765, duration=0.38s
Step 9406: loss=3.0251, lr=0.000439, tokens/sec=1373877.77, grad_norm=0.3259, duration=0.38s
Step 9407: loss=3.0496, lr=0.000439, tokens/sec=1375126.95, grad_norm=0.3043, duration=0.38s
Step 9408: loss=2.9713, lr=0.000439, tokens/sec=1374210.89, grad_norm=0.3003, duration=0.38s
Step 9409: loss=3.0101, lr=0.000439, tokens/sec=1373550.81, grad_norm=0.3112, duration=0.38s
Step 9410: loss=2.9579, lr=0.000439, tokens/sec=1379138.94, grad_norm=0.3059, duration=0.38s
Step 9411: loss=3.0200, lr=0.000439, tokens/sec=1372701.11, grad_norm=0.3150, duration=0.38s
Step 9412: loss=3.0343, lr=0.000439, tokens/sec=1375065.90, grad_norm=0.2929, duration=0.38s
Step 9413: loss=3.0204, lr=0.000439, tokens/sec=1378410.18, grad_norm=0.3119, duration=0.38s
Step 9414: loss=3.0444, lr=0.000439, tokens/sec=1375594.90, grad_norm=0.3169, duration=0.38s
Step 9415: loss=2.9793, lr=0.000439, tokens/sec=1375542.41, grad_norm=0.2849, duration=0.38s
Step 9416: loss=2.9878, lr=0.000439, tokens/sec=1380485.24, grad_norm=0.2998, duration=0.38s
Step 9417: loss=2.9999, lr=0.000439, tokens/sec=1373265.17, grad_norm=0.2986, duration=0.38s
Step 9418: loss=2.9986, lr=0.000438, tokens/sec=1373714.70, grad_norm=0.2860, duration=0.38s
Step 9419: loss=2.9664, lr=0.000438, tokens/sec=1377302.53, grad_norm=0.3081, duration=0.38s
Step 9420: loss=3.0112, lr=0.000438, tokens/sec=1377709.82, grad_norm=0.2900, duration=0.38s
Step 9421: loss=3.0239, lr=0.000438, tokens/sec=1375604.37, grad_norm=0.2881, duration=0.38s
Step 9422: loss=3.0143, lr=0.000438, tokens/sec=1372165.77, grad_norm=0.3130, duration=0.38s
Step 9423: loss=2.9492, lr=0.000438, tokens/sec=1379047.27, grad_norm=0.2632, duration=0.38s
Step 9424: loss=3.0187, lr=0.000438, tokens/sec=1374605.18, grad_norm=0.3024, duration=0.38s
Step 9425: loss=2.9688, lr=0.000438, tokens/sec=1379162.30, grad_norm=0.2867, duration=0.38s
Step 9426: loss=3.0057, lr=0.000438, tokens/sec=1376794.62, grad_norm=0.2748, duration=0.38s
Step 9427: loss=2.9808, lr=0.000438, tokens/sec=1376980.84, grad_norm=0.2802, duration=0.38s
Step 9428: loss=3.0074, lr=0.000438, tokens/sec=1376168.23, grad_norm=0.2929, duration=0.38s
Step 9429: loss=2.9865, lr=0.000438, tokens/sec=1373823.69, grad_norm=0.2949, duration=0.38s
Step 9430: loss=2.9835, lr=0.000438, tokens/sec=1372543.46, grad_norm=0.2744, duration=0.38s
Step 9431: loss=2.9732, lr=0.000438, tokens/sec=1375668.05, grad_norm=0.2911, duration=0.38s
Step 9432: loss=3.0136, lr=0.000438, tokens/sec=1374104.41, grad_norm=0.2935, duration=0.38s
Step 9433: loss=2.9933, lr=0.000438, tokens/sec=1372738.82, grad_norm=0.2942, duration=0.38s
Step 9434: loss=2.9832, lr=0.000438, tokens/sec=1371529.89, grad_norm=0.2840, duration=0.38s
Step 9435: loss=2.9768, lr=0.000438, tokens/sec=1374740.95, grad_norm=0.2899, duration=0.38s
Step 9436: loss=2.9734, lr=0.000438, tokens/sec=1378886.43, grad_norm=0.2716, duration=0.38s
Step 9437: loss=2.9565, lr=0.000438, tokens/sec=1377765.92, grad_norm=0.2920, duration=0.38s
Step 9438: loss=2.9728, lr=0.000438, tokens/sec=1377712.40, grad_norm=0.2815, duration=0.38s
Step 9439: loss=2.8975, lr=0.000438, tokens/sec=1375808.34, grad_norm=0.2755, duration=0.38s
Step 9440: loss=2.9208, lr=0.000438, tokens/sec=1373656.35, grad_norm=0.3004, duration=0.38s
Step 9441: loss=2.8546, lr=0.000438, tokens/sec=1373444.43, grad_norm=0.2726, duration=0.38s
Step 9442: loss=2.8994, lr=0.000438, tokens/sec=1373676.94, grad_norm=0.3035, duration=0.38s
Step 9443: loss=2.9020, lr=0.000438, tokens/sec=1376918.76, grad_norm=0.2946, duration=0.38s
Step 9444: loss=2.9663, lr=0.000438, tokens/sec=1374563.07, grad_norm=0.2799, duration=0.38s
Step 9445: loss=2.9116, lr=0.000438, tokens/sec=1374251.25, grad_norm=0.3275, duration=0.38s
Step 9446: loss=2.9043, lr=0.000438, tokens/sec=1375026.34, grad_norm=0.2834, duration=0.38s
Step 9447: loss=2.9369, lr=0.000438, tokens/sec=1375177.68, grad_norm=0.3004, duration=0.38s
Step 9448: loss=2.9714, lr=0.000438, tokens/sec=1374916.30, grad_norm=0.3148, duration=0.38s
Step 9449: loss=2.9922, lr=0.000438, tokens/sec=1381626.67, grad_norm=0.2864, duration=0.38s
Step 9450: loss=2.9957, lr=0.000437, tokens/sec=1374777.05, grad_norm=0.3246, duration=0.38s
Step 9451: loss=3.0303, lr=0.000437, tokens/sec=1375185.42, grad_norm=0.3504, duration=0.38s
Step 9452: loss=3.0186, lr=0.000437, tokens/sec=1376588.64, grad_norm=0.3323, duration=0.38s
Step 9453: loss=3.0221, lr=0.000437, tokens/sec=1376748.94, grad_norm=0.3999, duration=0.38s
Step 9454: loss=3.0059, lr=0.000437, tokens/sec=1375234.44, grad_norm=0.3175, duration=0.38s
Step 9455: loss=2.9842, lr=0.000437, tokens/sec=1375156.18, grad_norm=0.3451, duration=0.38s
Step 9456: loss=3.0023, lr=0.000437, tokens/sec=1374219.48, grad_norm=0.3405, duration=0.38s
Step 9457: loss=3.0752, lr=0.000437, tokens/sec=1376036.48, grad_norm=0.3074, duration=0.38s
Step 9458: loss=3.1463, lr=0.000437, tokens/sec=1377556.19, grad_norm=0.3482, duration=0.38s
Step 9459: loss=3.0630, lr=0.000437, tokens/sec=1376870.48, grad_norm=0.3134, duration=0.38s
Step 9460: loss=3.0460, lr=0.000437, tokens/sec=1378290.10, grad_norm=0.3020, duration=0.38s
Step 9461: loss=3.0217, lr=0.000437, tokens/sec=1377906.64, grad_norm=0.3061, duration=0.38s
Step 9462: loss=2.9900, lr=0.000437, tokens/sec=1375950.38, grad_norm=0.3189, duration=0.38s
Step 9463: loss=3.0008, lr=0.000437, tokens/sec=1378118.21, grad_norm=0.2952, duration=0.38s
Step 9464: loss=2.9640, lr=0.000437, tokens/sec=1377106.74, grad_norm=0.2994, duration=0.38s
Step 9465: loss=3.0730, lr=0.000437, tokens/sec=1375142.43, grad_norm=0.2996, duration=0.38s
Step 9466: loss=2.9419, lr=0.000437, tokens/sec=1376224.21, grad_norm=0.2947, duration=0.38s
Step 9467: loss=3.0487, lr=0.000437, tokens/sec=1378462.89, grad_norm=0.2942, duration=0.38s
Step 9468: loss=3.0283, lr=0.000437, tokens/sec=1378996.24, grad_norm=0.3407, duration=0.38s
Step 9469: loss=3.0268, lr=0.000437, tokens/sec=1376303.46, grad_norm=0.3132, duration=0.38s
Step 9470: loss=2.9762, lr=0.000437, tokens/sec=1373358.66, grad_norm=0.3040, duration=0.38s
Step 9471: loss=3.0072, lr=0.000437, tokens/sec=1374912.86, grad_norm=0.3119, duration=0.38s
Step 9472: loss=3.0513, lr=0.000437, tokens/sec=1377383.62, grad_norm=0.3160, duration=0.38s
Step 9473: loss=2.9937, lr=0.000437, tokens/sec=1379241.01, grad_norm=0.3040, duration=0.38s
Step 9474: loss=2.9260, lr=0.000437, tokens/sec=1375422.82, grad_norm=0.2891, duration=0.38s
Step 9475: loss=2.9775, lr=0.000437, tokens/sec=1379416.65, grad_norm=0.3164, duration=0.38s
Step 9476: loss=2.9878, lr=0.000437, tokens/sec=1374866.44, grad_norm=0.2921, duration=0.38s
Step 9477: loss=3.0411, lr=0.000437, tokens/sec=1375126.09, grad_norm=0.3200, duration=0.38s
Step 9478: loss=3.0332, lr=0.000437, tokens/sec=1370727.12, grad_norm=0.3236, duration=0.38s
Step 9479: loss=2.9460, lr=0.000437, tokens/sec=1374132.74, grad_norm=0.2843, duration=0.38s
Step 9480: loss=2.9686, lr=0.000437, tokens/sec=1372568.31, grad_norm=0.3113, duration=0.38s
Step 9481: loss=2.9834, lr=0.000437, tokens/sec=1376341.36, grad_norm=0.3137, duration=0.38s
Step 9482: loss=2.9554, lr=0.000436, tokens/sec=1377082.59, grad_norm=0.2853, duration=0.38s
Step 9483: loss=2.9663, lr=0.000436, tokens/sec=1374541.59, grad_norm=0.3012, duration=0.38s
Step 9484: loss=2.9415, lr=0.000436, tokens/sec=1379382.90, grad_norm=0.3090, duration=0.38s
Step 9485: loss=2.8955, lr=0.000436, tokens/sec=1378582.15, grad_norm=0.2922, duration=0.38s
Step 9486: loss=2.9236, lr=0.000436, tokens/sec=1375475.30, grad_norm=0.3002, duration=0.38s
Step 9487: loss=2.9571, lr=0.000436, tokens/sec=1375292.93, grad_norm=0.2998, duration=0.38s
Step 9488: loss=2.9400, lr=0.000436, tokens/sec=1376599.84, grad_norm=0.2938, duration=0.38s
Step 9489: loss=2.9107, lr=0.000436, tokens/sec=1378370.44, grad_norm=0.3080, duration=0.38s
Step 9490: loss=2.8597, lr=0.000436, tokens/sec=1376426.65, grad_norm=0.2950, duration=0.38s
Step 9491: loss=2.9283, lr=0.000436, tokens/sec=1376272.45, grad_norm=0.2875, duration=0.38s
Step 9492: loss=2.9502, lr=0.000436, tokens/sec=1378507.83, grad_norm=0.3023, duration=0.38s
Step 9493: loss=2.8716, lr=0.000436, tokens/sec=1377020.50, grad_norm=0.3045, duration=0.38s
Step 9494: loss=2.9402, lr=0.000436, tokens/sec=1379489.33, grad_norm=0.2949, duration=0.38s
Step 9495: loss=2.9868, lr=0.000436, tokens/sec=1375580.27, grad_norm=0.2827, duration=0.38s
Step 9496: loss=3.0199, lr=0.000436, tokens/sec=1372883.65, grad_norm=0.3129, duration=0.38s
Step 9497: loss=3.0134, lr=0.000436, tokens/sec=1373640.90, grad_norm=0.2933, duration=0.38s
Step 9498: loss=2.9888, lr=0.000436, tokens/sec=1374066.63, grad_norm=0.3042, duration=0.38s
Step 9499: loss=2.9972, lr=0.000436, tokens/sec=1376785.14, grad_norm=0.3281, duration=0.38s
Step 9500/19073 (49.8%), Elapsed time: 3766.90s, Steps per hour: 9079.09, Estimated hours remaining: 1.05
Validation loss at step 9500: 3.7577829360961914
Step 9500: loss=2.9856, lr=0.000436, tokens/sec=153749.85, grad_norm=0.2974, duration=3.41s
Step 9501: loss=3.0550, lr=0.000436, tokens/sec=1373263.46, grad_norm=0.3272, duration=0.38s
Step 9502: loss=3.0466, lr=0.000436, tokens/sec=1376117.42, grad_norm=0.3032, duration=0.38s
Step 9503: loss=3.0995, lr=0.000436, tokens/sec=1378762.80, grad_norm=0.3246, duration=0.38s
Step 9504: loss=3.0363, lr=0.000436, tokens/sec=1379328.39, grad_norm=0.3316, duration=0.38s
Step 9505: loss=3.0489, lr=0.000436, tokens/sec=1376126.89, grad_norm=0.2900, duration=0.38s
Step 9506: loss=3.0997, lr=0.000436, tokens/sec=1375476.16, grad_norm=0.3271, duration=0.38s
Step 9507: loss=3.0431, lr=0.000436, tokens/sec=1375051.28, grad_norm=0.3151, duration=0.38s
Step 9508: loss=3.0383, lr=0.000436, tokens/sec=1376605.87, grad_norm=0.3114, duration=0.38s
Step 9509: loss=3.0280, lr=0.000436, tokens/sec=1375093.41, grad_norm=0.3008, duration=0.38s
Step 9510: loss=3.0404, lr=0.000436, tokens/sec=1373783.35, grad_norm=0.3120, duration=0.38s
Step 9511: loss=3.0010, lr=0.000436, tokens/sec=1371961.16, grad_norm=0.2859, duration=0.38s
Step 9512: loss=3.0441, lr=0.000436, tokens/sec=1375988.26, grad_norm=0.3099, duration=0.38s
Step 9513: loss=2.9545, lr=0.000436, tokens/sec=1372038.20, grad_norm=0.2936, duration=0.38s
Step 9514: loss=3.0356, lr=0.000436, tokens/sec=1372469.79, grad_norm=0.2968, duration=0.38s
Step 9515: loss=3.0404, lr=0.000435, tokens/sec=1377355.15, grad_norm=0.3568, duration=0.38s
Step 9516: loss=2.9550, lr=0.000435, tokens/sec=1373164.84, grad_norm=0.3204, duration=0.38s
Step 9517: loss=3.0556, lr=0.000435, tokens/sec=1372866.51, grad_norm=0.3107, duration=0.38s
Step 9518: loss=2.9610, lr=0.000435, tokens/sec=1377002.40, grad_norm=0.2978, duration=0.38s
Step 9519: loss=2.9094, lr=0.000435, tokens/sec=1372192.31, grad_norm=0.3157, duration=0.38s
Step 9520: loss=2.9734, lr=0.000435, tokens/sec=1373777.35, grad_norm=0.2915, duration=0.38s
Step 9521: loss=3.0002, lr=0.000435, tokens/sec=1376492.99, grad_norm=0.2784, duration=0.38s
Step 9522: loss=3.0882, lr=0.000435, tokens/sec=1373415.27, grad_norm=0.2883, duration=0.38s
Step 9523: loss=2.9363, lr=0.000435, tokens/sec=1373363.80, grad_norm=0.3053, duration=0.38s
Step 9524: loss=3.0319, lr=0.000435, tokens/sec=1377139.51, grad_norm=0.2985, duration=0.38s
Step 9525: loss=3.0057, lr=0.000435, tokens/sec=1375669.77, grad_norm=0.2810, duration=0.38s
Step 9526: loss=2.9608, lr=0.000435, tokens/sec=1375268.85, grad_norm=0.3135, duration=0.38s
Step 9527: loss=3.0164, lr=0.000435, tokens/sec=1372531.47, grad_norm=0.3058, duration=0.38s
Step 9528: loss=3.0041, lr=0.000435, tokens/sec=1375261.11, grad_norm=0.2957, duration=0.38s
Step 9529: loss=2.9833, lr=0.000435, tokens/sec=1376001.18, grad_norm=0.2907, duration=0.38s
Step 9530: loss=2.9527, lr=0.000435, tokens/sec=1374514.10, grad_norm=0.2821, duration=0.38s
Step 9531: loss=2.9929, lr=0.000435, tokens/sec=1375285.19, grad_norm=0.2925, duration=0.38s
Step 9532: loss=3.0059, lr=0.000435, tokens/sec=1374618.92, grad_norm=0.3044, duration=0.38s
Step 9533: loss=3.0059, lr=0.000435, tokens/sec=1378342.79, grad_norm=0.2702, duration=0.38s
Step 9534: loss=2.9622, lr=0.000435, tokens/sec=1374332.84, grad_norm=0.2808, duration=0.38s
Step 9535: loss=2.9030, lr=0.000435, tokens/sec=1374756.42, grad_norm=0.2852, duration=0.38s
Step 9536: loss=2.9458, lr=0.000435, tokens/sec=1374575.96, grad_norm=0.2735, duration=0.38s
Step 9537: loss=2.9009, lr=0.000435, tokens/sec=1376441.29, grad_norm=0.3001, duration=0.38s
Step 9538: loss=2.8996, lr=0.000435, tokens/sec=1377928.23, grad_norm=0.2987, duration=0.38s
Step 9539: loss=2.8541, lr=0.000435, tokens/sec=1374675.64, grad_norm=0.3026, duration=0.38s
Step 9540: loss=2.9257, lr=0.000435, tokens/sec=1373449.58, grad_norm=0.3150, duration=0.38s
Step 9541: loss=2.9268, lr=0.000435, tokens/sec=1376627.41, grad_norm=0.3089, duration=0.38s
Step 9542: loss=2.9686, lr=0.000435, tokens/sec=1377528.58, grad_norm=0.3202, duration=0.38s
Step 9543: loss=2.9795, lr=0.000435, tokens/sec=1373181.99, grad_norm=0.3191, duration=0.38s
Step 9544: loss=2.9537, lr=0.000435, tokens/sec=1373930.13, grad_norm=0.3084, duration=0.38s
Step 9545: loss=2.9443, lr=0.000435, tokens/sec=1373031.09, grad_norm=0.3175, duration=0.38s
Step 9546: loss=2.9578, lr=0.000435, tokens/sec=1369950.89, grad_norm=0.3273, duration=0.38s
Step 9547: loss=3.0089, lr=0.000434, tokens/sec=1374952.41, grad_norm=0.3169, duration=0.38s
Step 9548: loss=3.0271, lr=0.000434, tokens/sec=1372570.02, grad_norm=0.3351, duration=0.38s
Step 9549: loss=3.0316, lr=0.000434, tokens/sec=1373136.55, grad_norm=0.3492, duration=0.38s
Step 9550: loss=3.0285, lr=0.000434, tokens/sec=1373905.23, grad_norm=0.3352, duration=0.38s
Step 9551: loss=3.0424, lr=0.000434, tokens/sec=1376013.23, grad_norm=0.3616, duration=0.38s
Step 9552: loss=3.0619, lr=0.000434, tokens/sec=1375923.69, grad_norm=0.3207, duration=0.38s
Step 9553: loss=3.0282, lr=0.000434, tokens/sec=1372539.18, grad_norm=0.3634, duration=0.38s
Step 9554: loss=3.0097, lr=0.000434, tokens/sec=1373686.38, grad_norm=0.3321, duration=0.38s
Step 9555: loss=2.9937, lr=0.000434, tokens/sec=1374260.70, grad_norm=0.3235, duration=0.38s
Step 9556: loss=3.0101, lr=0.000434, tokens/sec=1374001.38, grad_norm=0.3468, duration=0.38s
Step 9557: loss=3.0148, lr=0.000434, tokens/sec=1375236.16, grad_norm=0.3741, duration=0.38s
Step 9558: loss=2.9519, lr=0.000434, tokens/sec=1375966.74, grad_norm=0.3001, duration=0.38s
Step 9559: loss=2.9765, lr=0.000434, tokens/sec=1377891.96, grad_norm=0.3267, duration=0.38s
Step 9560: loss=2.9524, lr=0.000434, tokens/sec=1377159.35, grad_norm=0.3443, duration=0.38s
Step 9561: loss=2.9053, lr=0.000434, tokens/sec=1377677.02, grad_norm=0.3079, duration=0.38s
Step 9562: loss=2.9566, lr=0.000434, tokens/sec=1377585.53, grad_norm=0.3213, duration=0.38s
Step 9563: loss=2.9705, lr=0.000434, tokens/sec=1373132.26, grad_norm=0.3130, duration=0.38s
Step 9564: loss=2.9860, lr=0.000434, tokens/sec=1375146.72, grad_norm=0.3155, duration=0.38s
Step 9565: loss=3.0163, lr=0.000434, tokens/sec=1378398.95, grad_norm=0.2885, duration=0.38s
Step 9566: loss=2.9716, lr=0.000434, tokens/sec=1377150.72, grad_norm=0.2916, duration=0.38s
Step 9567: loss=3.0618, lr=0.000434, tokens/sec=1378446.48, grad_norm=0.3021, duration=0.38s
Step 9568: loss=3.0241, lr=0.000434, tokens/sec=1380198.44, grad_norm=0.2994, duration=0.38s
Step 9569: loss=2.9957, lr=0.000434, tokens/sec=1372171.76, grad_norm=0.3151, duration=0.38s
Step 9570: loss=3.0223, lr=0.000434, tokens/sec=1372815.09, grad_norm=0.2824, duration=0.38s
Step 9571: loss=2.9829, lr=0.000434, tokens/sec=1373174.28, grad_norm=0.3156, duration=0.38s
Step 9572: loss=2.9658, lr=0.000434, tokens/sec=1375779.93, grad_norm=0.2932, duration=0.38s
Step 9573: loss=2.9951, lr=0.000434, tokens/sec=1380123.08, grad_norm=0.3018, duration=0.38s
Step 9574: loss=2.9940, lr=0.000434, tokens/sec=1376485.23, grad_norm=0.3049, duration=0.38s
Step 9575: loss=2.9723, lr=0.000434, tokens/sec=1374680.80, grad_norm=0.2822, duration=0.38s
Step 9576: loss=2.9927, lr=0.000434, tokens/sec=1372659.13, grad_norm=0.3160, duration=0.38s
Step 9577: loss=2.9597, lr=0.000434, tokens/sec=1376679.99, grad_norm=0.2815, duration=0.38s
Step 9578: loss=2.9704, lr=0.000434, tokens/sec=1375464.12, grad_norm=0.2911, duration=0.38s
Step 9579: loss=2.9547, lr=0.000433, tokens/sec=1374325.97, grad_norm=0.2904, duration=0.38s
Step 9580: loss=2.9834, lr=0.000433, tokens/sec=1378144.98, grad_norm=0.2642, duration=0.38s
Step 9581: loss=2.9031, lr=0.000433, tokens/sec=1377735.71, grad_norm=0.2935, duration=0.38s
Step 9582: loss=3.0335, lr=0.000433, tokens/sec=1373741.30, grad_norm=0.2897, duration=0.38s
Step 9583: loss=2.9610, lr=0.000433, tokens/sec=1381416.63, grad_norm=0.2625, duration=0.38s
Step 9584: loss=2.9335, lr=0.000433, tokens/sec=1373587.70, grad_norm=0.2848, duration=0.38s
Step 9585: loss=2.8942, lr=0.000433, tokens/sec=1375605.23, grad_norm=0.2891, duration=0.38s
Step 9586: loss=2.8934, lr=0.000433, tokens/sec=1376062.31, grad_norm=0.2780, duration=0.38s
Step 9587: loss=2.9548, lr=0.000433, tokens/sec=1375122.65, grad_norm=0.2992, duration=0.38s
Step 9588: loss=2.9222, lr=0.000433, tokens/sec=1375057.30, grad_norm=0.2979, duration=0.38s
Step 9589: loss=2.9342, lr=0.000433, tokens/sec=1375748.09, grad_norm=0.2864, duration=0.38s
Step 9590: loss=2.9504, lr=0.000433, tokens/sec=1375134.69, grad_norm=0.3351, duration=0.38s
Step 9591: loss=2.9845, lr=0.000433, tokens/sec=1375687.84, grad_norm=0.2850, duration=0.38s
Step 9592: loss=2.9373, lr=0.000433, tokens/sec=1380007.89, grad_norm=0.3190, duration=0.38s
Step 9593: loss=3.0005, lr=0.000433, tokens/sec=1377405.19, grad_norm=0.3209, duration=0.38s
Step 9594: loss=3.0100, lr=0.000433, tokens/sec=1381207.52, grad_norm=0.3214, duration=0.38s
Step 9595: loss=3.0106, lr=0.000433, tokens/sec=1371895.26, grad_norm=0.3112, duration=0.38s
Step 9596: loss=3.0431, lr=0.000433, tokens/sec=1369822.03, grad_norm=0.3174, duration=0.38s
Step 9597: loss=3.0302, lr=0.000433, tokens/sec=1376462.83, grad_norm=0.3331, duration=0.38s
Step 9598: loss=2.9593, lr=0.000433, tokens/sec=1374777.05, grad_norm=0.3053, duration=0.38s
Step 9599: loss=2.9880, lr=0.000433, tokens/sec=1375770.46, grad_norm=0.3490, duration=0.38s
Step 9600/19073 (50.3%), Elapsed time: 3808.14s, Steps per hour: 9075.30, Estimated hours remaining: 1.04
Step 9600: loss=2.9516, lr=0.000433, tokens/sec=1376602.42, grad_norm=0.3085, duration=0.38s
Step 9601: loss=3.0367, lr=0.000433, tokens/sec=1375786.82, grad_norm=0.3327, duration=0.38s
Step 9602: loss=3.0256, lr=0.000433, tokens/sec=1380519.04, grad_norm=0.3058, duration=0.38s
Step 9603: loss=3.0372, lr=0.000433, tokens/sec=1376768.76, grad_norm=0.2922, duration=0.38s
Step 9604: loss=3.0107, lr=0.000433, tokens/sec=1375477.88, grad_norm=0.3233, duration=0.38s
Step 9605: loss=2.9720, lr=0.000433, tokens/sec=1375151.02, grad_norm=0.2936, duration=0.38s
Step 9606: loss=2.9921, lr=0.000433, tokens/sec=1371210.04, grad_norm=0.2976, duration=0.38s
Step 9607: loss=2.9821, lr=0.000433, tokens/sec=1374544.17, grad_norm=0.2931, duration=0.38s
Step 9608: loss=3.0129, lr=0.000433, tokens/sec=1371638.54, grad_norm=0.2996, duration=0.38s
Step 9609: loss=2.9532, lr=0.000433, tokens/sec=1377986.94, grad_norm=0.2982, duration=0.38s
Step 9610: loss=3.0299, lr=0.000433, tokens/sec=1374003.95, grad_norm=0.3145, duration=0.38s
Step 9611: loss=2.9972, lr=0.000432, tokens/sec=1380181.12, grad_norm=0.2859, duration=0.38s
Step 9612: loss=3.0040, lr=0.000432, tokens/sec=1377066.21, grad_norm=0.3281, duration=0.38s
Step 9613: loss=2.9429, lr=0.000432, tokens/sec=1375986.54, grad_norm=0.2989, duration=0.38s
Step 9614: loss=3.0185, lr=0.000432, tokens/sec=1373658.92, grad_norm=0.3083, duration=0.38s
Step 9615: loss=2.9477, lr=0.000432, tokens/sec=1377046.37, grad_norm=0.3173, duration=0.38s
Step 9616: loss=3.0074, lr=0.000432, tokens/sec=1373171.70, grad_norm=0.2910, duration=0.38s
Step 9617: loss=2.9808, lr=0.000432, tokens/sec=1378697.10, grad_norm=0.3084, duration=0.38s
Step 9618: loss=3.0027, lr=0.000432, tokens/sec=1375938.33, grad_norm=0.2919, duration=0.38s
Step 9619: loss=2.9706, lr=0.000432, tokens/sec=1375429.70, grad_norm=0.3054, duration=0.38s
Step 9620: loss=2.9712, lr=0.000432, tokens/sec=1379504.91, grad_norm=0.2918, duration=0.38s
Step 9621: loss=2.9992, lr=0.000432, tokens/sec=1377335.31, grad_norm=0.3088, duration=0.38s
Step 9622: loss=3.0066, lr=0.000432, tokens/sec=1374282.17, grad_norm=0.3207, duration=0.38s
Step 9623: loss=2.9583, lr=0.000432, tokens/sec=1377010.16, grad_norm=0.2904, duration=0.38s
Step 9624: loss=2.9713, lr=0.000432, tokens/sec=1374497.78, grad_norm=0.3198, duration=0.38s
Step 9625: loss=2.9818, lr=0.000432, tokens/sec=1378167.44, grad_norm=0.3062, duration=0.38s
Step 9626: loss=2.9637, lr=0.000432, tokens/sec=1377172.28, grad_norm=0.3073, duration=0.38s
Step 9627: loss=2.9704, lr=0.000432, tokens/sec=1376216.46, grad_norm=0.3092, duration=0.38s
Step 9628: loss=2.9514, lr=0.000432, tokens/sec=1376434.40, grad_norm=0.3169, duration=0.38s
Step 9629: loss=2.8767, lr=0.000432, tokens/sec=1375640.51, grad_norm=0.3029, duration=0.38s
Step 9630: loss=2.9288, lr=0.000432, tokens/sec=1378170.89, grad_norm=0.3301, duration=0.38s
Step 9631: loss=2.8493, lr=0.000432, tokens/sec=1375284.33, grad_norm=0.2935, duration=0.38s
Step 9632: loss=2.8994, lr=0.000432, tokens/sec=1377304.25, grad_norm=0.3078, duration=0.38s
Step 9633: loss=2.8993, lr=0.000432, tokens/sec=1377994.71, grad_norm=0.3556, duration=0.38s
Step 9634: loss=2.9316, lr=0.000432, tokens/sec=1375648.25, grad_norm=0.2948, duration=0.38s
Step 9635: loss=2.9234, lr=0.000432, tokens/sec=1376414.58, grad_norm=0.3112, duration=0.38s
Step 9636: loss=2.9460, lr=0.000432, tokens/sec=1372089.57, grad_norm=0.3515, duration=0.38s
Step 9637: loss=2.9351, lr=0.000432, tokens/sec=1377796.14, grad_norm=0.2917, duration=0.38s
Step 9638: loss=2.9530, lr=0.000432, tokens/sec=1376785.14, grad_norm=0.3069, duration=0.38s
Step 9639: loss=2.9722, lr=0.000432, tokens/sec=1373529.36, grad_norm=0.3107, duration=0.38s
Step 9640: loss=3.0057, lr=0.000432, tokens/sec=1374405.86, grad_norm=0.3067, duration=0.38s
Step 9641: loss=3.0079, lr=0.000432, tokens/sec=1375788.54, grad_norm=0.3352, duration=0.38s
Step 9642: loss=3.0353, lr=0.000432, tokens/sec=1377042.06, grad_norm=0.3041, duration=0.38s
Step 9643: loss=2.9713, lr=0.000431, tokens/sec=1377658.89, grad_norm=0.3381, duration=0.38s
Step 9644: loss=3.0287, lr=0.000431, tokens/sec=1377450.06, grad_norm=0.3619, duration=0.38s
Step 9645: loss=2.9676, lr=0.000431, tokens/sec=1373930.99, grad_norm=0.3096, duration=0.38s
Step 9646: loss=3.0234, lr=0.000431, tokens/sec=1373105.68, grad_norm=0.3630, duration=0.38s
Step 9647: loss=3.0783, lr=0.000431, tokens/sec=1377584.67, grad_norm=0.3298, duration=0.38s
Step 9648: loss=3.1150, lr=0.000431, tokens/sec=1375042.68, grad_norm=0.3082, duration=0.38s
Step 9649: loss=3.0554, lr=0.000431, tokens/sec=1371245.95, grad_norm=0.3460, duration=0.38s
Step 9650: loss=3.0686, lr=0.000431, tokens/sec=1374179.11, grad_norm=0.3113, duration=0.38s
Step 9651: loss=2.9484, lr=0.000431, tokens/sec=1379714.37, grad_norm=0.2874, duration=0.38s
Step 9652: loss=3.0306, lr=0.000431, tokens/sec=1376722.22, grad_norm=0.3302, duration=0.38s
Step 9653: loss=2.9903, lr=0.000431, tokens/sec=1374513.24, grad_norm=0.2993, duration=0.38s
Step 9654: loss=2.9766, lr=0.000431, tokens/sec=1374954.98, grad_norm=0.3045, duration=0.38s
Step 9655: loss=3.0213, lr=0.000431, tokens/sec=1376322.41, grad_norm=0.2974, duration=0.38s
Step 9656: loss=2.9532, lr=0.000431, tokens/sec=1373889.78, grad_norm=0.2996, duration=0.38s
Step 9657: loss=2.9957, lr=0.000431, tokens/sec=1377429.35, grad_norm=0.3324, duration=0.38s
Step 9658: loss=3.0723, lr=0.000431, tokens/sec=1375551.88, grad_norm=0.2986, duration=0.38s
Step 9659: loss=3.0209, lr=0.000431, tokens/sec=1375467.56, grad_norm=0.2940, duration=0.38s
Step 9660: loss=2.9522, lr=0.000431, tokens/sec=1374484.03, grad_norm=0.2952, duration=0.38s
Step 9661: loss=3.0166, lr=0.000431, tokens/sec=1378255.54, grad_norm=0.2894, duration=0.38s
Step 9662: loss=3.0219, lr=0.000431, tokens/sec=1375240.46, grad_norm=0.3105, duration=0.38s
Step 9663: loss=2.9919, lr=0.000431, tokens/sec=1375261.97, grad_norm=0.2877, duration=0.38s
Step 9664: loss=2.9126, lr=0.000431, tokens/sec=1374628.38, grad_norm=0.2931, duration=0.38s
Step 9665: loss=2.9799, lr=0.000431, tokens/sec=1377915.28, grad_norm=0.2983, duration=0.38s
Step 9666: loss=3.0084, lr=0.000431, tokens/sec=1373043.09, grad_norm=0.2848, duration=0.38s
Step 9667: loss=2.9934, lr=0.000431, tokens/sec=1375618.13, grad_norm=0.2934, duration=0.38s
Step 9668: loss=3.0223, lr=0.000431, tokens/sec=1379012.67, grad_norm=0.3167, duration=0.38s
Step 9669: loss=2.9527, lr=0.000431, tokens/sec=1379613.96, grad_norm=0.2860, duration=0.38s
Step 9670: loss=2.9760, lr=0.000431, tokens/sec=1377611.42, grad_norm=0.2767, duration=0.38s
Step 9671: loss=2.9924, lr=0.000431, tokens/sec=1375909.92, grad_norm=0.3362, duration=0.38s
Step 9672: loss=2.9141, lr=0.000431, tokens/sec=1376663.61, grad_norm=0.2687, duration=0.38s
Step 9673: loss=2.9686, lr=0.000431, tokens/sec=1377676.15, grad_norm=0.2996, duration=0.38s
Step 9674: loss=2.9029, lr=0.000431, tokens/sec=1377718.45, grad_norm=0.2854, duration=0.38s
Step 9675: loss=2.9116, lr=0.000430, tokens/sec=1378148.43, grad_norm=0.2937, duration=0.38s
Step 9676: loss=2.8874, lr=0.000430, tokens/sec=1378243.45, grad_norm=0.2778, duration=0.38s
Step 9677: loss=2.9794, lr=0.000430, tokens/sec=1378392.90, grad_norm=0.2925, duration=0.38s
Step 9678: loss=2.9262, lr=0.000430, tokens/sec=1376338.77, grad_norm=0.2991, duration=0.38s
Step 9679: loss=2.8828, lr=0.000430, tokens/sec=1369914.20, grad_norm=0.2759, duration=0.38s
Step 9680: loss=2.8942, lr=0.000430, tokens/sec=1375727.43, grad_norm=0.3111, duration=0.38s
Step 9681: loss=2.9095, lr=0.000430, tokens/sec=1375251.65, grad_norm=0.3004, duration=0.38s
Step 9682: loss=2.8802, lr=0.000430, tokens/sec=1373584.27, grad_norm=0.3086, duration=0.38s
Step 9683: loss=2.9146, lr=0.000430, tokens/sec=1376831.69, grad_norm=0.2934, duration=0.38s
Step 9684: loss=2.9371, lr=0.000430, tokens/sec=1376647.24, grad_norm=0.3055, duration=0.38s
Step 9685: loss=3.0145, lr=0.000430, tokens/sec=1377800.45, grad_norm=0.3034, duration=0.38s
Step 9686: loss=3.0011, lr=0.000430, tokens/sec=1372231.70, grad_norm=0.3076, duration=0.38s
Step 9687: loss=3.0101, lr=0.000430, tokens/sec=1373591.13, grad_norm=0.3089, duration=0.38s
Step 9688: loss=2.9582, lr=0.000430, tokens/sec=1371539.30, grad_norm=0.2983, duration=0.38s
Step 9689: loss=3.0016, lr=0.000430, tokens/sec=1373626.31, grad_norm=0.3318, duration=0.38s
Step 9690: loss=3.0117, lr=0.000430, tokens/sec=1374469.43, grad_norm=0.2990, duration=0.38s
Step 9691: loss=3.0354, lr=0.000430, tokens/sec=1377690.83, grad_norm=0.3201, duration=0.38s
Step 9692: loss=3.0221, lr=0.000430, tokens/sec=1376327.58, grad_norm=0.3205, duration=0.38s
Step 9693: loss=3.0953, lr=0.000430, tokens/sec=1374533.86, grad_norm=0.3027, duration=0.38s
Step 9694: loss=3.0209, lr=0.000430, tokens/sec=1377318.06, grad_norm=0.3289, duration=0.38s
Step 9695: loss=3.0492, lr=0.000430, tokens/sec=1378414.51, grad_norm=0.3168, duration=0.38s
Step 9696: loss=3.1016, lr=0.000430, tokens/sec=1374199.72, grad_norm=0.3033, duration=0.38s
Step 9697: loss=3.0119, lr=0.000430, tokens/sec=1375863.43, grad_norm=0.3047, duration=0.38s
Step 9698: loss=3.0398, lr=0.000430, tokens/sec=1378304.78, grad_norm=0.3196, duration=0.38s
Step 9699: loss=3.0188, lr=0.000430, tokens/sec=1378915.83, grad_norm=0.2843, duration=0.38s
Step 9700/19073 (50.9%), Elapsed time: 3846.32s, Steps per hour: 9078.80, Estimated hours remaining: 1.03
Step 9700: loss=3.0316, lr=0.000430, tokens/sec=1376413.72, grad_norm=0.2983, duration=0.38s
Step 9701: loss=3.0255, lr=0.000430, tokens/sec=1378689.32, grad_norm=0.2976, duration=0.38s
Step 9702: loss=3.0097, lr=0.000430, tokens/sec=1374221.19, grad_norm=0.2961, duration=0.38s
Step 9703: loss=2.9592, lr=0.000430, tokens/sec=1372695.97, grad_norm=0.2887, duration=0.38s
Step 9704: loss=3.0481, lr=0.000430, tokens/sec=1371832.78, grad_norm=0.3113, duration=0.38s
Step 9705: loss=2.9963, lr=0.000430, tokens/sec=1371932.92, grad_norm=0.3205, duration=0.38s
Step 9706: loss=2.9789, lr=0.000430, tokens/sec=1375759.28, grad_norm=0.3143, duration=0.38s
Step 9707: loss=3.0344, lr=0.000429, tokens/sec=1372973.66, grad_norm=0.2929, duration=0.38s
Step 9708: loss=2.9373, lr=0.000429, tokens/sec=1371868.73, grad_norm=0.2979, duration=0.38s
Step 9709: loss=2.9167, lr=0.000429, tokens/sec=1374655.01, grad_norm=0.3049, duration=0.38s
Step 9710: loss=2.9579, lr=0.000429, tokens/sec=1373906.95, grad_norm=0.3144, duration=0.38s
Step 9711: loss=3.0066, lr=0.000429, tokens/sec=1376617.94, grad_norm=0.2811, duration=0.38s
Step 9712: loss=3.0830, lr=0.000429, tokens/sec=1372415.83, grad_norm=0.3112, duration=0.38s
Step 9713: loss=2.9218, lr=0.000429, tokens/sec=1377976.58, grad_norm=0.2946, duration=0.38s
Step 9714: loss=3.0153, lr=0.000429, tokens/sec=1374549.33, grad_norm=0.3317, duration=0.38s
Step 9715: loss=3.0063, lr=0.000429, tokens/sec=1375409.92, grad_norm=0.2959, duration=0.38s
Step 9716: loss=2.9790, lr=0.000429, tokens/sec=1377819.44, grad_norm=0.3052, duration=0.38s
Step 9717: loss=2.9759, lr=0.000429, tokens/sec=1371554.70, grad_norm=0.3112, duration=0.38s
Step 9718: loss=3.0025, lr=0.000429, tokens/sec=1373846.87, grad_norm=0.3027, duration=0.38s
Step 9719: loss=2.9741, lr=0.000429, tokens/sec=1374602.60, grad_norm=0.2995, duration=0.38s
Step 9720: loss=2.9540, lr=0.000429, tokens/sec=1377324.10, grad_norm=0.2989, duration=0.38s
Step 9721: loss=2.9869, lr=0.000429, tokens/sec=1376931.69, grad_norm=0.3040, duration=0.38s
Step 9722: loss=2.9637, lr=0.000429, tokens/sec=1377457.82, grad_norm=0.3189, duration=0.38s
Step 9723: loss=3.0161, lr=0.000429, tokens/sec=1367312.84, grad_norm=0.2959, duration=0.38s
Step 9724: loss=2.9490, lr=0.000429, tokens/sec=1372773.95, grad_norm=0.2886, duration=0.38s
Step 9725: loss=2.8933, lr=0.000429, tokens/sec=1376221.63, grad_norm=0.3063, duration=0.38s
Step 9726: loss=2.9494, lr=0.000429, tokens/sec=1372389.28, grad_norm=0.2862, duration=0.38s
Step 9727: loss=2.9005, lr=0.000429, tokens/sec=1371399.03, grad_norm=0.3016, duration=0.38s
Step 9728: loss=2.8285, lr=0.000429, tokens/sec=1376804.97, grad_norm=0.3190, duration=0.38s
Step 9729: loss=2.9068, lr=0.000429, tokens/sec=1371759.19, grad_norm=0.3166, duration=0.38s
Step 9730: loss=2.9161, lr=0.000429, tokens/sec=1375372.93, grad_norm=0.3165, duration=0.38s
Step 9731: loss=2.9584, lr=0.000429, tokens/sec=1373500.19, grad_norm=0.3220, duration=0.38s
Step 9732: loss=2.9562, lr=0.000429, tokens/sec=1373526.79, grad_norm=0.3222, duration=0.38s
Step 9733: loss=2.9727, lr=0.000429, tokens/sec=1376960.15, grad_norm=0.3228, duration=0.38s
Step 9734: loss=2.9416, lr=0.000429, tokens/sec=1373251.45, grad_norm=0.3140, duration=0.38s
Step 9735: loss=2.9383, lr=0.000429, tokens/sec=1377973.13, grad_norm=0.3066, duration=0.38s
Step 9736: loss=2.9344, lr=0.000429, tokens/sec=1376866.17, grad_norm=0.3211, duration=0.38s
Step 9737: loss=2.9814, lr=0.000429, tokens/sec=1370842.48, grad_norm=0.3298, duration=0.38s
Step 9738: loss=3.0669, lr=0.000429, tokens/sec=1372410.69, grad_norm=0.3035, duration=0.38s
Step 9739: loss=3.0245, lr=0.000428, tokens/sec=1375034.94, grad_norm=0.3222, duration=0.38s
Step 9740: loss=3.0144, lr=0.000428, tokens/sec=1375499.39, grad_norm=0.3205, duration=0.38s
Step 9741: loss=3.0641, lr=0.000428, tokens/sec=1375746.36, grad_norm=0.3536, duration=0.38s
Step 9742: loss=3.0210, lr=0.000428, tokens/sec=1381471.30, grad_norm=0.3119, duration=0.38s
Step 9743: loss=3.0387, lr=0.000428, tokens/sec=1371705.28, grad_norm=0.3371, duration=0.38s
Step 9744: loss=2.9920, lr=0.000428, tokens/sec=1375574.25, grad_norm=0.3622, duration=0.38s
Step 9745: loss=2.9775, lr=0.000428, tokens/sec=1376222.49, grad_norm=0.3315, duration=0.38s
Step 9746: loss=3.0209, lr=0.000428, tokens/sec=1375323.90, grad_norm=0.3376, duration=0.38s
Step 9747: loss=2.9548, lr=0.000428, tokens/sec=1377310.29, grad_norm=0.3550, duration=0.38s
Step 9748: loss=2.9748, lr=0.000428, tokens/sec=1374530.42, grad_norm=0.3395, duration=0.38s
Step 9749: loss=2.9419, lr=0.000428, tokens/sec=1376599.84, grad_norm=0.2960, duration=0.38s
Validation loss at step 9750: 3.7713534832000732
Step 9750: loss=2.9769, lr=0.000428, tokens/sec=156166.43, grad_norm=0.3315, duration=3.36s
Step 9751: loss=2.8938, lr=0.000428, tokens/sec=1376886.00, grad_norm=0.3227, duration=0.38s
Step 9752: loss=2.9169, lr=0.000428, tokens/sec=1377607.11, grad_norm=0.2944, duration=0.38s
Step 9753: loss=2.9884, lr=0.000428, tokens/sec=1375665.46, grad_norm=0.2968, duration=0.38s
Step 9754: loss=2.9667, lr=0.000428, tokens/sec=1379507.51, grad_norm=0.3084, duration=0.38s
Step 9755: loss=3.0272, lr=0.000428, tokens/sec=1377400.01, grad_norm=0.2882, duration=0.38s
Step 9756: loss=2.9774, lr=0.000428, tokens/sec=1379395.01, grad_norm=0.2895, duration=0.38s
Step 9757: loss=3.0525, lr=0.000428, tokens/sec=1373742.16, grad_norm=0.2764, duration=0.38s
Step 9758: loss=3.0032, lr=0.000428, tokens/sec=1375023.76, grad_norm=0.2883, duration=0.38s
Step 9759: loss=3.0146, lr=0.000428, tokens/sec=1375937.46, grad_norm=0.3113, duration=0.38s
Step 9760: loss=2.9735, lr=0.000428, tokens/sec=1373964.46, grad_norm=0.2714, duration=0.38s
Step 9761: loss=2.9829, lr=0.000428, tokens/sec=1374337.14, grad_norm=0.2976, duration=0.38s
Step 9762: loss=2.9705, lr=0.000428, tokens/sec=1376126.89, grad_norm=0.3017, duration=0.38s
Step 9763: loss=2.9859, lr=0.000428, tokens/sec=1377639.90, grad_norm=0.2891, duration=0.38s
Step 9764: loss=2.9686, lr=0.000428, tokens/sec=1374962.72, grad_norm=0.3025, duration=0.38s
Step 9765: loss=2.9825, lr=0.000428, tokens/sec=1377016.19, grad_norm=0.2778, duration=0.38s
Step 9766: loss=2.9973, lr=0.000428, tokens/sec=1375483.90, grad_norm=0.2984, duration=0.38s
Step 9767: loss=2.9249, lr=0.000428, tokens/sec=1379403.67, grad_norm=0.2817, duration=0.38s
Step 9768: loss=2.9530, lr=0.000428, tokens/sec=1376314.65, grad_norm=0.2758, duration=0.38s
Step 9769: loss=2.9630, lr=0.000428, tokens/sec=1373952.45, grad_norm=0.2954, duration=0.38s
Step 9770: loss=2.9685, lr=0.000428, tokens/sec=1378957.33, grad_norm=0.2706, duration=0.38s
Step 9771: loss=2.9639, lr=0.000427, tokens/sec=1374213.46, grad_norm=0.2693, duration=0.38s
Step 9772: loss=3.0096, lr=0.000427, tokens/sec=1377042.92, grad_norm=0.2841, duration=0.38s
Step 9773: loss=2.9169, lr=0.000427, tokens/sec=1378049.12, grad_norm=0.2815, duration=0.38s
Step 9774: loss=2.9348, lr=0.000427, tokens/sec=1373225.73, grad_norm=0.2747, duration=0.38s
Step 9775: loss=2.8546, lr=0.000427, tokens/sec=1365160.20, grad_norm=0.2868, duration=0.38s
Step 9776: loss=2.9278, lr=0.000427, tokens/sec=1375126.09, grad_norm=0.2669, duration=0.38s
Step 9777: loss=2.9601, lr=0.000427, tokens/sec=1374822.60, grad_norm=0.2977, duration=0.38s
Step 9778: loss=2.9039, lr=0.000427, tokens/sec=1376151.87, grad_norm=0.2982, duration=0.38s
Step 9779: loss=2.9620, lr=0.000427, tokens/sec=1378114.75, grad_norm=0.2913, duration=0.38s
Step 9780: loss=2.9280, lr=0.000427, tokens/sec=1377556.19, grad_norm=0.3089, duration=0.38s
Step 9781: loss=2.9480, lr=0.000427, tokens/sec=1374068.35, grad_norm=0.2848, duration=0.38s
Step 9782: loss=2.9681, lr=0.000427, tokens/sec=1374337.14, grad_norm=0.2990, duration=0.38s
Step 9783: loss=2.9580, lr=0.000427, tokens/sec=1381108.63, grad_norm=0.3152, duration=0.38s
Step 9784: loss=2.9971, lr=0.000427, tokens/sec=1375828.14, grad_norm=0.2940, duration=0.38s
Step 9785: loss=3.0320, lr=0.000427, tokens/sec=1373478.75, grad_norm=0.3455, duration=0.38s
Step 9786: loss=3.0202, lr=0.000427, tokens/sec=1374602.60, grad_norm=0.2894, duration=0.38s
Step 9787: loss=3.0187, lr=0.000427, tokens/sec=1371540.16, grad_norm=0.3464, duration=0.38s
Step 9788: loss=2.9322, lr=0.000427, tokens/sec=1375284.33, grad_norm=0.3154, duration=0.38s
Step 9789: loss=2.9782, lr=0.000427, tokens/sec=1376453.35, grad_norm=0.3160, duration=0.38s
Step 9790: loss=2.9642, lr=0.000427, tokens/sec=1373830.56, grad_norm=0.3155, duration=0.38s
Step 9791: loss=3.0263, lr=0.000427, tokens/sec=1375427.12, grad_norm=0.3179, duration=0.38s
Step 9792: loss=3.0417, lr=0.000427, tokens/sec=1377458.68, grad_norm=0.3159, duration=0.38s
Step 9793: loss=3.0024, lr=0.000427, tokens/sec=1377618.33, grad_norm=0.3040, duration=0.38s
Step 9794: loss=3.0024, lr=0.000427, tokens/sec=1376468.00, grad_norm=0.3354, duration=0.38s
Step 9795: loss=2.9739, lr=0.000427, tokens/sec=1374863.86, grad_norm=0.3180, duration=0.38s
Step 9796: loss=2.9769, lr=0.000427, tokens/sec=1374998.83, grad_norm=0.2981, duration=0.38s
Step 9797: loss=2.9978, lr=0.000427, tokens/sec=1374521.83, grad_norm=0.3283, duration=0.38s
Step 9798: loss=3.0024, lr=0.000427, tokens/sec=1378167.44, grad_norm=0.2983, duration=0.38s
Step 9799: loss=2.9751, lr=0.000427, tokens/sec=1375372.07, grad_norm=0.3002, duration=0.38s
Step 9800/19073 (51.4%), Elapsed time: 3887.52s, Steps per hour: 9075.20, Estimated hours remaining: 1.02
Step 9800: loss=3.0026, lr=0.000427, tokens/sec=1376080.39, grad_norm=0.3035, duration=0.38s
Step 9801: loss=2.9851, lr=0.000427, tokens/sec=1375808.34, grad_norm=0.2860, duration=0.38s
Step 9802: loss=2.9964, lr=0.000427, tokens/sec=1377571.73, grad_norm=0.2942, duration=0.38s
Step 9803: loss=2.9452, lr=0.000427, tokens/sec=1375734.32, grad_norm=0.3164, duration=0.38s
Step 9804: loss=2.9956, lr=0.000426, tokens/sec=1375485.62, grad_norm=0.3051, duration=0.38s
Step 9805: loss=2.9456, lr=0.000426, tokens/sec=1374904.26, grad_norm=0.3046, duration=0.38s
Step 9806: loss=3.0061, lr=0.000426, tokens/sec=1376564.51, grad_norm=0.3040, duration=0.38s
Step 9807: loss=2.9763, lr=0.000426, tokens/sec=1379443.47, grad_norm=0.2954, duration=0.38s
Step 9808: loss=2.9865, lr=0.000426, tokens/sec=1377748.66, grad_norm=0.2906, duration=0.38s
Step 9809: loss=2.9556, lr=0.000426, tokens/sec=1375334.22, grad_norm=0.2941, duration=0.38s
Step 9810: loss=2.9948, lr=0.000426, tokens/sec=1376783.42, grad_norm=0.2998, duration=0.38s
Step 9811: loss=2.9928, lr=0.000426, tokens/sec=1375434.00, grad_norm=0.2915, duration=0.38s
Step 9812: loss=2.9747, lr=0.000426, tokens/sec=1375581.99, grad_norm=0.3058, duration=0.38s
Step 9813: loss=2.9452, lr=0.000426, tokens/sec=1376247.47, grad_norm=0.2948, duration=0.38s
Step 9814: loss=2.9740, lr=0.000426, tokens/sec=1376382.71, grad_norm=0.2939, duration=0.38s
Step 9815: loss=2.9694, lr=0.000426, tokens/sec=1373979.92, grad_norm=0.3058, duration=0.38s
Step 9816: loss=2.9756, lr=0.000426, tokens/sec=1374921.46, grad_norm=0.2951, duration=0.38s
Step 9817: loss=2.9442, lr=0.000426, tokens/sec=1377300.80, grad_norm=0.3002, duration=0.38s
Step 9818: loss=2.9243, lr=0.000426, tokens/sec=1375680.96, grad_norm=0.3073, duration=0.38s
Step 9819: loss=2.8846, lr=0.000426, tokens/sec=1376062.31, grad_norm=0.2983, duration=0.38s
Step 9820: loss=2.9204, lr=0.000426, tokens/sec=1370274.43, grad_norm=0.3033, duration=0.38s
Step 9821: loss=2.8493, lr=0.000426, tokens/sec=1378004.21, grad_norm=0.3166, duration=0.38s
Step 9822: loss=2.8945, lr=0.000426, tokens/sec=1375499.39, grad_norm=0.2800, duration=0.38s
Step 9823: loss=2.8641, lr=0.000426, tokens/sec=1373754.17, grad_norm=0.3244, duration=0.38s
Step 9824: loss=2.9477, lr=0.000426, tokens/sec=1377391.39, grad_norm=0.3059, duration=0.38s
Step 9825: loss=2.9604, lr=0.000426, tokens/sec=1371138.22, grad_norm=0.2893, duration=0.38s
Step 9826: loss=2.9413, lr=0.000426, tokens/sec=1377038.61, grad_norm=0.3011, duration=0.38s
Step 9827: loss=2.9159, lr=0.000426, tokens/sec=1375128.67, grad_norm=0.3018, duration=0.38s
Step 9828: loss=2.9283, lr=0.000426, tokens/sec=1374040.01, grad_norm=0.2963, duration=0.38s
Step 9829: loss=2.9820, lr=0.000426, tokens/sec=1374567.37, grad_norm=0.3021, duration=0.38s
Step 9830: loss=2.9835, lr=0.000426, tokens/sec=1375292.07, grad_norm=0.3001, duration=0.38s
Step 9831: loss=3.0233, lr=0.000426, tokens/sec=1376349.11, grad_norm=0.3197, duration=0.38s
Step 9832: loss=2.9868, lr=0.000426, tokens/sec=1376845.48, grad_norm=0.3062, duration=0.38s
Step 9833: loss=2.9934, lr=0.000426, tokens/sec=1377042.92, grad_norm=0.3311, duration=0.38s
Step 9834: loss=3.0177, lr=0.000426, tokens/sec=1375882.37, grad_norm=0.3381, duration=0.38s
Step 9835: loss=2.9925, lr=0.000426, tokens/sec=1382211.12, grad_norm=0.3126, duration=0.38s
Step 9836: loss=3.0245, lr=0.000425, tokens/sec=1372965.94, grad_norm=0.3429, duration=0.38s
Step 9837: loss=3.0506, lr=0.000425, tokens/sec=1379941.21, grad_norm=0.3385, duration=0.38s
Step 9838: loss=3.1056, lr=0.000425, tokens/sec=1377960.17, grad_norm=0.3131, duration=0.38s
Step 9839: loss=3.0778, lr=0.000425, tokens/sec=1378015.44, grad_norm=0.3214, duration=0.38s
Step 9840: loss=2.9977, lr=0.000425, tokens/sec=1376020.12, grad_norm=0.3316, duration=0.38s
Step 9841: loss=2.9917, lr=0.000425, tokens/sec=1376574.85, grad_norm=0.3116, duration=0.38s
Step 9842: loss=3.0206, lr=0.000425, tokens/sec=1372963.37, grad_norm=0.3176, duration=0.38s
Step 9843: loss=3.0038, lr=0.000425, tokens/sec=1376455.08, grad_norm=0.2994, duration=0.38s
Step 9844: loss=2.9294, lr=0.000425, tokens/sec=1377391.39, grad_norm=0.3444, duration=0.38s
Step 9845: loss=3.0374, lr=0.000425, tokens/sec=1373353.51, grad_norm=0.2997, duration=0.38s
Step 9846: loss=2.9042, lr=0.000425, tokens/sec=1377111.05, grad_norm=0.4130, duration=0.38s
Step 9847: loss=3.0415, lr=0.000425, tokens/sec=1376512.81, grad_norm=0.3188, duration=0.38s
Step 9848: loss=3.0710, lr=0.000425, tokens/sec=1375905.61, grad_norm=0.3344, duration=0.38s
Step 9849: loss=2.9971, lr=0.000425, tokens/sec=1378391.18, grad_norm=0.3081, duration=0.38s
Step 9850: loss=2.9622, lr=0.000425, tokens/sec=1376246.61, grad_norm=0.3099, duration=0.38s
Step 9851: loss=2.9909, lr=0.000425, tokens/sec=1377808.22, grad_norm=0.2938, duration=0.38s
Step 9852: loss=3.0196, lr=0.000425, tokens/sec=1374685.09, grad_norm=0.3080, duration=0.38s
Step 9853: loss=2.9764, lr=0.000425, tokens/sec=1380351.79, grad_norm=0.2853, duration=0.38s
Step 9854: loss=2.9171, lr=0.000425, tokens/sec=1377229.21, grad_norm=0.2845, duration=0.38s
Step 9855: loss=3.0057, lr=0.000425, tokens/sec=1375500.25, grad_norm=0.3071, duration=0.38s
Step 9856: loss=2.9653, lr=0.000425, tokens/sec=1371861.02, grad_norm=0.2943, duration=0.38s
Step 9857: loss=2.9835, lr=0.000425, tokens/sec=1377319.78, grad_norm=0.2791, duration=0.38s
Step 9858: loss=3.0264, lr=0.000425, tokens/sec=1374307.94, grad_norm=0.3084, duration=0.38s
Step 9859: loss=2.9620, lr=0.000425, tokens/sec=1379036.02, grad_norm=0.2905, duration=0.38s
Step 9860: loss=2.9858, lr=0.000425, tokens/sec=1379356.08, grad_norm=0.2676, duration=0.38s
Step 9861: loss=2.9545, lr=0.000425, tokens/sec=1376973.94, grad_norm=0.3026, duration=0.38s
Step 9862: loss=2.9200, lr=0.000425, tokens/sec=1375267.99, grad_norm=0.2775, duration=0.38s
Step 9863: loss=2.9317, lr=0.000425, tokens/sec=1376195.79, grad_norm=0.2930, duration=0.38s
Step 9864: loss=2.9207, lr=0.000425, tokens/sec=1373316.63, grad_norm=0.2947, duration=0.38s
Step 9865: loss=2.8769, lr=0.000425, tokens/sec=1376865.31, grad_norm=0.2863, duration=0.38s
Step 9866: loss=2.9129, lr=0.000425, tokens/sec=1373691.53, grad_norm=0.2780, duration=0.38s
Step 9867: loss=2.9653, lr=0.000425, tokens/sec=1377447.47, grad_norm=0.2727, duration=0.38s
Step 9868: loss=2.8974, lr=0.000424, tokens/sec=1375628.46, grad_norm=0.2973, duration=0.38s
Step 9869: loss=2.9158, lr=0.000424, tokens/sec=1372902.51, grad_norm=0.2662, duration=0.38s
Step 9870: loss=2.8718, lr=0.000424, tokens/sec=1373204.29, grad_norm=0.2711, duration=0.38s
Step 9871: loss=2.8414, lr=0.000424, tokens/sec=1376660.16, grad_norm=0.3002, duration=0.38s
Step 9872: loss=2.9221, lr=0.000424, tokens/sec=1374799.40, grad_norm=0.2738, duration=0.38s
Step 9873: loss=2.9085, lr=0.000424, tokens/sec=1371874.72, grad_norm=0.2877, duration=0.38s
Step 9874: loss=2.9619, lr=0.000424, tokens/sec=1372251.40, grad_norm=0.2971, duration=0.38s
Step 9875: loss=2.9988, lr=0.000424, tokens/sec=1372633.42, grad_norm=0.2887, duration=0.38s
Step 9876: loss=2.9999, lr=0.000424, tokens/sec=1377334.45, grad_norm=0.3163, duration=0.38s
Step 9877: loss=2.9807, lr=0.000424, tokens/sec=1373457.30, grad_norm=0.2994, duration=0.38s
Step 9878: loss=2.9637, lr=0.000424, tokens/sec=1374286.46, grad_norm=0.3069, duration=0.38s
Step 9879: loss=3.0303, lr=0.000424, tokens/sec=1378074.16, grad_norm=0.3484, duration=0.38s
Step 9880: loss=2.9951, lr=0.000424, tokens/sec=1380477.44, grad_norm=0.2813, duration=0.38s
Step 9881: loss=3.0141, lr=0.000424, tokens/sec=1377040.34, grad_norm=0.3117, duration=0.38s
Step 9882: loss=3.0186, lr=0.000424, tokens/sec=1375699.89, grad_norm=0.3103, duration=0.38s
Step 9883: loss=3.0807, lr=0.000424, tokens/sec=1372829.66, grad_norm=0.3006, duration=0.38s
Step 9884: loss=3.0197, lr=0.000424, tokens/sec=1377981.76, grad_norm=0.3231, duration=0.38s
Step 9885: loss=3.0499, lr=0.000424, tokens/sec=1377716.72, grad_norm=0.3017, duration=0.38s
Step 9886: loss=3.0714, lr=0.000424, tokens/sec=1376824.79, grad_norm=0.2987, duration=0.38s
Step 9887: loss=3.0160, lr=0.000424, tokens/sec=1379058.51, grad_norm=0.2987, duration=0.38s
Step 9888: loss=3.0326, lr=0.000424, tokens/sec=1375668.91, grad_norm=0.3036, duration=0.38s
Step 9889: loss=3.0088, lr=0.000424, tokens/sec=1378250.36, grad_norm=0.2962, duration=0.38s
Step 9890: loss=3.0541, lr=0.000424, tokens/sec=1375824.69, grad_norm=0.2922, duration=0.38s
Step 9891: loss=2.9933, lr=0.000424, tokens/sec=1372050.19, grad_norm=0.3005, duration=0.38s
Step 9892: loss=3.0139, lr=0.000424, tokens/sec=1374040.87, grad_norm=0.2946, duration=0.38s
Step 9893: loss=2.9720, lr=0.000424, tokens/sec=1376181.15, grad_norm=0.3000, duration=0.38s
Step 9894: loss=2.9982, lr=0.000424, tokens/sec=1375776.49, grad_norm=0.3080, duration=0.38s
Step 9895: loss=3.0152, lr=0.000424, tokens/sec=1374742.67, grad_norm=0.3449, duration=0.38s
Step 9896: loss=2.9599, lr=0.000424, tokens/sec=1375519.18, grad_norm=0.3056, duration=0.38s
Step 9897: loss=3.0138, lr=0.000424, tokens/sec=1373801.38, grad_norm=0.3240, duration=0.38s
Step 9898: loss=2.9442, lr=0.000424, tokens/sec=1376762.73, grad_norm=0.3023, duration=0.38s
Step 9899: loss=2.8993, lr=0.000424, tokens/sec=1378775.77, grad_norm=0.3357, duration=0.38s
Step 9900/19073 (51.9%), Elapsed time: 3925.71s, Steps per hour: 9078.61, Estimated hours remaining: 1.01
Step 9900: loss=2.9632, lr=0.000423, tokens/sec=1378268.50, grad_norm=0.3265, duration=0.38s
Step 9901: loss=3.0026, lr=0.000423, tokens/sec=1376646.37, grad_norm=0.2955, duration=0.38s
Step 9902: loss=3.0684, lr=0.000423, tokens/sec=1377157.62, grad_norm=0.2960, duration=0.38s
Step 9903: loss=2.9047, lr=0.000423, tokens/sec=1372346.45, grad_norm=0.2947, duration=0.38s
Step 9904: loss=3.0171, lr=0.000423, tokens/sec=1378467.21, grad_norm=0.3128, duration=0.38s
Step 9905: loss=3.0270, lr=0.000423, tokens/sec=1375857.40, grad_norm=0.2937, duration=0.38s
Step 9906: loss=2.9383, lr=0.000423, tokens/sec=1377884.19, grad_norm=0.2967, duration=0.38s
Step 9907: loss=2.9746, lr=0.000423, tokens/sec=1374003.95, grad_norm=0.3115, duration=0.38s
Step 9908: loss=2.9925, lr=0.000423, tokens/sec=1379947.27, grad_norm=0.3052, duration=0.38s
Step 9909: loss=2.9762, lr=0.000423, tokens/sec=1377728.80, grad_norm=0.3116, duration=0.38s
Step 9910: loss=2.9518, lr=0.000423, tokens/sec=1376710.15, grad_norm=0.3017, duration=0.38s
Step 9911: loss=2.9519, lr=0.000423, tokens/sec=1375705.05, grad_norm=0.2954, duration=0.38s
Step 9912: loss=2.9839, lr=0.000423, tokens/sec=1376613.63, grad_norm=0.3595, duration=0.38s
Step 9913: loss=3.0056, lr=0.000423, tokens/sec=1377247.32, grad_norm=0.2963, duration=0.38s
Step 9914: loss=2.9382, lr=0.000423, tokens/sec=1378934.85, grad_norm=0.2909, duration=0.38s
Step 9915: loss=2.8957, lr=0.000423, tokens/sec=1378705.75, grad_norm=0.3100, duration=0.38s
Step 9916: loss=2.9475, lr=0.000423, tokens/sec=1375704.19, grad_norm=0.3067, duration=0.38s
Step 9917: loss=2.8309, lr=0.000423, tokens/sec=1375804.89, grad_norm=0.3018, duration=0.38s
Step 9918: loss=2.8829, lr=0.000423, tokens/sec=1375689.56, grad_norm=0.3187, duration=0.38s
Step 9919: loss=2.8968, lr=0.000423, tokens/sec=1376606.73, grad_norm=0.3049, duration=0.38s
Step 9920: loss=2.9488, lr=0.000423, tokens/sec=1369491.89, grad_norm=0.3158, duration=0.38s
Step 9921: loss=2.9502, lr=0.000423, tokens/sec=1375674.07, grad_norm=0.3174, duration=0.38s
Step 9922: loss=2.9501, lr=0.000423, tokens/sec=1376361.17, grad_norm=0.3040, duration=0.38s
Step 9923: loss=2.9596, lr=0.000423, tokens/sec=1374551.04, grad_norm=0.3092, duration=0.38s
Step 9924: loss=2.9355, lr=0.000423, tokens/sec=1372669.41, grad_norm=0.3198, duration=0.38s
Step 9925: loss=2.9177, lr=0.000423, tokens/sec=1379758.52, grad_norm=0.3018, duration=0.38s
Step 9926: loss=2.9021, lr=0.000423, tokens/sec=1377826.35, grad_norm=0.3100, duration=0.38s
Step 9927: loss=3.0201, lr=0.000423, tokens/sec=1368690.65, grad_norm=0.3313, duration=0.38s
Step 9928: loss=3.0602, lr=0.000423, tokens/sec=1374145.62, grad_norm=0.3412, duration=0.38s
Step 9929: loss=3.0087, lr=0.000423, tokens/sec=1374480.59, grad_norm=0.3149, duration=0.38s
Step 9930: loss=3.0355, lr=0.000423, tokens/sec=1373329.50, grad_norm=0.3493, duration=0.38s
Step 9931: loss=3.0233, lr=0.000423, tokens/sec=1373350.94, grad_norm=0.3298, duration=0.38s
Step 9932: loss=3.0365, lr=0.000422, tokens/sec=1377529.44, grad_norm=0.3291, duration=0.38s
Step 9933: loss=3.0228, lr=0.000422, tokens/sec=1373610.87, grad_norm=0.3394, duration=0.38s
Step 9934: loss=2.9778, lr=0.000422, tokens/sec=1373340.65, grad_norm=0.3463, duration=0.38s
Step 9935: loss=2.9892, lr=0.000422, tokens/sec=1375839.33, grad_norm=0.3330, duration=0.38s
Step 9936: loss=2.9620, lr=0.000422, tokens/sec=1371363.11, grad_norm=0.3457, duration=0.38s
Step 9937: loss=2.9749, lr=0.000422, tokens/sec=1374356.89, grad_norm=0.3250, duration=0.38s
Step 9938: loss=2.9398, lr=0.000422, tokens/sec=1374250.39, grad_norm=0.3389, duration=0.38s
Step 9939: loss=2.9702, lr=0.000422, tokens/sec=1373670.08, grad_norm=0.3282, duration=0.38s
Step 9940: loss=2.9627, lr=0.000422, tokens/sec=1376506.77, grad_norm=0.3024, duration=0.38s
Step 9941: loss=2.8554, lr=0.000422, tokens/sec=1372042.48, grad_norm=0.3315, duration=0.38s
Step 9942: loss=2.9374, lr=0.000422, tokens/sec=1370945.04, grad_norm=0.2952, duration=0.38s
Step 9943: loss=2.9710, lr=0.000422, tokens/sec=1374965.30, grad_norm=0.3069, duration=0.38s
Step 9944: loss=2.9796, lr=0.000422, tokens/sec=1366045.56, grad_norm=0.2968, duration=0.38s
Step 9945: loss=3.0347, lr=0.000422, tokens/sec=1375657.72, grad_norm=0.2994, duration=0.38s
Step 9946: loss=2.9706, lr=0.000422, tokens/sec=1375823.83, grad_norm=0.2906, duration=0.38s
Step 9947: loss=3.0352, lr=0.000422, tokens/sec=1374999.69, grad_norm=0.3012, duration=0.38s
Step 9948: loss=3.0237, lr=0.000422, tokens/sec=1375165.64, grad_norm=0.3157, duration=0.38s
Step 9949: loss=2.9660, lr=0.000422, tokens/sec=1377341.35, grad_norm=0.2976, duration=0.38s
Step 9950: loss=2.9754, lr=0.000422, tokens/sec=1378432.65, grad_norm=0.3058, duration=0.38s
Step 9951: loss=2.9888, lr=0.000422, tokens/sec=1371747.21, grad_norm=0.3047, duration=0.38s
Step 9952: loss=2.9593, lr=0.000422, tokens/sec=1372840.80, grad_norm=0.3250, duration=0.38s
Step 9953: loss=2.9613, lr=0.000422, tokens/sec=1372158.06, grad_norm=0.2997, duration=0.38s
Step 9954: loss=2.9798, lr=0.000422, tokens/sec=1374460.83, grad_norm=0.3148, duration=0.38s
Step 9955: loss=2.9898, lr=0.000422, tokens/sec=1375257.67, grad_norm=0.3027, duration=0.38s
Step 9956: loss=2.9665, lr=0.000422, tokens/sec=1377237.83, grad_norm=0.2974, duration=0.38s
Step 9957: loss=2.9140, lr=0.000422, tokens/sec=1375255.09, grad_norm=0.3095, duration=0.38s
Step 9958: loss=2.9645, lr=0.000422, tokens/sec=1376730.84, grad_norm=0.2801, duration=0.38s
Step 9959: loss=2.9515, lr=0.000422, tokens/sec=1376060.59, grad_norm=0.3156, duration=0.38s
Step 9960: loss=3.0312, lr=0.000422, tokens/sec=1373039.67, grad_norm=0.2972, duration=0.38s
Step 9961: loss=2.9441, lr=0.000422, tokens/sec=1371523.05, grad_norm=0.3016, duration=0.38s
Step 9962: loss=2.9674, lr=0.000422, tokens/sec=1372588.87, grad_norm=0.3124, duration=0.38s
Step 9963: loss=2.9197, lr=0.000422, tokens/sec=1376520.56, grad_norm=0.2883, duration=0.38s
Step 9964: loss=2.8962, lr=0.000421, tokens/sec=1375136.41, grad_norm=0.2999, duration=0.38s
Step 9965: loss=2.8915, lr=0.000421, tokens/sec=1374056.33, grad_norm=0.2918, duration=0.38s
Step 9966: loss=2.9366, lr=0.000421, tokens/sec=1375022.90, grad_norm=0.2770, duration=0.38s
Step 9967: loss=2.9466, lr=0.000421, tokens/sec=1379583.67, grad_norm=0.3054, duration=0.38s
Step 9968: loss=2.9314, lr=0.000421, tokens/sec=1377181.77, grad_norm=0.3081, duration=0.38s
Step 9969: loss=2.9393, lr=0.000421, tokens/sec=1375539.83, grad_norm=0.2852, duration=0.38s
Step 9970: loss=2.8893, lr=0.000421, tokens/sec=1373231.73, grad_norm=0.3234, duration=0.38s
Step 9971: loss=2.9766, lr=0.000421, tokens/sec=1372249.68, grad_norm=0.2889, duration=0.38s
Step 9972: loss=2.9270, lr=0.000421, tokens/sec=1373716.41, grad_norm=0.2834, duration=0.38s
Step 9973: loss=2.9467, lr=0.000421, tokens/sec=1373052.52, grad_norm=0.3286, duration=0.38s
Step 9974: loss=3.0172, lr=0.000421, tokens/sec=1376530.90, grad_norm=0.3019, duration=0.38s
Step 9975: loss=3.0079, lr=0.000421, tokens/sec=1377074.83, grad_norm=0.3231, duration=0.38s
Step 9976: loss=3.0089, lr=0.000421, tokens/sec=1375305.83, grad_norm=0.3149, duration=0.38s
Step 9977: loss=2.9940, lr=0.000421, tokens/sec=1376695.50, grad_norm=0.3160, duration=0.38s
Step 9978: loss=2.9286, lr=0.000421, tokens/sec=1377496.65, grad_norm=0.3221, duration=0.38s
Step 9979: loss=2.9943, lr=0.000421, tokens/sec=1375352.28, grad_norm=0.3107, duration=0.38s
Step 9980: loss=2.9560, lr=0.000421, tokens/sec=1370411.06, grad_norm=0.3160, duration=0.38s
Step 9981: loss=3.0402, lr=0.000421, tokens/sec=1374376.65, grad_norm=0.3233, duration=0.38s
Step 9982: loss=3.0052, lr=0.000421, tokens/sec=1374999.69, grad_norm=0.3224, duration=0.38s
Step 9983: loss=2.9926, lr=0.000421, tokens/sec=1375299.81, grad_norm=0.3106, duration=0.38s
Step 9984: loss=3.0077, lr=0.000421, tokens/sec=1374324.25, grad_norm=0.3304, duration=0.38s
Step 9985: loss=2.9620, lr=0.000421, tokens/sec=1375310.99, grad_norm=0.3429, duration=0.38s
Step 9986: loss=2.9879, lr=0.000421, tokens/sec=1372588.01, grad_norm=0.2948, duration=0.38s
Step 9987: loss=2.9878, lr=0.000421, tokens/sec=1376258.67, grad_norm=0.3262, duration=0.38s
Step 9988: loss=3.0166, lr=0.000421, tokens/sec=1377933.41, grad_norm=0.3303, duration=0.38s
Step 9989: loss=2.9455, lr=0.000421, tokens/sec=1375811.78, grad_norm=0.2913, duration=0.38s
Step 9990: loss=2.9922, lr=0.000421, tokens/sec=1371091.20, grad_norm=0.3252, duration=0.38s
Step 9991: loss=2.9791, lr=0.000421, tokens/sec=1374247.82, grad_norm=0.3098, duration=0.38s
Step 9992: loss=2.9978, lr=0.000421, tokens/sec=1377407.78, grad_norm=0.2954, duration=0.38s
Step 9993: loss=2.9185, lr=0.000421, tokens/sec=1376459.39, grad_norm=0.3008, duration=0.38s
Step 9994: loss=2.9970, lr=0.000421, tokens/sec=1372389.28, grad_norm=0.3178, duration=0.38s
Step 9995: loss=2.9454, lr=0.000421, tokens/sec=1374930.91, grad_norm=0.2909, duration=0.38s
Step 9996: loss=3.0018, lr=0.000420, tokens/sec=1376580.02, grad_norm=0.3000, duration=0.38s
Step 9997: loss=2.9591, lr=0.000420, tokens/sec=1368017.14, grad_norm=0.2941, duration=0.38s
Step 9998: loss=2.9706, lr=0.000420, tokens/sec=1375436.59, grad_norm=0.2979, duration=0.38s
Step 9999: loss=2.9795, lr=0.000420, tokens/sec=1373834.85, grad_norm=0.3022, duration=0.38s
Step 10000/19073 (52.4%), Elapsed time: 3963.93s, Steps per hour: 9081.91, Estimated hours remaining: 1.00
Validation loss at step 10000: 3.766824245452881
Checkpoint saved to model_checkpoints/model_10000.pt
Step 10000: loss=2.9869, lr=0.000420, tokens/sec=93826.29, grad_norm=0.2826, duration=5.59s
Step 10001: loss=2.9593, lr=0.000420, tokens/sec=1380901.35, grad_norm=0.3009, duration=0.38s
Step 10002: loss=2.9592, lr=0.000420, tokens/sec=1381542.47, grad_norm=0.3005, duration=0.38s
Step 10003: loss=2.9502, lr=0.000420, tokens/sec=1381355.02, grad_norm=0.3082, duration=0.38s
Step 10004: loss=2.9588, lr=0.000420, tokens/sec=1378956.47, grad_norm=0.2786, duration=0.38s
Step 10005: loss=2.9801, lr=0.000420, tokens/sec=1376426.65, grad_norm=0.3136, duration=0.38s
Step 10006: loss=2.9517, lr=0.000420, tokens/sec=1376200.10, grad_norm=0.3071, duration=0.38s
Step 10007: loss=2.9209, lr=0.000420, tokens/sec=1376720.49, grad_norm=0.2817, duration=0.38s
Step 10008: loss=2.9330, lr=0.000420, tokens/sec=1378921.88, grad_norm=0.3050, duration=0.38s
Step 10009: loss=2.8778, lr=0.000420, tokens/sec=1375251.65, grad_norm=0.3082, duration=0.38s
Step 10010: loss=2.9172, lr=0.000420, tokens/sec=1372749.96, grad_norm=0.2983, duration=0.38s
Step 10011: loss=2.8462, lr=0.000420, tokens/sec=1370782.66, grad_norm=0.3119, duration=0.38s
Step 10012: loss=2.8603, lr=0.000420, tokens/sec=1373540.51, grad_norm=0.2828, duration=0.38s
Step 10013: loss=2.8756, lr=0.000420, tokens/sec=1369101.38, grad_norm=0.2928, duration=0.38s
Step 10014: loss=2.9847, lr=0.000420, tokens/sec=1370276.99, grad_norm=0.3176, duration=0.38s
Step 10015: loss=2.9580, lr=0.000420, tokens/sec=1372681.40, grad_norm=0.2936, duration=0.38s
Step 10016: loss=2.9219, lr=0.000420, tokens/sec=1373552.53, grad_norm=0.2924, duration=0.38s
Step 10017: loss=2.8937, lr=0.000420, tokens/sec=1374688.53, grad_norm=0.2882, duration=0.38s
Step 10018: loss=2.9415, lr=0.000420, tokens/sec=1374988.51, grad_norm=0.2792, duration=0.38s
Step 10019: loss=2.9609, lr=0.000420, tokens/sec=1371372.52, grad_norm=0.2969, duration=0.38s
Step 10020: loss=3.0008, lr=0.000420, tokens/sec=1372039.06, grad_norm=0.2972, duration=0.38s
Step 10021: loss=2.9744, lr=0.000420, tokens/sec=1372707.97, grad_norm=0.2900, duration=0.38s
Step 10022: loss=3.0097, lr=0.000420, tokens/sec=1367260.98, grad_norm=0.3030, duration=0.38s
Step 10023: loss=2.9791, lr=0.000420, tokens/sec=1370733.10, grad_norm=0.2931, duration=0.38s
Step 10024: loss=3.0359, lr=0.000420, tokens/sec=1371778.01, grad_norm=0.3105, duration=0.38s
Step 10025: loss=2.9942, lr=0.000420, tokens/sec=1374316.52, grad_norm=0.3161, duration=0.38s
Step 10026: loss=2.9961, lr=0.000420, tokens/sec=1373497.62, grad_norm=0.3190, duration=0.38s
Step 10027: loss=3.0394, lr=0.000420, tokens/sec=1374588.85, grad_norm=0.3421, duration=0.38s
Step 10028: loss=3.1258, lr=0.000419, tokens/sec=1370800.61, grad_norm=0.3144, duration=0.38s
Step 10029: loss=3.0059, lr=0.000419, tokens/sec=1371354.56, grad_norm=0.3193, duration=0.38s
Step 10030: loss=3.0382, lr=0.000419, tokens/sec=1372003.11, grad_norm=0.3346, duration=0.38s
Step 10031: loss=2.9831, lr=0.000419, tokens/sec=1375206.06, grad_norm=0.3086, duration=0.38s
Step 10032: loss=3.0337, lr=0.000419, tokens/sec=1373875.19, grad_norm=0.3216, duration=0.38s
Step 10033: loss=2.9556, lr=0.000419, tokens/sec=1370428.99, grad_norm=0.3171, duration=0.38s
Step 10034: loss=2.9390, lr=0.000419, tokens/sec=1373504.48, grad_norm=0.3109, duration=0.38s
Step 10035: loss=2.9834, lr=0.000419, tokens/sec=1373428.14, grad_norm=0.3694, duration=0.38s
Step 10036: loss=2.9446, lr=0.000419, tokens/sec=1374293.33, grad_norm=0.3236, duration=0.38s
Step 10037: loss=3.0358, lr=0.000419, tokens/sec=1372990.80, grad_norm=0.3585, duration=0.38s
Step 10038: loss=3.0430, lr=0.000419, tokens/sec=1372302.78, grad_norm=0.3314, duration=0.38s
Step 10039: loss=3.0074, lr=0.000419, tokens/sec=1374835.50, grad_norm=0.3171, duration=0.38s
Step 10040: loss=2.9345, lr=0.000419, tokens/sec=1373441.00, grad_norm=0.3134, duration=0.38s
Step 10041: loss=2.9920, lr=0.000419, tokens/sec=1376172.54, grad_norm=0.3124, duration=0.38s
Step 10042: loss=3.0042, lr=0.000419, tokens/sec=1376351.70, grad_norm=0.2970, duration=0.38s
Step 10043: loss=2.9809, lr=0.000419, tokens/sec=1374325.11, grad_norm=0.3083, duration=0.38s
Step 10044: loss=2.9372, lr=0.000419, tokens/sec=1375089.11, grad_norm=0.2788, duration=0.38s
Step 10045: loss=2.9598, lr=0.000419, tokens/sec=1372779.09, grad_norm=0.2905, duration=0.38s
Step 10046: loss=2.9577, lr=0.000419, tokens/sec=1370908.28, grad_norm=0.2842, duration=0.38s
Step 10047: loss=2.9891, lr=0.000419, tokens/sec=1371240.82, grad_norm=0.2807, duration=0.38s
Step 10048: loss=3.0320, lr=0.000419, tokens/sec=1375918.52, grad_norm=0.2945, duration=0.38s
Step 10049: loss=2.9730, lr=0.000419, tokens/sec=1374444.51, grad_norm=0.2900, duration=0.38s
Step 10050: loss=2.9468, lr=0.000419, tokens/sec=1375820.39, grad_norm=0.2684, duration=0.38s
Step 10051: loss=2.9588, lr=0.000419, tokens/sec=1378512.15, grad_norm=0.2917, duration=0.38s
Step 10052: loss=2.8816, lr=0.000419, tokens/sec=1378063.80, grad_norm=0.2898, duration=0.38s
Step 10053: loss=2.9502, lr=0.000419, tokens/sec=1372567.45, grad_norm=0.3019, duration=0.38s
Step 10054: loss=2.8881, lr=0.000419, tokens/sec=1374861.28, grad_norm=0.2830, duration=0.38s
Step 10055: loss=2.9015, lr=0.000419, tokens/sec=1377200.75, grad_norm=0.3006, duration=0.38s
Step 10056: loss=2.8993, lr=0.000419, tokens/sec=1376465.42, grad_norm=0.2899, duration=0.38s
Step 10057: loss=2.9382, lr=0.000419, tokens/sec=1377945.50, grad_norm=0.2691, duration=0.38s
Step 10058: loss=2.9304, lr=0.000419, tokens/sec=1374910.28, grad_norm=0.3000, duration=0.38s
Step 10059: loss=2.8955, lr=0.000419, tokens/sec=1375631.90, grad_norm=0.2766, duration=0.38s
Step 10060: loss=2.8059, lr=0.000418, tokens/sec=1374082.08, grad_norm=0.3035, duration=0.38s
Step 10061: loss=2.8837, lr=0.000418, tokens/sec=1379430.49, grad_norm=0.2978, duration=0.38s
Step 10062: loss=2.9183, lr=0.000418, tokens/sec=1377457.82, grad_norm=0.2812, duration=0.38s
Step 10063: loss=2.9335, lr=0.000418, tokens/sec=1377415.54, grad_norm=0.2954, duration=0.38s
Step 10064: loss=2.9497, lr=0.000418, tokens/sec=1377609.70, grad_norm=0.3198, duration=0.38s
Step 10065: loss=2.9951, lr=0.000418, tokens/sec=1377396.56, grad_norm=0.2834, duration=0.38s
Step 10066: loss=2.9717, lr=0.000418, tokens/sec=1371710.41, grad_norm=0.3095, duration=0.38s
Step 10067: loss=2.9889, lr=0.000418, tokens/sec=1372564.88, grad_norm=0.3248, duration=0.38s
Step 10068: loss=2.9918, lr=0.000418, tokens/sec=1378227.04, grad_norm=0.2792, duration=0.38s
Step 10069: loss=3.0130, lr=0.000418, tokens/sec=1371567.53, grad_norm=0.3669, duration=0.38s
Step 10070: loss=2.9718, lr=0.000418, tokens/sec=1371079.24, grad_norm=0.3132, duration=0.38s
Step 10071: loss=3.0088, lr=0.000418, tokens/sec=1371521.34, grad_norm=0.2935, duration=0.38s
Step 10072: loss=3.0066, lr=0.000418, tokens/sec=1376391.32, grad_norm=0.3649, duration=0.38s
Step 10073: loss=3.0809, lr=0.000418, tokens/sec=1376062.31, grad_norm=0.3224, duration=0.38s
Step 10074: loss=3.0226, lr=0.000418, tokens/sec=1367381.70, grad_norm=0.3166, duration=0.38s
Step 10075: loss=3.0225, lr=0.000418, tokens/sec=1376439.57, grad_norm=0.3672, duration=0.38s
Step 10076: loss=3.0760, lr=0.000418, tokens/sec=1376837.72, grad_norm=0.3286, duration=0.38s
Step 10077: loss=3.0096, lr=0.000418, tokens/sec=1376485.23, grad_norm=0.3404, duration=0.38s
Step 10078: loss=3.0243, lr=0.000418, tokens/sec=1376760.14, grad_norm=0.3387, duration=0.38s
Step 10079: loss=3.0318, lr=0.000418, tokens/sec=1379305.03, grad_norm=0.3326, duration=0.38s
Step 10080: loss=3.0191, lr=0.000418, tokens/sec=1375902.17, grad_norm=0.3094, duration=0.38s
Step 10081: loss=2.9949, lr=0.000418, tokens/sec=1377223.17, grad_norm=0.3169, duration=0.38s
Step 10082: loss=3.0285, lr=0.000418, tokens/sec=1373749.02, grad_norm=0.3449, duration=0.38s
Step 10083: loss=2.9239, lr=0.000418, tokens/sec=1377696.87, grad_norm=0.3075, duration=0.38s
Step 10084: loss=3.0258, lr=0.000418, tokens/sec=1374865.58, grad_norm=0.3324, duration=0.38s
Step 10085: loss=2.9959, lr=0.000418, tokens/sec=1376070.06, grad_norm=0.3435, duration=0.38s
Step 10086: loss=2.9395, lr=0.000418, tokens/sec=1373889.78, grad_norm=0.3307, duration=0.38s
Step 10087: loss=3.0242, lr=0.000418, tokens/sec=1373833.99, grad_norm=0.3273, duration=0.38s
Step 10088: loss=2.9326, lr=0.000418, tokens/sec=1375117.49, grad_norm=0.3537, duration=0.38s
Step 10089: loss=2.9110, lr=0.000418, tokens/sec=1373246.31, grad_norm=0.4006, duration=0.38s
Step 10090: loss=2.9557, lr=0.000418, tokens/sec=1373617.73, grad_norm=0.3103, duration=0.38s
Step 10091: loss=2.9872, lr=0.000418, tokens/sec=1375596.62, grad_norm=0.3209, duration=0.38s
Step 10092: loss=3.0535, lr=0.000417, tokens/sec=1377358.60, grad_norm=0.3280, duration=0.38s
Step 10093: loss=2.9055, lr=0.000417, tokens/sec=1378463.76, grad_norm=0.3067, duration=0.38s
Step 10094: loss=3.0351, lr=0.000417, tokens/sec=1376493.85, grad_norm=0.3034, duration=0.38s
Step 10095: loss=2.9879, lr=0.000417, tokens/sec=1376520.56, grad_norm=0.3323, duration=0.38s
Step 10096: loss=2.9382, lr=0.000417, tokens/sec=1373955.88, grad_norm=0.3067, duration=0.38s
Step 10097: loss=2.9659, lr=0.000417, tokens/sec=1375933.16, grad_norm=0.3161, duration=0.38s
Step 10098: loss=2.9950, lr=0.000417, tokens/sec=1377319.78, grad_norm=0.3201, duration=0.38s
Step 10099: loss=2.9738, lr=0.000417, tokens/sec=1365350.06, grad_norm=0.3124, duration=0.38s
Step 10100/19073 (53.0%), Elapsed time: 4007.36s, Steps per hour: 9073.30, Estimated hours remaining: 0.99
Step 10100: loss=2.9152, lr=0.000417, tokens/sec=1377636.45, grad_norm=0.3333, duration=0.38s
Step 10101: loss=2.9620, lr=0.000417, tokens/sec=1376228.52, grad_norm=0.2951, duration=0.38s
Step 10102: loss=2.9692, lr=0.000417, tokens/sec=1375196.60, grad_norm=0.3414, duration=0.38s
Step 10103: loss=2.9949, lr=0.000417, tokens/sec=1378571.78, grad_norm=0.3281, duration=0.38s
Step 10104: loss=2.9416, lr=0.000417, tokens/sec=1376430.95, grad_norm=0.2933, duration=0.38s
Step 10105: loss=2.8942, lr=0.000417, tokens/sec=1377981.76, grad_norm=0.3161, duration=0.38s
Step 10106: loss=2.8766, lr=0.000417, tokens/sec=1376911.00, grad_norm=0.3181, duration=0.38s
Step 10107: loss=2.8836, lr=0.000417, tokens/sec=1375059.88, grad_norm=0.2879, duration=0.38s
Step 10108: loss=2.8764, lr=0.000417, tokens/sec=1379017.00, grad_norm=0.3437, duration=0.38s
Step 10109: loss=2.9302, lr=0.000417, tokens/sec=1377733.98, grad_norm=0.3267, duration=0.38s
Step 10110: loss=2.9383, lr=0.000417, tokens/sec=1375194.02, grad_norm=0.3159, duration=0.38s
Step 10111: loss=2.9428, lr=0.000417, tokens/sec=1376178.57, grad_norm=0.3473, duration=0.38s
Step 10112: loss=2.9362, lr=0.000417, tokens/sec=1378841.47, grad_norm=0.3216, duration=0.38s
Step 10113: loss=2.9554, lr=0.000417, tokens/sec=1376195.79, grad_norm=0.3103, duration=0.38s
Step 10114: loss=2.9130, lr=0.000417, tokens/sec=1377181.77, grad_norm=0.3232, duration=0.38s
Step 10115: loss=2.8845, lr=0.000417, tokens/sec=1373432.42, grad_norm=0.3057, duration=0.38s
Step 10116: loss=2.9418, lr=0.000417, tokens/sec=1379359.54, grad_norm=0.3214, duration=0.38s
Step 10117: loss=3.0128, lr=0.000417, tokens/sec=1376070.06, grad_norm=0.3301, duration=0.38s
Step 10118: loss=3.0479, lr=0.000417, tokens/sec=1376457.66, grad_norm=0.3278, duration=0.38s
Step 10119: loss=3.0297, lr=0.000417, tokens/sec=1377738.30, grad_norm=0.3626, duration=0.38s
Step 10120: loss=2.9934, lr=0.000417, tokens/sec=1378627.09, grad_norm=0.3250, duration=0.38s
Step 10121: loss=3.0372, lr=0.000417, tokens/sec=1375783.38, grad_norm=0.3466, duration=0.38s
Step 10122: loss=3.0192, lr=0.000417, tokens/sec=1377685.65, grad_norm=0.3349, duration=0.38s
Step 10123: loss=3.0076, lr=0.000417, tokens/sec=1375786.82, grad_norm=0.3308, duration=0.38s
Step 10124: loss=2.9900, lr=0.000416, tokens/sec=1375980.51, grad_norm=0.3735, duration=0.38s
Step 10125: loss=2.9297, lr=0.000416, tokens/sec=1372460.37, grad_norm=0.3312, duration=0.38s
Step 10126: loss=2.9857, lr=0.000416, tokens/sec=1376561.92, grad_norm=0.3506, duration=0.38s
Step 10127: loss=2.9420, lr=0.000416, tokens/sec=1377911.82, grad_norm=0.3724, duration=0.38s
Step 10128: loss=2.9673, lr=0.000416, tokens/sec=1377136.92, grad_norm=0.3365, duration=0.38s
Step 10129: loss=2.9609, lr=0.000416, tokens/sec=1374429.05, grad_norm=0.3548, duration=0.38s
Step 10130: loss=2.9276, lr=0.000416, tokens/sec=1376271.59, grad_norm=0.3692, duration=0.38s
Step 10131: loss=2.8781, lr=0.000416, tokens/sec=1376480.06, grad_norm=0.3342, duration=0.38s
Step 10132: loss=2.9217, lr=0.000416, tokens/sec=1373465.02, grad_norm=0.3169, duration=0.38s
Step 10133: loss=2.9835, lr=0.000416, tokens/sec=1373532.79, grad_norm=0.3215, duration=0.38s
Step 10134: loss=2.9887, lr=0.000416, tokens/sec=1373725.00, grad_norm=0.3137, duration=0.38s
Step 10135: loss=3.0300, lr=0.000416, tokens/sec=1373001.09, grad_norm=0.3272, duration=0.38s
Step 10136: loss=2.9537, lr=0.000416, tokens/sec=1375432.28, grad_norm=0.3263, duration=0.38s
Step 10137: loss=3.0563, lr=0.000416, tokens/sec=1372192.31, grad_norm=0.3223, duration=0.38s
Step 10138: loss=2.9773, lr=0.000416, tokens/sec=1372392.70, grad_norm=0.3294, duration=0.38s
Step 10139: loss=2.9665, lr=0.000416, tokens/sec=1372734.53, grad_norm=0.3554, duration=0.38s
Step 10140: loss=2.9811, lr=0.000416, tokens/sec=1375307.55, grad_norm=0.3357, duration=0.38s
Step 10141: loss=2.9785, lr=0.000416, tokens/sec=1375900.45, grad_norm=0.3002, duration=0.38s
Step 10142: loss=2.9354, lr=0.000416, tokens/sec=1373151.98, grad_norm=0.3306, duration=0.38s
Step 10143: loss=2.9764, lr=0.000416, tokens/sec=1371405.01, grad_norm=0.3459, duration=0.38s
Step 10144: loss=2.9859, lr=0.000416, tokens/sec=1379723.89, grad_norm=0.3076, duration=0.38s
Step 10145: loss=2.9571, lr=0.000416, tokens/sec=1374266.71, grad_norm=0.3057, duration=0.38s
Step 10146: loss=2.9534, lr=0.000416, tokens/sec=1376140.67, grad_norm=0.3191, duration=0.38s
Step 10147: loss=2.9241, lr=0.000416, tokens/sec=1375647.39, grad_norm=0.3202, duration=0.38s
Step 10148: loss=2.9545, lr=0.000416, tokens/sec=1380900.48, grad_norm=0.3096, duration=0.38s
Step 10149: loss=3.0121, lr=0.000416, tokens/sec=1377011.02, grad_norm=0.3210, duration=0.38s
Step 10150: loss=3.0116, lr=0.000416, tokens/sec=1377444.88, grad_norm=0.3361, duration=0.38s
Step 10151: loss=2.8990, lr=0.000416, tokens/sec=1378168.30, grad_norm=0.3150, duration=0.38s
Step 10152: loss=2.9699, lr=0.000416, tokens/sec=1376134.65, grad_norm=0.3253, duration=0.38s
Step 10153: loss=2.8808, lr=0.000416, tokens/sec=1373390.39, grad_norm=0.2937, duration=0.38s
Step 10154: loss=2.9353, lr=0.000416, tokens/sec=1373561.96, grad_norm=0.3082, duration=0.38s
Step 10155: loss=2.8991, lr=0.000416, tokens/sec=1376763.59, grad_norm=0.3316, duration=0.38s
Step 10156: loss=2.9195, lr=0.000415, tokens/sec=1373292.62, grad_norm=0.2896, duration=0.38s
Step 10157: loss=2.9714, lr=0.000415, tokens/sec=1374942.95, grad_norm=0.2992, duration=0.38s
Step 10158: loss=2.9112, lr=0.000415, tokens/sec=1377116.22, grad_norm=0.3424, duration=0.38s
Step 10159: loss=2.9010, lr=0.000415, tokens/sec=1378004.21, grad_norm=0.2937, duration=0.38s
Step 10160: loss=2.9180, lr=0.000415, tokens/sec=1377335.31, grad_norm=0.3254, duration=0.38s
Step 10161: loss=2.9393, lr=0.000415, tokens/sec=1376754.11, grad_norm=0.3161, duration=0.38s
Step 10162: loss=2.9170, lr=0.000415, tokens/sec=1375065.90, grad_norm=0.3021, duration=0.38s
Step 10163: loss=2.9692, lr=0.000415, tokens/sec=1376954.97, grad_norm=0.3148, duration=0.38s
Step 10164: loss=2.9952, lr=0.000415, tokens/sec=1375296.37, grad_norm=0.3081, duration=0.38s
Step 10165: loss=2.9976, lr=0.000415, tokens/sec=1376054.56, grad_norm=0.3174, duration=0.38s
Step 10166: loss=2.9869, lr=0.000415, tokens/sec=1378854.44, grad_norm=0.3255, duration=0.38s
Step 10167: loss=2.9870, lr=0.000415, tokens/sec=1374822.60, grad_norm=0.3273, duration=0.38s
Step 10168: loss=2.9444, lr=0.000415, tokens/sec=1378603.75, grad_norm=0.3105, duration=0.38s
Step 10169: loss=2.9866, lr=0.000415, tokens/sec=1377285.28, grad_norm=0.3315, duration=0.38s
Step 10170: loss=2.9717, lr=0.000415, tokens/sec=1372013.38, grad_norm=0.3358, duration=0.38s
Step 10171: loss=3.0063, lr=0.000415, tokens/sec=1375304.11, grad_norm=0.3506, duration=0.38s
Step 10172: loss=2.9956, lr=0.000415, tokens/sec=1377010.16, grad_norm=0.3507, duration=0.38s
Step 10173: loss=2.9987, lr=0.000415, tokens/sec=1374112.14, grad_norm=0.3414, duration=0.38s
Step 10174: loss=2.9912, lr=0.000415, tokens/sec=1377344.80, grad_norm=0.3283, duration=0.38s
Step 10175: loss=2.9731, lr=0.000415, tokens/sec=1376206.99, grad_norm=0.3518, duration=0.38s
Step 10176: loss=2.9789, lr=0.000415, tokens/sec=1374677.36, grad_norm=0.3272, duration=0.38s
Step 10177: loss=3.0031, lr=0.000415, tokens/sec=1378624.50, grad_norm=0.2933, duration=0.38s
Step 10178: loss=2.9926, lr=0.000415, tokens/sec=1374723.76, grad_norm=0.3412, duration=0.38s
Step 10179: loss=2.9351, lr=0.000415, tokens/sec=1377803.91, grad_norm=0.3220, duration=0.38s
Step 10180: loss=2.9848, lr=0.000415, tokens/sec=1376784.28, grad_norm=0.2995, duration=0.38s
Step 10181: loss=2.9794, lr=0.000415, tokens/sec=1376083.84, grad_norm=0.3183, duration=0.38s
Step 10182: loss=2.9728, lr=0.000415, tokens/sec=1375483.90, grad_norm=0.2766, duration=0.38s
Step 10183: loss=2.9210, lr=0.000415, tokens/sec=1368348.28, grad_norm=0.2940, duration=0.38s
Step 10184: loss=2.9959, lr=0.000415, tokens/sec=1377070.52, grad_norm=0.3104, duration=0.38s
Step 10185: loss=2.9399, lr=0.000415, tokens/sec=1377003.26, grad_norm=0.2893, duration=0.38s
Step 10186: loss=2.9822, lr=0.000415, tokens/sec=1371409.29, grad_norm=0.3065, duration=0.38s
Step 10187: loss=2.9434, lr=0.000415, tokens/sec=1378493.14, grad_norm=0.2857, duration=0.38s
Step 10188: loss=2.9945, lr=0.000414, tokens/sec=1373409.26, grad_norm=0.2972, duration=0.38s
Step 10189: loss=2.9732, lr=0.000414, tokens/sec=1369324.74, grad_norm=0.2871, duration=0.38s
Step 10190: loss=2.9559, lr=0.000414, tokens/sec=1374869.02, grad_norm=0.3039, duration=0.38s
Step 10191: loss=2.9450, lr=0.000414, tokens/sec=1372594.87, grad_norm=0.3025, duration=0.38s
Step 10192: loss=2.9627, lr=0.000414, tokens/sec=1371151.05, grad_norm=0.3055, duration=0.38s
Step 10193: loss=2.9380, lr=0.000414, tokens/sec=1373039.67, grad_norm=0.3295, duration=0.38s
Step 10194: loss=2.9728, lr=0.000414, tokens/sec=1376201.82, grad_norm=0.2986, duration=0.38s
Step 10195: loss=2.9546, lr=0.000414, tokens/sec=1373397.26, grad_norm=0.2964, duration=0.38s
Step 10196: loss=2.9268, lr=0.000414, tokens/sec=1372517.76, grad_norm=0.3040, duration=0.38s
Step 10197: loss=2.9324, lr=0.000414, tokens/sec=1375681.82, grad_norm=0.3008, duration=0.38s
Step 10198: loss=2.9264, lr=0.000414, tokens/sec=1377234.38, grad_norm=0.2861, duration=0.38s
Step 10199: loss=2.8776, lr=0.000414, tokens/sec=1376021.84, grad_norm=0.2944, duration=0.38s
Step 10200/19073 (53.5%), Elapsed time: 4045.56s, Steps per hour: 9076.61, Estimated hours remaining: 0.98
Step 10200: loss=2.9138, lr=0.000414, tokens/sec=1375237.88, grad_norm=0.2824, duration=0.38s
Step 10201: loss=2.8093, lr=0.000414, tokens/sec=1375281.75, grad_norm=0.2970, duration=0.38s
Step 10202: loss=2.8738, lr=0.000414, tokens/sec=1373073.10, grad_norm=0.2951, duration=0.38s
Step 10203: loss=2.9165, lr=0.000414, tokens/sec=1375654.28, grad_norm=0.2832, duration=0.38s
Step 10204: loss=2.9830, lr=0.000414, tokens/sec=1371103.17, grad_norm=0.2958, duration=0.38s
Step 10205: loss=2.9388, lr=0.000414, tokens/sec=1368129.49, grad_norm=0.3083, duration=0.38s
Step 10206: loss=2.8990, lr=0.000414, tokens/sec=1374881.91, grad_norm=0.2872, duration=0.38s
Step 10207: loss=2.9037, lr=0.000414, tokens/sec=1373898.37, grad_norm=0.2923, duration=0.38s
Step 10208: loss=2.9198, lr=0.000414, tokens/sec=1371894.40, grad_norm=0.3009, duration=0.38s
Step 10209: loss=2.9776, lr=0.000414, tokens/sec=1373310.63, grad_norm=0.3041, duration=0.38s
Step 10210: loss=2.9515, lr=0.000414, tokens/sec=1375358.30, grad_norm=0.2989, duration=0.38s
Step 10211: loss=2.9979, lr=0.000414, tokens/sec=1375021.19, grad_norm=0.3098, duration=0.38s
Step 10212: loss=2.9967, lr=0.000414, tokens/sec=1376300.87, grad_norm=0.3048, duration=0.38s
Step 10213: loss=3.0006, lr=0.000414, tokens/sec=1378010.26, grad_norm=0.3075, duration=0.38s
Step 10214: loss=3.0375, lr=0.000414, tokens/sec=1373931.84, grad_norm=0.3249, duration=0.38s
Step 10215: loss=2.9658, lr=0.000414, tokens/sec=1376613.63, grad_norm=0.3146, duration=0.38s
Step 10216: loss=2.9879, lr=0.000414, tokens/sec=1372713.97, grad_norm=0.3127, duration=0.38s
Step 10217: loss=3.0618, lr=0.000414, tokens/sec=1374221.19, grad_norm=0.3491, duration=0.38s
Step 10218: loss=3.0554, lr=0.000414, tokens/sec=1376736.87, grad_norm=0.3225, duration=0.38s
Step 10219: loss=3.0445, lr=0.000414, tokens/sec=1377061.03, grad_norm=0.3080, duration=0.38s
Step 10220: loss=3.0301, lr=0.000413, tokens/sec=1376369.79, grad_norm=0.3244, duration=0.38s
Step 10221: loss=2.9969, lr=0.000413, tokens/sec=1375711.08, grad_norm=0.3093, duration=0.38s
Step 10222: loss=2.9854, lr=0.000413, tokens/sec=1372192.31, grad_norm=0.3169, duration=0.38s
Step 10223: loss=2.9665, lr=0.000413, tokens/sec=1376344.80, grad_norm=0.3141, duration=0.38s
Step 10224: loss=2.8840, lr=0.000413, tokens/sec=1371813.10, grad_norm=0.3320, duration=0.38s
Step 10225: loss=3.0309, lr=0.000413, tokens/sec=1374502.07, grad_norm=0.3232, duration=0.38s
Step 10226: loss=2.9410, lr=0.000413, tokens/sec=1376490.40, grad_norm=0.3163, duration=0.38s
Step 10227: loss=3.0088, lr=0.000413, tokens/sec=1372277.94, grad_norm=0.3082, duration=0.38s
Step 10228: loss=3.0540, lr=0.000413, tokens/sec=1376356.00, grad_norm=0.3303, duration=0.38s
Step 10229: loss=2.9798, lr=0.000413, tokens/sec=1376004.62, grad_norm=0.2919, duration=0.38s
Step 10230: loss=2.9346, lr=0.000413, tokens/sec=1375128.67, grad_norm=0.3087, duration=0.38s
Step 10231: loss=2.9812, lr=0.000413, tokens/sec=1374023.70, grad_norm=0.3414, duration=0.38s
Step 10232: loss=3.0081, lr=0.000413, tokens/sec=1376593.81, grad_norm=0.2749, duration=0.38s
Step 10233: loss=3.0035, lr=0.000413, tokens/sec=1379039.48, grad_norm=0.3052, duration=0.38s
Step 10234: loss=2.8957, lr=0.000413, tokens/sec=1377145.55, grad_norm=0.2920, duration=0.38s
Step 10235: loss=2.9509, lr=0.000413, tokens/sec=1376352.56, grad_norm=0.2845, duration=0.38s
Step 10236: loss=2.9629, lr=0.000413, tokens/sec=1376474.03, grad_norm=0.2950, duration=0.38s
Step 10237: loss=2.9977, lr=0.000413, tokens/sec=1376555.89, grad_norm=0.2778, duration=0.38s
Step 10238: loss=3.0426, lr=0.000413, tokens/sec=1372682.26, grad_norm=0.2904, duration=0.38s
Step 10239: loss=2.9336, lr=0.000413, tokens/sec=1372043.34, grad_norm=0.2901, duration=0.38s
Step 10240: loss=2.9509, lr=0.000413, tokens/sec=1375317.87, grad_norm=0.2786, duration=0.38s
Step 10241: loss=2.9240, lr=0.000413, tokens/sec=1377994.71, grad_norm=0.2832, duration=0.38s
Step 10242: loss=2.9019, lr=0.000413, tokens/sec=1373658.92, grad_norm=0.2845, duration=0.38s
Step 10243: loss=2.9142, lr=0.000413, tokens/sec=1378276.27, grad_norm=0.2831, duration=0.38s
Step 10244: loss=2.9109, lr=0.000413, tokens/sec=1372607.72, grad_norm=0.2775, duration=0.38s
Step 10245: loss=2.8898, lr=0.000413, tokens/sec=1377090.35, grad_norm=0.2810, duration=0.38s
Step 10246: loss=2.8739, lr=0.000413, tokens/sec=1379221.12, grad_norm=0.2952, duration=0.38s
Step 10247: loss=2.9725, lr=0.000413, tokens/sec=1376418.03, grad_norm=0.2772, duration=0.38s
Step 10248: loss=2.9101, lr=0.000413, tokens/sec=1376409.42, grad_norm=0.2819, duration=0.38s
Step 10249: loss=2.8319, lr=0.000413, tokens/sec=1378390.31, grad_norm=0.2909, duration=0.38s
Validation loss at step 10250: 3.768247604370117
Step 10250: loss=2.8493, lr=0.000413, tokens/sec=155188.14, grad_norm=0.2818, duration=3.38s
Step 10251: loss=2.8795, lr=0.000413, tokens/sec=1379003.16, grad_norm=0.2796, duration=0.38s
Step 10252: loss=2.9445, lr=0.000412, tokens/sec=1377723.63, grad_norm=0.2811, duration=0.38s
Step 10253: loss=2.9201, lr=0.000412, tokens/sec=1375825.55, grad_norm=0.2908, duration=0.38s
Step 10254: loss=2.9466, lr=0.000412, tokens/sec=1374538.16, grad_norm=0.2985, duration=0.38s
Step 10255: loss=2.9684, lr=0.000412, tokens/sec=1378997.11, grad_norm=0.3024, duration=0.38s
Step 10256: loss=2.9795, lr=0.000412, tokens/sec=1379865.88, grad_norm=0.3045, duration=0.38s
Step 10257: loss=3.0192, lr=0.000412, tokens/sec=1376517.98, grad_norm=0.3565, duration=0.38s
Step 10258: loss=2.9750, lr=0.000412, tokens/sec=1376146.70, grad_norm=0.2952, duration=0.38s
Step 10259: loss=2.9910, lr=0.000412, tokens/sec=1376380.99, grad_norm=0.3664, duration=0.38s
Step 10260: loss=2.9680, lr=0.000412, tokens/sec=1374182.55, grad_norm=0.3598, duration=0.38s
Step 10261: loss=2.9948, lr=0.000412, tokens/sec=1369178.95, grad_norm=0.2917, duration=0.38s
Step 10262: loss=3.0063, lr=0.000412, tokens/sec=1379030.84, grad_norm=0.3547, duration=0.38s
Step 10263: loss=3.0813, lr=0.000412, tokens/sec=1374619.78, grad_norm=0.3204, duration=0.38s
Step 10264: loss=2.9921, lr=0.000412, tokens/sec=1374486.61, grad_norm=0.2982, duration=0.38s
Step 10265: loss=3.0245, lr=0.000412, tokens/sec=1375642.23, grad_norm=0.3460, duration=0.38s
Step 10266: loss=3.0673, lr=0.000412, tokens/sec=1369312.81, grad_norm=0.3044, duration=0.38s
Step 10267: loss=3.0002, lr=0.000412, tokens/sec=1373784.21, grad_norm=0.3255, duration=0.38s
Step 10268: loss=3.0467, lr=0.000412, tokens/sec=1377902.32, grad_norm=0.2949, duration=0.38s
Step 10269: loss=3.0013, lr=0.000412, tokens/sec=1376045.09, grad_norm=0.3109, duration=0.38s
Step 10270: loss=3.0241, lr=0.000412, tokens/sec=1375292.07, grad_norm=0.3214, duration=0.38s
Step 10271: loss=3.0092, lr=0.000412, tokens/sec=1377686.51, grad_norm=0.3131, duration=0.38s
Step 10272: loss=2.9809, lr=0.000412, tokens/sec=1375164.78, grad_norm=0.3329, duration=0.38s
Step 10273: loss=2.9516, lr=0.000412, tokens/sec=1374182.55, grad_norm=0.3095, duration=0.38s
Step 10274: loss=3.0071, lr=0.000412, tokens/sec=1376742.90, grad_norm=0.3025, duration=0.38s
Step 10275: loss=2.9774, lr=0.000412, tokens/sec=1373356.94, grad_norm=0.3669, duration=0.38s
Step 10276: loss=2.9511, lr=0.000412, tokens/sec=1373308.91, grad_norm=0.3196, duration=0.38s
Step 10277: loss=3.0074, lr=0.000412, tokens/sec=1376912.73, grad_norm=0.3079, duration=0.38s
Step 10278: loss=2.9422, lr=0.000412, tokens/sec=1377824.62, grad_norm=0.3314, duration=0.38s
Step 10279: loss=2.9029, lr=0.000412, tokens/sec=1373472.74, grad_norm=0.3360, duration=0.38s
Step 10280: loss=2.9411, lr=0.000412, tokens/sec=1375179.40, grad_norm=0.2998, duration=0.38s
Step 10281: loss=2.9694, lr=0.000412, tokens/sec=1377343.07, grad_norm=0.2987, duration=0.38s
Step 10282: loss=3.0521, lr=0.000412, tokens/sec=1376121.73, grad_norm=0.3308, duration=0.38s
Step 10283: loss=2.9236, lr=0.000412, tokens/sec=1376123.45, grad_norm=0.3087, duration=0.38s
Step 10284: loss=2.9955, lr=0.000411, tokens/sec=1372833.09, grad_norm=0.3031, duration=0.38s
Step 10285: loss=2.9844, lr=0.000411, tokens/sec=1375740.34, grad_norm=0.3230, duration=0.38s
Step 10286: loss=2.9237, lr=0.000411, tokens/sec=1378125.12, grad_norm=0.3017, duration=0.38s
Step 10287: loss=2.9676, lr=0.000411, tokens/sec=1378946.09, grad_norm=0.2923, duration=0.38s
Step 10288: loss=2.9939, lr=0.000411, tokens/sec=1375317.01, grad_norm=0.3280, duration=0.38s
Step 10289: loss=2.9384, lr=0.000411, tokens/sec=1373797.08, grad_norm=0.2949, duration=0.38s
Step 10290: loss=2.9233, lr=0.000411, tokens/sec=1374210.03, grad_norm=0.3244, duration=0.38s
Step 10291: loss=2.9503, lr=0.000411, tokens/sec=1376157.04, grad_norm=0.3014, duration=0.38s
Step 10292: loss=2.9538, lr=0.000411, tokens/sec=1375446.05, grad_norm=0.3244, duration=0.38s
Step 10293: loss=2.9980, lr=0.000411, tokens/sec=1374727.20, grad_norm=0.3330, duration=0.38s
Step 10294: loss=2.9420, lr=0.000411, tokens/sec=1379381.17, grad_norm=0.2954, duration=0.38s
Step 10295: loss=2.8214, lr=0.000411, tokens/sec=1377918.73, grad_norm=0.3124, duration=0.38s
Step 10296: loss=2.9311, lr=0.000411, tokens/sec=1377588.12, grad_norm=0.3480, duration=0.38s
Step 10297: loss=2.8737, lr=0.000411, tokens/sec=1377748.66, grad_norm=0.2857, duration=0.38s
Step 10298: loss=2.9087, lr=0.000411, tokens/sec=1375915.08, grad_norm=0.3510, duration=0.38s
Step 10299: loss=2.9183, lr=0.000411, tokens/sec=1377787.50, grad_norm=0.3355, duration=0.38s
Step 10300/19073 (54.0%), Elapsed time: 4086.76s, Steps per hour: 9073.20, Estimated hours remaining: 0.97
Step 10300: loss=2.9307, lr=0.000411, tokens/sec=1373731.00, grad_norm=0.3254, duration=0.38s
Step 10301: loss=2.9283, lr=0.000411, tokens/sec=1379159.70, grad_norm=0.3346, duration=0.38s
Step 10302: loss=2.9317, lr=0.000411, tokens/sec=1378734.27, grad_norm=0.3268, duration=0.38s
Step 10303: loss=2.9315, lr=0.000411, tokens/sec=1378601.16, grad_norm=0.3281, duration=0.38s
Step 10304: loss=2.8827, lr=0.000411, tokens/sec=1372462.08, grad_norm=0.3076, duration=0.38s
Step 10305: loss=2.9256, lr=0.000411, tokens/sec=1373676.08, grad_norm=0.3115, duration=0.38s
Step 10306: loss=2.9377, lr=0.000411, tokens/sec=1377130.89, grad_norm=0.3153, duration=0.38s
Step 10307: loss=3.0012, lr=0.000411, tokens/sec=1374932.63, grad_norm=0.3318, duration=0.38s
Step 10308: loss=3.0676, lr=0.000411, tokens/sec=1373496.76, grad_norm=0.3104, duration=0.38s
Step 10309: loss=2.9900, lr=0.000411, tokens/sec=1375289.49, grad_norm=0.3389, duration=0.38s
Step 10310: loss=3.0078, lr=0.000411, tokens/sec=1374685.09, grad_norm=0.3364, duration=0.38s
Step 10311: loss=3.0205, lr=0.000411, tokens/sec=1376660.16, grad_norm=0.3461, duration=0.38s
Step 10312: loss=3.0048, lr=0.000411, tokens/sec=1377358.60, grad_norm=0.3612, duration=0.38s
Step 10313: loss=3.0212, lr=0.000411, tokens/sec=1377406.05, grad_norm=0.3168, duration=0.38s
Step 10314: loss=2.9255, lr=0.000411, tokens/sec=1374593.15, grad_norm=0.3647, duration=0.38s
Step 10315: loss=2.9540, lr=0.000411, tokens/sec=1373701.83, grad_norm=0.3563, duration=0.38s
Step 10316: loss=2.9476, lr=0.000410, tokens/sec=1376637.76, grad_norm=0.3289, duration=0.38s
Step 10317: loss=2.9679, lr=0.000410, tokens/sec=1375627.60, grad_norm=0.3862, duration=0.38s
Step 10318: loss=2.9578, lr=0.000410, tokens/sec=1372965.94, grad_norm=0.3569, duration=0.38s
Step 10319: loss=2.9216, lr=0.000410, tokens/sec=1371561.55, grad_norm=0.3192, duration=0.38s
Step 10320: loss=2.9488, lr=0.000410, tokens/sec=1370930.51, grad_norm=0.3381, duration=0.38s
Step 10321: loss=2.8598, lr=0.000410, tokens/sec=1375423.68, grad_norm=0.3467, duration=0.38s
Step 10322: loss=2.9339, lr=0.000410, tokens/sec=1371025.38, grad_norm=0.3122, duration=0.38s
Step 10323: loss=2.9895, lr=0.000410, tokens/sec=1378168.30, grad_norm=0.3231, duration=0.38s
Step 10324: loss=2.9813, lr=0.000410, tokens/sec=1377172.28, grad_norm=0.3189, duration=0.38s
Step 10325: loss=3.0101, lr=0.000410, tokens/sec=1374592.29, grad_norm=0.3138, duration=0.38s
Step 10326: loss=2.9754, lr=0.000410, tokens/sec=1371278.45, grad_norm=0.3438, duration=0.38s
Step 10327: loss=3.0080, lr=0.000410, tokens/sec=1376334.47, grad_norm=0.3356, duration=0.38s
Step 10328: loss=2.9758, lr=0.000410, tokens/sec=1374211.75, grad_norm=0.3366, duration=0.38s
Step 10329: loss=2.9707, lr=0.000410, tokens/sec=1371842.20, grad_norm=0.3165, duration=0.38s
Step 10330: loss=2.9713, lr=0.000410, tokens/sec=1370750.19, grad_norm=0.3530, duration=0.38s
Step 10331: loss=2.9556, lr=0.000410, tokens/sec=1378410.18, grad_norm=0.3068, duration=0.38s
Step 10332: loss=2.9485, lr=0.000410, tokens/sec=1373261.74, grad_norm=0.3026, duration=0.38s
Step 10333: loss=2.9811, lr=0.000410, tokens/sec=1375653.42, grad_norm=0.3220, duration=0.38s
Step 10334: loss=2.9546, lr=0.000410, tokens/sec=1375716.24, grad_norm=0.3182, duration=0.38s
Step 10335: loss=2.9427, lr=0.000410, tokens/sec=1373460.73, grad_norm=0.3077, duration=0.38s
Step 10336: loss=2.9630, lr=0.000410, tokens/sec=1376950.66, grad_norm=0.3034, duration=0.38s
Step 10337: loss=2.9107, lr=0.000410, tokens/sec=1375588.02, grad_norm=0.3216, duration=0.38s
Step 10338: loss=3.0169, lr=0.000410, tokens/sec=1373402.40, grad_norm=0.3135, duration=0.38s
Step 10339: loss=2.9895, lr=0.000410, tokens/sec=1377003.26, grad_norm=0.3085, duration=0.38s
Step 10340: loss=2.9655, lr=0.000410, tokens/sec=1378937.44, grad_norm=0.3230, duration=0.38s
Step 10341: loss=2.8997, lr=0.000410, tokens/sec=1380843.25, grad_norm=0.3070, duration=0.38s
Step 10342: loss=2.9290, lr=0.000410, tokens/sec=1373877.77, grad_norm=0.3051, duration=0.38s
Step 10343: loss=2.9166, lr=0.000410, tokens/sec=1375282.61, grad_norm=0.2989, duration=0.38s
Step 10344: loss=2.9413, lr=0.000410, tokens/sec=1375648.25, grad_norm=0.2866, duration=0.38s
Step 10345: loss=2.8807, lr=0.000410, tokens/sec=1375706.77, grad_norm=0.3041, duration=0.38s
Step 10346: loss=2.9469, lr=0.000410, tokens/sec=1376762.73, grad_norm=0.3124, duration=0.38s
Step 10347: loss=2.9503, lr=0.000410, tokens/sec=1375647.39, grad_norm=0.2915, duration=0.38s
Step 10348: loss=2.8719, lr=0.000409, tokens/sec=1373057.67, grad_norm=0.3033, duration=0.38s
Step 10349: loss=2.9283, lr=0.000409, tokens/sec=1379523.09, grad_norm=0.3066, duration=0.38s
Step 10350: loss=2.8752, lr=0.000409, tokens/sec=1378071.57, grad_norm=0.3071, duration=0.38s
Step 10351: loss=2.9268, lr=0.000409, tokens/sec=1376273.31, grad_norm=0.2999, duration=0.38s
Step 10352: loss=2.9369, lr=0.000409, tokens/sec=1377327.55, grad_norm=0.2903, duration=0.38s
Step 10353: loss=2.9477, lr=0.000409, tokens/sec=1374064.91, grad_norm=0.3138, duration=0.38s
Step 10354: loss=2.9847, lr=0.000409, tokens/sec=1374776.19, grad_norm=0.3051, duration=0.38s
Step 10355: loss=2.9726, lr=0.000409, tokens/sec=1375484.76, grad_norm=0.3070, duration=0.38s
Step 10356: loss=2.9759, lr=0.000409, tokens/sec=1377773.69, grad_norm=0.3281, duration=0.38s
Step 10357: loss=3.0018, lr=0.000409, tokens/sec=1380017.42, grad_norm=0.3229, duration=0.38s
Step 10358: loss=2.9350, lr=0.000409, tokens/sec=1374898.25, grad_norm=0.3260, duration=0.38s
Step 10359: loss=3.0020, lr=0.000409, tokens/sec=1374268.43, grad_norm=0.3187, duration=0.38s
Step 10360: loss=2.9376, lr=0.000409, tokens/sec=1376713.60, grad_norm=0.3320, duration=0.38s
Step 10361: loss=2.9956, lr=0.000409, tokens/sec=1374587.99, grad_norm=0.3271, duration=0.38s
Step 10362: loss=3.0012, lr=0.000409, tokens/sec=1375608.67, grad_norm=0.3589, duration=0.38s
Step 10363: loss=2.9804, lr=0.000409, tokens/sec=1375557.04, grad_norm=0.3578, duration=0.38s
Step 10364: loss=3.0028, lr=0.000409, tokens/sec=1377595.03, grad_norm=0.3031, duration=0.38s
Step 10365: loss=2.9635, lr=0.000409, tokens/sec=1374450.53, grad_norm=0.3538, duration=0.38s
Step 10366: loss=2.9976, lr=0.000409, tokens/sec=1373919.83, grad_norm=0.3457, duration=0.38s
Step 10367: loss=2.9757, lr=0.000409, tokens/sec=1376897.21, grad_norm=0.2860, duration=0.38s
Step 10368: loss=2.9784, lr=0.000409, tokens/sec=1374670.48, grad_norm=0.3272, duration=0.38s
Step 10369: loss=2.9282, lr=0.000409, tokens/sec=1375522.62, grad_norm=0.3153, duration=0.38s
Step 10370: loss=2.9867, lr=0.000409, tokens/sec=1376231.97, grad_norm=0.2901, duration=0.38s
Step 10371: loss=2.9571, lr=0.000409, tokens/sec=1374976.48, grad_norm=0.3148, duration=0.38s
Step 10372: loss=2.9749, lr=0.000409, tokens/sec=1376879.97, grad_norm=0.2838, duration=0.38s
Step 10373: loss=2.9194, lr=0.000409, tokens/sec=1376183.73, grad_norm=0.2926, duration=0.38s
Step 10374: loss=2.9897, lr=0.000409, tokens/sec=1375766.16, grad_norm=0.2915, duration=0.38s
Step 10375: loss=2.9221, lr=0.000409, tokens/sec=1373986.78, grad_norm=0.2985, duration=0.38s
Step 10376: loss=2.9691, lr=0.000409, tokens/sec=1374708.30, grad_norm=0.2873, duration=0.38s
Step 10377: loss=2.9672, lr=0.000409, tokens/sec=1378151.03, grad_norm=0.2799, duration=0.38s
Step 10378: loss=2.9872, lr=0.000409, tokens/sec=1373000.23, grad_norm=0.2905, duration=0.38s
Step 10379: loss=2.9395, lr=0.000409, tokens/sec=1375304.97, grad_norm=0.2722, duration=0.38s
Step 10380: loss=2.9425, lr=0.000409, tokens/sec=1379401.07, grad_norm=0.2910, duration=0.38s
Step 10381: loss=2.9480, lr=0.000408, tokens/sec=1379811.33, grad_norm=0.2865, duration=0.38s
Step 10382: loss=2.9505, lr=0.000408, tokens/sec=1375421.96, grad_norm=0.2993, duration=0.38s
Step 10383: loss=2.9504, lr=0.000408, tokens/sec=1375236.16, grad_norm=0.3135, duration=0.38s
Step 10384: loss=2.9485, lr=0.000408, tokens/sec=1377764.20, grad_norm=0.2711, duration=0.38s
Step 10385: loss=2.9308, lr=0.000408, tokens/sec=1375623.30, grad_norm=0.3015, duration=0.38s
Step 10386: loss=2.9371, lr=0.000408, tokens/sec=1378431.79, grad_norm=0.3019, duration=0.38s
Step 10387: loss=2.9247, lr=0.000408, tokens/sec=1378076.75, grad_norm=0.2882, duration=0.38s
Step 10388: loss=2.9253, lr=0.000408, tokens/sec=1377140.37, grad_norm=0.2913, duration=0.38s
Step 10389: loss=2.8720, lr=0.000408, tokens/sec=1374420.46, grad_norm=0.2896, duration=0.38s
Step 10390: loss=2.8807, lr=0.000408, tokens/sec=1374125.02, grad_norm=0.2850, duration=0.38s
Step 10391: loss=2.8248, lr=0.000408, tokens/sec=1376959.28, grad_norm=0.3118, duration=0.38s
Step 10392: loss=2.9112, lr=0.000408, tokens/sec=1375778.21, grad_norm=0.2911, duration=0.38s
Step 10393: loss=2.9128, lr=0.000408, tokens/sec=1375335.08, grad_norm=0.2896, duration=0.38s
Step 10394: loss=2.9634, lr=0.000408, tokens/sec=1378697.10, grad_norm=0.2875, duration=0.38s
Step 10395: loss=2.9139, lr=0.000408, tokens/sec=1375958.13, grad_norm=0.2986, duration=0.38s
Step 10396: loss=2.9090, lr=0.000408, tokens/sec=1374457.40, grad_norm=0.3004, duration=0.38s
Step 10397: loss=2.8817, lr=0.000408, tokens/sec=1374888.79, grad_norm=0.2840, duration=0.38s
Step 10398: loss=2.9360, lr=0.000408, tokens/sec=1371722.39, grad_norm=0.2999, duration=0.38s
Step 10399: loss=2.9289, lr=0.000408, tokens/sec=1377820.31, grad_norm=0.2858, duration=0.38s
Step 10400/19073 (54.5%), Elapsed time: 4124.96s, Steps per hour: 9076.46, Estimated hours remaining: 0.96
Step 10400: loss=2.9720, lr=0.000408, tokens/sec=1372897.37, grad_norm=0.3029, duration=0.38s
Step 10401: loss=2.9863, lr=0.000408, tokens/sec=1375448.63, grad_norm=0.3089, duration=0.38s
Step 10402: loss=3.0189, lr=0.000408, tokens/sec=1375228.42, grad_norm=0.3253, duration=0.38s
Step 10403: loss=3.0028, lr=0.000408, tokens/sec=1374258.12, grad_norm=0.3170, duration=0.38s
Step 10404: loss=3.0092, lr=0.000408, tokens/sec=1374843.23, grad_norm=0.3364, duration=0.38s
Step 10405: loss=2.9581, lr=0.000408, tokens/sec=1374365.48, grad_norm=0.3385, duration=0.38s
Step 10406: loss=3.0094, lr=0.000408, tokens/sec=1374062.34, grad_norm=0.3180, duration=0.38s
Step 10407: loss=2.9906, lr=0.000408, tokens/sec=1373430.71, grad_norm=0.3504, duration=0.38s
Step 10408: loss=3.0975, lr=0.000408, tokens/sec=1375445.19, grad_norm=0.3419, duration=0.38s
Step 10409: loss=3.0369, lr=0.000408, tokens/sec=1375082.23, grad_norm=0.3132, duration=0.38s
Step 10410: loss=3.0475, lr=0.000408, tokens/sec=1373094.53, grad_norm=0.3370, duration=0.38s
Step 10411: loss=2.9514, lr=0.000408, tokens/sec=1375649.11, grad_norm=0.3260, duration=0.38s
Step 10412: loss=2.9965, lr=0.000408, tokens/sec=1373077.39, grad_norm=0.3158, duration=0.38s
Step 10413: loss=2.9106, lr=0.000407, tokens/sec=1372308.77, grad_norm=0.3580, duration=0.38s
Step 10414: loss=2.9302, lr=0.000407, tokens/sec=1372685.69, grad_norm=0.3181, duration=0.38s
Step 10415: loss=3.0242, lr=0.000407, tokens/sec=1374501.21, grad_norm=0.3110, duration=0.38s
Step 10416: loss=2.9162, lr=0.000407, tokens/sec=1374531.28, grad_norm=0.3168, duration=0.38s
Step 10417: loss=3.0225, lr=0.000407, tokens/sec=1375818.67, grad_norm=0.3140, duration=0.38s
Step 10418: loss=3.0270, lr=0.000407, tokens/sec=1371720.68, grad_norm=0.3346, duration=0.38s
Step 10419: loss=2.9780, lr=0.000407, tokens/sec=1378931.39, grad_norm=0.3113, duration=0.38s
Step 10420: loss=2.9201, lr=0.000407, tokens/sec=1377424.17, grad_norm=0.3071, duration=0.38s
Step 10421: loss=2.9852, lr=0.000407, tokens/sec=1374283.03, grad_norm=0.3372, duration=0.38s
Step 10422: loss=3.0326, lr=0.000407, tokens/sec=1373736.15, grad_norm=0.3021, duration=0.38s
Step 10423: loss=2.9601, lr=0.000407, tokens/sec=1376992.05, grad_norm=0.3049, duration=0.38s
Step 10424: loss=2.8873, lr=0.000407, tokens/sec=1373674.37, grad_norm=0.3222, duration=0.38s
Step 10425: loss=2.9566, lr=0.000407, tokens/sec=1376878.24, grad_norm=0.3206, duration=0.38s
Step 10426: loss=2.9751, lr=0.000407, tokens/sec=1374355.18, grad_norm=0.3196, duration=0.38s
Step 10427: loss=3.0075, lr=0.000407, tokens/sec=1379612.23, grad_norm=0.3091, duration=0.38s
Step 10428: loss=3.0048, lr=0.000407, tokens/sec=1376311.21, grad_norm=0.3016, duration=0.38s
Step 10429: loss=2.9369, lr=0.000407, tokens/sec=1376166.51, grad_norm=0.3208, duration=0.38s
Step 10430: loss=2.9166, lr=0.000407, tokens/sec=1379695.33, grad_norm=0.2933, duration=0.38s
Step 10431: loss=2.9417, lr=0.000407, tokens/sec=1374337.14, grad_norm=0.3011, duration=0.38s
Step 10432: loss=2.8720, lr=0.000407, tokens/sec=1377419.00, grad_norm=0.2921, duration=0.38s
Step 10433: loss=2.9389, lr=0.000407, tokens/sec=1375873.76, grad_norm=0.2830, duration=0.38s
Step 10434: loss=2.8993, lr=0.000407, tokens/sec=1373117.68, grad_norm=0.2995, duration=0.38s
Step 10435: loss=2.8634, lr=0.000407, tokens/sec=1377215.41, grad_norm=0.2752, duration=0.38s
Step 10436: loss=2.9063, lr=0.000407, tokens/sec=1375069.33, grad_norm=0.2849, duration=0.38s
Step 10437: loss=2.9526, lr=0.000407, tokens/sec=1371716.40, grad_norm=0.2876, duration=0.38s
Step 10438: loss=2.8406, lr=0.000407, tokens/sec=1377235.25, grad_norm=0.2733, duration=0.38s
Step 10439: loss=2.8709, lr=0.000407, tokens/sec=1377990.40, grad_norm=0.2747, duration=0.38s
Step 10440: loss=2.8437, lr=0.000407, tokens/sec=1378081.07, grad_norm=0.2755, duration=0.38s
Step 10441: loss=2.9061, lr=0.000407, tokens/sec=1372564.88, grad_norm=0.2800, duration=0.38s
Step 10442: loss=2.9300, lr=0.000407, tokens/sec=1374047.74, grad_norm=0.2782, duration=0.38s
Step 10443: loss=2.9163, lr=0.000407, tokens/sec=1379287.73, grad_norm=0.2774, duration=0.38s
Step 10444: loss=2.9159, lr=0.000407, tokens/sec=1375513.16, grad_norm=0.3016, duration=0.38s
Step 10445: loss=2.9721, lr=0.000406, tokens/sec=1376410.28, grad_norm=0.2798, duration=0.38s
Step 10446: loss=3.0064, lr=0.000406, tokens/sec=1377126.57, grad_norm=0.3083, duration=0.38s
Step 10447: loss=3.0016, lr=0.000406, tokens/sec=1376428.37, grad_norm=0.3324, duration=0.38s
Step 10448: loss=2.9521, lr=0.000406, tokens/sec=1375637.07, grad_norm=0.2990, duration=0.38s
Step 10449: loss=2.9849, lr=0.000406, tokens/sec=1375976.21, grad_norm=0.3232, duration=0.38s
Step 10450: loss=2.9533, lr=0.000406, tokens/sec=1376631.72, grad_norm=0.3563, duration=0.38s
Step 10451: loss=2.9935, lr=0.000406, tokens/sec=1376008.06, grad_norm=0.3072, duration=0.38s
Step 10452: loss=3.0069, lr=0.000406, tokens/sec=1373025.09, grad_norm=0.3271, duration=0.38s
Step 10453: loss=3.0503, lr=0.000406, tokens/sec=1379497.12, grad_norm=0.3363, duration=0.38s
Step 10454: loss=2.9944, lr=0.000406, tokens/sec=1379036.02, grad_norm=0.3148, duration=0.38s
Step 10455: loss=3.0166, lr=0.000406, tokens/sec=1372932.51, grad_norm=0.3122, duration=0.38s
Step 10456: loss=3.0620, lr=0.000406, tokens/sec=1377629.55, grad_norm=0.3457, duration=0.38s
Step 10457: loss=3.0265, lr=0.000406, tokens/sec=1377336.17, grad_norm=0.3477, duration=0.38s
Step 10458: loss=3.0146, lr=0.000406, tokens/sec=1372146.93, grad_norm=0.3080, duration=0.38s
Step 10459: loss=3.0042, lr=0.000406, tokens/sec=1376156.18, grad_norm=0.3317, duration=0.38s
Step 10460: loss=3.0358, lr=0.000406, tokens/sec=1378467.21, grad_norm=0.3442, duration=0.38s
Step 10461: loss=2.9591, lr=0.000406, tokens/sec=1376324.99, grad_norm=0.2986, duration=0.38s
Step 10462: loss=3.0091, lr=0.000406, tokens/sec=1377859.16, grad_norm=0.3468, duration=0.38s
Step 10463: loss=2.9329, lr=0.000406, tokens/sec=1376007.20, grad_norm=0.3304, duration=0.38s
Step 10464: loss=2.9864, lr=0.000406, tokens/sec=1375485.62, grad_norm=0.2990, duration=0.38s
Step 10465: loss=2.9861, lr=0.000406, tokens/sec=1378972.03, grad_norm=0.3614, duration=0.38s
Step 10466: loss=2.9349, lr=0.000406, tokens/sec=1378742.05, grad_norm=0.3676, duration=0.38s
Step 10467: loss=3.0166, lr=0.000406, tokens/sec=1374822.60, grad_norm=0.3230, duration=0.38s
Step 10468: loss=2.9336, lr=0.000406, tokens/sec=1380275.55, grad_norm=0.3376, duration=0.38s
Step 10469: loss=2.8900, lr=0.000406, tokens/sec=1378660.80, grad_norm=0.4070, duration=0.38s
Step 10470: loss=2.9274, lr=0.000406, tokens/sec=1374477.16, grad_norm=0.3484, duration=0.38s
Step 10471: loss=2.9702, lr=0.000406, tokens/sec=1375722.27, grad_norm=0.2962, duration=0.38s
Step 10472: loss=3.0723, lr=0.000406, tokens/sec=1375591.46, grad_norm=0.3631, duration=0.38s
Step 10473: loss=2.8840, lr=0.000406, tokens/sec=1375631.90, grad_norm=0.3389, duration=0.38s
Step 10474: loss=2.9938, lr=0.000406, tokens/sec=1376673.95, grad_norm=0.2939, duration=0.38s
Step 10475: loss=2.9728, lr=0.000406, tokens/sec=1374596.58, grad_norm=0.3186, duration=0.38s
Step 10476: loss=2.9320, lr=0.000406, tokens/sec=1374574.24, grad_norm=0.3430, duration=0.38s
Step 10477: loss=2.9653, lr=0.000405, tokens/sec=1374704.00, grad_norm=0.2974, duration=0.38s
Step 10478: loss=2.9585, lr=0.000405, tokens/sec=1378695.37, grad_norm=0.3397, duration=0.38s
Step 10479: loss=2.9500, lr=0.000405, tokens/sec=1378054.30, grad_norm=0.3052, duration=0.38s
Step 10480: loss=2.9152, lr=0.000405, tokens/sec=1374896.53, grad_norm=0.3200, duration=0.38s
Step 10481: loss=2.9404, lr=0.000405, tokens/sec=1376431.82, grad_norm=0.3336, duration=0.38s
Step 10482: loss=2.9636, lr=0.000405, tokens/sec=1378924.47, grad_norm=0.3182, duration=0.38s
Step 10483: loss=2.9968, lr=0.000405, tokens/sec=1376345.67, grad_norm=0.3281, duration=0.38s
Step 10484: loss=2.8714, lr=0.000405, tokens/sec=1378275.41, grad_norm=0.3299, duration=0.38s
Step 10485: loss=2.8769, lr=0.000405, tokens/sec=1377520.81, grad_norm=0.3029, duration=0.38s
Step 10486: loss=2.9222, lr=0.000405, tokens/sec=1377162.80, grad_norm=0.3189, duration=0.38s
Step 10487: loss=2.9078, lr=0.000405, tokens/sec=1374360.33, grad_norm=0.2977, duration=0.38s
Step 10488: loss=2.8989, lr=0.000405, tokens/sec=1372278.80, grad_norm=0.3324, duration=0.38s
Step 10489: loss=2.9119, lr=0.000405, tokens/sec=1372095.56, grad_norm=0.3208, duration=0.38s
Step 10490: loss=2.9162, lr=0.000405, tokens/sec=1376146.70, grad_norm=0.3175, duration=0.38s
Step 10491: loss=2.9244, lr=0.000405, tokens/sec=1377185.22, grad_norm=0.3244, duration=0.38s
Step 10492: loss=2.9085, lr=0.000405, tokens/sec=1374246.96, grad_norm=0.3299, duration=0.38s
Step 10493: loss=2.9031, lr=0.000405, tokens/sec=1377781.46, grad_norm=0.3231, duration=0.38s
Step 10494: loss=2.9231, lr=0.000405, tokens/sec=1375135.55, grad_norm=0.3142, duration=0.38s
Step 10495: loss=2.9211, lr=0.000405, tokens/sec=1378736.86, grad_norm=0.3159, duration=0.38s
Step 10496: loss=2.9236, lr=0.000405, tokens/sec=1376873.93, grad_norm=0.2936, duration=0.38s
Step 10497: loss=3.0220, lr=0.000405, tokens/sec=1376567.95, grad_norm=0.3291, duration=0.38s
Step 10498: loss=3.0298, lr=0.000405, tokens/sec=1376327.58, grad_norm=0.3094, duration=0.38s
Step 10499: loss=3.0035, lr=0.000405, tokens/sec=1376962.73, grad_norm=0.3235, duration=0.38s
Step 10500/19073 (55.1%), Elapsed time: 4163.15s, Steps per hour: 9079.66, Estimated hours remaining: 0.94
Validation loss at step 10500: 3.7671127319335938
Step 10500: loss=2.9894, lr=0.000405, tokens/sec=156331.30, grad_norm=0.3139, duration=3.35s
Step 10501: loss=3.0049, lr=0.000405, tokens/sec=1378716.12, grad_norm=0.3179, duration=0.38s
Step 10502: loss=3.0167, lr=0.000405, tokens/sec=1376788.59, grad_norm=0.3478, duration=0.38s
Step 10503: loss=2.9603, lr=0.000405, tokens/sec=1373053.38, grad_norm=0.3152, duration=0.38s
Step 10504: loss=2.9528, lr=0.000405, tokens/sec=1378411.91, grad_norm=0.3330, duration=0.38s
Step 10505: loss=2.9168, lr=0.000405, tokens/sec=1375472.72, grad_norm=0.3687, duration=0.38s
Step 10506: loss=2.9746, lr=0.000405, tokens/sec=1375589.74, grad_norm=0.3254, duration=0.38s
Step 10507: loss=2.9581, lr=0.000405, tokens/sec=1374090.67, grad_norm=0.3597, duration=0.38s
Step 10508: loss=2.9203, lr=0.000405, tokens/sec=1372043.34, grad_norm=0.3458, duration=0.38s
Step 10509: loss=2.9425, lr=0.000404, tokens/sec=1377993.85, grad_norm=0.3185, duration=0.38s
Step 10510: loss=2.9326, lr=0.000404, tokens/sec=1376081.25, grad_norm=0.3481, duration=0.38s
Step 10511: loss=2.8708, lr=0.000404, tokens/sec=1375851.38, grad_norm=0.3453, duration=0.38s
Step 10512: loss=2.9420, lr=0.000404, tokens/sec=1375506.27, grad_norm=0.3224, duration=0.38s
Step 10513: loss=2.9834, lr=0.000404, tokens/sec=1377972.26, grad_norm=0.3204, duration=0.38s
Step 10514: loss=2.9607, lr=0.000404, tokens/sec=1376567.95, grad_norm=0.3213, duration=0.38s
Step 10515: loss=3.0294, lr=0.000404, tokens/sec=1374288.18, grad_norm=0.3215, duration=0.38s
Step 10516: loss=2.9262, lr=0.000404, tokens/sec=1369892.86, grad_norm=0.3171, duration=0.38s
Step 10517: loss=3.0072, lr=0.000404, tokens/sec=1376557.61, grad_norm=0.3312, duration=0.38s
Step 10518: loss=2.9809, lr=0.000404, tokens/sec=1374198.86, grad_norm=0.3371, duration=0.38s
Step 10519: loss=2.9604, lr=0.000404, tokens/sec=1377141.24, grad_norm=0.3136, duration=0.38s
Step 10520: loss=2.9463, lr=0.000404, tokens/sec=1376980.84, grad_norm=0.3193, duration=0.38s
Step 10521: loss=2.9689, lr=0.000404, tokens/sec=1372188.89, grad_norm=0.3285, duration=0.38s
Step 10522: loss=2.9540, lr=0.000404, tokens/sec=1374925.76, grad_norm=0.2933, duration=0.38s
Step 10523: loss=2.9461, lr=0.000404, tokens/sec=1377258.54, grad_norm=0.3154, duration=0.38s
Step 10524: loss=2.9409, lr=0.000404, tokens/sec=1373161.41, grad_norm=0.3161, duration=0.38s
Step 10525: loss=2.9530, lr=0.000404, tokens/sec=1374955.84, grad_norm=0.3038, duration=0.38s
Step 10526: loss=2.9483, lr=0.000404, tokens/sec=1374198.86, grad_norm=0.3155, duration=0.38s
Step 10527: loss=2.9719, lr=0.000404, tokens/sec=1374874.18, grad_norm=0.3138, duration=0.38s
Step 10528: loss=2.9968, lr=0.000404, tokens/sec=1371742.93, grad_norm=0.3256, duration=0.38s
Step 10529: loss=2.9454, lr=0.000404, tokens/sec=1375114.05, grad_norm=0.3041, duration=0.38s
Step 10530: loss=2.9671, lr=0.000404, tokens/sec=1375797.15, grad_norm=0.3112, duration=0.38s
Step 10531: loss=2.8609, lr=0.000404, tokens/sec=1378642.65, grad_norm=0.3118, duration=0.38s
Step 10532: loss=2.9654, lr=0.000404, tokens/sec=1376430.09, grad_norm=0.2880, duration=0.38s
Step 10533: loss=2.9257, lr=0.000404, tokens/sec=1376557.61, grad_norm=0.3171, duration=0.38s
Step 10534: loss=2.9228, lr=0.000404, tokens/sec=1375314.43, grad_norm=0.2851, duration=0.38s
Step 10535: loss=2.9077, lr=0.000404, tokens/sec=1375349.70, grad_norm=0.2856, duration=0.38s
Step 10536: loss=2.9264, lr=0.000404, tokens/sec=1373635.75, grad_norm=0.3048, duration=0.38s
Step 10537: loss=2.9127, lr=0.000404, tokens/sec=1376998.95, grad_norm=0.2829, duration=0.38s
Step 10538: loss=2.9017, lr=0.000404, tokens/sec=1374220.33, grad_norm=0.2796, duration=0.38s
Step 10539: loss=2.8904, lr=0.000404, tokens/sec=1378203.71, grad_norm=0.2944, duration=0.38s
Step 10540: loss=2.8660, lr=0.000404, tokens/sec=1372604.29, grad_norm=0.2915, duration=0.38s
Step 10541: loss=2.9474, lr=0.000403, tokens/sec=1375232.72, grad_norm=0.2965, duration=0.38s
Step 10542: loss=2.9168, lr=0.000403, tokens/sec=1373301.19, grad_norm=0.2843, duration=0.38s
Step 10543: loss=2.9368, lr=0.000403, tokens/sec=1378537.21, grad_norm=0.3075, duration=0.38s
Step 10544: loss=2.9621, lr=0.000403, tokens/sec=1376147.56, grad_norm=0.3008, duration=0.38s
Step 10545: loss=2.9640, lr=0.000403, tokens/sec=1376804.97, grad_norm=0.3050, duration=0.38s
Step 10546: loss=2.9931, lr=0.000403, tokens/sec=1374443.65, grad_norm=0.3163, duration=0.38s
Step 10547: loss=2.9906, lr=0.000403, tokens/sec=1368470.90, grad_norm=0.3012, duration=0.38s
Step 10548: loss=2.9485, lr=0.000403, tokens/sec=1375665.46, grad_norm=0.3112, duration=0.38s
Step 10549: loss=2.9672, lr=0.000403, tokens/sec=1378634.87, grad_norm=0.3144, duration=0.38s
Step 10550: loss=2.9277, lr=0.000403, tokens/sec=1374027.14, grad_norm=0.3306, duration=0.38s
Step 10551: loss=3.0022, lr=0.000403, tokens/sec=1377412.09, grad_norm=0.3221, duration=0.38s
Step 10552: loss=2.9833, lr=0.000403, tokens/sec=1376871.35, grad_norm=0.3195, duration=0.38s
Step 10553: loss=2.9934, lr=0.000403, tokens/sec=1376156.18, grad_norm=0.3526, duration=0.38s
Step 10554: loss=2.9896, lr=0.000403, tokens/sec=1373748.17, grad_norm=0.3162, duration=0.38s
Step 10555: loss=2.9817, lr=0.000403, tokens/sec=1378624.50, grad_norm=0.3217, duration=0.38s
Step 10556: loss=2.9690, lr=0.000403, tokens/sec=1377746.93, grad_norm=0.3548, duration=0.38s
Step 10557: loss=2.9645, lr=0.000403, tokens/sec=1375588.88, grad_norm=0.2972, duration=0.38s
Step 10558: loss=2.9735, lr=0.000403, tokens/sec=1376299.15, grad_norm=0.3238, duration=0.38s
Step 10559: loss=2.9259, lr=0.000403, tokens/sec=1376611.90, grad_norm=0.3286, duration=0.38s
Step 10560: loss=2.9595, lr=0.000403, tokens/sec=1373754.17, grad_norm=0.2920, duration=0.38s
Step 10561: loss=2.9568, lr=0.000403, tokens/sec=1375290.35, grad_norm=0.3205, duration=0.38s
Step 10562: loss=2.9730, lr=0.000403, tokens/sec=1376204.40, grad_norm=0.2913, duration=0.38s
Step 10563: loss=2.9121, lr=0.000403, tokens/sec=1374094.96, grad_norm=0.2834, duration=0.38s
Step 10564: loss=2.9723, lr=0.000403, tokens/sec=1376565.37, grad_norm=0.3088, duration=0.38s
Step 10565: loss=2.9072, lr=0.000403, tokens/sec=1375021.19, grad_norm=0.2914, duration=0.38s
Step 10566: loss=2.9928, lr=0.000403, tokens/sec=1377975.72, grad_norm=0.3054, duration=0.38s
Step 10567: loss=2.9607, lr=0.000403, tokens/sec=1374215.18, grad_norm=0.2792, duration=0.38s
Step 10568: loss=2.9552, lr=0.000403, tokens/sec=1374814.01, grad_norm=0.2985, duration=0.38s
Step 10569: loss=2.9259, lr=0.000403, tokens/sec=1372627.42, grad_norm=0.2728, duration=0.38s
Step 10570: loss=2.9477, lr=0.000403, tokens/sec=1374494.34, grad_norm=0.3042, duration=0.38s
Step 10571: loss=2.9371, lr=0.000403, tokens/sec=1376911.00, grad_norm=0.2957, duration=0.38s
Step 10572: loss=2.9610, lr=0.000403, tokens/sec=1376390.46, grad_norm=0.2833, duration=0.38s
Step 10573: loss=2.9247, lr=0.000402, tokens/sec=1375474.44, grad_norm=0.3359, duration=0.38s
Step 10574: loss=2.9251, lr=0.000402, tokens/sec=1381045.31, grad_norm=0.2882, duration=0.38s
Step 10575: loss=2.9419, lr=0.000402, tokens/sec=1370642.54, grad_norm=0.3024, duration=0.38s
Step 10576: loss=2.9308, lr=0.000402, tokens/sec=1373450.44, grad_norm=0.3294, duration=0.38s
Step 10577: loss=2.9262, lr=0.000402, tokens/sec=1375795.43, grad_norm=0.2885, duration=0.38s
Step 10578: loss=2.9220, lr=0.000402, tokens/sec=1373824.55, grad_norm=0.3021, duration=0.38s
Step 10579: loss=2.8387, lr=0.000402, tokens/sec=1373833.13, grad_norm=0.3278, duration=0.38s
Step 10580: loss=2.8932, lr=0.000402, tokens/sec=1373008.80, grad_norm=0.2876, duration=0.38s
Step 10581: loss=2.8604, lr=0.000402, tokens/sec=1374050.32, grad_norm=0.3097, duration=0.38s
Step 10582: loss=2.9094, lr=0.000402, tokens/sec=1375628.46, grad_norm=0.3061, duration=0.38s
Step 10583: loss=2.8959, lr=0.000402, tokens/sec=1375563.92, grad_norm=0.2910, duration=0.38s
Step 10584: loss=2.9419, lr=0.000402, tokens/sec=1375900.45, grad_norm=0.3012, duration=0.38s
Step 10585: loss=2.9256, lr=0.000402, tokens/sec=1374064.91, grad_norm=0.3031, duration=0.38s
Step 10586: loss=2.8872, lr=0.000402, tokens/sec=1374000.52, grad_norm=0.2910, duration=0.38s
Step 10587: loss=2.8993, lr=0.000402, tokens/sec=1375013.45, grad_norm=0.3015, duration=0.38s
Step 10588: loss=2.8874, lr=0.000402, tokens/sec=1378804.29, grad_norm=0.2966, duration=0.38s
Step 10589: loss=2.9518, lr=0.000402, tokens/sec=1373194.00, grad_norm=0.2900, duration=0.38s
Step 10590: loss=2.9615, lr=0.000402, tokens/sec=1375533.81, grad_norm=0.3053, duration=0.38s
Step 10591: loss=3.0067, lr=0.000402, tokens/sec=1374301.92, grad_norm=0.2962, duration=0.38s
Step 10592: loss=3.0220, lr=0.000402, tokens/sec=1374934.35, grad_norm=0.3148, duration=0.38s
Step 10593: loss=2.9753, lr=0.000402, tokens/sec=1374624.94, grad_norm=0.3088, duration=0.38s
Step 10594: loss=2.9991, lr=0.000402, tokens/sec=1374553.62, grad_norm=0.3165, duration=0.38s
Step 10595: loss=2.9782, lr=0.000402, tokens/sec=1373819.40, grad_norm=0.3306, duration=0.38s
Step 10596: loss=2.9374, lr=0.000402, tokens/sec=1375335.94, grad_norm=0.3110, duration=0.38s
Step 10597: loss=3.0314, lr=0.000402, tokens/sec=1377111.91, grad_norm=0.3518, duration=0.38s
Step 10598: loss=3.0907, lr=0.000402, tokens/sec=1375051.28, grad_norm=0.3532, duration=0.38s
Step 10599: loss=3.0518, lr=0.000402, tokens/sec=1369326.45, grad_norm=0.3162, duration=0.38s
Step 10600/19073 (55.6%), Elapsed time: 4204.34s, Steps per hour: 9076.34, Estimated hours remaining: 0.93
Step 10600: loss=2.9965, lr=0.000402, tokens/sec=1375513.16, grad_norm=0.3306, duration=0.38s
Step 10601: loss=2.9592, lr=0.000402, tokens/sec=1376193.21, grad_norm=0.3269, duration=0.38s
Step 10602: loss=2.9430, lr=0.000402, tokens/sec=1373758.46, grad_norm=0.3589, duration=0.38s
Step 10603: loss=2.9570, lr=0.000402, tokens/sec=1379304.17, grad_norm=0.3305, duration=0.38s
Step 10604: loss=2.9262, lr=0.000402, tokens/sec=1375591.46, grad_norm=0.3469, duration=0.38s
Step 10605: loss=2.9999, lr=0.000402, tokens/sec=1375818.67, grad_norm=0.3156, duration=0.38s
Step 10606: loss=2.9305, lr=0.000401, tokens/sec=1375536.39, grad_norm=0.3342, duration=0.38s
Step 10607: loss=2.9950, lr=0.000401, tokens/sec=1373070.53, grad_norm=0.3074, duration=0.38s
Step 10608: loss=3.0244, lr=0.000401, tokens/sec=1372552.03, grad_norm=0.3274, duration=0.38s
Step 10609: loss=2.9631, lr=0.000401, tokens/sec=1372660.84, grad_norm=0.3136, duration=0.38s
Step 10610: loss=2.9245, lr=0.000401, tokens/sec=1377431.94, grad_norm=0.3073, duration=0.38s
Step 10611: loss=3.0053, lr=0.000401, tokens/sec=1377351.70, grad_norm=0.3306, duration=0.38s
Step 10612: loss=2.9914, lr=0.000401, tokens/sec=1375005.71, grad_norm=0.3255, duration=0.38s
Step 10613: loss=2.9520, lr=0.000401, tokens/sec=1376247.47, grad_norm=0.3117, duration=0.38s
Step 10614: loss=2.8929, lr=0.000401, tokens/sec=1376964.46, grad_norm=0.3159, duration=0.38s
Step 10615: loss=2.9640, lr=0.000401, tokens/sec=1375916.80, grad_norm=0.3384, duration=0.38s
Step 10616: loss=2.9832, lr=0.000401, tokens/sec=1375581.99, grad_norm=0.3272, duration=0.38s
Step 10617: loss=2.9716, lr=0.000401, tokens/sec=1381927.95, grad_norm=0.3048, duration=0.38s
Step 10618: loss=3.0074, lr=0.000401, tokens/sec=1373955.88, grad_norm=0.3161, duration=0.38s
Step 10619: loss=2.9008, lr=0.000401, tokens/sec=1376369.79, grad_norm=0.3313, duration=0.38s
Step 10620: loss=2.9347, lr=0.000401, tokens/sec=1376520.56, grad_norm=0.2852, duration=0.38s
Step 10621: loss=2.9074, lr=0.000401, tokens/sec=1378407.59, grad_norm=0.3080, duration=0.38s
Step 10622: loss=2.8960, lr=0.000401, tokens/sec=1376821.35, grad_norm=0.3193, duration=0.38s
Step 10623: loss=2.9269, lr=0.000401, tokens/sec=1374514.96, grad_norm=0.2776, duration=0.38s
Step 10624: loss=2.8711, lr=0.000401, tokens/sec=1373088.53, grad_norm=0.2883, duration=0.38s
Step 10625: loss=2.8994, lr=0.000401, tokens/sec=1371833.64, grad_norm=0.2926, duration=0.38s
Step 10626: loss=2.8875, lr=0.000401, tokens/sec=1374316.52, grad_norm=0.2762, duration=0.38s
Step 10627: loss=2.8845, lr=0.000401, tokens/sec=1373797.94, grad_norm=0.2933, duration=0.38s
Step 10628: loss=2.8869, lr=0.000401, tokens/sec=1376276.75, grad_norm=0.2895, duration=0.38s
Step 10629: loss=2.8655, lr=0.000401, tokens/sec=1373841.72, grad_norm=0.2743, duration=0.38s
Step 10630: loss=2.8693, lr=0.000401, tokens/sec=1377053.27, grad_norm=0.2738, duration=0.38s
Step 10631: loss=2.8898, lr=0.000401, tokens/sec=1375865.15, grad_norm=0.2883, duration=0.38s
Step 10632: loss=2.9290, lr=0.000401, tokens/sec=1373818.54, grad_norm=0.2849, duration=0.38s
Step 10633: loss=2.8879, lr=0.000401, tokens/sec=1373254.88, grad_norm=0.2688, duration=0.38s
Step 10634: loss=2.9219, lr=0.000401, tokens/sec=1376698.09, grad_norm=0.3001, duration=0.38s
Step 10635: loss=3.0029, lr=0.000401, tokens/sec=1376094.17, grad_norm=0.3155, duration=0.38s
Step 10636: loss=2.9883, lr=0.000401, tokens/sec=1377327.55, grad_norm=0.2748, duration=0.38s
Step 10637: loss=2.9798, lr=0.000401, tokens/sec=1376378.40, grad_norm=0.3036, duration=0.38s
Step 10638: loss=2.9492, lr=0.000400, tokens/sec=1372466.37, grad_norm=0.3145, duration=0.38s
Step 10639: loss=2.9688, lr=0.000400, tokens/sec=1376931.69, grad_norm=0.2888, duration=0.38s
Step 10640: loss=2.9540, lr=0.000400, tokens/sec=1373651.20, grad_norm=0.3089, duration=0.38s
Step 10641: loss=2.9960, lr=0.000400, tokens/sec=1376694.64, grad_norm=0.3320, duration=0.38s
Step 10642: loss=2.9748, lr=0.000400, tokens/sec=1377391.39, grad_norm=0.3102, duration=0.38s
Step 10643: loss=3.0537, lr=0.000400, tokens/sec=1379437.41, grad_norm=0.3176, duration=0.38s
Step 10644: loss=2.9878, lr=0.000400, tokens/sec=1375319.59, grad_norm=0.3212, duration=0.38s
Step 10645: loss=3.0083, lr=0.000400, tokens/sec=1375907.33, grad_norm=0.3342, duration=0.38s
Step 10646: loss=3.0810, lr=0.000400, tokens/sec=1375763.58, grad_norm=0.3184, duration=0.38s
Step 10647: loss=2.9939, lr=0.000400, tokens/sec=1378295.28, grad_norm=0.3633, duration=0.38s
Step 10648: loss=3.0193, lr=0.000400, tokens/sec=1374940.37, grad_norm=0.3494, duration=0.38s
Step 10649: loss=3.0171, lr=0.000400, tokens/sec=1376324.13, grad_norm=0.3331, duration=0.38s
Step 10650: loss=2.9907, lr=0.000400, tokens/sec=1374993.67, grad_norm=0.3742, duration=0.38s
Step 10651: loss=2.9896, lr=0.000400, tokens/sec=1374453.10, grad_norm=0.3160, duration=0.38s
Step 10652: loss=2.9893, lr=0.000400, tokens/sec=1379658.97, grad_norm=0.3363, duration=0.38s
Step 10653: loss=2.9130, lr=0.000400, tokens/sec=1377573.45, grad_norm=0.3461, duration=0.38s
Step 10654: loss=2.9973, lr=0.000400, tokens/sec=1378249.49, grad_norm=0.3133, duration=0.38s
Step 10655: loss=2.9765, lr=0.000400, tokens/sec=1376160.48, grad_norm=0.3446, duration=0.38s
Step 10656: loss=2.9413, lr=0.000400, tokens/sec=1372937.65, grad_norm=0.3533, duration=0.38s
Step 10657: loss=3.0081, lr=0.000400, tokens/sec=1372049.33, grad_norm=0.3411, duration=0.38s
Step 10658: loss=2.9214, lr=0.000400, tokens/sec=1374507.23, grad_norm=0.3417, duration=0.38s
Step 10659: loss=2.8720, lr=0.000400, tokens/sec=1375977.07, grad_norm=0.3534, duration=0.38s
Step 10660: loss=2.9283, lr=0.000400, tokens/sec=1377363.78, grad_norm=0.3608, duration=0.38s
Step 10661: loss=2.9923, lr=0.000400, tokens/sec=1374881.05, grad_norm=0.3326, duration=0.38s
Step 10662: loss=3.0303, lr=0.000400, tokens/sec=1376547.27, grad_norm=0.3089, duration=0.38s
Step 10663: loss=2.8852, lr=0.000400, tokens/sec=1374255.54, grad_norm=0.3492, duration=0.38s
Step 10664: loss=2.9844, lr=0.000400, tokens/sec=1374649.00, grad_norm=0.3434, duration=0.38s
Step 10665: loss=2.9790, lr=0.000400, tokens/sec=1378496.59, grad_norm=0.3064, duration=0.38s
Step 10666: loss=2.9295, lr=0.000400, tokens/sec=1376089.00, grad_norm=0.3227, duration=0.38s
Step 10667: loss=2.9320, lr=0.000400, tokens/sec=1373973.05, grad_norm=0.3309, duration=0.38s
Step 10668: loss=2.9658, lr=0.000400, tokens/sec=1376941.18, grad_norm=0.3108, duration=0.38s
Step 10669: loss=2.9383, lr=0.000400, tokens/sec=1377406.92, grad_norm=0.3040, duration=0.38s
Step 10670: loss=2.9044, lr=0.000399, tokens/sec=1376664.47, grad_norm=0.3202, duration=0.38s
Step 10671: loss=2.9434, lr=0.000399, tokens/sec=1379196.90, grad_norm=0.3263, duration=0.38s
Step 10672: loss=2.9588, lr=0.000399, tokens/sec=1378954.74, grad_norm=0.3329, duration=0.38s
Step 10673: loss=2.9234, lr=0.000399, tokens/sec=1378212.35, grad_norm=0.3221, duration=0.38s
Step 10674: loss=2.9246, lr=0.000399, tokens/sec=1376438.71, grad_norm=0.3087, duration=0.38s
Step 10675: loss=2.8670, lr=0.000399, tokens/sec=1374841.51, grad_norm=0.3025, duration=0.38s
Step 10676: loss=2.9512, lr=0.000399, tokens/sec=1375625.02, grad_norm=0.3010, duration=0.38s
Step 10677: loss=2.8984, lr=0.000399, tokens/sec=1376928.25, grad_norm=0.2934, duration=0.38s
Step 10678: loss=2.8922, lr=0.000399, tokens/sec=1374460.83, grad_norm=0.2991, duration=0.38s
Step 10679: loss=2.8991, lr=0.000399, tokens/sec=1376614.49, grad_norm=0.3026, duration=0.38s
Step 10680: loss=2.9122, lr=0.000399, tokens/sec=1376244.02, grad_norm=0.2950, duration=0.38s
Step 10681: loss=2.8990, lr=0.000399, tokens/sec=1380054.66, grad_norm=0.2990, duration=0.38s
Step 10682: loss=2.8792, lr=0.000399, tokens/sec=1373001.09, grad_norm=0.3078, duration=0.38s
Step 10683: loss=2.9443, lr=0.000399, tokens/sec=1374438.50, grad_norm=0.3098, duration=0.38s
Step 10684: loss=2.9177, lr=0.000399, tokens/sec=1376215.60, grad_norm=0.2959, duration=0.38s
Step 10685: loss=2.9054, lr=0.000399, tokens/sec=1377123.12, grad_norm=0.2969, duration=0.38s
Step 10686: loss=2.9445, lr=0.000399, tokens/sec=1374781.35, grad_norm=0.2853, duration=0.38s
Step 10687: loss=2.9822, lr=0.000399, tokens/sec=1377218.86, grad_norm=0.3105, duration=0.38s
Step 10688: loss=3.0404, lr=0.000399, tokens/sec=1377329.27, grad_norm=0.3022, duration=0.38s
Step 10689: loss=2.9865, lr=0.000399, tokens/sec=1374078.65, grad_norm=0.3076, duration=0.38s
Step 10690: loss=2.9714, lr=0.000399, tokens/sec=1374020.27, grad_norm=0.2947, duration=0.38s
Step 10691: loss=3.0184, lr=0.000399, tokens/sec=1372338.75, grad_norm=0.3283, duration=0.38s
Step 10692: loss=2.9560, lr=0.000399, tokens/sec=1374382.66, grad_norm=0.3312, duration=0.38s
Step 10693: loss=2.9816, lr=0.000399, tokens/sec=1373631.46, grad_norm=0.2786, duration=0.38s
Step 10694: loss=2.9143, lr=0.000399, tokens/sec=1372149.50, grad_norm=0.3157, duration=0.38s
Step 10695: loss=2.9451, lr=0.000399, tokens/sec=1375445.19, grad_norm=0.3274, duration=0.38s
Step 10696: loss=2.9613, lr=0.000399, tokens/sec=1374740.09, grad_norm=0.2919, duration=0.38s
Step 10697: loss=2.9182, lr=0.000399, tokens/sec=1373282.33, grad_norm=0.3227, duration=0.38s
Step 10698: loss=2.9359, lr=0.000399, tokens/sec=1373827.12, grad_norm=0.2971, duration=0.38s
Step 10699: loss=2.9236, lr=0.000399, tokens/sec=1375278.31, grad_norm=0.2953, duration=0.38s
Step 10700/19073 (56.1%), Elapsed time: 4242.54s, Steps per hour: 9079.47, Estimated hours remaining: 0.92
Step 10700: loss=2.9418, lr=0.000399, tokens/sec=1373805.67, grad_norm=0.3099, duration=0.38s
Step 10701: loss=2.8772, lr=0.000399, tokens/sec=1374948.11, grad_norm=0.3133, duration=0.38s
Step 10702: loss=2.9301, lr=0.000398, tokens/sec=1377724.49, grad_norm=0.2869, duration=0.38s
Step 10703: loss=2.9624, lr=0.000398, tokens/sec=1374414.45, grad_norm=0.3184, duration=0.38s
Step 10704: loss=2.9809, lr=0.000398, tokens/sec=1372444.10, grad_norm=0.3186, duration=0.38s
Step 10705: loss=2.9789, lr=0.000398, tokens/sec=1374425.61, grad_norm=0.2987, duration=0.38s
Step 10706: loss=2.9264, lr=0.000398, tokens/sec=1377384.49, grad_norm=0.3158, duration=0.38s
Step 10707: loss=3.0123, lr=0.000398, tokens/sec=1374484.89, grad_norm=0.3151, duration=0.38s
Step 10708: loss=2.9703, lr=0.000398, tokens/sec=1375046.12, grad_norm=0.2978, duration=0.38s
Step 10709: loss=2.9384, lr=0.000398, tokens/sec=1376040.78, grad_norm=0.3170, duration=0.38s
Step 10710: loss=2.9571, lr=0.000398, tokens/sec=1377797.86, grad_norm=0.2930, duration=0.38s
Step 10711: loss=2.9737, lr=0.000398, tokens/sec=1376880.83, grad_norm=0.3059, duration=0.38s
Step 10712: loss=2.9206, lr=0.000398, tokens/sec=1376538.66, grad_norm=0.2950, duration=0.38s
Step 10713: loss=2.9336, lr=0.000398, tokens/sec=1380259.95, grad_norm=0.3014, duration=0.38s
Step 10714: loss=2.9488, lr=0.000398, tokens/sec=1371991.12, grad_norm=0.3033, duration=0.38s
Step 10715: loss=2.9400, lr=0.000398, tokens/sec=1374398.98, grad_norm=0.2885, duration=0.38s
Step 10716: loss=3.0118, lr=0.000398, tokens/sec=1376845.48, grad_norm=0.3225, duration=0.38s
Step 10717: loss=2.9529, lr=0.000398, tokens/sec=1376107.95, grad_norm=0.3134, duration=0.38s
Step 10718: loss=2.9504, lr=0.000398, tokens/sec=1374331.13, grad_norm=0.3123, duration=0.38s
Step 10719: loss=2.9464, lr=0.000398, tokens/sec=1379642.52, grad_norm=0.3004, duration=0.38s
Step 10720: loss=2.9248, lr=0.000398, tokens/sec=1377732.26, grad_norm=0.3063, duration=0.38s
Step 10721: loss=2.8966, lr=0.000398, tokens/sec=1372156.35, grad_norm=0.3049, duration=0.38s
Step 10722: loss=2.9709, lr=0.000398, tokens/sec=1376161.34, grad_norm=0.2927, duration=0.38s
Step 10723: loss=2.9079, lr=0.000398, tokens/sec=1371408.44, grad_norm=0.2981, duration=0.38s
Step 10724: loss=2.9481, lr=0.000398, tokens/sec=1380936.03, grad_norm=0.3026, duration=0.38s
Step 10725: loss=2.8852, lr=0.000398, tokens/sec=1373618.59, grad_norm=0.2784, duration=0.38s
Step 10726: loss=2.8855, lr=0.000398, tokens/sec=1379308.49, grad_norm=0.2883, duration=0.38s
Step 10727: loss=2.9424, lr=0.000398, tokens/sec=1377755.56, grad_norm=0.2869, duration=0.38s
Step 10728: loss=2.8601, lr=0.000398, tokens/sec=1378247.77, grad_norm=0.2781, duration=0.38s
Step 10729: loss=2.8788, lr=0.000398, tokens/sec=1375762.72, grad_norm=0.2847, duration=0.38s
Step 10730: loss=2.8876, lr=0.000398, tokens/sec=1374948.97, grad_norm=0.2886, duration=0.38s
Step 10731: loss=2.9246, lr=0.000398, tokens/sec=1376889.45, grad_norm=0.2838, duration=0.38s
Step 10732: loss=2.9044, lr=0.000398, tokens/sec=1377082.59, grad_norm=0.2684, duration=0.38s
Step 10733: loss=2.9135, lr=0.000398, tokens/sec=1377052.41, grad_norm=0.2881, duration=0.38s
Step 10734: loss=2.9530, lr=0.000398, tokens/sec=1377900.60, grad_norm=0.2826, duration=0.38s
Step 10735: loss=2.9787, lr=0.000397, tokens/sec=1377768.51, grad_norm=0.2836, duration=0.38s
Step 10736: loss=2.9823, lr=0.000397, tokens/sec=1373050.81, grad_norm=0.2906, duration=0.38s
Step 10737: loss=3.0066, lr=0.000397, tokens/sec=1376051.12, grad_norm=0.2986, duration=0.38s
Step 10738: loss=2.9141, lr=0.000397, tokens/sec=1377954.13, grad_norm=0.2986, duration=0.38s
Step 10739: loss=2.9573, lr=0.000397, tokens/sec=1376267.28, grad_norm=0.3065, duration=0.38s
Step 10740: loss=2.9351, lr=0.000397, tokens/sec=1375947.80, grad_norm=0.3154, duration=0.38s
Step 10741: loss=2.9851, lr=0.000397, tokens/sec=1376489.54, grad_norm=0.3172, duration=0.38s
Step 10742: loss=2.9966, lr=0.000397, tokens/sec=1379751.59, grad_norm=0.3017, duration=0.38s
Step 10743: loss=2.9809, lr=0.000397, tokens/sec=1373223.15, grad_norm=0.3253, duration=0.38s
Step 10744: loss=3.0086, lr=0.000397, tokens/sec=1374271.86, grad_norm=0.3137, duration=0.38s
Step 10745: loss=2.9535, lr=0.000397, tokens/sec=1372708.82, grad_norm=0.3015, duration=0.38s
Step 10746: loss=2.9589, lr=0.000397, tokens/sec=1378931.39, grad_norm=0.3301, duration=0.38s
Step 10747: loss=2.9581, lr=0.000397, tokens/sec=1374481.45, grad_norm=0.3061, duration=0.38s
Step 10748: loss=2.9708, lr=0.000397, tokens/sec=1377582.94, grad_norm=0.3006, duration=0.38s
Step 10749: loss=2.9025, lr=0.000397, tokens/sec=1376221.63, grad_norm=0.3149, duration=0.38s
Validation loss at step 10750: 3.772195816040039
Step 10750: loss=2.9628, lr=0.000397, tokens/sec=156558.86, grad_norm=0.3138, duration=3.35s
Step 10751: loss=2.9569, lr=0.000397, tokens/sec=1372849.37, grad_norm=0.3080, duration=0.38s
Step 10752: loss=2.9684, lr=0.000397, tokens/sec=1380016.55, grad_norm=0.3010, duration=0.38s
Step 10753: loss=2.8968, lr=0.000397, tokens/sec=1373741.30, grad_norm=0.3133, duration=0.38s
Step 10754: loss=2.9598, lr=0.000397, tokens/sec=1374203.16, grad_norm=0.3021, duration=0.38s
Step 10755: loss=2.9312, lr=0.000397, tokens/sec=1372730.25, grad_norm=0.2986, duration=0.38s
Step 10756: loss=2.9846, lr=0.000397, tokens/sec=1375866.01, grad_norm=0.3086, duration=0.38s
Step 10757: loss=2.9281, lr=0.000397, tokens/sec=1376435.26, grad_norm=0.2924, duration=0.38s
Step 10758: loss=2.9404, lr=0.000397, tokens/sec=1376090.73, grad_norm=0.2849, duration=0.38s
Step 10759: loss=2.9318, lr=0.000397, tokens/sec=1377613.15, grad_norm=0.2819, duration=0.38s
Step 10760: loss=2.9342, lr=0.000397, tokens/sec=1377080.00, grad_norm=0.2772, duration=0.38s
Step 10761: loss=2.9510, lr=0.000397, tokens/sec=1372959.08, grad_norm=0.2902, duration=0.38s
Step 10762: loss=2.9377, lr=0.000397, tokens/sec=1373525.07, grad_norm=0.2807, duration=0.38s
Step 10763: loss=2.8989, lr=0.000397, tokens/sec=1375259.39, grad_norm=0.2752, duration=0.38s
Step 10764: loss=2.9372, lr=0.000397, tokens/sec=1369541.36, grad_norm=0.3001, duration=0.38s
Step 10765: loss=2.9364, lr=0.000397, tokens/sec=1375121.79, grad_norm=0.2933, duration=0.38s
Step 10766: loss=2.9310, lr=0.000397, tokens/sec=1374881.91, grad_norm=0.2906, duration=0.38s
Step 10767: loss=2.9213, lr=0.000396, tokens/sec=1374771.03, grad_norm=0.2909, duration=0.38s
Step 10768: loss=2.8884, lr=0.000396, tokens/sec=1373655.49, grad_norm=0.2877, duration=0.38s
Step 10769: loss=2.8503, lr=0.000396, tokens/sec=1373245.45, grad_norm=0.2863, duration=0.38s
Step 10770: loss=2.9303, lr=0.000396, tokens/sec=1376597.25, grad_norm=0.2926, duration=0.38s
Step 10771: loss=2.8604, lr=0.000396, tokens/sec=1374630.95, grad_norm=0.2991, duration=0.38s
Step 10772: loss=2.8894, lr=0.000396, tokens/sec=1374589.71, grad_norm=0.2886, duration=0.38s
Step 10773: loss=2.8711, lr=0.000396, tokens/sec=1376925.66, grad_norm=0.2739, duration=0.38s
Step 10774: loss=2.9530, lr=0.000396, tokens/sec=1377483.71, grad_norm=0.3001, duration=0.38s
Step 10775: loss=2.9054, lr=0.000396, tokens/sec=1372709.68, grad_norm=0.2979, duration=0.38s
Step 10776: loss=2.9057, lr=0.000396, tokens/sec=1373592.85, grad_norm=0.2816, duration=0.38s
Step 10777: loss=2.8496, lr=0.000396, tokens/sec=1376769.63, grad_norm=0.2821, duration=0.38s
Step 10778: loss=2.9093, lr=0.000396, tokens/sec=1378685.86, grad_norm=0.2928, duration=0.38s
Step 10779: loss=2.9365, lr=0.000396, tokens/sec=1374919.74, grad_norm=0.2866, duration=0.38s
Step 10780: loss=2.9820, lr=0.000396, tokens/sec=1378344.52, grad_norm=0.2917, duration=0.38s
Step 10781: loss=3.0087, lr=0.000396, tokens/sec=1380356.99, grad_norm=0.2979, duration=0.38s
Step 10782: loss=2.9930, lr=0.000396, tokens/sec=1375054.72, grad_norm=0.3115, duration=0.38s
Step 10783: loss=2.9660, lr=0.000396, tokens/sec=1375879.78, grad_norm=0.3021, duration=0.38s
Step 10784: loss=3.0209, lr=0.000396, tokens/sec=1369633.48, grad_norm=0.2984, duration=0.38s
Step 10785: loss=2.9051, lr=0.000396, tokens/sec=1377055.00, grad_norm=0.3092, duration=0.38s
Step 10786: loss=2.9778, lr=0.000396, tokens/sec=1376060.59, grad_norm=0.3066, duration=0.38s
Step 10787: loss=3.0200, lr=0.000396, tokens/sec=1372263.38, grad_norm=0.3098, duration=0.38s
Step 10788: loss=3.1019, lr=0.000396, tokens/sec=1379595.78, grad_norm=0.3436, duration=0.38s
Step 10789: loss=3.0038, lr=0.000396, tokens/sec=1377336.17, grad_norm=0.3298, duration=0.38s
Step 10790: loss=3.0056, lr=0.000396, tokens/sec=1374919.74, grad_norm=0.3032, duration=0.38s
Step 10791: loss=2.9076, lr=0.000396, tokens/sec=1374718.61, grad_norm=0.3833, duration=0.38s
Step 10792: loss=2.9908, lr=0.000396, tokens/sec=1377626.10, grad_norm=0.3274, duration=0.38s
Step 10793: loss=2.9509, lr=0.000396, tokens/sec=1374385.24, grad_norm=0.3461, duration=0.38s
Step 10794: loss=2.9001, lr=0.000396, tokens/sec=1373961.03, grad_norm=0.3306, duration=0.38s
Step 10795: loss=3.0086, lr=0.000396, tokens/sec=1372295.93, grad_norm=0.3086, duration=0.38s
Step 10796: loss=2.9027, lr=0.000396, tokens/sec=1378068.12, grad_norm=0.3239, duration=0.38s
Step 10797: loss=2.9939, lr=0.000396, tokens/sec=1374839.79, grad_norm=0.3029, duration=0.38s
Step 10798: loss=3.0054, lr=0.000396, tokens/sec=1376713.60, grad_norm=0.3071, duration=0.38s
Step 10799: loss=2.9657, lr=0.000395, tokens/sec=1376051.98, grad_norm=0.2986, duration=0.38s
Step 10800/19073 (56.6%), Elapsed time: 4283.70s, Steps per hour: 9076.26, Estimated hours remaining: 0.91
Step 10800: loss=2.9449, lr=0.000395, tokens/sec=1377754.70, grad_norm=0.2945, duration=0.38s
Step 10801: loss=2.9584, lr=0.000395, tokens/sec=1373531.08, grad_norm=0.3020, duration=0.38s
Step 10802: loss=2.9806, lr=0.000395, tokens/sec=1375162.20, grad_norm=0.3028, duration=0.38s
Step 10803: loss=2.9565, lr=0.000395, tokens/sec=1374599.16, grad_norm=0.2921, duration=0.38s
Step 10804: loss=2.8994, lr=0.000395, tokens/sec=1375686.12, grad_norm=0.2934, duration=0.38s
Step 10805: loss=2.9702, lr=0.000395, tokens/sec=1377318.92, grad_norm=0.3185, duration=0.38s
Step 10806: loss=2.9457, lr=0.000395, tokens/sec=1377100.70, grad_norm=0.3000, duration=0.38s
Step 10807: loss=2.9758, lr=0.000395, tokens/sec=1377498.38, grad_norm=0.2997, duration=0.38s
Step 10808: loss=2.9710, lr=0.000395, tokens/sec=1380143.87, grad_norm=0.3016, duration=0.38s
Step 10809: loss=2.9203, lr=0.000395, tokens/sec=1374345.73, grad_norm=0.3229, duration=0.38s
Step 10810: loss=2.9023, lr=0.000395, tokens/sec=1376027.87, grad_norm=0.2976, duration=0.38s
Step 10811: loss=2.9313, lr=0.000395, tokens/sec=1374028.85, grad_norm=0.2847, duration=0.38s
Step 10812: loss=2.8829, lr=0.000395, tokens/sec=1377825.49, grad_norm=0.3171, duration=0.38s
Step 10813: loss=2.8991, lr=0.000395, tokens/sec=1378334.15, grad_norm=0.2803, duration=0.38s
Step 10814: loss=2.9054, lr=0.000395, tokens/sec=1378447.34, grad_norm=0.2945, duration=0.38s
Step 10815: loss=2.8776, lr=0.000395, tokens/sec=1374234.07, grad_norm=0.3033, duration=0.38s
Step 10816: loss=2.8182, lr=0.000395, tokens/sec=1379283.40, grad_norm=0.2775, duration=0.38s
Step 10817: loss=2.9300, lr=0.000395, tokens/sec=1374459.12, grad_norm=0.2970, duration=0.38s
Step 10818: loss=2.8789, lr=0.000395, tokens/sec=1374787.36, grad_norm=0.2828, duration=0.38s
Step 10819: loss=2.8889, lr=0.000395, tokens/sec=1374797.68, grad_norm=0.2805, duration=0.38s
Step 10820: loss=2.8521, lr=0.000395, tokens/sec=1379517.89, grad_norm=0.2833, duration=0.38s
Step 10821: loss=2.8855, lr=0.000395, tokens/sec=1378474.99, grad_norm=0.2901, duration=0.38s
Step 10822: loss=2.9018, lr=0.000395, tokens/sec=1374997.11, grad_norm=0.2945, duration=0.38s
Step 10823: loss=2.8971, lr=0.000395, tokens/sec=1377523.40, grad_norm=0.2777, duration=0.38s
Step 10824: loss=2.9496, lr=0.000395, tokens/sec=1379210.74, grad_norm=0.2969, duration=0.38s
Step 10825: loss=2.9857, lr=0.000395, tokens/sec=1376441.29, grad_norm=0.3013, duration=0.38s
Step 10826: loss=2.9642, lr=0.000395, tokens/sec=1375114.05, grad_norm=0.2870, duration=0.38s
Step 10827: loss=2.9748, lr=0.000395, tokens/sec=1375111.47, grad_norm=0.2922, duration=0.38s
Step 10828: loss=2.9333, lr=0.000395, tokens/sec=1379418.38, grad_norm=0.3220, duration=0.38s
Step 10829: loss=2.9701, lr=0.000395, tokens/sec=1376157.90, grad_norm=0.2920, duration=0.38s
Step 10830: loss=2.9534, lr=0.000395, tokens/sec=1377318.92, grad_norm=0.3024, duration=0.38s
Step 10831: loss=2.9663, lr=0.000395, tokens/sec=1378462.89, grad_norm=0.3288, duration=0.38s
Step 10832: loss=2.9786, lr=0.000394, tokens/sec=1374716.89, grad_norm=0.3108, duration=0.38s
Step 10833: loss=3.0474, lr=0.000394, tokens/sec=1372844.23, grad_norm=0.2970, duration=0.38s
Step 10834: loss=2.9790, lr=0.000394, tokens/sec=1375512.30, grad_norm=0.3147, duration=0.38s
Step 10835: loss=3.0324, lr=0.000394, tokens/sec=1377552.74, grad_norm=0.3545, duration=0.38s
Step 10836: loss=3.0496, lr=0.000394, tokens/sec=1375397.87, grad_norm=0.2967, duration=0.38s
Step 10837: loss=2.9973, lr=0.000394, tokens/sec=1377551.01, grad_norm=0.3518, duration=0.38s
Step 10838: loss=3.0324, lr=0.000394, tokens/sec=1377899.73, grad_norm=0.3685, duration=0.38s
Step 10839: loss=2.9699, lr=0.000394, tokens/sec=1377289.59, grad_norm=0.3181, duration=0.38s
Step 10840: loss=3.0165, lr=0.000394, tokens/sec=1378065.52, grad_norm=0.3682, duration=0.38s
Step 10841: loss=2.9676, lr=0.000394, tokens/sec=1374167.09, grad_norm=0.3196, duration=0.38s
Step 10842: loss=2.9664, lr=0.000394, tokens/sec=1376739.46, grad_norm=0.3147, duration=0.38s
Step 10843: loss=2.9238, lr=0.000394, tokens/sec=1378474.99, grad_norm=0.3459, duration=0.38s
Step 10844: loss=2.9842, lr=0.000394, tokens/sec=1375009.15, grad_norm=0.3266, duration=0.38s
Step 10845: loss=2.9803, lr=0.000394, tokens/sec=1373552.53, grad_norm=0.3445, duration=0.38s
Step 10846: loss=2.9345, lr=0.000394, tokens/sec=1378527.70, grad_norm=0.3444, duration=0.38s
Step 10847: loss=2.9963, lr=0.000394, tokens/sec=1376654.13, grad_norm=0.3365, duration=0.38s
Step 10848: loss=2.9040, lr=0.000394, tokens/sec=1378738.59, grad_norm=0.3403, duration=0.38s
Step 10849: loss=2.8764, lr=0.000394, tokens/sec=1375721.41, grad_norm=0.3538, duration=0.38s
Step 10850: loss=2.9498, lr=0.000394, tokens/sec=1376786.86, grad_norm=0.3420, duration=0.38s
Step 10851: loss=2.9517, lr=0.000394, tokens/sec=1375566.50, grad_norm=0.3150, duration=0.38s
Step 10852: loss=3.0307, lr=0.000394, tokens/sec=1375779.93, grad_norm=0.3171, duration=0.38s
Step 10853: loss=2.8721, lr=0.000394, tokens/sec=1376907.55, grad_norm=0.3152, duration=0.38s
Step 10854: loss=2.9861, lr=0.000394, tokens/sec=1375563.06, grad_norm=0.3183, duration=0.38s
Step 10855: loss=2.9755, lr=0.000394, tokens/sec=1378704.02, grad_norm=0.3152, duration=0.38s
Step 10856: loss=2.8923, lr=0.000394, tokens/sec=1376377.54, grad_norm=0.2842, duration=0.38s
Step 10857: loss=2.9413, lr=0.000394, tokens/sec=1375535.53, grad_norm=0.3094, duration=0.38s
Step 10858: loss=2.9580, lr=0.000394, tokens/sec=1376086.42, grad_norm=0.3266, duration=0.38s
Step 10859: loss=2.9284, lr=0.000394, tokens/sec=1373189.71, grad_norm=0.2854, duration=0.38s
Step 10860: loss=2.9041, lr=0.000394, tokens/sec=1372152.07, grad_norm=0.3040, duration=0.38s
Step 10861: loss=2.9425, lr=0.000394, tokens/sec=1374325.97, grad_norm=0.3180, duration=0.38s
Step 10862: loss=2.8879, lr=0.000394, tokens/sec=1379569.82, grad_norm=0.3189, duration=0.38s
Step 10863: loss=2.9806, lr=0.000394, tokens/sec=1375158.76, grad_norm=0.2970, duration=0.38s
Step 10864: loss=2.9178, lr=0.000393, tokens/sec=1377261.99, grad_norm=0.3103, duration=0.38s
Step 10865: loss=2.8981, lr=0.000393, tokens/sec=1378074.16, grad_norm=0.2807, duration=0.38s
Step 10866: loss=2.9418, lr=0.000393, tokens/sec=1375305.83, grad_norm=0.3121, duration=0.38s
Step 10867: loss=2.8910, lr=0.000393, tokens/sec=1374238.37, grad_norm=0.3096, duration=0.38s
Step 10868: loss=2.8774, lr=0.000393, tokens/sec=1374559.64, grad_norm=0.2798, duration=0.38s
Step 10869: loss=2.8966, lr=0.000393, tokens/sec=1370677.57, grad_norm=0.3269, duration=0.38s
Step 10870: loss=2.8900, lr=0.000393, tokens/sec=1369851.05, grad_norm=0.2937, duration=0.38s
Step 10871: loss=2.8702, lr=0.000393, tokens/sec=1373988.50, grad_norm=0.2963, duration=0.38s
Step 10872: loss=2.9193, lr=0.000393, tokens/sec=1377230.93, grad_norm=0.3225, duration=0.38s
Step 10873: loss=2.9365, lr=0.000393, tokens/sec=1377415.54, grad_norm=0.3125, duration=0.38s
Step 10874: loss=2.9031, lr=0.000393, tokens/sec=1376309.49, grad_norm=0.3108, duration=0.38s
Step 10875: loss=2.9261, lr=0.000393, tokens/sec=1375098.57, grad_norm=0.2911, duration=0.38s
Step 10876: loss=2.9045, lr=0.000393, tokens/sec=1379433.09, grad_norm=0.2952, duration=0.38s
Step 10877: loss=2.9964, lr=0.000393, tokens/sec=1376948.94, grad_norm=0.3097, duration=0.38s
Step 10878: loss=3.0245, lr=0.000393, tokens/sec=1373646.91, grad_norm=0.3024, duration=0.38s
Step 10879: loss=2.9742, lr=0.000393, tokens/sec=1374861.28, grad_norm=0.3099, duration=0.38s
Step 10880: loss=2.9865, lr=0.000393, tokens/sec=1376197.51, grad_norm=0.2794, duration=0.38s
Step 10881: loss=2.9569, lr=0.000393, tokens/sec=1374034.00, grad_norm=0.2951, duration=0.38s
Step 10882: loss=2.9796, lr=0.000393, tokens/sec=1376126.89, grad_norm=0.3285, duration=0.38s
Step 10883: loss=2.9455, lr=0.000393, tokens/sec=1375909.05, grad_norm=0.2852, duration=0.38s
Step 10884: loss=2.9417, lr=0.000393, tokens/sec=1375309.27, grad_norm=0.2927, duration=0.38s
Step 10885: loss=2.9333, lr=0.000393, tokens/sec=1374921.46, grad_norm=0.3224, duration=0.38s
Step 10886: loss=2.9240, lr=0.000393, tokens/sec=1377081.73, grad_norm=0.3029, duration=0.38s
Step 10887: loss=2.9358, lr=0.000393, tokens/sec=1373886.35, grad_norm=0.3013, duration=0.38s
Step 10888: loss=2.9218, lr=0.000393, tokens/sec=1372366.15, grad_norm=0.3142, duration=0.38s
Step 10889: loss=2.9346, lr=0.000393, tokens/sec=1375721.41, grad_norm=0.2832, duration=0.38s
Step 10890: loss=2.9495, lr=0.000393, tokens/sec=1374684.23, grad_norm=0.3013, duration=0.38s
Step 10891: loss=2.8732, lr=0.000393, tokens/sec=1375153.60, grad_norm=0.3339, duration=0.38s
Step 10892: loss=2.9128, lr=0.000393, tokens/sec=1373991.94, grad_norm=0.2865, duration=0.38s
Step 10893: loss=2.9830, lr=0.000393, tokens/sec=1376418.03, grad_norm=0.3138, duration=0.38s
Step 10894: loss=2.9335, lr=0.000393, tokens/sec=1376082.12, grad_norm=0.3274, duration=0.38s
Step 10895: loss=2.9785, lr=0.000393, tokens/sec=1375345.40, grad_norm=0.2934, duration=0.38s
Step 10896: loss=2.9324, lr=0.000392, tokens/sec=1373848.58, grad_norm=0.3202, duration=0.38s
Step 10897: loss=3.0021, lr=0.000392, tokens/sec=1373240.30, grad_norm=0.3380, duration=0.38s
Step 10898: loss=2.9453, lr=0.000392, tokens/sec=1372091.28, grad_norm=0.3122, duration=0.38s
Step 10899: loss=2.9488, lr=0.000392, tokens/sec=1378365.26, grad_norm=0.3187, duration=0.38s
Step 10900/19073 (57.1%), Elapsed time: 4321.90s, Steps per hour: 9079.34, Estimated hours remaining: 0.90
Step 10900: loss=2.9642, lr=0.000392, tokens/sec=1374025.42, grad_norm=0.3297, duration=0.38s
Step 10901: loss=2.9427, lr=0.000392, tokens/sec=1372431.25, grad_norm=0.3119, duration=0.38s
Step 10902: loss=2.9095, lr=0.000392, tokens/sec=1379405.40, grad_norm=0.3041, duration=0.38s
Step 10903: loss=2.9432, lr=0.000392, tokens/sec=1376277.62, grad_norm=0.3024, duration=0.38s
Step 10904: loss=2.9381, lr=0.000392, tokens/sec=1376788.59, grad_norm=0.3158, duration=0.38s
Step 10905: loss=3.0032, lr=0.000392, tokens/sec=1373546.52, grad_norm=0.3229, duration=0.38s
Step 10906: loss=2.9917, lr=0.000392, tokens/sec=1377011.02, grad_norm=0.2938, duration=0.38s
Step 10907: loss=2.9088, lr=0.000392, tokens/sec=1374630.09, grad_norm=0.3175, duration=0.38s
Step 10908: loss=2.9546, lr=0.000392, tokens/sec=1377736.57, grad_norm=0.3235, duration=0.38s
Step 10909: loss=2.9095, lr=0.000392, tokens/sec=1375129.53, grad_norm=0.3101, duration=0.38s
Step 10910: loss=2.9616, lr=0.000392, tokens/sec=1379132.02, grad_norm=0.2953, duration=0.38s
Step 10911: loss=2.9023, lr=0.000392, tokens/sec=1376773.07, grad_norm=0.3083, duration=0.38s
Step 10912: loss=2.9564, lr=0.000392, tokens/sec=1377602.79, grad_norm=0.3306, duration=0.38s
Step 10913: loss=2.9328, lr=0.000392, tokens/sec=1372049.33, grad_norm=0.2990, duration=0.38s
Step 10914: loss=2.9285, lr=0.000392, tokens/sec=1378997.11, grad_norm=0.2911, duration=0.38s
Step 10915: loss=2.8485, lr=0.000392, tokens/sec=1380328.40, grad_norm=0.3048, duration=0.38s
Step 10916: loss=2.9171, lr=0.000392, tokens/sec=1379729.95, grad_norm=0.3000, duration=0.38s
Step 10917: loss=2.9046, lr=0.000392, tokens/sec=1378257.27, grad_norm=0.2911, duration=0.38s
Step 10918: loss=2.8526, lr=0.000392, tokens/sec=1375656.00, grad_norm=0.3008, duration=0.38s
Step 10919: loss=2.8991, lr=0.000392, tokens/sec=1374661.89, grad_norm=0.2869, duration=0.38s
Step 10920: loss=2.8645, lr=0.000392, tokens/sec=1375514.88, grad_norm=0.2911, duration=0.38s
Step 10921: loss=2.9152, lr=0.000392, tokens/sec=1375886.67, grad_norm=0.2984, duration=0.38s
Step 10922: loss=2.8803, lr=0.000392, tokens/sec=1377276.65, grad_norm=0.2905, duration=0.38s
Step 10923: loss=2.9029, lr=0.000392, tokens/sec=1375885.81, grad_norm=0.2895, duration=0.38s
Step 10924: loss=2.9694, lr=0.000392, tokens/sec=1381284.74, grad_norm=0.2893, duration=0.38s
Step 10925: loss=2.9705, lr=0.000392, tokens/sec=1374339.71, grad_norm=0.2933, duration=0.38s
Step 10926: loss=2.9986, lr=0.000392, tokens/sec=1376992.05, grad_norm=0.2930, duration=0.38s
Step 10927: loss=2.9718, lr=0.000392, tokens/sec=1375052.14, grad_norm=0.2963, duration=0.38s
Step 10928: loss=2.9046, lr=0.000392, tokens/sec=1376924.80, grad_norm=0.3072, duration=0.38s
Step 10929: loss=2.9631, lr=0.000391, tokens/sec=1370657.92, grad_norm=0.3025, duration=0.38s
Step 10930: loss=2.9202, lr=0.000391, tokens/sec=1379193.44, grad_norm=0.3044, duration=0.38s
Step 10931: loss=2.9999, lr=0.000391, tokens/sec=1370158.31, grad_norm=0.3060, duration=0.38s
Step 10932: loss=2.9839, lr=0.000391, tokens/sec=1376459.39, grad_norm=0.2984, duration=0.38s
Step 10933: loss=2.9985, lr=0.000391, tokens/sec=1374024.56, grad_norm=0.3064, duration=0.38s
Step 10934: loss=2.9824, lr=0.000391, tokens/sec=1376861.00, grad_norm=0.3048, duration=0.38s
Step 10935: loss=2.9408, lr=0.000391, tokens/sec=1376182.87, grad_norm=0.2874, duration=0.38s
Step 10936: loss=2.9515, lr=0.000391, tokens/sec=1377776.28, grad_norm=0.3032, duration=0.38s
Step 10937: loss=2.9590, lr=0.000391, tokens/sec=1375019.47, grad_norm=0.2945, duration=0.38s
Step 10938: loss=2.9474, lr=0.000391, tokens/sec=1374852.69, grad_norm=0.2789, duration=0.38s
Step 10939: loss=2.9039, lr=0.000391, tokens/sec=1378416.23, grad_norm=0.3074, duration=0.38s
Step 10940: loss=2.9612, lr=0.000391, tokens/sec=1376780.83, grad_norm=0.3236, duration=0.38s
Step 10941: loss=2.9522, lr=0.000391, tokens/sec=1375413.36, grad_norm=0.2903, duration=0.38s
Step 10942: loss=2.9521, lr=0.000391, tokens/sec=1375668.91, grad_norm=0.3030, duration=0.38s
Step 10943: loss=2.8834, lr=0.000391, tokens/sec=1373918.11, grad_norm=0.3051, duration=0.38s
Step 10944: loss=2.9842, lr=0.000391, tokens/sec=1376225.94, grad_norm=0.3143, duration=0.38s
Step 10945: loss=2.9261, lr=0.000391, tokens/sec=1376094.17, grad_norm=0.2933, duration=0.38s
Step 10946: loss=2.9521, lr=0.000391, tokens/sec=1376531.76, grad_norm=0.3047, duration=0.38s
Step 10947: loss=2.9155, lr=0.000391, tokens/sec=1382815.20, grad_norm=0.3039, duration=0.38s
Step 10948: loss=2.9460, lr=0.000391, tokens/sec=1374321.68, grad_norm=0.2821, duration=0.38s
Step 10949: loss=2.9210, lr=0.000391, tokens/sec=1374911.14, grad_norm=0.2906, duration=0.38s
Step 10950: loss=2.9466, lr=0.000391, tokens/sec=1377029.99, grad_norm=0.2941, duration=0.38s
Step 10951: loss=2.9270, lr=0.000391, tokens/sec=1379382.04, grad_norm=0.2790, duration=0.38s
Step 10952: loss=2.9143, lr=0.000391, tokens/sec=1373006.23, grad_norm=0.2910, duration=0.38s
Step 10953: loss=2.9133, lr=0.000391, tokens/sec=1370815.13, grad_norm=0.2937, duration=0.38s
Step 10954: loss=2.9284, lr=0.000391, tokens/sec=1374057.18, grad_norm=0.2810, duration=0.38s
Step 10955: loss=2.9357, lr=0.000391, tokens/sec=1375540.69, grad_norm=0.3055, duration=0.38s
Step 10956: loss=2.9271, lr=0.000391, tokens/sec=1374059.76, grad_norm=0.3017, duration=0.38s
Step 10957: loss=2.8879, lr=0.000391, tokens/sec=1377293.90, grad_norm=0.2860, duration=0.38s
Step 10958: loss=2.9041, lr=0.000391, tokens/sec=1375215.52, grad_norm=0.3086, duration=0.38s
Step 10959: loss=2.8885, lr=0.000391, tokens/sec=1372874.22, grad_norm=0.2995, duration=0.38s
Step 10960: loss=2.9301, lr=0.000391, tokens/sec=1375828.14, grad_norm=0.2905, duration=0.38s
Step 10961: loss=2.8409, lr=0.000390, tokens/sec=1376743.77, grad_norm=0.3040, duration=0.38s
Step 10962: loss=2.8707, lr=0.000390, tokens/sec=1376714.46, grad_norm=0.3012, duration=0.38s
Step 10963: loss=2.8857, lr=0.000390, tokens/sec=1375100.29, grad_norm=0.2902, duration=0.38s
Step 10964: loss=2.9319, lr=0.000390, tokens/sec=1372485.21, grad_norm=0.2852, duration=0.38s
Step 10965: loss=2.9226, lr=0.000390, tokens/sec=1373255.74, grad_norm=0.3083, duration=0.38s
Step 10966: loss=2.8570, lr=0.000390, tokens/sec=1373565.40, grad_norm=0.3010, duration=0.38s
Step 10967: loss=2.8712, lr=0.000390, tokens/sec=1376049.39, grad_norm=0.2866, duration=0.38s
Step 10968: loss=2.8981, lr=0.000390, tokens/sec=1375465.84, grad_norm=0.3012, duration=0.38s
Step 10969: loss=2.9602, lr=0.000390, tokens/sec=1370166.00, grad_norm=0.3022, duration=0.38s
Step 10970: loss=2.9854, lr=0.000390, tokens/sec=1375668.05, grad_norm=0.3043, duration=0.38s
Step 10971: loss=2.9824, lr=0.000390, tokens/sec=1376062.31, grad_norm=0.3148, duration=0.38s
Step 10972: loss=2.9828, lr=0.000390, tokens/sec=1375323.90, grad_norm=0.3091, duration=0.38s
Step 10973: loss=2.9885, lr=0.000390, tokens/sec=1371315.22, grad_norm=0.2963, duration=0.38s
Step 10974: loss=2.9496, lr=0.000390, tokens/sec=1374189.42, grad_norm=0.3023, duration=0.38s
Step 10975: loss=2.9462, lr=0.000390, tokens/sec=1376390.46, grad_norm=0.3167, duration=0.38s
Step 10976: loss=2.9721, lr=0.000390, tokens/sec=1375805.75, grad_norm=0.3256, duration=0.38s
Step 10977: loss=3.0342, lr=0.000390, tokens/sec=1375385.83, grad_norm=0.2976, duration=0.38s
Step 10978: loss=3.0531, lr=0.000390, tokens/sec=1371715.55, grad_norm=0.3413, duration=0.38s
Step 10979: loss=3.0148, lr=0.000390, tokens/sec=1374710.87, grad_norm=0.3284, duration=0.38s
Step 10980: loss=2.9536, lr=0.000390, tokens/sec=1374679.94, grad_norm=0.3238, duration=0.38s
Step 10981: loss=2.9524, lr=0.000390, tokens/sec=1372303.63, grad_norm=0.3261, duration=0.38s
Step 10982: loss=2.9821, lr=0.000390, tokens/sec=1374476.30, grad_norm=0.3230, duration=0.38s
Step 10983: loss=2.9266, lr=0.000390, tokens/sec=1372491.21, grad_norm=0.3197, duration=0.38s
Step 10984: loss=2.9117, lr=0.000390, tokens/sec=1377419.00, grad_norm=0.3367, duration=0.38s
Step 10985: loss=2.9819, lr=0.000390, tokens/sec=1374814.87, grad_norm=0.3109, duration=0.38s
Step 10986: loss=2.8989, lr=0.000390, tokens/sec=1377066.21, grad_norm=0.3047, duration=0.38s
Step 10987: loss=2.9795, lr=0.000390, tokens/sec=1373752.46, grad_norm=0.3256, duration=0.38s
Step 10988: loss=3.0128, lr=0.000390, tokens/sec=1375253.37, grad_norm=0.3124, duration=0.38s
Step 10989: loss=2.9889, lr=0.000390, tokens/sec=1374262.42, grad_norm=0.3016, duration=0.38s
Step 10990: loss=2.9023, lr=0.000390, tokens/sec=1376998.09, grad_norm=0.3031, duration=0.38s
Step 10991: loss=2.9537, lr=0.000390, tokens/sec=1373692.39, grad_norm=0.3200, duration=0.38s
Step 10992: loss=2.9857, lr=0.000390, tokens/sec=1380941.24, grad_norm=0.3105, duration=0.38s
Step 10993: loss=2.9650, lr=0.000390, tokens/sec=1376557.61, grad_norm=0.3070, duration=0.38s
Step 10994: loss=2.9083, lr=0.000389, tokens/sec=1376393.05, grad_norm=0.2964, duration=0.38s
Step 10995: loss=2.9359, lr=0.000389, tokens/sec=1378125.98, grad_norm=0.3056, duration=0.38s
Step 10996: loss=2.9484, lr=0.000389, tokens/sec=1372289.93, grad_norm=0.3128, duration=0.38s
Step 10997: loss=2.9379, lr=0.000389, tokens/sec=1378447.34, grad_norm=0.2867, duration=0.38s
Step 10998: loss=2.9910, lr=0.000389, tokens/sec=1376697.22, grad_norm=0.3060, duration=0.38s
Step 10999: loss=2.8838, lr=0.000389, tokens/sec=1377490.61, grad_norm=0.2922, duration=0.38s
Step 11000/19073 (57.7%), Elapsed time: 4360.10s, Steps per hour: 9082.36, Estimated hours remaining: 0.89
Validation loss at step 11000: 3.7760303020477295
Step 11000: loss=2.9253, lr=0.000389, tokens/sec=156181.84, grad_norm=0.2990, duration=3.36s
Step 11001: loss=2.9185, lr=0.000389, tokens/sec=1375704.19, grad_norm=0.2960, duration=0.38s
Step 11002: loss=2.8560, lr=0.000389, tokens/sec=1376387.02, grad_norm=0.2996, duration=0.38s
Step 11003: loss=2.9351, lr=0.000389, tokens/sec=1376278.48, grad_norm=0.2972, duration=0.38s
Step 11004: loss=2.8860, lr=0.000389, tokens/sec=1376132.92, grad_norm=0.2872, duration=0.38s
Step 11005: loss=2.8090, lr=0.000389, tokens/sec=1376798.93, grad_norm=0.3232, duration=0.38s
Step 11006: loss=2.8636, lr=0.000389, tokens/sec=1377984.35, grad_norm=0.2868, duration=0.38s
Step 11007: loss=2.9233, lr=0.000389, tokens/sec=1374848.39, grad_norm=0.2820, duration=0.38s
Step 11008: loss=2.9078, lr=0.000389, tokens/sec=1375904.75, grad_norm=0.3132, duration=0.38s
Step 11009: loss=2.8756, lr=0.000389, tokens/sec=1375955.54, grad_norm=0.2813, duration=0.38s
Step 11010: loss=2.8514, lr=0.000389, tokens/sec=1373079.10, grad_norm=0.2889, duration=0.38s
Step 11011: loss=2.8598, lr=0.000389, tokens/sec=1376673.95, grad_norm=0.2950, duration=0.38s
Step 11012: loss=2.9095, lr=0.000389, tokens/sec=1369167.02, grad_norm=0.2930, duration=0.38s
Step 11013: loss=2.9255, lr=0.000389, tokens/sec=1374344.87, grad_norm=0.2948, duration=0.38s
Step 11014: loss=2.9322, lr=0.000389, tokens/sec=1371127.96, grad_norm=0.2860, duration=0.38s
Step 11015: loss=2.9636, lr=0.000389, tokens/sec=1376184.60, grad_norm=0.3031, duration=0.38s
Step 11016: loss=2.9618, lr=0.000389, tokens/sec=1371534.17, grad_norm=0.2986, duration=0.38s
Step 11017: loss=2.9583, lr=0.000389, tokens/sec=1378591.65, grad_norm=0.2850, duration=0.38s
Step 11018: loss=2.9355, lr=0.000389, tokens/sec=1376178.57, grad_norm=0.3126, duration=0.38s
Step 11019: loss=2.9711, lr=0.000389, tokens/sec=1373643.47, grad_norm=0.2895, duration=0.38s
Step 11020: loss=2.9253, lr=0.000389, tokens/sec=1375607.81, grad_norm=0.3032, duration=0.38s
Step 11021: loss=2.9670, lr=0.000389, tokens/sec=1376776.52, grad_norm=0.3030, duration=0.38s
Step 11022: loss=2.9744, lr=0.000389, tokens/sec=1372327.61, grad_norm=0.2898, duration=0.38s
Step 11023: loss=3.0383, lr=0.000389, tokens/sec=1371930.35, grad_norm=0.3139, duration=0.38s
Step 11024: loss=3.0027, lr=0.000389, tokens/sec=1373431.57, grad_norm=0.2962, duration=0.38s
Step 11025: loss=2.9972, lr=0.000389, tokens/sec=1372620.57, grad_norm=0.3179, duration=0.38s
Step 11026: loss=3.0536, lr=0.000388, tokens/sec=1376182.01, grad_norm=0.3398, duration=0.38s
Step 11027: loss=3.0081, lr=0.000388, tokens/sec=1373640.04, grad_norm=0.3434, duration=0.38s
Step 11028: loss=2.9839, lr=0.000388, tokens/sec=1375711.08, grad_norm=0.3793, duration=0.38s
Step 11029: loss=2.9972, lr=0.000388, tokens/sec=1373648.62, grad_norm=0.3261, duration=0.38s
Step 11030: loss=2.9991, lr=0.000388, tokens/sec=1369908.22, grad_norm=0.3474, duration=0.38s
Step 11031: loss=2.9509, lr=0.000388, tokens/sec=1375502.83, grad_norm=0.3520, duration=0.38s
Step 11032: loss=2.9774, lr=0.000388, tokens/sec=1377527.72, grad_norm=0.3139, duration=0.38s
Step 11033: loss=2.9112, lr=0.000388, tokens/sec=1376396.49, grad_norm=0.3492, duration=0.38s
Step 11034: loss=2.9939, lr=0.000388, tokens/sec=1375536.39, grad_norm=0.3439, duration=0.38s
Step 11035: loss=2.9715, lr=0.000388, tokens/sec=1377600.20, grad_norm=0.3244, duration=0.38s
Step 11036: loss=2.9217, lr=0.000388, tokens/sec=1372890.51, grad_norm=0.3456, duration=0.38s
Step 11037: loss=2.9789, lr=0.000388, tokens/sec=1374257.26, grad_norm=0.3496, duration=0.38s
Step 11038: loss=2.9035, lr=0.000388, tokens/sec=1375273.15, grad_norm=0.3288, duration=0.38s
Step 11039: loss=2.8958, lr=0.000388, tokens/sec=1377171.42, grad_norm=0.3805, duration=0.38s
Step 11040: loss=2.9103, lr=0.000388, tokens/sec=1376033.03, grad_norm=0.3590, duration=0.38s
Step 11041: loss=2.9494, lr=0.000388, tokens/sec=1377366.37, grad_norm=0.3022, duration=0.38s
Step 11042: loss=3.0208, lr=0.000388, tokens/sec=1375500.25, grad_norm=0.3375, duration=0.38s
Step 11043: loss=2.8754, lr=0.000388, tokens/sec=1375280.89, grad_norm=0.3264, duration=0.38s
Step 11044: loss=2.9841, lr=0.000388, tokens/sec=1375022.04, grad_norm=0.3191, duration=0.38s
Step 11045: loss=2.9422, lr=0.000388, tokens/sec=1377362.92, grad_norm=0.3260, duration=0.38s
Step 11046: loss=2.9020, lr=0.000388, tokens/sec=1376672.23, grad_norm=0.3069, duration=0.38s
Step 11047: loss=2.9302, lr=0.000388, tokens/sec=1377038.61, grad_norm=0.2925, duration=0.38s
Step 11048: loss=2.9486, lr=0.000388, tokens/sec=1372726.82, grad_norm=0.3240, duration=0.38s
Step 11049: loss=2.9325, lr=0.000388, tokens/sec=1378410.18, grad_norm=0.3024, duration=0.38s
Step 11050: loss=2.9048, lr=0.000388, tokens/sec=1375341.10, grad_norm=0.2968, duration=0.38s
Step 11051: loss=2.8703, lr=0.000388, tokens/sec=1378384.26, grad_norm=0.3169, duration=0.38s
Step 11052: loss=2.9440, lr=0.000388, tokens/sec=1375291.21, grad_norm=0.3367, duration=0.38s
Step 11053: loss=2.9706, lr=0.000388, tokens/sec=1376855.83, grad_norm=0.2883, duration=0.38s
Step 11054: loss=2.9484, lr=0.000388, tokens/sec=1376155.31, grad_norm=0.3039, duration=0.38s
Step 11055: loss=2.8892, lr=0.000388, tokens/sec=1377551.01, grad_norm=0.2950, duration=0.38s
Step 11056: loss=2.9338, lr=0.000388, tokens/sec=1376560.20, grad_norm=0.2965, duration=0.38s
Step 11057: loss=2.8777, lr=0.000388, tokens/sec=1376829.97, grad_norm=0.2928, duration=0.38s
Step 11058: loss=2.8769, lr=0.000388, tokens/sec=1376392.19, grad_norm=0.3063, duration=0.38s
Step 11059: loss=2.8719, lr=0.000387, tokens/sec=1378657.34, grad_norm=0.2913, duration=0.38s
Step 11060: loss=2.8592, lr=0.000387, tokens/sec=1376861.00, grad_norm=0.2949, duration=0.38s
Step 11061: loss=2.9095, lr=0.000387, tokens/sec=1376678.26, grad_norm=0.2975, duration=0.38s
Step 11062: loss=2.9130, lr=0.000387, tokens/sec=1375451.21, grad_norm=0.2903, duration=0.38s
Step 11063: loss=2.9252, lr=0.000387, tokens/sec=1376192.35, grad_norm=0.3148, duration=0.38s
Step 11064: loss=2.9279, lr=0.000387, tokens/sec=1375872.04, grad_norm=0.3042, duration=0.38s
Step 11065: loss=2.8901, lr=0.000387, tokens/sec=1376556.75, grad_norm=0.2909, duration=0.38s
Step 11066: loss=2.9209, lr=0.000387, tokens/sec=1376515.39, grad_norm=0.2853, duration=0.38s
Step 11067: loss=2.9765, lr=0.000387, tokens/sec=1374856.13, grad_norm=0.3149, duration=0.38s
Step 11068: loss=3.0122, lr=0.000387, tokens/sec=1372588.01, grad_norm=0.3107, duration=0.38s
Step 11069: loss=2.9869, lr=0.000387, tokens/sec=1370050.76, grad_norm=0.3067, duration=0.38s
Step 11070: loss=2.9299, lr=0.000387, tokens/sec=1373192.28, grad_norm=0.3125, duration=0.38s
Step 11071: loss=2.9800, lr=0.000387, tokens/sec=1376022.70, grad_norm=0.3002, duration=0.38s
Step 11072: loss=2.9430, lr=0.000387, tokens/sec=1372390.99, grad_norm=0.3158, duration=0.38s
Step 11073: loss=2.9759, lr=0.000387, tokens/sec=1372497.20, grad_norm=0.2968, duration=0.38s
Step 11074: loss=2.9345, lr=0.000387, tokens/sec=1377877.29, grad_norm=0.2990, duration=0.38s
Step 11075: loss=2.8953, lr=0.000387, tokens/sec=1370962.13, grad_norm=0.3268, duration=0.38s
Step 11076: loss=2.9465, lr=0.000387, tokens/sec=1372283.08, grad_norm=0.2974, duration=0.38s
Step 11077: loss=2.9187, lr=0.000387, tokens/sec=1375518.32, grad_norm=0.3309, duration=0.38s
Step 11078: loss=2.9323, lr=0.000387, tokens/sec=1375231.00, grad_norm=0.3179, duration=0.38s
Step 11079: loss=2.9440, lr=0.000387, tokens/sec=1374999.69, grad_norm=0.2854, duration=0.38s
Step 11080: loss=2.9449, lr=0.000387, tokens/sec=1375470.14, grad_norm=0.3155, duration=0.38s
Step 11081: loss=2.8524, lr=0.000387, tokens/sec=1376620.52, grad_norm=0.3345, duration=0.38s
Step 11082: loss=2.9334, lr=0.000387, tokens/sec=1374613.77, grad_norm=0.3038, duration=0.38s
Step 11083: loss=2.9328, lr=0.000387, tokens/sec=1376271.59, grad_norm=0.3125, duration=0.38s
Step 11084: loss=2.9335, lr=0.000387, tokens/sec=1371656.51, grad_norm=0.3349, duration=0.38s
Step 11085: loss=2.9851, lr=0.000387, tokens/sec=1377062.76, grad_norm=0.3142, duration=0.38s
Step 11086: loss=2.9234, lr=0.000387, tokens/sec=1379329.26, grad_norm=0.3098, duration=0.38s
Step 11087: loss=2.9788, lr=0.000387, tokens/sec=1379167.49, grad_norm=0.3457, duration=0.38s
Step 11088: loss=2.9614, lr=0.000387, tokens/sec=1376132.06, grad_norm=0.3244, duration=0.38s
Step 11089: loss=2.9558, lr=0.000387, tokens/sec=1376943.77, grad_norm=0.3024, duration=0.38s
Step 11090: loss=2.9318, lr=0.000387, tokens/sec=1378004.21, grad_norm=0.3260, duration=0.38s
Step 11091: loss=2.9274, lr=0.000387, tokens/sec=1376107.09, grad_norm=0.3159, duration=0.38s
Step 11092: loss=2.9196, lr=0.000386, tokens/sec=1377015.33, grad_norm=0.3000, duration=0.38s
Step 11093: loss=2.9321, lr=0.000386, tokens/sec=1377513.91, grad_norm=0.3058, duration=0.38s
Step 11094: loss=3.0007, lr=0.000386, tokens/sec=1377080.87, grad_norm=0.3116, duration=0.38s
Step 11095: loss=2.9824, lr=0.000386, tokens/sec=1374845.81, grad_norm=0.3296, duration=0.38s
Step 11096: loss=2.9459, lr=0.000386, tokens/sec=1379707.44, grad_norm=0.3039, duration=0.38s
Step 11097: loss=2.9090, lr=0.000386, tokens/sec=1374795.10, grad_norm=0.2977, duration=0.38s
Step 11098: loss=2.9146, lr=0.000386, tokens/sec=1379246.21, grad_norm=0.3116, duration=0.38s
Step 11099: loss=2.9480, lr=0.000386, tokens/sec=1378597.70, grad_norm=0.3314, duration=0.38s
Step 11100/19073 (58.2%), Elapsed time: 4401.28s, Steps per hour: 9079.18, Estimated hours remaining: 0.88
Step 11100: loss=2.9716, lr=0.000386, tokens/sec=1376133.78, grad_norm=0.3125, duration=0.38s
Step 11101: loss=2.8852, lr=0.000386, tokens/sec=1376760.14, grad_norm=0.2940, duration=0.38s
Step 11102: loss=2.9818, lr=0.000386, tokens/sec=1379602.71, grad_norm=0.3439, duration=0.38s
Step 11103: loss=2.9147, lr=0.000386, tokens/sec=1374627.52, grad_norm=0.3201, duration=0.38s
Step 11104: loss=2.8899, lr=0.000386, tokens/sec=1378007.67, grad_norm=0.2869, duration=0.38s
Step 11105: loss=2.8805, lr=0.000386, tokens/sec=1374357.75, grad_norm=0.3277, duration=0.38s
Step 11106: loss=2.8792, lr=0.000386, tokens/sec=1377347.39, grad_norm=0.3089, duration=0.38s
Step 11107: loss=2.8931, lr=0.000386, tokens/sec=1377412.09, grad_norm=0.2853, duration=0.38s
Step 11108: loss=2.8736, lr=0.000386, tokens/sec=1377419.00, grad_norm=0.2935, duration=0.38s
Step 11109: loss=2.8798, lr=0.000386, tokens/sec=1376704.12, grad_norm=0.2968, duration=0.38s
Step 11110: loss=2.8571, lr=0.000386, tokens/sec=1374809.71, grad_norm=0.2956, duration=0.38s
Step 11111: loss=2.8925, lr=0.000386, tokens/sec=1373479.60, grad_norm=0.3023, duration=0.38s
Step 11112: loss=2.8760, lr=0.000386, tokens/sec=1375939.19, grad_norm=0.3055, duration=0.38s
Step 11113: loss=2.9197, lr=0.000386, tokens/sec=1379032.56, grad_norm=0.2903, duration=0.38s
Step 11114: loss=2.9596, lr=0.000386, tokens/sec=1372006.53, grad_norm=0.3095, duration=0.38s
Step 11115: loss=2.9878, lr=0.000386, tokens/sec=1374326.83, grad_norm=0.3063, duration=0.38s
Step 11116: loss=2.9657, lr=0.000386, tokens/sec=1378684.14, grad_norm=0.3119, duration=0.38s
Step 11117: loss=2.9637, lr=0.000386, tokens/sec=1378704.88, grad_norm=0.2963, duration=0.38s
Step 11118: loss=2.9114, lr=0.000386, tokens/sec=1374143.05, grad_norm=0.3309, duration=0.38s
Step 11119: loss=2.9484, lr=0.000386, tokens/sec=1375071.91, grad_norm=0.3296, duration=0.38s
Step 11120: loss=2.9308, lr=0.000386, tokens/sec=1378529.43, grad_norm=0.3134, duration=0.38s
Step 11121: loss=2.9898, lr=0.000386, tokens/sec=1378778.36, grad_norm=0.3401, duration=0.38s
Step 11122: loss=3.0034, lr=0.000386, tokens/sec=1373526.79, grad_norm=0.3105, duration=0.38s
Step 11123: loss=2.9712, lr=0.000386, tokens/sec=1374664.47, grad_norm=0.3097, duration=0.38s
Step 11124: loss=2.9712, lr=0.000385, tokens/sec=1376557.61, grad_norm=0.3203, duration=0.38s
Step 11125: loss=2.9364, lr=0.000385, tokens/sec=1375201.76, grad_norm=0.3181, duration=0.38s
Step 11126: loss=2.9495, lr=0.000385, tokens/sec=1376748.08, grad_norm=0.3011, duration=0.38s
Step 11127: loss=2.9359, lr=0.000385, tokens/sec=1377718.45, grad_norm=0.3238, duration=0.38s
Step 11128: loss=2.9488, lr=0.000385, tokens/sec=1377462.14, grad_norm=0.3074, duration=0.38s
Step 11129: loss=2.9027, lr=0.000385, tokens/sec=1374195.43, grad_norm=0.3018, duration=0.38s
Step 11130: loss=2.9555, lr=0.000385, tokens/sec=1378331.56, grad_norm=0.3277, duration=0.38s
Step 11131: loss=2.9339, lr=0.000385, tokens/sec=1378771.44, grad_norm=0.3179, duration=0.38s
Step 11132: loss=2.9393, lr=0.000385, tokens/sec=1378753.29, grad_norm=0.2985, duration=0.38s
Step 11133: loss=2.9091, lr=0.000385, tokens/sec=1375143.29, grad_norm=0.3355, duration=0.38s
Step 11134: loss=2.9729, lr=0.000385, tokens/sec=1376933.42, grad_norm=0.3260, duration=0.38s
Step 11135: loss=2.8945, lr=0.000385, tokens/sec=1375242.18, grad_norm=0.3025, duration=0.38s
Step 11136: loss=2.9399, lr=0.000385, tokens/sec=1377230.07, grad_norm=0.3030, duration=0.38s
Step 11137: loss=2.9212, lr=0.000385, tokens/sec=1377888.51, grad_norm=0.3098, duration=0.38s
Step 11138: loss=2.9316, lr=0.000385, tokens/sec=1379734.28, grad_norm=0.3011, duration=0.38s
Step 11139: loss=2.9318, lr=0.000385, tokens/sec=1377028.27, grad_norm=0.2995, duration=0.38s
Step 11140: loss=2.9251, lr=0.000385, tokens/sec=1377235.25, grad_norm=0.3020, duration=0.38s
Step 11141: loss=2.9048, lr=0.000385, tokens/sec=1379352.62, grad_norm=0.3023, duration=0.38s
Step 11142: loss=2.9267, lr=0.000385, tokens/sec=1377358.60, grad_norm=0.3046, duration=0.38s
Step 11143: loss=2.9085, lr=0.000385, tokens/sec=1375286.05, grad_norm=0.3025, duration=0.38s
Step 11144: loss=2.9290, lr=0.000385, tokens/sec=1375050.42, grad_norm=0.2826, duration=0.38s
Step 11145: loss=2.9314, lr=0.000385, tokens/sec=1379122.51, grad_norm=0.3087, duration=0.38s
Step 11146: loss=2.8926, lr=0.000385, tokens/sec=1376752.39, grad_norm=0.3077, duration=0.38s
Step 11147: loss=2.9041, lr=0.000385, tokens/sec=1374685.95, grad_norm=0.3091, duration=0.38s
Step 11148: loss=2.9401, lr=0.000385, tokens/sec=1378151.89, grad_norm=0.2949, duration=0.38s
Step 11149: loss=2.8906, lr=0.000385, tokens/sec=1374538.16, grad_norm=0.3086, duration=0.38s
Step 11150: loss=2.9096, lr=0.000385, tokens/sec=1376956.70, grad_norm=0.3077, duration=0.38s
Step 11151: loss=2.8187, lr=0.000385, tokens/sec=1377940.31, grad_norm=0.3035, duration=0.38s
Step 11152: loss=2.8829, lr=0.000385, tokens/sec=1376887.73, grad_norm=0.3131, duration=0.38s
Step 11153: loss=2.8652, lr=0.000385, tokens/sec=1375816.08, grad_norm=0.3041, duration=0.38s
Step 11154: loss=2.9500, lr=0.000385, tokens/sec=1376592.08, grad_norm=0.2905, duration=0.38s
Step 11155: loss=2.8770, lr=0.000385, tokens/sec=1375996.01, grad_norm=0.3054, duration=0.38s
Step 11156: loss=2.8790, lr=0.000385, tokens/sec=1376914.45, grad_norm=0.2907, duration=0.38s
Step 11157: loss=2.8599, lr=0.000384, tokens/sec=1372937.65, grad_norm=0.2990, duration=0.38s
Step 11158: loss=2.9189, lr=0.000384, tokens/sec=1376569.68, grad_norm=0.3092, duration=0.38s
Step 11159: loss=2.9635, lr=0.000384, tokens/sec=1374107.84, grad_norm=0.3121, duration=0.38s
Step 11160: loss=2.9566, lr=0.000384, tokens/sec=1377280.96, grad_norm=0.3095, duration=0.38s
Step 11161: loss=2.9718, lr=0.000384, tokens/sec=1379758.52, grad_norm=0.3140, duration=0.38s
Step 11162: loss=3.0073, lr=0.000384, tokens/sec=1378018.03, grad_norm=0.3151, duration=0.38s
Step 11163: loss=2.9184, lr=0.000384, tokens/sec=1375630.18, grad_norm=0.3125, duration=0.38s
Step 11164: loss=2.9908, lr=0.000384, tokens/sec=1376332.74, grad_norm=0.2997, duration=0.38s
Step 11165: loss=2.9378, lr=0.000384, tokens/sec=1377744.34, grad_norm=0.3121, duration=0.38s
Step 11166: loss=2.9836, lr=0.000384, tokens/sec=1377399.15, grad_norm=0.3146, duration=0.38s
Step 11167: loss=2.9868, lr=0.000384, tokens/sec=1378291.82, grad_norm=0.3105, duration=0.38s
Step 11168: loss=3.0655, lr=0.000384, tokens/sec=1374070.92, grad_norm=0.3255, duration=0.38s
Step 11169: loss=2.9564, lr=0.000384, tokens/sec=1378546.71, grad_norm=0.3458, duration=0.38s
Step 11170: loss=3.0026, lr=0.000384, tokens/sec=1378246.04, grad_norm=0.3191, duration=0.38s
Step 11171: loss=2.9484, lr=0.000384, tokens/sec=1376845.48, grad_norm=0.3089, duration=0.38s
Step 11172: loss=2.9602, lr=0.000384, tokens/sec=1371825.08, grad_norm=0.3136, duration=0.38s
Step 11173: loss=2.9394, lr=0.000384, tokens/sec=1377575.18, grad_norm=0.3110, duration=0.38s
Step 11174: loss=2.8858, lr=0.000384, tokens/sec=1372855.37, grad_norm=0.3325, duration=0.38s
Step 11175: loss=2.9802, lr=0.000384, tokens/sec=1375987.40, grad_norm=0.3160, duration=0.38s
Step 11176: loss=2.8849, lr=0.000384, tokens/sec=1376552.44, grad_norm=0.3083, duration=0.38s
Step 11177: loss=2.9848, lr=0.000384, tokens/sec=1379375.11, grad_norm=0.3157, duration=0.38s
Step 11178: loss=3.0342, lr=0.000384, tokens/sec=1373843.43, grad_norm=0.3239, duration=0.38s
Step 11179: loss=2.9466, lr=0.000384, tokens/sec=1378228.76, grad_norm=0.2851, duration=0.38s
Step 11180: loss=2.8946, lr=0.000384, tokens/sec=1372256.53, grad_norm=0.2996, duration=0.38s
Step 11181: loss=2.9588, lr=0.000384, tokens/sec=1371399.88, grad_norm=0.3237, duration=0.38s
Step 11182: loss=2.9942, lr=0.000384, tokens/sec=1376320.68, grad_norm=0.2978, duration=0.38s
Step 11183: loss=2.9731, lr=0.000384, tokens/sec=1376001.18, grad_norm=0.3152, duration=0.38s
Step 11184: loss=2.8719, lr=0.000384, tokens/sec=1378212.35, grad_norm=0.2990, duration=0.38s
Step 11185: loss=2.9372, lr=0.000384, tokens/sec=1374948.97, grad_norm=0.2900, duration=0.38s
Step 11186: loss=2.9118, lr=0.000384, tokens/sec=1378864.81, grad_norm=0.3078, duration=0.38s
Step 11187: loss=2.9566, lr=0.000384, tokens/sec=1376830.83, grad_norm=0.3123, duration=0.38s
Step 11188: loss=2.9576, lr=0.000384, tokens/sec=1375470.14, grad_norm=0.2911, duration=0.38s
Step 11189: loss=2.9088, lr=0.000384, tokens/sec=1371818.23, grad_norm=0.2888, duration=0.38s
Step 11190: loss=2.9119, lr=0.000383, tokens/sec=1372937.65, grad_norm=0.2945, duration=0.38s
Step 11191: loss=2.8945, lr=0.000383, tokens/sec=1376558.47, grad_norm=0.2980, duration=0.38s
Step 11192: loss=2.8909, lr=0.000383, tokens/sec=1379294.65, grad_norm=0.3023, duration=0.38s
Step 11193: loss=2.9134, lr=0.000383, tokens/sec=1380942.11, grad_norm=0.3096, duration=0.38s
Step 11194: loss=2.8156, lr=0.000383, tokens/sec=1374883.63, grad_norm=0.2976, duration=0.38s
Step 11195: loss=2.8520, lr=0.000383, tokens/sec=1377920.46, grad_norm=0.3078, duration=0.38s
Step 11196: loss=2.8562, lr=0.000383, tokens/sec=1380465.31, grad_norm=0.3066, duration=0.38s
Step 11197: loss=2.9507, lr=0.000383, tokens/sec=1374789.94, grad_norm=0.2832, duration=0.38s
Step 11198: loss=2.8923, lr=0.000383, tokens/sec=1371465.74, grad_norm=0.3090, duration=0.38s
Step 11199: loss=2.8732, lr=0.000383, tokens/sec=1373775.63, grad_norm=0.2868, duration=0.38s
Step 11200/19073 (58.7%), Elapsed time: 4439.45s, Steps per hour: 9082.20, Estimated hours remaining: 0.87
Step 11200: loss=2.8248, lr=0.000383, tokens/sec=1374346.59, grad_norm=0.2901, duration=0.38s
Step 11201: loss=2.8660, lr=0.000383, tokens/sec=1376453.35, grad_norm=0.2844, duration=0.38s
Step 11202: loss=2.9372, lr=0.000383, tokens/sec=1374213.46, grad_norm=0.2801, duration=0.38s
Step 11203: loss=2.9104, lr=0.000383, tokens/sec=1376110.53, grad_norm=0.2909, duration=0.38s
Step 11204: loss=2.9140, lr=0.000383, tokens/sec=1377583.81, grad_norm=0.2839, duration=0.38s
Step 11205: loss=2.9574, lr=0.000383, tokens/sec=1377962.76, grad_norm=0.2854, duration=0.38s
Step 11206: loss=2.9490, lr=0.000383, tokens/sec=1376933.42, grad_norm=0.2897, duration=0.38s
Step 11207: loss=2.9595, lr=0.000383, tokens/sec=1374957.56, grad_norm=0.2833, duration=0.38s
Step 11208: loss=2.9371, lr=0.000383, tokens/sec=1376309.49, grad_norm=0.2956, duration=0.38s
Step 11209: loss=2.9427, lr=0.000383, tokens/sec=1378407.59, grad_norm=0.2823, duration=0.38s
Step 11210: loss=2.9251, lr=0.000383, tokens/sec=1377141.24, grad_norm=0.2827, duration=0.38s
Step 11211: loss=2.9634, lr=0.000383, tokens/sec=1369939.80, grad_norm=0.3099, duration=0.38s
Step 11212: loss=2.9623, lr=0.000383, tokens/sec=1377535.48, grad_norm=0.2732, duration=0.38s
Step 11213: loss=3.0634, lr=0.000383, tokens/sec=1378056.89, grad_norm=0.3079, duration=0.38s
Step 11214: loss=2.9709, lr=0.000383, tokens/sec=1375596.62, grad_norm=0.3077, duration=0.38s
Step 11215: loss=3.0007, lr=0.000383, tokens/sec=1376583.46, grad_norm=0.3246, duration=0.38s
Step 11216: loss=3.0664, lr=0.000383, tokens/sec=1372231.70, grad_norm=0.3519, duration=0.38s
Step 11217: loss=2.9594, lr=0.000383, tokens/sec=1377412.96, grad_norm=0.3165, duration=0.38s
Step 11218: loss=3.0116, lr=0.000383, tokens/sec=1379238.42, grad_norm=0.3539, duration=0.38s
Step 11219: loss=2.9775, lr=0.000383, tokens/sec=1375854.82, grad_norm=0.3500, duration=0.38s
Step 11220: loss=2.9774, lr=0.000383, tokens/sec=1375694.73, grad_norm=0.3197, duration=0.38s
Step 11221: loss=2.9627, lr=0.000383, tokens/sec=1379715.24, grad_norm=0.3504, duration=0.38s
Step 11222: loss=2.9644, lr=0.000382, tokens/sec=1376255.22, grad_norm=0.3299, duration=0.38s
Step 11223: loss=2.9169, lr=0.000382, tokens/sec=1374200.58, grad_norm=0.3417, duration=0.38s
Step 11224: loss=2.9860, lr=0.000382, tokens/sec=1375398.73, grad_norm=0.3368, duration=0.38s
Step 11225: loss=2.9585, lr=0.000382, tokens/sec=1376253.50, grad_norm=0.3492, duration=0.38s
Step 11226: loss=2.9044, lr=0.000382, tokens/sec=1379357.81, grad_norm=0.3335, duration=0.38s
Step 11227: loss=2.9782, lr=0.000382, tokens/sec=1379577.61, grad_norm=0.3469, duration=0.38s
Step 11228: loss=2.9237, lr=0.000382, tokens/sec=1378511.28, grad_norm=0.3274, duration=0.38s
Step 11229: loss=2.8553, lr=0.000382, tokens/sec=1373485.61, grad_norm=0.3589, duration=0.38s
Step 11230: loss=2.9072, lr=0.000382, tokens/sec=1376820.48, grad_norm=0.3543, duration=0.38s
Step 11231: loss=2.9380, lr=0.000382, tokens/sec=1374412.73, grad_norm=0.2960, duration=0.38s
Step 11232: loss=3.0234, lr=0.000382, tokens/sec=1376029.59, grad_norm=0.3185, duration=0.38s
Step 11233: loss=2.8758, lr=0.000382, tokens/sec=1372833.09, grad_norm=0.3358, duration=0.38s
Step 11234: loss=2.9493, lr=0.000382, tokens/sec=1377035.16, grad_norm=0.3074, duration=0.38s
Step 11235: loss=2.9512, lr=0.000382, tokens/sec=1376020.12, grad_norm=0.3067, duration=0.38s
Step 11236: loss=2.8907, lr=0.000382, tokens/sec=1375384.97, grad_norm=0.2996, duration=0.38s
Step 11237: loss=2.9182, lr=0.000382, tokens/sec=1376935.14, grad_norm=0.2771, duration=0.38s
Step 11238: loss=2.9524, lr=0.000382, tokens/sec=1376318.10, grad_norm=0.3082, duration=0.38s
Step 11239: loss=2.9341, lr=0.000382, tokens/sec=1374299.35, grad_norm=0.3213, duration=0.38s
Step 11240: loss=2.8308, lr=0.000382, tokens/sec=1376249.19, grad_norm=0.2865, duration=0.38s
Step 11241: loss=2.9241, lr=0.000382, tokens/sec=1376368.93, grad_norm=0.3060, duration=0.38s
Step 11242: loss=2.9296, lr=0.000382, tokens/sec=1378511.28, grad_norm=0.3229, duration=0.38s
Step 11243: loss=3.0024, lr=0.000382, tokens/sec=1376781.69, grad_norm=0.3015, duration=0.38s
Step 11244: loss=2.9389, lr=0.000382, tokens/sec=1375473.58, grad_norm=0.2907, duration=0.38s
Step 11245: loss=2.8837, lr=0.000382, tokens/sec=1379869.34, grad_norm=0.3008, duration=0.38s
Step 11246: loss=2.9216, lr=0.000382, tokens/sec=1374988.51, grad_norm=0.2894, duration=0.38s
Step 11247: loss=2.8741, lr=0.000382, tokens/sec=1377571.73, grad_norm=0.2852, duration=0.38s
Step 11248: loss=2.8540, lr=0.000382, tokens/sec=1377965.35, grad_norm=0.3087, duration=0.38s
Step 11249: loss=2.8429, lr=0.000382, tokens/sec=1376780.83, grad_norm=0.2944, duration=0.38s
Validation loss at step 11250: 3.7749667167663574
Step 11250: loss=2.9030, lr=0.000382, tokens/sec=156738.30, grad_norm=0.2950, duration=3.34s
Step 11251: loss=2.9058, lr=0.000382, tokens/sec=1376869.62, grad_norm=0.3026, duration=0.38s
Step 11252: loss=2.9004, lr=0.000382, tokens/sec=1376080.39, grad_norm=0.2867, duration=0.38s
Step 11253: loss=2.9460, lr=0.000382, tokens/sec=1378979.81, grad_norm=0.3168, duration=0.38s
Step 11254: loss=2.8871, lr=0.000382, tokens/sec=1376448.19, grad_norm=0.2997, duration=0.38s
Step 11255: loss=2.9037, lr=0.000381, tokens/sec=1375685.26, grad_norm=0.3036, duration=0.38s
Step 11256: loss=2.9048, lr=0.000381, tokens/sec=1378049.12, grad_norm=0.2909, duration=0.38s
Step 11257: loss=2.9627, lr=0.000381, tokens/sec=1379241.01, grad_norm=0.3139, duration=0.38s
Step 11258: loss=3.0236, lr=0.000381, tokens/sec=1375871.18, grad_norm=0.3035, duration=0.38s
Step 11259: loss=2.9280, lr=0.000381, tokens/sec=1374650.72, grad_norm=0.3095, duration=0.38s
Step 11260: loss=2.9522, lr=0.000381, tokens/sec=1375309.27, grad_norm=0.3066, duration=0.38s
Step 11261: loss=2.9425, lr=0.000381, tokens/sec=1371581.22, grad_norm=0.3057, duration=0.38s
Step 11262: loss=2.9732, lr=0.000381, tokens/sec=1375443.47, grad_norm=0.3219, duration=0.38s
Step 11263: loss=2.9673, lr=0.000381, tokens/sec=1374657.59, grad_norm=0.3015, duration=0.38s
Step 11264: loss=2.8958, lr=0.000381, tokens/sec=1375233.58, grad_norm=0.3079, duration=0.38s
Step 11265: loss=2.9159, lr=0.000381, tokens/sec=1375003.99, grad_norm=0.3397, duration=0.38s
Step 11266: loss=2.9291, lr=0.000381, tokens/sec=1372966.80, grad_norm=0.3073, duration=0.38s
Step 11267: loss=2.9348, lr=0.000381, tokens/sec=1371134.80, grad_norm=0.3328, duration=0.38s
Step 11268: loss=2.9410, lr=0.000381, tokens/sec=1372232.56, grad_norm=0.3498, duration=0.38s
Step 11269: loss=2.9385, lr=0.000381, tokens/sec=1374594.01, grad_norm=0.3101, duration=0.38s
Step 11270: loss=2.9270, lr=0.000381, tokens/sec=1370328.22, grad_norm=0.3346, duration=0.38s
Step 11271: loss=2.8764, lr=0.000381, tokens/sec=1375284.33, grad_norm=0.3449, duration=0.38s
Step 11272: loss=2.8864, lr=0.000381, tokens/sec=1374581.12, grad_norm=0.3224, duration=0.38s
Step 11273: loss=2.9352, lr=0.000381, tokens/sec=1375363.46, grad_norm=0.3319, duration=0.38s
Step 11274: loss=2.9385, lr=0.000381, tokens/sec=1370300.90, grad_norm=0.3389, duration=0.38s
Step 11275: loss=2.9754, lr=0.000381, tokens/sec=1377452.65, grad_norm=0.3336, duration=0.38s
Step 11276: loss=2.8982, lr=0.000381, tokens/sec=1378867.41, grad_norm=0.3215, duration=0.38s
Step 11277: loss=2.9935, lr=0.000381, tokens/sec=1376646.37, grad_norm=0.3529, duration=0.38s
Step 11278: loss=2.9659, lr=0.000381, tokens/sec=1374884.49, grad_norm=0.3396, duration=0.38s
Step 11279: loss=2.9238, lr=0.000381, tokens/sec=1375040.10, grad_norm=0.3142, duration=0.38s
Step 11280: loss=2.9172, lr=0.000381, tokens/sec=1376736.87, grad_norm=0.3120, duration=0.38s
Step 11281: loss=2.9390, lr=0.000381, tokens/sec=1377064.48, grad_norm=0.3412, duration=0.38s
Step 11282: loss=2.9092, lr=0.000381, tokens/sec=1375301.53, grad_norm=0.3418, duration=0.38s
Step 11283: loss=2.9937, lr=0.000381, tokens/sec=1373168.27, grad_norm=0.3067, duration=0.38s
Step 11284: loss=2.9800, lr=0.000381, tokens/sec=1375353.14, grad_norm=0.3249, duration=0.38s
Step 11285: loss=2.9369, lr=0.000381, tokens/sec=1377393.98, grad_norm=0.3451, duration=0.38s
Step 11286: loss=2.9502, lr=0.000381, tokens/sec=1373826.27, grad_norm=0.3134, duration=0.38s
Step 11287: loss=2.8706, lr=0.000381, tokens/sec=1371116.85, grad_norm=0.3197, duration=0.38s
Step 11288: loss=2.9498, lr=0.000380, tokens/sec=1372027.93, grad_norm=0.3182, duration=0.38s
Step 11289: loss=2.9535, lr=0.000380, tokens/sec=1375715.38, grad_norm=0.3185, duration=0.38s
Step 11290: loss=2.9565, lr=0.000380, tokens/sec=1375830.72, grad_norm=0.3483, duration=0.38s
Step 11291: loss=2.9128, lr=0.000380, tokens/sec=1378463.76, grad_norm=0.3096, duration=0.38s
Step 11292: loss=2.9608, lr=0.000380, tokens/sec=1375749.81, grad_norm=0.3143, duration=0.38s
Step 11293: loss=2.8778, lr=0.000380, tokens/sec=1376713.60, grad_norm=0.3505, duration=0.38s
Step 11294: loss=2.9217, lr=0.000380, tokens/sec=1373019.09, grad_norm=0.3018, duration=0.38s
Step 11295: loss=2.8401, lr=0.000380, tokens/sec=1380700.20, grad_norm=0.2870, duration=0.38s
Step 11296: loss=2.8682, lr=0.000380, tokens/sec=1371711.27, grad_norm=0.3486, duration=0.38s
Step 11297: loss=2.9159, lr=0.000380, tokens/sec=1377973.99, grad_norm=0.3208, duration=0.38s
Step 11298: loss=2.8556, lr=0.000380, tokens/sec=1376928.25, grad_norm=0.2812, duration=0.38s
Step 11299: loss=2.8712, lr=0.000380, tokens/sec=1378319.47, grad_norm=0.3027, duration=0.38s
Step 11300/19073 (59.2%), Elapsed time: 4480.61s, Steps per hour: 9079.11, Estimated hours remaining: 0.86
Step 11300: loss=2.8341, lr=0.000380, tokens/sec=1376299.15, grad_norm=0.2973, duration=0.38s
Step 11301: loss=2.8859, lr=0.000380, tokens/sec=1371490.55, grad_norm=0.2840, duration=0.38s
Step 11302: loss=2.8919, lr=0.000380, tokens/sec=1375738.62, grad_norm=0.2955, duration=0.38s
Step 11303: loss=2.9123, lr=0.000380, tokens/sec=1380531.17, grad_norm=0.2951, duration=0.38s
Step 11304: loss=2.9744, lr=0.000380, tokens/sec=1373398.97, grad_norm=0.2898, duration=0.38s
Step 11305: loss=2.9518, lr=0.000380, tokens/sec=1377439.70, grad_norm=0.3050, duration=0.38s
Step 11306: loss=2.9553, lr=0.000380, tokens/sec=1379790.55, grad_norm=0.3168, duration=0.38s
Step 11307: loss=2.9704, lr=0.000380, tokens/sec=1373497.62, grad_norm=0.3064, duration=0.38s
Step 11308: loss=2.8959, lr=0.000380, tokens/sec=1377865.20, grad_norm=0.3356, duration=0.38s
Step 11309: loss=2.9621, lr=0.000380, tokens/sec=1372993.37, grad_norm=0.3212, duration=0.38s
Step 11310: loss=2.9207, lr=0.000380, tokens/sec=1370698.93, grad_norm=0.3368, duration=0.38s
Step 11311: loss=3.0066, lr=0.000380, tokens/sec=1375304.11, grad_norm=0.3173, duration=0.38s
Step 11312: loss=2.9769, lr=0.000380, tokens/sec=1374634.39, grad_norm=0.3457, duration=0.38s
Step 11313: loss=2.9598, lr=0.000380, tokens/sec=1374791.66, grad_norm=0.3206, duration=0.38s
Step 11314: loss=2.9633, lr=0.000380, tokens/sec=1368632.72, grad_norm=0.3156, duration=0.38s
Step 11315: loss=2.9356, lr=0.000380, tokens/sec=1374569.09, grad_norm=0.3302, duration=0.38s
Step 11316: loss=2.9273, lr=0.000380, tokens/sec=1375242.18, grad_norm=0.3214, duration=0.38s
Step 11317: loss=2.9385, lr=0.000380, tokens/sec=1375439.17, grad_norm=0.3098, duration=0.38s
Step 11318: loss=2.9493, lr=0.000380, tokens/sec=1371685.60, grad_norm=0.3432, duration=0.38s
Step 11319: loss=2.8981, lr=0.000380, tokens/sec=1377929.09, grad_norm=0.3037, duration=0.38s
Step 11320: loss=2.9383, lr=0.000380, tokens/sec=1376883.41, grad_norm=0.3316, duration=0.38s
Step 11321: loss=2.9193, lr=0.000379, tokens/sec=1372379.00, grad_norm=0.3345, duration=0.38s
Step 11322: loss=2.9636, lr=0.000379, tokens/sec=1371799.41, grad_norm=0.2965, duration=0.38s
Step 11323: loss=2.9001, lr=0.000379, tokens/sec=1374265.85, grad_norm=0.3166, duration=0.38s
Step 11324: loss=2.9443, lr=0.000379, tokens/sec=1373855.45, grad_norm=0.3234, duration=0.38s
Step 11325: loss=2.8810, lr=0.000379, tokens/sec=1374061.48, grad_norm=0.3108, duration=0.38s
Step 11326: loss=2.9446, lr=0.000379, tokens/sec=1374793.38, grad_norm=0.3268, duration=0.38s
Step 11327: loss=2.9095, lr=0.000379, tokens/sec=1374049.46, grad_norm=0.3119, duration=0.38s
Step 11328: loss=2.9461, lr=0.000379, tokens/sec=1374245.24, grad_norm=0.2948, duration=0.38s
Step 11329: loss=2.9106, lr=0.000379, tokens/sec=1373954.16, grad_norm=0.3136, duration=0.38s
Step 11330: loss=2.9036, lr=0.000379, tokens/sec=1373839.14, grad_norm=0.3254, duration=0.38s
Step 11331: loss=2.9150, lr=0.000379, tokens/sec=1378736.00, grad_norm=0.2923, duration=0.38s
Step 11332: loss=2.9200, lr=0.000379, tokens/sec=1374596.58, grad_norm=0.3054, duration=0.38s
Step 11333: loss=2.9082, lr=0.000379, tokens/sec=1374144.77, grad_norm=0.3132, duration=0.38s
Step 11334: loss=2.9265, lr=0.000379, tokens/sec=1377557.05, grad_norm=0.2965, duration=0.38s
Step 11335: loss=2.8961, lr=0.000379, tokens/sec=1373867.47, grad_norm=0.3075, duration=0.38s
Step 11336: loss=2.9085, lr=0.000379, tokens/sec=1377097.25, grad_norm=0.3230, duration=0.38s
Step 11337: loss=2.9404, lr=0.000379, tokens/sec=1377933.41, grad_norm=0.3091, duration=0.38s
Step 11338: loss=2.9397, lr=0.000379, tokens/sec=1374454.82, grad_norm=0.2767, duration=0.38s
Step 11339: loss=2.8721, lr=0.000379, tokens/sec=1374540.73, grad_norm=0.3169, duration=0.38s
Step 11340: loss=2.8894, lr=0.000379, tokens/sec=1375792.84, grad_norm=0.2972, duration=0.38s
Step 11341: loss=2.8292, lr=0.000379, tokens/sec=1379598.38, grad_norm=0.2900, duration=0.38s
Step 11342: loss=2.8593, lr=0.000379, tokens/sec=1373834.85, grad_norm=0.2978, duration=0.38s
Step 11343: loss=2.8838, lr=0.000379, tokens/sec=1373486.47, grad_norm=0.3027, duration=0.38s
Step 11344: loss=2.9033, lr=0.000379, tokens/sec=1378893.35, grad_norm=0.2816, duration=0.38s
Step 11345: loss=2.8967, lr=0.000379, tokens/sec=1377327.55, grad_norm=0.2851, duration=0.38s
Step 11346: loss=2.8706, lr=0.000379, tokens/sec=1376653.27, grad_norm=0.3009, duration=0.38s
Step 11347: loss=2.8838, lr=0.000379, tokens/sec=1376135.51, grad_norm=0.3098, duration=0.38s
Step 11348: loss=2.9206, lr=0.000379, tokens/sec=1377509.59, grad_norm=0.2843, duration=0.38s
Step 11349: loss=2.9372, lr=0.000379, tokens/sec=1376739.46, grad_norm=0.3025, duration=0.38s
Step 11350: loss=2.9490, lr=0.000379, tokens/sec=1375014.31, grad_norm=0.3237, duration=0.38s
Step 11351: loss=2.9961, lr=0.000379, tokens/sec=1376343.08, grad_norm=0.3169, duration=0.38s
Step 11352: loss=2.9329, lr=0.000379, tokens/sec=1377438.84, grad_norm=0.3116, duration=0.38s
Step 11353: loss=2.9580, lr=0.000379, tokens/sec=1376553.30, grad_norm=0.3229, duration=0.38s
Step 11354: loss=2.9840, lr=0.000378, tokens/sec=1376795.48, grad_norm=0.3316, duration=0.38s
Step 11355: loss=2.9505, lr=0.000378, tokens/sec=1379613.96, grad_norm=0.3171, duration=0.38s
Step 11356: loss=2.9349, lr=0.000378, tokens/sec=1374718.61, grad_norm=0.3152, duration=0.38s
Step 11357: loss=2.9993, lr=0.000378, tokens/sec=1376433.54, grad_norm=0.3271, duration=0.38s
Step 11358: loss=3.0077, lr=0.000378, tokens/sec=1374667.90, grad_norm=0.3670, duration=0.38s
Step 11359: loss=3.0066, lr=0.000378, tokens/sec=1377561.37, grad_norm=0.3104, duration=0.38s
Step 11360: loss=2.9969, lr=0.000378, tokens/sec=1375738.62, grad_norm=0.3361, duration=0.38s
Step 11361: loss=2.9257, lr=0.000378, tokens/sec=1376909.28, grad_norm=0.3198, duration=0.38s
Step 11362: loss=2.9718, lr=0.000378, tokens/sec=1375508.86, grad_norm=0.3080, duration=0.38s
Step 11363: loss=2.9139, lr=0.000378, tokens/sec=1372465.51, grad_norm=0.3052, duration=0.38s
Step 11364: loss=2.8856, lr=0.000378, tokens/sec=1377273.20, grad_norm=0.3195, duration=0.38s
Step 11365: loss=2.9684, lr=0.000378, tokens/sec=1375814.36, grad_norm=0.3244, duration=0.38s
Step 11366: loss=2.8883, lr=0.000378, tokens/sec=1376257.80, grad_norm=0.2969, duration=0.38s
Step 11367: loss=3.0071, lr=0.000378, tokens/sec=1375180.26, grad_norm=0.3075, duration=0.38s
Step 11368: loss=2.9933, lr=0.000378, tokens/sec=1371397.32, grad_norm=0.3341, duration=0.38s
Step 11369: loss=2.9370, lr=0.000378, tokens/sec=1371433.24, grad_norm=0.2956, duration=0.38s
Step 11370: loss=2.9011, lr=0.000378, tokens/sec=1374638.69, grad_norm=0.2904, duration=0.38s
Step 11371: loss=2.9688, lr=0.000378, tokens/sec=1378089.71, grad_norm=0.3384, duration=0.38s
Step 11372: loss=3.0024, lr=0.000378, tokens/sec=1373182.85, grad_norm=0.2941, duration=0.38s
Step 11373: loss=2.9395, lr=0.000378, tokens/sec=1376961.01, grad_norm=0.2867, duration=0.38s
Step 11374: loss=2.8781, lr=0.000378, tokens/sec=1373670.93, grad_norm=0.3227, duration=0.38s
Step 11375: loss=2.9022, lr=0.000378, tokens/sec=1377200.75, grad_norm=0.3104, duration=0.38s
Step 11376: loss=2.9311, lr=0.000378, tokens/sec=1377284.41, grad_norm=0.2933, duration=0.38s
Step 11377: loss=2.9254, lr=0.000378, tokens/sec=1375779.07, grad_norm=0.3200, duration=0.38s
Step 11378: loss=2.9820, lr=0.000378, tokens/sec=1373368.09, grad_norm=0.3174, duration=0.38s
Step 11379: loss=2.8962, lr=0.000378, tokens/sec=1377203.33, grad_norm=0.2852, duration=0.38s
Step 11380: loss=2.8870, lr=0.000378, tokens/sec=1374584.55, grad_norm=0.3161, duration=0.38s
Step 11381: loss=2.9298, lr=0.000378, tokens/sec=1376762.73, grad_norm=0.3189, duration=0.38s
Step 11382: loss=2.8718, lr=0.000378, tokens/sec=1377117.95, grad_norm=0.2910, duration=0.38s
Step 11383: loss=2.8470, lr=0.000378, tokens/sec=1379852.02, grad_norm=0.3222, duration=0.38s
Step 11384: loss=2.8597, lr=0.000378, tokens/sec=1377363.78, grad_norm=0.2889, duration=0.38s
Step 11385: loss=2.8477, lr=0.000378, tokens/sec=1375480.46, grad_norm=0.3190, duration=0.38s
Step 11386: loss=2.8828, lr=0.000378, tokens/sec=1375170.80, grad_norm=0.2989, duration=0.38s
Step 11387: loss=2.9328, lr=0.000377, tokens/sec=1376756.70, grad_norm=0.2771, duration=0.38s
Step 11388: loss=2.8884, lr=0.000377, tokens/sec=1376932.56, grad_norm=0.2876, duration=0.38s
Step 11389: loss=2.8473, lr=0.000377, tokens/sec=1375785.96, grad_norm=0.3041, duration=0.38s
Step 11390: loss=2.8318, lr=0.000377, tokens/sec=1381719.56, grad_norm=0.2774, duration=0.38s
Step 11391: loss=2.8933, lr=0.000377, tokens/sec=1377506.14, grad_norm=0.3041, duration=0.38s
Step 11392: loss=2.9203, lr=0.000377, tokens/sec=1375647.39, grad_norm=0.2939, duration=0.38s
Step 11393: loss=2.8856, lr=0.000377, tokens/sec=1375191.44, grad_norm=0.2722, duration=0.38s
Step 11394: loss=2.9106, lr=0.000377, tokens/sec=1372217.14, grad_norm=0.3122, duration=0.38s
Step 11395: loss=2.9468, lr=0.000377, tokens/sec=1375171.66, grad_norm=0.3077, duration=0.38s
Step 11396: loss=2.9499, lr=0.000377, tokens/sec=1375810.92, grad_norm=0.2893, duration=0.38s
Step 11397: loss=2.9617, lr=0.000377, tokens/sec=1372205.16, grad_norm=0.2845, duration=0.38s
Step 11398: loss=2.9080, lr=0.000377, tokens/sec=1379820.85, grad_norm=0.3049, duration=0.38s
Step 11399: loss=2.9462, lr=0.000377, tokens/sec=1377561.37, grad_norm=0.3014, duration=0.38s
Step 11400/19073 (59.8%), Elapsed time: 4518.82s, Steps per hour: 9082.01, Estimated hours remaining: 0.84
Step 11400: loss=2.9218, lr=0.000377, tokens/sec=1375804.89, grad_norm=0.2858, duration=0.38s
Step 11401: loss=2.9540, lr=0.000377, tokens/sec=1376337.91, grad_norm=0.3072, duration=0.38s
Step 11402: loss=2.9885, lr=0.000377, tokens/sec=1376967.04, grad_norm=0.2855, duration=0.38s
Step 11403: loss=3.0304, lr=0.000377, tokens/sec=1378898.53, grad_norm=0.2991, duration=0.38s
Step 11404: loss=2.9734, lr=0.000377, tokens/sec=1371268.19, grad_norm=0.3051, duration=0.38s
Step 11405: loss=3.0147, lr=0.000377, tokens/sec=1376786.00, grad_norm=0.3128, duration=0.38s
Step 11406: loss=3.0178, lr=0.000377, tokens/sec=1380841.52, grad_norm=0.3367, duration=0.38s
Step 11407: loss=2.9868, lr=0.000377, tokens/sec=1376219.05, grad_norm=0.2919, duration=0.38s
Step 11408: loss=2.9899, lr=0.000377, tokens/sec=1374869.02, grad_norm=0.3321, duration=0.38s
Step 11409: loss=2.9586, lr=0.000377, tokens/sec=1372247.97, grad_norm=0.3421, duration=0.38s
Step 11410: loss=2.9859, lr=0.000377, tokens/sec=1378606.35, grad_norm=0.3089, duration=0.38s
Step 11411: loss=2.9463, lr=0.000377, tokens/sec=1378152.75, grad_norm=0.3423, duration=0.38s
Step 11412: loss=2.9703, lr=0.000377, tokens/sec=1375320.45, grad_norm=0.3173, duration=0.38s
Step 11413: loss=2.9094, lr=0.000377, tokens/sec=1374149.06, grad_norm=0.3290, duration=0.38s
Step 11414: loss=2.9744, lr=0.000377, tokens/sec=1379667.63, grad_norm=0.3524, duration=0.38s
Step 11415: loss=2.9421, lr=0.000377, tokens/sec=1379041.21, grad_norm=0.3282, duration=0.38s
Step 11416: loss=2.9033, lr=0.000377, tokens/sec=1375439.17, grad_norm=0.3270, duration=0.38s
Step 11417: loss=3.0018, lr=0.000377, tokens/sec=1374887.93, grad_norm=0.3771, duration=0.38s
Step 11418: loss=2.8841, lr=0.000377, tokens/sec=1379812.20, grad_norm=0.3166, duration=0.38s
Step 11419: loss=2.8505, lr=0.000377, tokens/sec=1373576.55, grad_norm=0.3383, duration=0.38s
Step 11420: loss=2.8986, lr=0.000376, tokens/sec=1374603.46, grad_norm=0.3829, duration=0.38s
Step 11421: loss=2.9419, lr=0.000376, tokens/sec=1374412.73, grad_norm=0.3343, duration=0.38s
Step 11422: loss=3.0193, lr=0.000376, tokens/sec=1377314.61, grad_norm=0.2939, duration=0.38s
Step 11423: loss=2.8409, lr=0.000376, tokens/sec=1377751.25, grad_norm=0.3339, duration=0.38s
Step 11424: loss=2.9602, lr=0.000376, tokens/sec=1377681.33, grad_norm=0.3318, duration=0.38s
Step 11425: loss=2.9407, lr=0.000376, tokens/sec=1377522.54, grad_norm=0.2986, duration=0.38s
Step 11426: loss=2.8803, lr=0.000376, tokens/sec=1374920.60, grad_norm=0.3134, duration=0.38s
Step 11427: loss=2.9241, lr=0.000376, tokens/sec=1379949.00, grad_norm=0.3216, duration=0.38s
Step 11428: loss=2.9509, lr=0.000376, tokens/sec=1380386.45, grad_norm=0.2781, duration=0.38s
Step 11429: loss=2.8615, lr=0.000376, tokens/sec=1372982.23, grad_norm=0.3252, duration=0.38s
Step 11430: loss=2.8865, lr=0.000376, tokens/sec=1377266.30, grad_norm=0.3192, duration=0.38s
Step 11431: loss=2.9175, lr=0.000376, tokens/sec=1372675.41, grad_norm=0.2934, duration=0.38s
Step 11432: loss=2.9611, lr=0.000376, tokens/sec=1373091.96, grad_norm=0.3185, duration=0.38s
Step 11433: loss=2.9925, lr=0.000376, tokens/sec=1374429.05, grad_norm=0.3292, duration=0.38s
Step 11434: loss=2.9336, lr=0.000376, tokens/sec=1371466.60, grad_norm=0.3051, duration=0.38s
Step 11435: loss=2.8722, lr=0.000376, tokens/sec=1377847.07, grad_norm=0.3039, duration=0.38s
Step 11436: loss=2.9181, lr=0.000376, tokens/sec=1376752.39, grad_norm=0.3219, duration=0.38s
Step 11437: loss=2.8503, lr=0.000376, tokens/sec=1372001.39, grad_norm=0.2995, duration=0.38s
Step 11438: loss=2.8242, lr=0.000376, tokens/sec=1377161.07, grad_norm=0.3085, duration=0.38s
Step 11439: loss=2.8873, lr=0.000376, tokens/sec=1372521.19, grad_norm=0.3130, duration=0.38s
Step 11440: loss=2.8971, lr=0.000376, tokens/sec=1376540.38, grad_norm=0.3071, duration=0.38s
Step 11441: loss=2.8930, lr=0.000376, tokens/sec=1374257.26, grad_norm=0.3021, duration=0.38s
Step 11442: loss=2.9231, lr=0.000376, tokens/sec=1375722.27, grad_norm=0.3108, duration=0.38s
Step 11443: loss=2.9064, lr=0.000376, tokens/sec=1378541.53, grad_norm=0.3302, duration=0.38s
Step 11444: loss=2.9024, lr=0.000376, tokens/sec=1375158.76, grad_norm=0.2972, duration=0.38s
Step 11445: loss=2.8871, lr=0.000376, tokens/sec=1372713.97, grad_norm=0.3134, duration=0.38s
Step 11446: loss=2.8912, lr=0.000376, tokens/sec=1377656.30, grad_norm=0.3092, duration=0.38s
Step 11447: loss=2.9768, lr=0.000376, tokens/sec=1374345.73, grad_norm=0.3068, duration=0.38s
Step 11448: loss=2.9660, lr=0.000376, tokens/sec=1373266.03, grad_norm=0.3073, duration=0.38s
Step 11449: loss=2.9532, lr=0.000376, tokens/sec=1378249.49, grad_norm=0.3247, duration=0.38s
Step 11450: loss=2.9153, lr=0.000376, tokens/sec=1372659.13, grad_norm=0.3139, duration=0.38s
Step 11451: loss=2.9726, lr=0.000376, tokens/sec=1374207.45, grad_norm=0.3177, duration=0.38s
Step 11452: loss=2.9636, lr=0.000376, tokens/sec=1376430.95, grad_norm=0.3083, duration=0.38s
Step 11453: loss=2.9270, lr=0.000375, tokens/sec=1373250.60, grad_norm=0.3253, duration=0.38s
Step 11454: loss=2.9176, lr=0.000375, tokens/sec=1377224.90, grad_norm=0.3224, duration=0.38s
Step 11455: loss=2.8982, lr=0.000375, tokens/sec=1376040.78, grad_norm=0.3304, duration=0.38s
Step 11456: loss=2.9426, lr=0.000375, tokens/sec=1373592.85, grad_norm=0.3176, duration=0.38s
Step 11457: loss=2.9423, lr=0.000375, tokens/sec=1376708.43, grad_norm=0.3241, duration=0.38s
Step 11458: loss=2.9355, lr=0.000375, tokens/sec=1377376.72, grad_norm=0.3246, duration=0.38s
Step 11459: loss=2.9201, lr=0.000375, tokens/sec=1370256.50, grad_norm=0.3113, duration=0.38s
Step 11460: loss=2.9477, lr=0.000375, tokens/sec=1377361.19, grad_norm=0.3252, duration=0.38s
Step 11461: loss=2.8269, lr=0.000375, tokens/sec=1375218.96, grad_norm=0.3343, duration=0.38s
Step 11462: loss=2.8891, lr=0.000375, tokens/sec=1376789.45, grad_norm=0.3216, duration=0.38s
Step 11463: loss=2.9383, lr=0.000375, tokens/sec=1374923.18, grad_norm=0.3170, duration=0.38s
Step 11464: loss=2.9288, lr=0.000375, tokens/sec=1377556.19, grad_norm=0.3349, duration=0.38s
Step 11465: loss=2.9512, lr=0.000375, tokens/sec=1375203.48, grad_norm=0.3171, duration=0.38s
Step 11466: loss=2.9101, lr=0.000375, tokens/sec=1376911.00, grad_norm=0.3225, duration=0.38s
Step 11467: loss=3.0021, lr=0.000375, tokens/sec=1372062.17, grad_norm=0.3331, duration=0.38s
Step 11468: loss=2.9326, lr=0.000375, tokens/sec=1377369.82, grad_norm=0.3160, duration=0.38s
Step 11469: loss=2.9128, lr=0.000375, tokens/sec=1378722.17, grad_norm=0.3176, duration=0.38s
Step 11470: loss=2.9285, lr=0.000375, tokens/sec=1376444.74, grad_norm=0.3141, duration=0.38s
Step 11471: loss=2.9285, lr=0.000375, tokens/sec=1374714.31, grad_norm=0.3123, duration=0.38s
Step 11472: loss=2.9689, lr=0.000375, tokens/sec=1376527.45, grad_norm=0.3492, duration=0.38s
Step 11473: loss=2.9762, lr=0.000375, tokens/sec=1374832.92, grad_norm=0.3157, duration=0.38s
Step 11474: loss=2.9350, lr=0.000375, tokens/sec=1374009.96, grad_norm=0.3066, duration=0.38s
Step 11475: loss=2.9368, lr=0.000375, tokens/sec=1375674.07, grad_norm=0.3306, duration=0.38s
Step 11476: loss=2.9096, lr=0.000375, tokens/sec=1374832.06, grad_norm=0.3035, duration=0.38s
Step 11477: loss=2.9091, lr=0.000375, tokens/sec=1376809.28, grad_norm=0.3231, duration=0.38s
Step 11478: loss=2.9595, lr=0.000375, tokens/sec=1376908.42, grad_norm=0.3145, duration=0.38s
Step 11479: loss=2.9360, lr=0.000375, tokens/sec=1379420.97, grad_norm=0.2949, duration=0.38s
Step 11480: loss=2.9805, lr=0.000375, tokens/sec=1371882.42, grad_norm=0.3299, duration=0.38s
Step 11481: loss=2.8932, lr=0.000375, tokens/sec=1378166.57, grad_norm=0.3307, duration=0.38s
Step 11482: loss=2.9232, lr=0.000375, tokens/sec=1375565.64, grad_norm=0.2946, duration=0.38s
Step 11483: loss=2.9078, lr=0.000375, tokens/sec=1375890.98, grad_norm=0.3391, duration=0.38s
Step 11484: loss=2.8861, lr=0.000375, tokens/sec=1376619.66, grad_norm=0.3404, duration=0.38s
Step 11485: loss=2.8315, lr=0.000375, tokens/sec=1377366.37, grad_norm=0.2932, duration=0.38s
Step 11486: loss=2.8901, lr=0.000374, tokens/sec=1376640.34, grad_norm=0.3369, duration=0.38s
Step 11487: loss=2.8968, lr=0.000374, tokens/sec=1375767.88, grad_norm=0.3359, duration=0.38s
Step 11488: loss=2.8463, lr=0.000374, tokens/sec=1376630.86, grad_norm=0.2917, duration=0.38s
Step 11489: loss=2.8474, lr=0.000374, tokens/sec=1378759.34, grad_norm=0.3123, duration=0.38s
Step 11490: loss=2.8260, lr=0.000374, tokens/sec=1374234.07, grad_norm=0.3076, duration=0.38s
Step 11491: loss=2.9024, lr=0.000374, tokens/sec=1374425.61, grad_norm=0.2875, duration=0.38s
Step 11492: loss=2.8831, lr=0.000374, tokens/sec=1379934.28, grad_norm=0.2885, duration=0.38s
Step 11493: loss=2.9277, lr=0.000374, tokens/sec=1379141.54, grad_norm=0.3067, duration=0.38s
Step 11494: loss=2.9429, lr=0.000374, tokens/sec=1375142.43, grad_norm=0.2977, duration=0.38s
Step 11495: loss=2.9421, lr=0.000374, tokens/sec=1376325.85, grad_norm=0.2942, duration=0.38s
Step 11496: loss=2.9623, lr=0.000374, tokens/sec=1377296.49, grad_norm=0.3196, duration=0.38s
Step 11497: loss=2.9528, lr=0.000374, tokens/sec=1379209.01, grad_norm=0.3153, duration=0.38s
Step 11498: loss=2.9072, lr=0.000374, tokens/sec=1377873.83, grad_norm=0.3007, duration=0.38s
Step 11499: loss=2.9517, lr=0.000374, tokens/sec=1373630.60, grad_norm=0.3342, duration=0.38s
Step 11500/19073 (60.3%), Elapsed time: 4557.01s, Steps per hour: 9084.91, Estimated hours remaining: 0.83
Validation loss at step 11500: 3.7759480476379395
Step 11500: loss=2.9423, lr=0.000374, tokens/sec=153771.91, grad_norm=0.3335, duration=3.41s
Step 11501: loss=2.9804, lr=0.000374, tokens/sec=1377571.73, grad_norm=0.3031, duration=0.38s
Step 11502: loss=2.9642, lr=0.000374, tokens/sec=1377539.80, grad_norm=0.3309, duration=0.38s
Step 11503: loss=2.9521, lr=0.000374, tokens/sec=1376798.93, grad_norm=0.3449, duration=0.38s
Step 11504: loss=2.9600, lr=0.000374, tokens/sec=1375283.47, grad_norm=0.3057, duration=0.38s
Step 11505: loss=2.9117, lr=0.000374, tokens/sec=1373806.53, grad_norm=0.3200, duration=0.38s
Step 11506: loss=2.9308, lr=0.000374, tokens/sec=1375975.35, grad_norm=0.3362, duration=0.38s
Step 11507: loss=2.9382, lr=0.000374, tokens/sec=1376826.52, grad_norm=0.3112, duration=0.38s
Step 11508: loss=2.9436, lr=0.000374, tokens/sec=1376150.15, grad_norm=0.2961, duration=0.38s
Step 11509: loss=2.8775, lr=0.000374, tokens/sec=1376796.35, grad_norm=0.2977, duration=0.38s
Step 11510: loss=2.9257, lr=0.000374, tokens/sec=1372122.96, grad_norm=0.3157, duration=0.38s
Step 11511: loss=2.9443, lr=0.000374, tokens/sec=1372264.24, grad_norm=0.3238, duration=0.38s
Step 11512: loss=2.9567, lr=0.000374, tokens/sec=1371230.56, grad_norm=0.2973, duration=0.38s
Step 11513: loss=2.8693, lr=0.000374, tokens/sec=1370448.64, grad_norm=0.2952, duration=0.38s
Step 11514: loss=2.9313, lr=0.000374, tokens/sec=1373209.43, grad_norm=0.3400, duration=0.38s
Step 11515: loss=2.8866, lr=0.000374, tokens/sec=1377684.78, grad_norm=0.3170, duration=0.38s
Step 11516: loss=2.9317, lr=0.000374, tokens/sec=1374119.01, grad_norm=0.3080, duration=0.38s
Step 11517: loss=2.9216, lr=0.000374, tokens/sec=1378530.30, grad_norm=0.3185, duration=0.38s
Step 11518: loss=2.9227, lr=0.000374, tokens/sec=1375631.90, grad_norm=0.3043, duration=0.38s
Step 11519: loss=2.8865, lr=0.000373, tokens/sec=1373758.46, grad_norm=0.3025, duration=0.38s
Step 11520: loss=2.9141, lr=0.000373, tokens/sec=1373666.64, grad_norm=0.3222, duration=0.38s
Step 11521: loss=2.9114, lr=0.000373, tokens/sec=1371267.33, grad_norm=0.3034, duration=0.38s
Step 11522: loss=2.9210, lr=0.000373, tokens/sec=1373479.60, grad_norm=0.3013, duration=0.38s
Step 11523: loss=2.9062, lr=0.000373, tokens/sec=1371299.82, grad_norm=0.3248, duration=0.38s
Step 11524: loss=2.8931, lr=0.000373, tokens/sec=1377645.08, grad_norm=0.2935, duration=0.38s
Step 11525: loss=2.9116, lr=0.000373, tokens/sec=1373778.20, grad_norm=0.3051, duration=0.38s
Step 11526: loss=2.9427, lr=0.000373, tokens/sec=1375009.15, grad_norm=0.3199, duration=0.38s
Step 11527: loss=2.9397, lr=0.000373, tokens/sec=1377742.62, grad_norm=0.3117, duration=0.38s
Step 11528: loss=2.9193, lr=0.000373, tokens/sec=1374992.81, grad_norm=0.2817, duration=0.38s
Step 11529: loss=2.8500, lr=0.000373, tokens/sec=1377955.86, grad_norm=0.2993, duration=0.38s
Step 11530: loss=2.9029, lr=0.000373, tokens/sec=1372639.42, grad_norm=0.2993, duration=0.38s
Step 11531: loss=2.8084, lr=0.000373, tokens/sec=1376866.17, grad_norm=0.2908, duration=0.38s
Step 11532: loss=2.8766, lr=0.000373, tokens/sec=1373325.21, grad_norm=0.2941, duration=0.38s
Step 11533: loss=2.8359, lr=0.000373, tokens/sec=1374847.53, grad_norm=0.2932, duration=0.38s
Step 11534: loss=2.9248, lr=0.000373, tokens/sec=1379748.13, grad_norm=0.2914, duration=0.38s
Step 11535: loss=2.8856, lr=0.000373, tokens/sec=1374263.27, grad_norm=0.2816, duration=0.38s
Step 11536: loss=2.8921, lr=0.000373, tokens/sec=1374998.83, grad_norm=0.2883, duration=0.38s
Step 11537: loss=2.8870, lr=0.000373, tokens/sec=1377879.88, grad_norm=0.3108, duration=0.38s
Step 11538: loss=2.8937, lr=0.000373, tokens/sec=1376632.59, grad_norm=0.2947, duration=0.38s
Step 11539: loss=2.9279, lr=0.000373, tokens/sec=1378936.58, grad_norm=0.2997, duration=0.38s
Step 11540: loss=2.9719, lr=0.000373, tokens/sec=1377931.68, grad_norm=0.3130, duration=0.38s
Step 11541: loss=2.9240, lr=0.000373, tokens/sec=1376174.26, grad_norm=0.3204, duration=0.38s
Step 11542: loss=2.9755, lr=0.000373, tokens/sec=1374877.62, grad_norm=0.3135, duration=0.38s
Step 11543: loss=2.9537, lr=0.000373, tokens/sec=1378591.65, grad_norm=0.3065, duration=0.38s
Step 11544: loss=3.0004, lr=0.000373, tokens/sec=1372803.09, grad_norm=0.3605, duration=0.38s
Step 11545: loss=2.9035, lr=0.000373, tokens/sec=1376449.91, grad_norm=0.3270, duration=0.38s
Step 11546: loss=2.9448, lr=0.000373, tokens/sec=1370939.05, grad_norm=0.3024, duration=0.38s
Step 11547: loss=2.9410, lr=0.000373, tokens/sec=1375315.29, grad_norm=0.3757, duration=0.38s
Step 11548: loss=3.0579, lr=0.000373, tokens/sec=1376085.56, grad_norm=0.3437, duration=0.38s
Step 11549: loss=3.0027, lr=0.000373, tokens/sec=1376590.36, grad_norm=0.3295, duration=0.38s
Step 11550: loss=2.9729, lr=0.000373, tokens/sec=1373161.41, grad_norm=0.3267, duration=0.38s
Step 11551: loss=2.9397, lr=0.000373, tokens/sec=1375516.60, grad_norm=0.3445, duration=0.38s
Step 11552: loss=2.9459, lr=0.000372, tokens/sec=1378278.87, grad_norm=0.3237, duration=0.38s
Step 11553: loss=2.9120, lr=0.000372, tokens/sec=1372695.11, grad_norm=0.3037, duration=0.38s
Step 11554: loss=2.8697, lr=0.000372, tokens/sec=1375351.42, grad_norm=0.3367, duration=0.38s
Step 11555: loss=2.9716, lr=0.000372, tokens/sec=1377626.96, grad_norm=0.3245, duration=0.38s
Step 11556: loss=2.9124, lr=0.000372, tokens/sec=1379594.05, grad_norm=0.3090, duration=0.38s
Step 11557: loss=2.9649, lr=0.000372, tokens/sec=1378706.61, grad_norm=0.3035, duration=0.38s
Step 11558: loss=2.9844, lr=0.000372, tokens/sec=1374807.99, grad_norm=0.3415, duration=0.38s
Step 11559: loss=2.9448, lr=0.000372, tokens/sec=1378001.62, grad_norm=0.3097, duration=0.38s
Step 11560: loss=2.9079, lr=0.000372, tokens/sec=1376051.12, grad_norm=0.2893, duration=0.38s
Step 11561: loss=2.9788, lr=0.000372, tokens/sec=1373500.19, grad_norm=0.3538, duration=0.38s
Step 11562: loss=2.9702, lr=0.000372, tokens/sec=1376067.48, grad_norm=0.3091, duration=0.38s
Step 11563: loss=2.9428, lr=0.000372, tokens/sec=1374757.28, grad_norm=0.2812, duration=0.38s
Step 11564: loss=2.8421, lr=0.000372, tokens/sec=1376321.55, grad_norm=0.3206, duration=0.38s
Step 11565: loss=2.9222, lr=0.000372, tokens/sec=1374760.72, grad_norm=0.3272, duration=0.38s
Step 11566: loss=2.8998, lr=0.000372, tokens/sec=1375095.99, grad_norm=0.3030, duration=0.38s
Step 11567: loss=2.9495, lr=0.000372, tokens/sec=1372260.82, grad_norm=0.3171, duration=0.38s
Step 11568: loss=2.9700, lr=0.000372, tokens/sec=1377554.47, grad_norm=0.3061, duration=0.38s
Step 11569: loss=2.8738, lr=0.000372, tokens/sec=1373957.60, grad_norm=0.2872, duration=0.38s
Step 11570: loss=2.9205, lr=0.000372, tokens/sec=1375668.05, grad_norm=0.2961, duration=0.38s
Step 11571: loss=2.9090, lr=0.000372, tokens/sec=1375550.16, grad_norm=0.3088, duration=0.38s
Step 11572: loss=2.8060, lr=0.000372, tokens/sec=1375077.93, grad_norm=0.3137, duration=0.38s
Step 11573: loss=2.8928, lr=0.000372, tokens/sec=1373626.31, grad_norm=0.3168, duration=0.38s
Step 11574: loss=2.8527, lr=0.000372, tokens/sec=1376873.07, grad_norm=0.2868, duration=0.38s
Step 11575: loss=2.8718, lr=0.000372, tokens/sec=1374643.84, grad_norm=0.2931, duration=0.38s
Step 11576: loss=2.8687, lr=0.000372, tokens/sec=1373462.45, grad_norm=0.3171, duration=0.38s
Step 11577: loss=2.9340, lr=0.000372, tokens/sec=1377418.13, grad_norm=0.2949, duration=0.38s
Step 11578: loss=2.8649, lr=0.000372, tokens/sec=1375138.13, grad_norm=0.2828, duration=0.38s
Step 11579: loss=2.8596, lr=0.000372, tokens/sec=1374271.00, grad_norm=0.3049, duration=0.38s
Step 11580: loss=2.8592, lr=0.000372, tokens/sec=1377143.82, grad_norm=0.2860, duration=0.38s
Step 11581: loss=2.8783, lr=0.000372, tokens/sec=1379904.84, grad_norm=0.2935, duration=0.38s
Step 11582: loss=2.9006, lr=0.000372, tokens/sec=1373537.08, grad_norm=0.3075, duration=0.38s
Step 11583: loss=2.8837, lr=0.000372, tokens/sec=1369446.69, grad_norm=0.2868, duration=0.38s
Step 11584: loss=2.8981, lr=0.000372, tokens/sec=1371048.46, grad_norm=0.2841, duration=0.38s
Step 11585: loss=2.9481, lr=0.000371, tokens/sec=1374352.60, grad_norm=0.3254, duration=0.38s
Step 11586: loss=2.9514, lr=0.000371, tokens/sec=1373861.46, grad_norm=0.3011, duration=0.38s
Step 11587: loss=2.9327, lr=0.000371, tokens/sec=1369851.90, grad_norm=0.2897, duration=0.38s
Step 11588: loss=2.9117, lr=0.000371, tokens/sec=1373954.16, grad_norm=0.3119, duration=0.38s
Step 11589: loss=2.9445, lr=0.000371, tokens/sec=1375286.91, grad_norm=0.3012, duration=0.38s
Step 11590: loss=2.9147, lr=0.000371, tokens/sec=1377108.46, grad_norm=0.2975, duration=0.38s
Step 11591: loss=2.9801, lr=0.000371, tokens/sec=1375745.50, grad_norm=0.3039, duration=0.38s
Step 11592: loss=2.9605, lr=0.000371, tokens/sec=1374933.49, grad_norm=0.2960, duration=0.38s
Step 11593: loss=3.0347, lr=0.000371, tokens/sec=1375679.23, grad_norm=0.2913, duration=0.38s
Step 11594: loss=2.9879, lr=0.000371, tokens/sec=1375958.99, grad_norm=0.3070, duration=0.38s
Step 11595: loss=2.9685, lr=0.000371, tokens/sec=1373787.64, grad_norm=0.3029, duration=0.38s
Step 11596: loss=3.0508, lr=0.000371, tokens/sec=1377557.92, grad_norm=0.3421, duration=0.38s
Step 11597: loss=2.9704, lr=0.000371, tokens/sec=1374924.04, grad_norm=0.2956, duration=0.38s
Step 11598: loss=2.9721, lr=0.000371, tokens/sec=1369852.75, grad_norm=0.3108, duration=0.38s
Step 11599: loss=2.9676, lr=0.000371, tokens/sec=1369953.45, grad_norm=0.3328, duration=0.38s
Step 11600/19073 (60.8%), Elapsed time: 4598.25s, Steps per hour: 9081.72, Estimated hours remaining: 0.82
Step 11600: loss=2.9735, lr=0.000371, tokens/sec=1376706.70, grad_norm=0.3048, duration=0.38s
Step 11601: loss=2.9541, lr=0.000371, tokens/sec=1376538.66, grad_norm=0.3081, duration=0.38s
Step 11602: loss=2.9643, lr=0.000371, tokens/sec=1373150.27, grad_norm=0.3149, duration=0.38s
Step 11603: loss=2.8943, lr=0.000371, tokens/sec=1375175.96, grad_norm=0.3035, duration=0.38s
Step 11604: loss=2.9559, lr=0.000371, tokens/sec=1373152.84, grad_norm=0.3228, duration=0.38s
Step 11605: loss=2.9432, lr=0.000371, tokens/sec=1375613.83, grad_norm=0.3420, duration=0.38s
Step 11606: loss=2.9244, lr=0.000371, tokens/sec=1377096.39, grad_norm=0.3056, duration=0.38s
Step 11607: loss=2.9594, lr=0.000371, tokens/sec=1376309.49, grad_norm=0.3446, duration=0.38s
Step 11608: loss=2.8791, lr=0.000371, tokens/sec=1373598.00, grad_norm=0.3234, duration=0.38s
Step 11609: loss=2.8429, lr=0.000371, tokens/sec=1375900.45, grad_norm=0.3647, duration=0.38s
Step 11610: loss=2.8994, lr=0.000371, tokens/sec=1377924.77, grad_norm=0.3575, duration=0.38s
Step 11611: loss=2.9391, lr=0.000371, tokens/sec=1375409.92, grad_norm=0.3297, duration=0.38s
Step 11612: loss=2.9853, lr=0.000371, tokens/sec=1376820.48, grad_norm=0.3031, duration=0.38s
Step 11613: loss=2.8507, lr=0.000371, tokens/sec=1374014.26, grad_norm=0.3119, duration=0.38s
Step 11614: loss=2.9513, lr=0.000371, tokens/sec=1379557.70, grad_norm=0.3323, duration=0.38s
Step 11615: loss=2.9318, lr=0.000371, tokens/sec=1377382.76, grad_norm=0.2977, duration=0.38s
Step 11616: loss=2.8851, lr=0.000371, tokens/sec=1379049.86, grad_norm=0.3040, duration=0.38s
Step 11617: loss=2.9252, lr=0.000371, tokens/sec=1376262.11, grad_norm=0.3187, duration=0.38s
Step 11618: loss=2.8805, lr=0.000371, tokens/sec=1378764.53, grad_norm=0.3015, duration=0.38s
Step 11619: loss=2.9141, lr=0.000370, tokens/sec=1375791.98, grad_norm=0.3028, duration=0.38s
Step 11620: loss=2.8802, lr=0.000370, tokens/sec=1377671.84, grad_norm=0.3233, duration=0.38s
Step 11621: loss=2.9493, lr=0.000370, tokens/sec=1375631.04, grad_norm=0.2933, duration=0.38s
Step 11622: loss=2.9571, lr=0.000370, tokens/sec=1375115.77, grad_norm=0.3276, duration=0.38s
Step 11623: loss=2.9864, lr=0.000370, tokens/sec=1374125.87, grad_norm=0.3190, duration=0.38s
Step 11624: loss=2.9200, lr=0.000370, tokens/sec=1371829.36, grad_norm=0.2927, duration=0.38s
Step 11625: loss=2.8692, lr=0.000370, tokens/sec=1377500.10, grad_norm=0.2861, duration=0.38s
Step 11626: loss=2.8999, lr=0.000370, tokens/sec=1377121.40, grad_norm=0.3117, duration=0.38s
Step 11627: loss=2.8224, lr=0.000370, tokens/sec=1369800.70, grad_norm=0.3000, duration=0.38s
Step 11628: loss=2.8645, lr=0.000370, tokens/sec=1373748.17, grad_norm=0.2992, duration=0.38s
Step 11629: loss=2.8820, lr=0.000370, tokens/sec=1372617.14, grad_norm=0.3004, duration=0.38s
Step 11630: loss=2.8852, lr=0.000370, tokens/sec=1378348.84, grad_norm=0.3027, duration=0.38s
Step 11631: loss=2.9170, lr=0.000370, tokens/sec=1373796.23, grad_norm=0.2944, duration=0.38s
Step 11632: loss=2.8832, lr=0.000370, tokens/sec=1375369.49, grad_norm=0.2881, duration=0.38s
Step 11633: loss=2.9222, lr=0.000370, tokens/sec=1374057.18, grad_norm=0.3155, duration=0.38s
Step 11634: loss=2.8860, lr=0.000370, tokens/sec=1373230.87, grad_norm=0.3020, duration=0.38s
Step 11635: loss=2.8740, lr=0.000370, tokens/sec=1373617.73, grad_norm=0.3004, duration=0.38s
Step 11636: loss=2.9044, lr=0.000370, tokens/sec=1374040.01, grad_norm=0.3218, duration=0.38s
Step 11637: loss=2.9186, lr=0.000370, tokens/sec=1377536.34, grad_norm=0.3075, duration=0.38s
Step 11638: loss=2.9893, lr=0.000370, tokens/sec=1376775.66, grad_norm=0.3090, duration=0.38s
Step 11639: loss=2.9148, lr=0.000370, tokens/sec=1373865.75, grad_norm=0.3135, duration=0.38s
Step 11640: loss=2.9456, lr=0.000370, tokens/sec=1375178.54, grad_norm=0.3142, duration=0.38s
Step 11641: loss=2.9657, lr=0.000370, tokens/sec=1371793.42, grad_norm=0.3083, duration=0.38s
Step 11642: loss=2.9245, lr=0.000370, tokens/sec=1372009.10, grad_norm=0.3125, duration=0.38s
Step 11643: loss=2.9495, lr=0.000370, tokens/sec=1376672.23, grad_norm=0.3126, duration=0.38s
Step 11644: loss=2.9019, lr=0.000370, tokens/sec=1378080.21, grad_norm=0.3156, duration=0.38s
Step 11645: loss=2.9129, lr=0.000370, tokens/sec=1373628.89, grad_norm=0.3223, duration=0.38s
Step 11646: loss=2.9490, lr=0.000370, tokens/sec=1376162.20, grad_norm=0.3143, duration=0.38s
Step 11647: loss=2.9378, lr=0.000370, tokens/sec=1381736.05, grad_norm=0.3240, duration=0.38s
Step 11648: loss=2.9179, lr=0.000370, tokens/sec=1373997.95, grad_norm=0.3294, duration=0.38s
Step 11649: loss=2.9391, lr=0.000370, tokens/sec=1377594.16, grad_norm=0.3192, duration=0.38s
Step 11650: loss=2.8972, lr=0.000370, tokens/sec=1375335.08, grad_norm=0.3282, duration=0.38s
Step 11651: loss=2.8270, lr=0.000370, tokens/sec=1376328.44, grad_norm=0.3301, duration=0.38s
Step 11652: loss=2.8922, lr=0.000369, tokens/sec=1373545.66, grad_norm=0.3089, duration=0.38s
Step 11653: loss=2.9305, lr=0.000369, tokens/sec=1375640.51, grad_norm=0.3098, duration=0.38s
Step 11654: loss=2.9045, lr=0.000369, tokens/sec=1376945.49, grad_norm=0.3358, duration=0.38s
Step 11655: loss=2.9681, lr=0.000369, tokens/sec=1379338.77, grad_norm=0.3251, duration=0.38s
Step 11656: loss=2.9156, lr=0.000369, tokens/sec=1372473.22, grad_norm=0.3059, duration=0.38s
Step 11657: loss=2.9683, lr=0.000369, tokens/sec=1373629.75, grad_norm=0.3217, duration=0.38s
Step 11658: loss=2.9231, lr=0.000369, tokens/sec=1375143.29, grad_norm=0.3171, duration=0.38s
Step 11659: loss=2.9209, lr=0.000369, tokens/sec=1373373.24, grad_norm=0.3041, duration=0.38s
Step 11660: loss=2.9169, lr=0.000369, tokens/sec=1374283.03, grad_norm=0.3093, duration=0.38s
Step 11661: loss=2.9894, lr=0.000369, tokens/sec=1374536.44, grad_norm=0.3100, duration=0.38s
Step 11662: loss=2.9490, lr=0.000369, tokens/sec=1373555.10, grad_norm=0.3236, duration=0.38s
Step 11663: loss=2.9302, lr=0.000369, tokens/sec=1371660.79, grad_norm=0.3068, duration=0.38s
Step 11664: loss=2.9375, lr=0.000369, tokens/sec=1375785.96, grad_norm=0.2965, duration=0.38s
Step 11665: loss=2.9001, lr=0.000369, tokens/sec=1376995.50, grad_norm=0.3125, duration=0.38s
Step 11666: loss=2.9439, lr=0.000369, tokens/sec=1370638.27, grad_norm=0.2926, duration=0.38s
Step 11667: loss=2.9147, lr=0.000369, tokens/sec=1373005.37, grad_norm=0.3063, duration=0.38s
Step 11668: loss=2.9401, lr=0.000369, tokens/sec=1373682.95, grad_norm=0.3164, duration=0.38s
Step 11669: loss=2.9640, lr=0.000369, tokens/sec=1373495.90, grad_norm=0.2964, duration=0.38s
Step 11670: loss=2.9590, lr=0.000369, tokens/sec=1374420.46, grad_norm=0.3003, duration=0.38s
Step 11671: loss=2.8585, lr=0.000369, tokens/sec=1372956.51, grad_norm=0.3449, duration=0.38s
Step 11672: loss=2.9556, lr=0.000369, tokens/sec=1374192.85, grad_norm=0.3100, duration=0.38s
Step 11673: loss=2.8684, lr=0.000369, tokens/sec=1373273.75, grad_norm=0.2965, duration=0.38s
Step 11674: loss=2.8742, lr=0.000369, tokens/sec=1370340.18, grad_norm=0.3545, duration=0.38s
Step 11675: loss=2.8540, lr=0.000369, tokens/sec=1369996.98, grad_norm=0.3163, duration=0.38s
Step 11676: loss=2.8705, lr=0.000369, tokens/sec=1377582.94, grad_norm=0.3026, duration=0.38s
Step 11677: loss=2.8862, lr=0.000369, tokens/sec=1381257.84, grad_norm=0.3610, duration=0.38s
Step 11678: loss=2.8226, lr=0.000369, tokens/sec=1374558.78, grad_norm=0.3198, duration=0.38s
Step 11679: loss=2.8415, lr=0.000369, tokens/sec=1376194.93, grad_norm=0.2997, duration=0.38s
Step 11680: loss=2.8423, lr=0.000369, tokens/sec=1376823.93, grad_norm=0.3070, duration=0.38s
Step 11681: loss=2.8940, lr=0.000369, tokens/sec=1375702.47, grad_norm=0.2970, duration=0.38s
Step 11682: loss=2.8990, lr=0.000369, tokens/sec=1376517.98, grad_norm=0.2897, duration=0.38s
Step 11683: loss=2.8945, lr=0.000369, tokens/sec=1373694.10, grad_norm=0.3005, duration=0.38s
Step 11684: loss=2.9331, lr=0.000369, tokens/sec=1377028.27, grad_norm=0.2969, duration=0.38s
Step 11685: loss=2.9497, lr=0.000368, tokens/sec=1376418.03, grad_norm=0.3017, duration=0.38s
Step 11686: loss=2.9462, lr=0.000368, tokens/sec=1373712.98, grad_norm=0.3078, duration=0.38s
Step 11687: loss=2.9681, lr=0.000368, tokens/sec=1376107.09, grad_norm=0.3179, duration=0.38s
Step 11688: loss=2.8978, lr=0.000368, tokens/sec=1371422.98, grad_norm=0.3258, duration=0.38s
Step 11689: loss=2.9707, lr=0.000368, tokens/sec=1375555.32, grad_norm=0.3126, duration=0.38s
Step 11690: loss=2.9144, lr=0.000368, tokens/sec=1376650.68, grad_norm=0.3326, duration=0.38s
Step 11691: loss=2.9677, lr=0.000368, tokens/sec=1375966.74, grad_norm=0.3218, duration=0.38s
Step 11692: loss=2.9588, lr=0.000368, tokens/sec=1374392.11, grad_norm=0.3115, duration=0.38s
Step 11693: loss=2.9521, lr=0.000368, tokens/sec=1373088.53, grad_norm=0.3487, duration=0.38s
Step 11694: loss=2.9392, lr=0.000368, tokens/sec=1377012.74, grad_norm=0.3394, duration=0.38s
Step 11695: loss=2.9135, lr=0.000368, tokens/sec=1372558.88, grad_norm=0.3218, duration=0.38s
Step 11696: loss=2.9314, lr=0.000368, tokens/sec=1378008.53, grad_norm=0.3193, duration=0.38s
Step 11697: loss=2.9348, lr=0.000368, tokens/sec=1371537.59, grad_norm=0.3195, duration=0.38s
Step 11698: loss=2.9262, lr=0.000368, tokens/sec=1379303.30, grad_norm=0.3165, duration=0.38s
Step 11699: loss=2.8656, lr=0.000368, tokens/sec=1376141.53, grad_norm=0.2961, duration=0.38s
Step 11700/19073 (61.3%), Elapsed time: 4636.47s, Steps per hour: 9084.50, Estimated hours remaining: 0.81
Step 11700: loss=2.9476, lr=0.000368, tokens/sec=1370969.82, grad_norm=0.3174, duration=0.38s
Step 11701: loss=2.9347, lr=0.000368, tokens/sec=1375395.29, grad_norm=0.3134, duration=0.38s
Step 11702: loss=2.9237, lr=0.000368, tokens/sec=1376053.70, grad_norm=0.3282, duration=0.38s
Step 11703: loss=2.8565, lr=0.000368, tokens/sec=1373381.82, grad_norm=0.2850, duration=0.38s
Step 11704: loss=2.9372, lr=0.000368, tokens/sec=1378150.16, grad_norm=0.3127, duration=0.38s
Step 11705: loss=2.8758, lr=0.000368, tokens/sec=1377494.06, grad_norm=0.3349, duration=0.38s
Step 11706: loss=2.9450, lr=0.000368, tokens/sec=1374922.32, grad_norm=0.3134, duration=0.38s
Step 11707: loss=2.8984, lr=0.000368, tokens/sec=1374258.12, grad_norm=0.3110, duration=0.38s
Step 11708: loss=2.9022, lr=0.000368, tokens/sec=1374090.67, grad_norm=0.3276, duration=0.38s
Step 11709: loss=2.8974, lr=0.000368, tokens/sec=1375355.72, grad_norm=0.3238, duration=0.38s
Step 11710: loss=2.9096, lr=0.000368, tokens/sec=1376293.12, grad_norm=0.3051, duration=0.38s
Step 11711: loss=2.9121, lr=0.000368, tokens/sec=1380735.74, grad_norm=0.3322, duration=0.38s
Step 11712: loss=2.9164, lr=0.000368, tokens/sec=1373978.20, grad_norm=0.3137, duration=0.38s
Step 11713: loss=2.8707, lr=0.000368, tokens/sec=1374966.16, grad_norm=0.3140, duration=0.38s
Step 11714: loss=2.9072, lr=0.000368, tokens/sec=1372253.96, grad_norm=0.3089, duration=0.38s
Step 11715: loss=2.9499, lr=0.000368, tokens/sec=1377283.55, grad_norm=0.3109, duration=0.38s
Step 11716: loss=2.9432, lr=0.000368, tokens/sec=1375345.40, grad_norm=0.3018, duration=0.38s
Step 11717: loss=2.9195, lr=0.000368, tokens/sec=1375804.89, grad_norm=0.3309, duration=0.38s
Step 11718: loss=2.8999, lr=0.000368, tokens/sec=1377643.36, grad_norm=0.3011, duration=0.38s
Step 11719: loss=2.8596, lr=0.000367, tokens/sec=1378319.47, grad_norm=0.2797, duration=0.38s
Step 11720: loss=2.8809, lr=0.000367, tokens/sec=1374722.91, grad_norm=0.3005, duration=0.38s
Step 11721: loss=2.8250, lr=0.000367, tokens/sec=1374088.09, grad_norm=0.2978, duration=0.38s
Step 11722: loss=2.8307, lr=0.000367, tokens/sec=1376560.20, grad_norm=0.2910, duration=0.38s
Step 11723: loss=2.8579, lr=0.000367, tokens/sec=1372811.66, grad_norm=0.2940, duration=0.38s
Step 11724: loss=2.9160, lr=0.000367, tokens/sec=1375683.54, grad_norm=0.2989, duration=0.38s
Step 11725: loss=2.9096, lr=0.000367, tokens/sec=1376616.21, grad_norm=0.2896, duration=0.38s
Step 11726: loss=2.8946, lr=0.000367, tokens/sec=1374556.20, grad_norm=0.2976, duration=0.38s
Step 11727: loss=2.8595, lr=0.000367, tokens/sec=1371945.76, grad_norm=0.3140, duration=0.38s
Step 11728: loss=2.8871, lr=0.000367, tokens/sec=1377709.82, grad_norm=0.3036, duration=0.38s
Step 11729: loss=2.9513, lr=0.000367, tokens/sec=1377456.96, grad_norm=0.2993, duration=0.38s
Step 11730: loss=2.8990, lr=0.000367, tokens/sec=1373783.35, grad_norm=0.3122, duration=0.38s
Step 11731: loss=2.9662, lr=0.000367, tokens/sec=1373266.89, grad_norm=0.3249, duration=0.38s
Step 11732: loss=2.9684, lr=0.000367, tokens/sec=1378564.86, grad_norm=0.3180, duration=0.38s
Step 11733: loss=2.9649, lr=0.000367, tokens/sec=1377979.17, grad_norm=0.2988, duration=0.38s
Step 11734: loss=2.9498, lr=0.000367, tokens/sec=1375317.01, grad_norm=0.3345, duration=0.38s
Step 11735: loss=2.9163, lr=0.000367, tokens/sec=1376130.34, grad_norm=0.3380, duration=0.38s
Step 11736: loss=2.8890, lr=0.000367, tokens/sec=1371097.19, grad_norm=0.3274, duration=0.38s
Step 11737: loss=2.9910, lr=0.000367, tokens/sec=1375177.68, grad_norm=0.3291, duration=0.38s
Step 11738: loss=3.0540, lr=0.000367, tokens/sec=1376835.14, grad_norm=0.3659, duration=0.38s
Step 11739: loss=2.9766, lr=0.000367, tokens/sec=1376898.93, grad_norm=0.3256, duration=0.38s
Step 11740: loss=2.9845, lr=0.000367, tokens/sec=1375824.69, grad_norm=0.2981, duration=0.38s
Step 11741: loss=2.9129, lr=0.000367, tokens/sec=1379135.48, grad_norm=0.3280, duration=0.38s
Step 11742: loss=2.9437, lr=0.000367, tokens/sec=1376640.34, grad_norm=0.3502, duration=0.38s
Step 11743: loss=2.8987, lr=0.000367, tokens/sec=1374594.01, grad_norm=0.2992, duration=0.38s
Step 11744: loss=2.8736, lr=0.000367, tokens/sec=1377696.87, grad_norm=0.3168, duration=0.38s
Step 11745: loss=2.9951, lr=0.000367, tokens/sec=1374650.72, grad_norm=0.3378, duration=0.38s
Step 11746: loss=2.8716, lr=0.000367, tokens/sec=1377246.46, grad_norm=0.3245, duration=0.38s
Step 11747: loss=2.9549, lr=0.000367, tokens/sec=1377230.93, grad_norm=0.3013, duration=0.38s
Step 11748: loss=2.9907, lr=0.000367, tokens/sec=1373586.84, grad_norm=0.3422, duration=0.38s
Step 11749: loss=2.9516, lr=0.000367, tokens/sec=1376597.25, grad_norm=0.3385, duration=0.38s
Validation loss at step 11750: 3.778826951980591
Step 11750: loss=2.9197, lr=0.000367, tokens/sec=156144.29, grad_norm=0.2908, duration=3.36s
Step 11751: loss=2.9431, lr=0.000367, tokens/sec=1378714.39, grad_norm=0.3372, duration=0.38s
Step 11752: loss=2.9744, lr=0.000366, tokens/sec=1373906.95, grad_norm=0.3366, duration=0.38s
Step 11753: loss=2.9078, lr=0.000366, tokens/sec=1375002.27, grad_norm=0.3059, duration=0.38s
Step 11754: loss=2.8596, lr=0.000366, tokens/sec=1379043.81, grad_norm=0.3385, duration=0.38s
Step 11755: loss=2.8911, lr=0.000366, tokens/sec=1376442.15, grad_norm=0.3376, duration=0.38s
Step 11756: loss=2.9237, lr=0.000366, tokens/sec=1374646.42, grad_norm=0.3108, duration=0.38s
Step 11757: loss=2.9383, lr=0.000366, tokens/sec=1371908.95, grad_norm=0.3200, duration=0.38s
Step 11758: loss=2.9454, lr=0.000366, tokens/sec=1375899.58, grad_norm=0.3372, duration=0.38s
Step 11759: loss=2.9083, lr=0.000366, tokens/sec=1372548.60, grad_norm=0.3089, duration=0.38s
Step 11760: loss=2.9028, lr=0.000366, tokens/sec=1376501.60, grad_norm=0.2988, duration=0.38s
Step 11761: loss=2.8412, lr=0.000366, tokens/sec=1374982.50, grad_norm=0.3180, duration=0.38s
Step 11762: loss=2.8498, lr=0.000366, tokens/sec=1377454.37, grad_norm=0.3110, duration=0.38s
Step 11763: loss=2.8890, lr=0.000366, tokens/sec=1376790.31, grad_norm=0.3116, duration=0.38s
Step 11764: loss=2.8833, lr=0.000366, tokens/sec=1373022.52, grad_norm=0.2956, duration=0.38s
Step 11765: loss=2.8574, lr=0.000366, tokens/sec=1374608.61, grad_norm=0.2890, duration=0.38s
Step 11766: loss=2.8674, lr=0.000366, tokens/sec=1376167.37, grad_norm=0.3031, duration=0.38s
Step 11767: loss=2.9084, lr=0.000366, tokens/sec=1375606.09, grad_norm=0.3086, duration=0.38s
Step 11768: loss=2.8740, lr=0.000366, tokens/sec=1372815.09, grad_norm=0.2813, duration=0.38s
Step 11769: loss=2.8877, lr=0.000366, tokens/sec=1378785.27, grad_norm=0.2993, duration=0.38s
Step 11770: loss=2.8466, lr=0.000366, tokens/sec=1380627.38, grad_norm=0.2982, duration=0.38s
Step 11771: loss=2.8576, lr=0.000366, tokens/sec=1375966.74, grad_norm=0.2781, duration=0.38s
Step 11772: loss=2.8972, lr=0.000366, tokens/sec=1379954.20, grad_norm=0.3038, duration=0.38s
Step 11773: loss=2.8735, lr=0.000366, tokens/sec=1371145.06, grad_norm=0.2975, duration=0.38s
Step 11774: loss=2.8975, lr=0.000366, tokens/sec=1371790.85, grad_norm=0.2883, duration=0.38s
Step 11775: loss=2.9517, lr=0.000366, tokens/sec=1375237.88, grad_norm=0.3147, duration=0.38s
Step 11776: loss=2.9258, lr=0.000366, tokens/sec=1376246.61, grad_norm=0.3147, duration=0.38s
Step 11777: loss=2.9343, lr=0.000366, tokens/sec=1377197.30, grad_norm=0.2873, duration=0.38s
Step 11778: loss=2.9075, lr=0.000366, tokens/sec=1377404.33, grad_norm=0.3011, duration=0.38s
Step 11779: loss=2.9342, lr=0.000366, tokens/sec=1377186.95, grad_norm=0.3110, duration=0.38s
Step 11780: loss=2.9396, lr=0.000366, tokens/sec=1377628.68, grad_norm=0.3106, duration=0.38s
Step 11781: loss=2.9494, lr=0.000366, tokens/sec=1379533.47, grad_norm=0.3011, duration=0.38s
Step 11782: loss=2.9624, lr=0.000366, tokens/sec=1374766.74, grad_norm=0.3137, duration=0.38s
Step 11783: loss=3.0482, lr=0.000366, tokens/sec=1373743.88, grad_norm=0.3213, duration=0.38s
Step 11784: loss=2.9416, lr=0.000366, tokens/sec=1378079.34, grad_norm=0.3007, duration=0.38s
Step 11785: loss=2.9968, lr=0.000366, tokens/sec=1377602.79, grad_norm=0.3174, duration=0.38s
Step 11786: loss=3.0272, lr=0.000365, tokens/sec=1376882.55, grad_norm=0.3226, duration=0.38s
Step 11787: loss=2.9519, lr=0.000365, tokens/sec=1379392.42, grad_norm=0.2978, duration=0.38s
Step 11788: loss=2.9825, lr=0.000365, tokens/sec=1376757.56, grad_norm=0.3025, duration=0.38s
Step 11789: loss=2.9550, lr=0.000365, tokens/sec=1376112.26, grad_norm=0.3256, duration=0.38s
Step 11790: loss=2.9794, lr=0.000365, tokens/sec=1374478.88, grad_norm=0.3135, duration=0.38s
Step 11791: loss=2.9480, lr=0.000365, tokens/sec=1378611.53, grad_norm=0.3048, duration=0.38s
Step 11792: loss=2.9499, lr=0.000365, tokens/sec=1371692.44, grad_norm=0.3062, duration=0.38s
Step 11793: loss=2.8776, lr=0.000365, tokens/sec=1377696.87, grad_norm=0.2973, duration=0.38s
Step 11794: loss=2.9550, lr=0.000365, tokens/sec=1378359.21, grad_norm=0.3102, duration=0.38s
Step 11795: loss=2.9651, lr=0.000365, tokens/sec=1378908.04, grad_norm=0.3637, duration=0.38s
Step 11796: loss=2.8864, lr=0.000365, tokens/sec=1377795.27, grad_norm=0.3003, duration=0.38s
Step 11797: loss=2.9582, lr=0.000365, tokens/sec=1378176.07, grad_norm=0.3301, duration=0.38s
Step 11798: loss=2.8704, lr=0.000365, tokens/sec=1378928.80, grad_norm=0.3336, duration=0.38s
Step 11799: loss=2.8453, lr=0.000365, tokens/sec=1376063.17, grad_norm=0.3180, duration=0.38s
Step 11800/19073 (61.9%), Elapsed time: 4677.63s, Steps per hour: 9081.53, Estimated hours remaining: 0.80
Step 11800: loss=2.8992, lr=0.000365, tokens/sec=1377766.79, grad_norm=0.3279, duration=0.38s
Step 11801: loss=2.9064, lr=0.000365, tokens/sec=1379462.51, grad_norm=0.3402, duration=0.38s
Step 11802: loss=2.9980, lr=0.000365, tokens/sec=1374196.29, grad_norm=0.3096, duration=0.38s
Step 11803: loss=2.8361, lr=0.000365, tokens/sec=1378446.48, grad_norm=0.2982, duration=0.38s
Step 11804: loss=2.9400, lr=0.000365, tokens/sec=1375974.49, grad_norm=0.3142, duration=0.38s
Step 11805: loss=2.9359, lr=0.000365, tokens/sec=1373779.92, grad_norm=0.3033, duration=0.38s
Step 11806: loss=2.8830, lr=0.000365, tokens/sec=1373332.93, grad_norm=0.3002, duration=0.38s
Step 11807: loss=2.8522, lr=0.000365, tokens/sec=1375499.39, grad_norm=0.3050, duration=0.38s
Step 11808: loss=2.9335, lr=0.000365, tokens/sec=1374335.42, grad_norm=0.3038, duration=0.38s
Step 11809: loss=2.9074, lr=0.000365, tokens/sec=1374479.73, grad_norm=0.2814, duration=0.38s
Step 11810: loss=2.9101, lr=0.000365, tokens/sec=1373808.24, grad_norm=0.3083, duration=0.38s
Step 11811: loss=2.9370, lr=0.000365, tokens/sec=1378055.16, grad_norm=0.3040, duration=0.38s
Step 11812: loss=2.9449, lr=0.000365, tokens/sec=1373223.15, grad_norm=0.3214, duration=0.38s
Step 11813: loss=2.9741, lr=0.000365, tokens/sec=1374871.60, grad_norm=0.3206, duration=0.38s
Step 11814: loss=2.9187, lr=0.000365, tokens/sec=1378354.89, grad_norm=0.2977, duration=0.38s
Step 11815: loss=2.8480, lr=0.000365, tokens/sec=1377117.95, grad_norm=0.2978, duration=0.38s
Step 11816: loss=2.8688, lr=0.000365, tokens/sec=1374797.68, grad_norm=0.3099, duration=0.38s
Step 11817: loss=2.8616, lr=0.000365, tokens/sec=1372175.19, grad_norm=0.2939, duration=0.38s
Step 11818: loss=2.8570, lr=0.000365, tokens/sec=1371723.25, grad_norm=0.3052, duration=0.38s
Step 11819: loss=2.8652, lr=0.000365, tokens/sec=1377342.21, grad_norm=0.2950, duration=0.38s
Step 11820: loss=2.9076, lr=0.000364, tokens/sec=1377321.51, grad_norm=0.2942, duration=0.38s
Step 11821: loss=2.8779, lr=0.000364, tokens/sec=1377360.33, grad_norm=0.3021, duration=0.38s
Step 11822: loss=2.8968, lr=0.000364, tokens/sec=1372540.04, grad_norm=0.3023, duration=0.38s
Step 11823: loss=2.9043, lr=0.000364, tokens/sec=1372415.83, grad_norm=0.3097, duration=0.38s
Step 11824: loss=2.8717, lr=0.000364, tokens/sec=1370812.57, grad_norm=0.3066, duration=0.38s
Step 11825: loss=2.8894, lr=0.000364, tokens/sec=1372989.09, grad_norm=0.3004, duration=0.38s
Step 11826: loss=2.8466, lr=0.000364, tokens/sec=1374724.62, grad_norm=0.3090, duration=0.38s
Step 11827: loss=2.9409, lr=0.000364, tokens/sec=1373722.42, grad_norm=0.3009, duration=0.38s
Step 11828: loss=2.9514, lr=0.000364, tokens/sec=1375452.07, grad_norm=0.3002, duration=0.38s
Step 11829: loss=2.9437, lr=0.000364, tokens/sec=1370730.54, grad_norm=0.3348, duration=0.38s
Step 11830: loss=2.9348, lr=0.000364, tokens/sec=1375505.41, grad_norm=0.2968, duration=0.38s
Step 11831: loss=2.9275, lr=0.000364, tokens/sec=1376260.39, grad_norm=0.3152, duration=0.38s
Step 11832: loss=2.9472, lr=0.000364, tokens/sec=1376704.12, grad_norm=0.3219, duration=0.38s
Step 11833: loss=2.9302, lr=0.000364, tokens/sec=1376304.32, grad_norm=0.2931, duration=0.38s
Step 11834: loss=2.9142, lr=0.000364, tokens/sec=1375542.41, grad_norm=0.3051, duration=0.38s
Step 11835: loss=2.9217, lr=0.000364, tokens/sec=1372971.94, grad_norm=0.3278, duration=0.38s
Step 11836: loss=2.9449, lr=0.000364, tokens/sec=1373224.87, grad_norm=0.3014, duration=0.38s
Step 11837: loss=2.9197, lr=0.000364, tokens/sec=1372466.37, grad_norm=0.3266, duration=0.38s
Step 11838: loss=2.9360, lr=0.000364, tokens/sec=1377488.88, grad_norm=0.3265, duration=0.38s
Step 11839: loss=2.8921, lr=0.000364, tokens/sec=1375853.10, grad_norm=0.3055, duration=0.38s
Step 11840: loss=2.8998, lr=0.000364, tokens/sec=1374386.96, grad_norm=0.3145, duration=0.38s
Step 11841: loss=2.8336, lr=0.000364, tokens/sec=1372833.94, grad_norm=0.3528, duration=0.38s
Step 11842: loss=2.8836, lr=0.000364, tokens/sec=1372985.66, grad_norm=0.3091, duration=0.38s
Step 11843: loss=2.9043, lr=0.000364, tokens/sec=1375242.18, grad_norm=0.2777, duration=0.38s
Step 11844: loss=2.9154, lr=0.000364, tokens/sec=1376726.53, grad_norm=0.3427, duration=0.38s
Step 11845: loss=2.9691, lr=0.000364, tokens/sec=1375157.90, grad_norm=0.3405, duration=0.38s
Step 11846: loss=2.8845, lr=0.000364, tokens/sec=1375575.97, grad_norm=0.2873, duration=0.38s
Step 11847: loss=2.9538, lr=0.000364, tokens/sec=1370786.08, grad_norm=0.3540, duration=0.38s
Step 11848: loss=2.9326, lr=0.000364, tokens/sec=1375215.52, grad_norm=0.3065, duration=0.38s
Step 11849: loss=2.9111, lr=0.000364, tokens/sec=1375627.60, grad_norm=0.2957, duration=0.38s
Step 11850: loss=2.9778, lr=0.000364, tokens/sec=1375995.15, grad_norm=0.3158, duration=0.38s
Step 11851: loss=2.9690, lr=0.000364, tokens/sec=1379102.62, grad_norm=0.3100, duration=0.38s
Step 11852: loss=2.9059, lr=0.000364, tokens/sec=1373211.15, grad_norm=0.3072, duration=0.38s
Step 11853: loss=2.9296, lr=0.000363, tokens/sec=1373757.61, grad_norm=0.2987, duration=0.38s
Step 11854: loss=2.9006, lr=0.000363, tokens/sec=1375360.02, grad_norm=0.2982, duration=0.38s
Step 11855: loss=2.9380, lr=0.000363, tokens/sec=1374965.30, grad_norm=0.3004, duration=0.38s
Step 11856: loss=2.9518, lr=0.000363, tokens/sec=1378316.88, grad_norm=0.2913, duration=0.38s
Step 11857: loss=2.8958, lr=0.000363, tokens/sec=1372494.63, grad_norm=0.2907, duration=0.38s
Step 11858: loss=2.9675, lr=0.000363, tokens/sec=1374669.62, grad_norm=0.3007, duration=0.38s
Step 11859: loss=2.9421, lr=0.000363, tokens/sec=1377161.07, grad_norm=0.2947, duration=0.38s
Step 11860: loss=2.9232, lr=0.000363, tokens/sec=1375044.40, grad_norm=0.2931, duration=0.38s
Step 11861: loss=2.8871, lr=0.000363, tokens/sec=1375292.07, grad_norm=0.3024, duration=0.38s
Step 11862: loss=2.9165, lr=0.000363, tokens/sec=1372492.06, grad_norm=0.2863, duration=0.38s
Step 11863: loss=2.8577, lr=0.000363, tokens/sec=1377645.95, grad_norm=0.2850, duration=0.38s
Step 11864: loss=2.8962, lr=0.000363, tokens/sec=1377348.25, grad_norm=0.3019, duration=0.38s
Step 11865: loss=2.8338, lr=0.000363, tokens/sec=1377792.68, grad_norm=0.2959, duration=0.38s
Step 11866: loss=2.8608, lr=0.000363, tokens/sec=1376786.00, grad_norm=0.2791, duration=0.38s
Step 11867: loss=2.8620, lr=0.000363, tokens/sec=1376975.67, grad_norm=0.3114, duration=0.38s
Step 11868: loss=2.8156, lr=0.000363, tokens/sec=1375637.07, grad_norm=0.2957, duration=0.38s
Step 11869: loss=2.8563, lr=0.000363, tokens/sec=1376735.15, grad_norm=0.2890, duration=0.38s
Step 11870: loss=2.8353, lr=0.000363, tokens/sec=1376296.57, grad_norm=0.2885, duration=0.38s
Step 11871: loss=2.9100, lr=0.000363, tokens/sec=1374336.28, grad_norm=0.2700, duration=0.38s
Step 11872: loss=2.8663, lr=0.000363, tokens/sec=1375508.86, grad_norm=0.2837, duration=0.38s
Step 11873: loss=2.8860, lr=0.000363, tokens/sec=1378933.98, grad_norm=0.2922, duration=0.38s
Step 11874: loss=2.9430, lr=0.000363, tokens/sec=1378199.39, grad_norm=0.2866, duration=0.38s
Step 11875: loss=2.9341, lr=0.000363, tokens/sec=1376709.29, grad_norm=0.3047, duration=0.38s
Step 11876: loss=2.9596, lr=0.000363, tokens/sec=1376852.38, grad_norm=0.2920, duration=0.38s
Step 11877: loss=2.9558, lr=0.000363, tokens/sec=1376375.82, grad_norm=0.2967, duration=0.38s
Step 11878: loss=2.9147, lr=0.000363, tokens/sec=1376603.28, grad_norm=0.3095, duration=0.38s
Step 11879: loss=2.9444, lr=0.000363, tokens/sec=1374968.74, grad_norm=0.3021, duration=0.38s
Step 11880: loss=2.9023, lr=0.000363, tokens/sec=1377096.39, grad_norm=0.3110, duration=0.38s
Step 11881: loss=2.9615, lr=0.000363, tokens/sec=1377357.74, grad_norm=0.3024, duration=0.38s
Step 11882: loss=2.9589, lr=0.000363, tokens/sec=1377245.60, grad_norm=0.2995, duration=0.38s
Step 11883: loss=2.9284, lr=0.000363, tokens/sec=1380190.65, grad_norm=0.3258, duration=0.38s
Step 11884: loss=2.9395, lr=0.000363, tokens/sec=1379816.53, grad_norm=0.3114, duration=0.38s
Step 11885: loss=2.9127, lr=0.000363, tokens/sec=1376191.49, grad_norm=0.2970, duration=0.38s
Step 11886: loss=2.9254, lr=0.000363, tokens/sec=1378006.80, grad_norm=0.2967, duration=0.38s
Step 11887: loss=2.9173, lr=0.000362, tokens/sec=1377150.72, grad_norm=0.2952, duration=0.38s
Step 11888: loss=2.9147, lr=0.000362, tokens/sec=1380120.49, grad_norm=0.3083, duration=0.38s
Step 11889: loss=2.8890, lr=0.000362, tokens/sec=1376881.69, grad_norm=0.2834, duration=0.38s
Step 11890: loss=2.9404, lr=0.000362, tokens/sec=1375826.41, grad_norm=0.2852, duration=0.38s
Step 11891: loss=2.9042, lr=0.000362, tokens/sec=1375200.90, grad_norm=0.3057, duration=0.38s
Step 11892: loss=2.9115, lr=0.000362, tokens/sec=1375629.32, grad_norm=0.3141, duration=0.38s
Step 11893: loss=2.8623, lr=0.000362, tokens/sec=1377658.89, grad_norm=0.2940, duration=0.38s
Step 11894: loss=2.9246, lr=0.000362, tokens/sec=1378982.41, grad_norm=0.3028, duration=0.38s
Step 11895: loss=2.8884, lr=0.000362, tokens/sec=1378051.71, grad_norm=0.3190, duration=0.38s
Step 11896: loss=2.9258, lr=0.000362, tokens/sec=1375541.55, grad_norm=0.3220, duration=0.38s
Step 11897: loss=2.8735, lr=0.000362, tokens/sec=1378532.02, grad_norm=0.2971, duration=0.38s
Step 11898: loss=2.9123, lr=0.000362, tokens/sec=1372791.09, grad_norm=0.3198, duration=0.38s
Step 11899: loss=2.8930, lr=0.000362, tokens/sec=1376581.74, grad_norm=0.3155, duration=0.38s
Step 11900/19073 (62.4%), Elapsed time: 4715.83s, Steps per hour: 9084.30, Estimated hours remaining: 0.79
Step 11900: loss=2.9085, lr=0.000362, tokens/sec=1375826.41, grad_norm=0.2956, duration=0.38s
Step 11901: loss=2.9081, lr=0.000362, tokens/sec=1375132.97, grad_norm=0.3115, duration=0.38s
Step 11902: loss=2.8849, lr=0.000362, tokens/sec=1378468.94, grad_norm=0.3231, duration=0.38s
Step 11903: loss=2.8832, lr=0.000362, tokens/sec=1376341.36, grad_norm=0.2962, duration=0.38s
Step 11904: loss=2.9441, lr=0.000362, tokens/sec=1375827.27, grad_norm=0.2949, duration=0.38s
Step 11905: loss=2.9527, lr=0.000362, tokens/sec=1377111.91, grad_norm=0.3099, duration=0.38s
Step 11906: loss=2.9240, lr=0.000362, tokens/sec=1378246.90, grad_norm=0.2946, duration=0.38s
Step 11907: loss=2.8990, lr=0.000362, tokens/sec=1374633.53, grad_norm=0.3054, duration=0.38s
Step 11908: loss=2.9121, lr=0.000362, tokens/sec=1376264.70, grad_norm=0.3089, duration=0.38s
Step 11909: loss=2.8400, lr=0.000362, tokens/sec=1376864.45, grad_norm=0.2927, duration=0.38s
Step 11910: loss=2.9001, lr=0.000362, tokens/sec=1377340.49, grad_norm=0.2879, duration=0.38s
Step 11911: loss=2.7814, lr=0.000362, tokens/sec=1374234.07, grad_norm=0.3098, duration=0.38s
Step 11912: loss=2.8533, lr=0.000362, tokens/sec=1373059.38, grad_norm=0.2976, duration=0.38s
Step 11913: loss=2.8469, lr=0.000362, tokens/sec=1377484.57, grad_norm=0.2909, duration=0.38s
Step 11914: loss=2.9390, lr=0.000362, tokens/sec=1375695.59, grad_norm=0.3149, duration=0.38s
Step 11915: loss=2.9155, lr=0.000362, tokens/sec=1379132.89, grad_norm=0.3071, duration=0.38s
Step 11916: loss=2.8687, lr=0.000362, tokens/sec=1373755.89, grad_norm=0.3019, duration=0.38s
Step 11917: loss=2.8509, lr=0.000362, tokens/sec=1373856.31, grad_norm=0.3068, duration=0.38s
Step 11918: loss=2.9086, lr=0.000362, tokens/sec=1372820.23, grad_norm=0.3150, duration=0.38s
Step 11919: loss=2.8779, lr=0.000362, tokens/sec=1378488.82, grad_norm=0.3099, duration=0.38s
Step 11920: loss=2.9391, lr=0.000362, tokens/sec=1372742.24, grad_norm=0.3103, duration=0.38s
Step 11921: loss=2.9614, lr=0.000361, tokens/sec=1374787.36, grad_norm=0.3270, duration=0.38s
Step 11922: loss=2.9832, lr=0.000361, tokens/sec=1375462.40, grad_norm=0.3229, duration=0.38s
Step 11923: loss=2.9166, lr=0.000361, tokens/sec=1376516.25, grad_norm=0.2895, duration=0.38s
Step 11924: loss=2.9625, lr=0.000361, tokens/sec=1374295.91, grad_norm=0.3113, duration=0.38s
Step 11925: loss=2.8590, lr=0.000361, tokens/sec=1375103.73, grad_norm=0.3691, duration=0.38s
Step 11926: loss=2.9402, lr=0.000361, tokens/sec=1373182.85, grad_norm=0.3145, duration=0.38s
Step 11927: loss=2.9837, lr=0.000361, tokens/sec=1374576.82, grad_norm=0.3048, duration=0.38s
Step 11928: loss=3.0291, lr=0.000361, tokens/sec=1374551.04, grad_norm=0.3327, duration=0.38s
Step 11929: loss=2.9878, lr=0.000361, tokens/sec=1375722.27, grad_norm=0.3227, duration=0.38s
Step 11930: loss=2.9582, lr=0.000361, tokens/sec=1377847.07, grad_norm=0.3008, duration=0.38s
Step 11931: loss=2.9092, lr=0.000361, tokens/sec=1378071.57, grad_norm=0.3058, duration=0.38s
Step 11932: loss=2.9294, lr=0.000361, tokens/sec=1376589.50, grad_norm=0.3415, duration=0.38s
Step 11933: loss=2.9028, lr=0.000361, tokens/sec=1378538.07, grad_norm=0.3094, duration=0.38s
Step 11934: loss=2.8979, lr=0.000361, tokens/sec=1374375.79, grad_norm=0.3026, duration=0.38s
Step 11935: loss=2.9524, lr=0.000361, tokens/sec=1375354.86, grad_norm=0.3404, duration=0.38s
Step 11936: loss=2.8618, lr=0.000361, tokens/sec=1374013.40, grad_norm=0.3485, duration=0.38s
Step 11937: loss=2.9583, lr=0.000361, tokens/sec=1373510.49, grad_norm=0.3028, duration=0.38s
Step 11938: loss=2.9956, lr=0.000361, tokens/sec=1379821.72, grad_norm=0.3359, duration=0.38s
Step 11939: loss=2.9611, lr=0.000361, tokens/sec=1376346.53, grad_norm=0.3392, duration=0.38s
Step 11940: loss=2.8825, lr=0.000361, tokens/sec=1376610.18, grad_norm=0.2899, duration=0.38s
Step 11941: loss=2.9434, lr=0.000361, tokens/sec=1375127.81, grad_norm=0.3206, duration=0.38s
Step 11942: loss=2.9368, lr=0.000361, tokens/sec=1374070.92, grad_norm=0.3181, duration=0.38s
Step 11943: loss=2.9255, lr=0.000361, tokens/sec=1376412.00, grad_norm=0.2955, duration=0.38s
Step 11944: loss=2.8262, lr=0.000361, tokens/sec=1371431.53, grad_norm=0.3064, duration=0.38s
Step 11945: loss=2.9135, lr=0.000361, tokens/sec=1376836.00, grad_norm=0.3303, duration=0.38s
Step 11946: loss=2.9112, lr=0.000361, tokens/sec=1371038.20, grad_norm=0.3103, duration=0.38s
Step 11947: loss=2.9116, lr=0.000361, tokens/sec=1381560.70, grad_norm=0.3201, duration=0.38s
Step 11948: loss=2.9791, lr=0.000361, tokens/sec=1375101.15, grad_norm=0.3519, duration=0.38s
Step 11949: loss=2.8890, lr=0.000361, tokens/sec=1374609.47, grad_norm=0.3059, duration=0.38s
Step 11950: loss=2.8332, lr=0.000361, tokens/sec=1375919.39, grad_norm=0.2981, duration=0.38s
Step 11951: loss=2.8845, lr=0.000361, tokens/sec=1373302.91, grad_norm=0.3071, duration=0.38s
Step 11952: loss=2.8440, lr=0.000361, tokens/sec=1370329.08, grad_norm=0.2986, duration=0.38s
Step 11953: loss=2.9129, lr=0.000361, tokens/sec=1371600.04, grad_norm=0.2987, duration=0.38s
Step 11954: loss=2.8669, lr=0.000361, tokens/sec=1377444.88, grad_norm=0.2927, duration=0.38s
Step 11955: loss=2.8536, lr=0.000360, tokens/sec=1376909.28, grad_norm=0.2842, duration=0.38s
Step 11956: loss=2.8409, lr=0.000360, tokens/sec=1377922.18, grad_norm=0.2943, duration=0.38s
Step 11957: loss=2.9167, lr=0.000360, tokens/sec=1372266.81, grad_norm=0.3111, duration=0.38s
Step 11958: loss=2.9015, lr=0.000360, tokens/sec=1377985.21, grad_norm=0.2829, duration=0.38s
Step 11959: loss=2.8694, lr=0.000360, tokens/sec=1377337.90, grad_norm=0.2833, duration=0.38s
Step 11960: loss=2.8266, lr=0.000360, tokens/sec=1375071.91, grad_norm=0.2906, duration=0.38s
Step 11961: loss=2.8561, lr=0.000360, tokens/sec=1373148.55, grad_norm=0.2851, duration=0.38s
Step 11962: loss=2.8831, lr=0.000360, tokens/sec=1373043.95, grad_norm=0.2962, duration=0.38s
Step 11963: loss=2.8720, lr=0.000360, tokens/sec=1377464.72, grad_norm=0.2852, duration=0.38s
Step 11964: loss=2.8987, lr=0.000360, tokens/sec=1374863.00, grad_norm=0.2763, duration=0.38s
Step 11965: loss=2.9214, lr=0.000360, tokens/sec=1374307.94, grad_norm=0.3172, duration=0.38s
Step 11966: loss=2.9277, lr=0.000360, tokens/sec=1373239.45, grad_norm=0.3071, duration=0.38s
Step 11967: loss=2.9311, lr=0.000360, tokens/sec=1376374.96, grad_norm=0.2862, duration=0.38s
Step 11968: loss=2.8995, lr=0.000360, tokens/sec=1376788.59, grad_norm=0.3017, duration=0.38s
Step 11969: loss=2.9575, lr=0.000360, tokens/sec=1375075.35, grad_norm=0.3036, duration=0.38s
Step 11970: loss=2.9067, lr=0.000360, tokens/sec=1372721.68, grad_norm=0.2986, duration=0.38s
Step 11971: loss=2.9512, lr=0.000360, tokens/sec=1377118.81, grad_norm=0.3045, duration=0.38s
Step 11972: loss=2.9730, lr=0.000360, tokens/sec=1373934.42, grad_norm=0.3121, duration=0.38s
Step 11973: loss=3.0001, lr=0.000360, tokens/sec=1366269.62, grad_norm=0.2987, duration=0.38s
Step 11974: loss=2.9687, lr=0.000360, tokens/sec=1375007.43, grad_norm=0.2942, duration=0.38s
Step 11975: loss=2.9754, lr=0.000360, tokens/sec=1373924.98, grad_norm=0.3166, duration=0.38s
Step 11976: loss=3.0093, lr=0.000360, tokens/sec=1373606.58, grad_norm=0.3241, duration=0.38s
Step 11977: loss=2.9597, lr=0.000360, tokens/sec=1379004.89, grad_norm=0.2943, duration=0.38s
Step 11978: loss=2.9681, lr=0.000360, tokens/sec=1376399.94, grad_norm=0.2948, duration=0.38s
Step 11979: loss=2.9609, lr=0.000360, tokens/sec=1376868.76, grad_norm=0.3182, duration=0.38s
Step 11980: loss=2.9762, lr=0.000360, tokens/sec=1376397.35, grad_norm=0.3327, duration=0.38s
Step 11981: loss=2.9306, lr=0.000360, tokens/sec=1370836.50, grad_norm=0.2881, duration=0.38s
Step 11982: loss=2.9348, lr=0.000360, tokens/sec=1373326.92, grad_norm=0.3131, duration=0.38s
Step 11983: loss=2.8808, lr=0.000360, tokens/sec=1380285.08, grad_norm=0.2984, duration=0.38s
Step 11984: loss=2.9769, lr=0.000360, tokens/sec=1379013.54, grad_norm=0.2955, duration=0.38s
Step 11985: loss=2.9266, lr=0.000360, tokens/sec=1375485.62, grad_norm=0.3514, duration=0.38s
Step 11986: loss=2.8837, lr=0.000360, tokens/sec=1376876.52, grad_norm=0.3214, duration=0.38s
Step 11987: loss=2.9459, lr=0.000360, tokens/sec=1379350.02, grad_norm=0.3141, duration=0.38s
Step 11988: loss=2.8715, lr=0.000360, tokens/sec=1373182.85, grad_norm=0.3259, duration=0.38s
Step 11989: loss=2.8441, lr=0.000359, tokens/sec=1375175.96, grad_norm=0.3539, duration=0.38s
Step 11990: loss=2.8649, lr=0.000359, tokens/sec=1376286.23, grad_norm=0.3283, duration=0.38s
Step 11991: loss=2.9166, lr=0.000359, tokens/sec=1377666.66, grad_norm=0.3170, duration=0.38s
Step 11992: loss=2.9861, lr=0.000359, tokens/sec=1376762.73, grad_norm=0.3238, duration=0.38s
Step 11993: loss=2.8273, lr=0.000359, tokens/sec=1377941.18, grad_norm=0.3095, duration=0.38s
Step 11994: loss=2.9430, lr=0.000359, tokens/sec=1371298.97, grad_norm=0.3035, duration=0.38s
Step 11995: loss=2.9337, lr=0.000359, tokens/sec=1378445.61, grad_norm=0.2884, duration=0.38s
Step 11996: loss=2.8136, lr=0.000359, tokens/sec=1371813.95, grad_norm=0.3033, duration=0.38s
Step 11997: loss=2.9082, lr=0.000359, tokens/sec=1377402.60, grad_norm=0.3140, duration=0.38s
Step 11998: loss=2.9286, lr=0.000359, tokens/sec=1375488.21, grad_norm=0.3056, duration=0.38s
Step 11999: loss=2.9384, lr=0.000359, tokens/sec=1373204.29, grad_norm=0.2739, duration=0.38s
Step 12000/19073 (62.9%), Elapsed time: 4754.04s, Steps per hour: 9087.02, Estimated hours remaining: 0.78
Validation loss at step 12000: 3.7809484004974365
Step 12000: loss=2.9008, lr=0.000359, tokens/sec=156628.18, grad_norm=0.3096, duration=3.35s
Step 12001: loss=2.9319, lr=0.000359, tokens/sec=1375709.36, grad_norm=0.3314, duration=0.38s
Step 12002: loss=2.9396, lr=0.000359, tokens/sec=1378186.44, grad_norm=0.3119, duration=0.38s
Step 12003: loss=2.9699, lr=0.000359, tokens/sec=1378474.13, grad_norm=0.3131, duration=0.38s
Step 12004: loss=2.8965, lr=0.000359, tokens/sec=1375734.32, grad_norm=0.3132, duration=0.38s
Step 12005: loss=2.8166, lr=0.000359, tokens/sec=1376387.88, grad_norm=0.2999, duration=0.38s
Step 12006: loss=2.9105, lr=0.000359, tokens/sec=1374442.79, grad_norm=0.3217, duration=0.38s
Step 12007: loss=2.8598, lr=0.000359, tokens/sec=1375005.71, grad_norm=0.3061, duration=0.38s
Step 12008: loss=2.8453, lr=0.000359, tokens/sec=1375966.74, grad_norm=0.2929, duration=0.38s
Step 12009: loss=2.8909, lr=0.000359, tokens/sec=1375789.40, grad_norm=0.3135, duration=0.38s
Step 12010: loss=2.8700, lr=0.000359, tokens/sec=1368049.48, grad_norm=0.3156, duration=0.38s
Step 12011: loss=2.8917, lr=0.000359, tokens/sec=1376737.73, grad_norm=0.2928, duration=0.38s
Step 12012: loss=2.8809, lr=0.000359, tokens/sec=1372485.21, grad_norm=0.2915, duration=0.38s
Step 12013: loss=2.8886, lr=0.000359, tokens/sec=1375845.35, grad_norm=0.3106, duration=0.38s
Step 12014: loss=2.8860, lr=0.000359, tokens/sec=1372377.29, grad_norm=0.2919, duration=0.38s
Step 12015: loss=2.8304, lr=0.000359, tokens/sec=1373725.85, grad_norm=0.2969, duration=0.38s
Step 12016: loss=2.8707, lr=0.000359, tokens/sec=1374850.97, grad_norm=0.2952, duration=0.38s
Step 12017: loss=2.9068, lr=0.000359, tokens/sec=1374531.28, grad_norm=0.3149, duration=0.38s
Step 12018: loss=2.9812, lr=0.000359, tokens/sec=1373133.98, grad_norm=0.3024, duration=0.38s
Step 12019: loss=2.9344, lr=0.000359, tokens/sec=1374436.78, grad_norm=0.3283, duration=0.38s
Step 12020: loss=2.8970, lr=0.000359, tokens/sec=1373587.70, grad_norm=0.3191, duration=0.38s
Step 12021: loss=2.9493, lr=0.000359, tokens/sec=1374398.12, grad_norm=0.3134, duration=0.38s
Step 12022: loss=2.9297, lr=0.000359, tokens/sec=1377178.32, grad_norm=0.3137, duration=0.38s
Step 12023: loss=2.9474, lr=0.000358, tokens/sec=1374648.14, grad_norm=0.3332, duration=0.38s
Step 12024: loss=2.9248, lr=0.000358, tokens/sec=1378745.51, grad_norm=0.3151, duration=0.38s
Step 12025: loss=2.9143, lr=0.000358, tokens/sec=1374931.77, grad_norm=0.3107, duration=0.38s
Step 12026: loss=2.9249, lr=0.000358, tokens/sec=1375620.72, grad_norm=0.3388, duration=0.38s
Step 12027: loss=2.9401, lr=0.000358, tokens/sec=1373992.79, grad_norm=0.3270, duration=0.38s
Step 12028: loss=2.8907, lr=0.000358, tokens/sec=1372673.69, grad_norm=0.3313, duration=0.38s
Step 12029: loss=2.8950, lr=0.000358, tokens/sec=1378209.76, grad_norm=0.3397, duration=0.38s
Step 12030: loss=2.9040, lr=0.000358, tokens/sec=1370201.00, grad_norm=0.2968, duration=0.38s
Step 12031: loss=2.8255, lr=0.000358, tokens/sec=1376293.12, grad_norm=0.3421, duration=0.38s
Step 12032: loss=2.8608, lr=0.000358, tokens/sec=1376550.72, grad_norm=0.3571, duration=0.38s
Step 12033: loss=2.9202, lr=0.000358, tokens/sec=1376322.41, grad_norm=0.2953, duration=0.38s
Step 12034: loss=2.9215, lr=0.000358, tokens/sec=1376114.84, grad_norm=0.3172, duration=0.38s
Step 12035: loss=2.9383, lr=0.000358, tokens/sec=1377124.85, grad_norm=0.3625, duration=0.38s
Step 12036: loss=2.8723, lr=0.000358, tokens/sec=1372548.60, grad_norm=0.3023, duration=0.38s
Step 12037: loss=2.9639, lr=0.000358, tokens/sec=1374177.40, grad_norm=0.3011, duration=0.38s
Step 12038: loss=2.9194, lr=0.000358, tokens/sec=1379632.14, grad_norm=0.3385, duration=0.38s
Step 12039: loss=2.9728, lr=0.000358, tokens/sec=1373061.10, grad_norm=0.4190, duration=0.38s
Step 12040: loss=2.9590, lr=0.000358, tokens/sec=1375232.72, grad_norm=0.2832, duration=0.38s
Step 12041: loss=2.9229, lr=0.000358, tokens/sec=1376524.87, grad_norm=0.3043, duration=0.38s
Step 12042: loss=2.9055, lr=0.000358, tokens/sec=1373929.27, grad_norm=0.3103, duration=0.38s
Step 12043: loss=2.8915, lr=0.000358, tokens/sec=1378938.31, grad_norm=0.3041, duration=0.38s
Step 12044: loss=2.9388, lr=0.000358, tokens/sec=1376330.16, grad_norm=0.3010, duration=0.38s
Step 12045: loss=2.9458, lr=0.000358, tokens/sec=1373657.20, grad_norm=0.2909, duration=0.38s
Step 12046: loss=2.9349, lr=0.000358, tokens/sec=1373278.90, grad_norm=0.2985, duration=0.38s
Step 12047: loss=2.9235, lr=0.000358, tokens/sec=1373930.99, grad_norm=0.2901, duration=0.38s
Step 12048: loss=2.9484, lr=0.000358, tokens/sec=1371446.07, grad_norm=0.2985, duration=0.38s
Step 12049: loss=2.9040, lr=0.000358, tokens/sec=1371204.06, grad_norm=0.2914, duration=0.38s
Step 12050: loss=2.9557, lr=0.000358, tokens/sec=1372312.20, grad_norm=0.3016, duration=0.38s
Step 12051: loss=2.8519, lr=0.000358, tokens/sec=1373369.81, grad_norm=0.3034, duration=0.38s
Step 12052: loss=2.9081, lr=0.000358, tokens/sec=1376070.92, grad_norm=0.3021, duration=0.38s
Step 12053: loss=2.8796, lr=0.000358, tokens/sec=1373022.52, grad_norm=0.2854, duration=0.38s
Step 12054: loss=2.8756, lr=0.000358, tokens/sec=1371611.16, grad_norm=0.2879, duration=0.38s
Step 12055: loss=2.8267, lr=0.000358, tokens/sec=1375276.59, grad_norm=0.2985, duration=0.38s
Step 12056: loss=2.8402, lr=0.000358, tokens/sec=1373072.24, grad_norm=0.2990, duration=0.38s
Step 12057: loss=2.8549, lr=0.000357, tokens/sec=1375030.64, grad_norm=0.2853, duration=0.38s
Step 12058: loss=2.8311, lr=0.000357, tokens/sec=1375087.39, grad_norm=0.2910, duration=0.38s
Step 12059: loss=2.8507, lr=0.000357, tokens/sec=1371174.13, grad_norm=0.2894, duration=0.38s
Step 12060: loss=2.8536, lr=0.000357, tokens/sec=1370556.26, grad_norm=0.2845, duration=0.38s
Step 12061: loss=2.8778, lr=0.000357, tokens/sec=1374249.53, grad_norm=0.2946, duration=0.38s
Step 12062: loss=2.8583, lr=0.000357, tokens/sec=1373561.96, grad_norm=0.2839, duration=0.38s
Step 12063: loss=2.8944, lr=0.000357, tokens/sec=1375200.90, grad_norm=0.2998, duration=0.38s
Step 12064: loss=2.9264, lr=0.000357, tokens/sec=1375064.18, grad_norm=0.2953, duration=0.38s
Step 12065: loss=2.9503, lr=0.000357, tokens/sec=1373780.78, grad_norm=0.3145, duration=0.38s
Step 12066: loss=2.9487, lr=0.000357, tokens/sec=1377053.27, grad_norm=0.2999, duration=0.38s
Step 12067: loss=2.9743, lr=0.000357, tokens/sec=1371745.50, grad_norm=0.2874, duration=0.38s
Step 12068: loss=2.8905, lr=0.000357, tokens/sec=1374588.85, grad_norm=0.3297, duration=0.38s
Step 12069: loss=2.9341, lr=0.000357, tokens/sec=1376562.78, grad_norm=0.3164, duration=0.38s
Step 12070: loss=2.8983, lr=0.000357, tokens/sec=1372468.08, grad_norm=0.3127, duration=0.38s
Step 12071: loss=2.9636, lr=0.000357, tokens/sec=1373668.36, grad_norm=0.2958, duration=0.38s
Step 12072: loss=2.9402, lr=0.000357, tokens/sec=1372474.08, grad_norm=0.3038, duration=0.38s
Step 12073: loss=2.9320, lr=0.000357, tokens/sec=1372807.37, grad_norm=0.3192, duration=0.38s
Step 12074: loss=2.9412, lr=0.000357, tokens/sec=1376206.99, grad_norm=0.3195, duration=0.38s
Step 12075: loss=2.9097, lr=0.000357, tokens/sec=1375373.79, grad_norm=0.2981, duration=0.38s
Step 12076: loss=2.9080, lr=0.000357, tokens/sec=1375415.08, grad_norm=0.3185, duration=0.38s
Step 12077: loss=2.9059, lr=0.000357, tokens/sec=1374640.41, grad_norm=0.2973, duration=0.38s
Step 12078: loss=2.9404, lr=0.000357, tokens/sec=1374920.60, grad_norm=0.3017, duration=0.38s
Step 12079: loss=2.8870, lr=0.000357, tokens/sec=1374128.45, grad_norm=0.3104, duration=0.38s
Step 12080: loss=2.9084, lr=0.000357, tokens/sec=1372381.57, grad_norm=0.2841, duration=0.38s
Step 12081: loss=2.8925, lr=0.000357, tokens/sec=1373390.39, grad_norm=0.2944, duration=0.38s
Step 12082: loss=2.9178, lr=0.000357, tokens/sec=1375260.25, grad_norm=0.3155, duration=0.38s
Step 12083: loss=2.8512, lr=0.000357, tokens/sec=1372307.92, grad_norm=0.2953, duration=0.38s
Step 12084: loss=2.9382, lr=0.000357, tokens/sec=1374476.30, grad_norm=0.3069, duration=0.38s
Step 12085: loss=2.8664, lr=0.000357, tokens/sec=1376566.23, grad_norm=0.3194, duration=0.38s
Step 12086: loss=2.9015, lr=0.000357, tokens/sec=1374690.25, grad_norm=0.3079, duration=0.38s
Step 12087: loss=2.8861, lr=0.000357, tokens/sec=1375297.23, grad_norm=0.3027, duration=0.38s
Step 12088: loss=2.9066, lr=0.000357, tokens/sec=1372875.08, grad_norm=0.3101, duration=0.38s
Step 12089: loss=2.8942, lr=0.000357, tokens/sec=1370446.08, grad_norm=0.2999, duration=0.38s
Step 12090: loss=2.9031, lr=0.000357, tokens/sec=1375731.73, grad_norm=0.3100, duration=0.38s
Step 12091: loss=2.8722, lr=0.000356, tokens/sec=1373437.57, grad_norm=0.3309, duration=0.38s
Step 12092: loss=2.8988, lr=0.000356, tokens/sec=1371505.09, grad_norm=0.3163, duration=0.38s
Step 12093: loss=2.9228, lr=0.000356, tokens/sec=1372438.10, grad_norm=0.3158, duration=0.38s
Step 12094: loss=2.9464, lr=0.000356, tokens/sec=1371838.77, grad_norm=0.3206, duration=0.38s
Step 12095: loss=2.9330, lr=0.000356, tokens/sec=1374706.58, grad_norm=0.3261, duration=0.38s
Step 12096: loss=2.9040, lr=0.000356, tokens/sec=1373516.49, grad_norm=0.2920, duration=0.38s
Step 12097: loss=2.9131, lr=0.000356, tokens/sec=1377567.41, grad_norm=0.3300, duration=0.38s
Step 12098: loss=2.8929, lr=0.000356, tokens/sec=1371593.20, grad_norm=0.3567, duration=0.38s
Step 12099: loss=2.8607, lr=0.000356, tokens/sec=1372725.11, grad_norm=0.2975, duration=0.38s
Step 12100/19073 (63.4%), Elapsed time: 4795.24s, Steps per hour: 9084.02, Estimated hours remaining: 0.77
Step 12100: loss=2.8535, lr=0.000356, tokens/sec=1372112.69, grad_norm=0.2778, duration=0.38s
Step 12101: loss=2.8024, lr=0.000356, tokens/sec=1375680.96, grad_norm=0.3198, duration=0.38s
Step 12102: loss=2.8431, lr=0.000356, tokens/sec=1372040.77, grad_norm=0.3152, duration=0.38s
Step 12103: loss=2.8712, lr=0.000356, tokens/sec=1369103.09, grad_norm=0.3029, duration=0.38s
Step 12104: loss=2.9435, lr=0.000356, tokens/sec=1376209.57, grad_norm=0.3184, duration=0.38s
Step 12105: loss=2.8869, lr=0.000356, tokens/sec=1374687.67, grad_norm=0.3218, duration=0.38s
Step 12106: loss=2.8599, lr=0.000356, tokens/sec=1374305.36, grad_norm=0.3277, duration=0.38s
Step 12107: loss=2.8733, lr=0.000356, tokens/sec=1377060.17, grad_norm=0.2947, duration=0.38s
Step 12108: loss=2.8380, lr=0.000356, tokens/sec=1373845.15, grad_norm=0.3003, duration=0.38s
Step 12109: loss=2.9194, lr=0.000356, tokens/sec=1375889.25, grad_norm=0.3478, duration=0.38s
Step 12110: loss=2.9348, lr=0.000356, tokens/sec=1376416.31, grad_norm=0.3139, duration=0.38s
Step 12111: loss=2.9728, lr=0.000356, tokens/sec=1374430.77, grad_norm=0.2893, duration=0.38s
Step 12112: loss=2.9382, lr=0.000356, tokens/sec=1372902.51, grad_norm=0.3518, duration=0.38s
Step 12113: loss=2.9330, lr=0.000356, tokens/sec=1373569.69, grad_norm=0.3353, duration=0.38s
Step 12114: loss=2.9028, lr=0.000356, tokens/sec=1373849.44, grad_norm=0.3177, duration=0.38s
Step 12115: loss=2.9111, lr=0.000356, tokens/sec=1374447.95, grad_norm=0.3260, duration=0.38s
Step 12116: loss=2.9363, lr=0.000356, tokens/sec=1378922.74, grad_norm=0.3478, duration=0.38s
Step 12117: loss=2.9617, lr=0.000356, tokens/sec=1375294.65, grad_norm=0.2966, duration=0.38s
Step 12118: loss=3.0405, lr=0.000356, tokens/sec=1372450.95, grad_norm=0.3149, duration=0.38s
Step 12119: loss=2.9657, lr=0.000356, tokens/sec=1375347.98, grad_norm=0.3477, duration=0.38s
Step 12120: loss=2.9561, lr=0.000356, tokens/sec=1373961.03, grad_norm=0.3194, duration=0.38s
Step 12121: loss=2.8948, lr=0.000356, tokens/sec=1373289.19, grad_norm=0.2802, duration=0.38s
Step 12122: loss=2.9354, lr=0.000356, tokens/sec=1372893.94, grad_norm=0.3380, duration=0.38s
Step 12123: loss=2.9276, lr=0.000356, tokens/sec=1376463.69, grad_norm=0.3652, duration=0.38s
Step 12124: loss=2.8546, lr=0.000356, tokens/sec=1374173.10, grad_norm=0.2943, duration=0.38s
Step 12125: loss=2.9428, lr=0.000356, tokens/sec=1373767.90, grad_norm=0.3207, duration=0.38s
Step 12126: loss=2.8683, lr=0.000355, tokens/sec=1370886.06, grad_norm=0.3769, duration=0.38s
Step 12127: loss=2.9661, lr=0.000355, tokens/sec=1375705.05, grad_norm=0.3109, duration=0.38s
Step 12128: loss=3.0041, lr=0.000355, tokens/sec=1375309.27, grad_norm=0.3229, duration=0.38s
Step 12129: loss=2.9241, lr=0.000355, tokens/sec=1373446.15, grad_norm=0.3393, duration=0.38s
Step 12130: loss=2.8883, lr=0.000355, tokens/sec=1376706.70, grad_norm=0.3191, duration=0.38s
Step 12131: loss=2.9092, lr=0.000355, tokens/sec=1373751.60, grad_norm=0.3089, duration=0.38s
Step 12132: loss=2.9564, lr=0.000355, tokens/sec=1373583.41, grad_norm=0.3164, duration=0.38s
Step 12133: loss=2.8960, lr=0.000355, tokens/sec=1375843.63, grad_norm=0.3115, duration=0.38s
Step 12134: loss=2.8502, lr=0.000355, tokens/sec=1375668.91, grad_norm=0.2936, duration=0.38s
Step 12135: loss=2.9016, lr=0.000355, tokens/sec=1373306.34, grad_norm=0.3201, duration=0.38s
Step 12136: loss=2.8853, lr=0.000355, tokens/sec=1372744.81, grad_norm=0.3174, duration=0.38s
Step 12137: loss=2.9448, lr=0.000355, tokens/sec=1373857.17, grad_norm=0.2911, duration=0.38s
Step 12138: loss=2.9602, lr=0.000355, tokens/sec=1371908.95, grad_norm=0.3119, duration=0.38s
Step 12139: loss=2.8192, lr=0.000355, tokens/sec=1370657.06, grad_norm=0.3211, duration=0.38s
Step 12140: loss=2.8796, lr=0.000355, tokens/sec=1370873.24, grad_norm=0.3046, duration=0.38s
Step 12141: loss=2.8799, lr=0.000355, tokens/sec=1373386.10, grad_norm=0.2834, duration=0.38s
Step 12142: loss=2.8677, lr=0.000355, tokens/sec=1373517.35, grad_norm=0.2886, duration=0.38s
Step 12143: loss=2.8989, lr=0.000355, tokens/sec=1373622.02, grad_norm=0.3172, duration=0.38s
Step 12144: loss=2.8653, lr=0.000355, tokens/sec=1373428.14, grad_norm=0.2881, duration=0.38s
Step 12145: loss=2.8292, lr=0.000355, tokens/sec=1374921.46, grad_norm=0.2938, duration=0.38s
Step 12146: loss=2.8496, lr=0.000355, tokens/sec=1376123.45, grad_norm=0.2833, duration=0.38s
Step 12147: loss=2.9444, lr=0.000355, tokens/sec=1374930.05, grad_norm=0.2975, duration=0.38s
Step 12148: loss=2.8843, lr=0.000355, tokens/sec=1374948.97, grad_norm=0.2882, duration=0.38s
Step 12149: loss=2.8489, lr=0.000355, tokens/sec=1376573.12, grad_norm=0.2753, duration=0.38s
Step 12150: loss=2.8224, lr=0.000355, tokens/sec=1369701.73, grad_norm=0.2793, duration=0.38s
Step 12151: loss=2.8403, lr=0.000355, tokens/sec=1370714.31, grad_norm=0.2737, duration=0.38s
Step 12152: loss=2.8852, lr=0.000355, tokens/sec=1370911.70, grad_norm=0.2909, duration=0.38s
Step 12153: loss=2.8746, lr=0.000355, tokens/sec=1374392.97, grad_norm=0.3094, duration=0.38s
Step 12154: loss=2.8708, lr=0.000355, tokens/sec=1374232.36, grad_norm=0.2724, duration=0.38s
Step 12155: loss=2.9218, lr=0.000355, tokens/sec=1374395.55, grad_norm=0.2952, duration=0.38s
Step 12156: loss=2.9248, lr=0.000355, tokens/sec=1369932.12, grad_norm=0.3196, duration=0.38s
Step 12157: loss=2.9233, lr=0.000355, tokens/sec=1371349.42, grad_norm=0.2931, duration=0.38s
Step 12158: loss=2.9234, lr=0.000355, tokens/sec=1372594.87, grad_norm=0.3011, duration=0.38s
Step 12159: loss=2.9268, lr=0.000355, tokens/sec=1373459.87, grad_norm=0.3107, duration=0.38s
Step 12160: loss=2.9107, lr=0.000354, tokens/sec=1374503.79, grad_norm=0.2993, duration=0.38s
Step 12161: loss=2.9611, lr=0.000354, tokens/sec=1371930.35, grad_norm=0.3059, duration=0.38s
Step 12162: loss=2.9289, lr=0.000354, tokens/sec=1367231.22, grad_norm=0.3223, duration=0.38s
Step 12163: loss=3.0278, lr=0.000354, tokens/sec=1372082.72, grad_norm=0.3066, duration=0.38s
Step 12164: loss=2.9495, lr=0.000354, tokens/sec=1372006.53, grad_norm=0.2869, duration=0.38s
Step 12165: loss=2.9586, lr=0.000354, tokens/sec=1373585.99, grad_norm=0.3270, duration=0.38s
Step 12166: loss=3.0202, lr=0.000354, tokens/sec=1373547.38, grad_norm=0.3423, duration=0.38s
Step 12167: loss=2.9477, lr=0.000354, tokens/sec=1371907.24, grad_norm=0.3142, duration=0.38s
Step 12168: loss=2.9757, lr=0.000354, tokens/sec=1372766.24, grad_norm=0.2989, duration=0.38s
Step 12169: loss=2.9577, lr=0.000354, tokens/sec=1374128.45, grad_norm=0.3199, duration=0.38s
Step 12170: loss=2.9615, lr=0.000354, tokens/sec=1373432.42, grad_norm=0.3163, duration=0.38s
Step 12171: loss=2.9162, lr=0.000354, tokens/sec=1371503.38, grad_norm=0.3022, duration=0.38s
Step 12172: loss=2.9339, lr=0.000354, tokens/sec=1373743.02, grad_norm=0.2990, duration=0.38s
Step 12173: loss=2.8995, lr=0.000354, tokens/sec=1370905.72, grad_norm=0.3012, duration=0.38s
Step 12174: loss=2.9382, lr=0.000354, tokens/sec=1374974.76, grad_norm=0.2914, duration=0.38s
Step 12175: loss=2.9250, lr=0.000354, tokens/sec=1377019.64, grad_norm=0.3400, duration=0.38s
Step 12176: loss=2.8747, lr=0.000354, tokens/sec=1372468.94, grad_norm=0.3353, duration=0.38s
Step 12177: loss=2.9520, lr=0.000354, tokens/sec=1373511.35, grad_norm=0.3243, duration=0.38s
Step 12178: loss=2.8694, lr=0.000354, tokens/sec=1374163.66, grad_norm=0.3093, duration=0.38s
Step 12179: loss=2.8088, lr=0.000354, tokens/sec=1371016.83, grad_norm=0.3631, duration=0.38s
Step 12180: loss=2.8727, lr=0.000354, tokens/sec=1371180.12, grad_norm=0.3209, duration=0.38s
Step 12181: loss=2.9059, lr=0.000354, tokens/sec=1366048.10, grad_norm=0.3144, duration=0.38s
Step 12182: loss=2.9780, lr=0.000354, tokens/sec=1374635.25, grad_norm=0.3115, duration=0.38s
Step 12183: loss=2.8329, lr=0.000354, tokens/sec=1373551.67, grad_norm=0.3046, duration=0.38s
Step 12184: loss=2.9434, lr=0.000354, tokens/sec=1371873.01, grad_norm=0.3005, duration=0.38s
Step 12185: loss=2.8636, lr=0.000354, tokens/sec=1371950.04, grad_norm=0.3031, duration=0.38s
Step 12186: loss=2.8675, lr=0.000354, tokens/sec=1370145.51, grad_norm=0.2970, duration=0.38s
Step 12187: loss=2.8997, lr=0.000354, tokens/sec=1369996.98, grad_norm=0.2985, duration=0.38s
Step 12188: loss=2.9595, lr=0.000354, tokens/sec=1370994.61, grad_norm=0.3106, duration=0.38s
Step 12189: loss=2.9299, lr=0.000354, tokens/sec=1374546.75, grad_norm=0.2810, duration=0.38s
Step 12190: loss=2.8943, lr=0.000354, tokens/sec=1370609.22, grad_norm=0.2937, duration=0.38s
Step 12191: loss=2.9214, lr=0.000354, tokens/sec=1373344.93, grad_norm=0.3175, duration=0.38s
Step 12192: loss=2.9353, lr=0.000354, tokens/sec=1376161.34, grad_norm=0.3345, duration=0.38s
Step 12193: loss=2.9486, lr=0.000354, tokens/sec=1374327.69, grad_norm=0.3049, duration=0.38s
Step 12194: loss=2.8672, lr=0.000353, tokens/sec=1374465.13, grad_norm=0.3184, duration=0.38s
Step 12195: loss=2.8596, lr=0.000353, tokens/sec=1375546.71, grad_norm=0.2981, duration=0.38s
Step 12196: loss=2.9028, lr=0.000353, tokens/sec=1373290.90, grad_norm=0.2969, duration=0.38s
Step 12197: loss=2.8489, lr=0.000353, tokens/sec=1375032.36, grad_norm=0.3215, duration=0.38s
Step 12198: loss=2.8701, lr=0.000353, tokens/sec=1375166.50, grad_norm=0.3082, duration=0.38s
Step 12199: loss=2.8525, lr=0.000353, tokens/sec=1371959.45, grad_norm=0.2980, duration=0.38s
Step 12200/19073 (64.0%), Elapsed time: 4833.50s, Steps per hour: 9086.57, Estimated hours remaining: 0.76
Step 12200: loss=2.8853, lr=0.000353, tokens/sec=1369809.24, grad_norm=0.3362, duration=0.38s
Step 12201: loss=2.8763, lr=0.000353, tokens/sec=1375323.04, grad_norm=0.3100, duration=0.38s
Step 12202: loss=2.8660, lr=0.000353, tokens/sec=1367420.82, grad_norm=0.2795, duration=0.38s
Step 12203: loss=2.9045, lr=0.000353, tokens/sec=1370119.04, grad_norm=0.3125, duration=0.38s
Step 12204: loss=2.8266, lr=0.000353, tokens/sec=1375838.46, grad_norm=0.3068, duration=0.38s
Step 12205: loss=2.8518, lr=0.000353, tokens/sec=1374002.24, grad_norm=0.2947, duration=0.38s
Step 12206: loss=2.8353, lr=0.000353, tokens/sec=1373457.30, grad_norm=0.3021, duration=0.38s
Step 12207: loss=2.9371, lr=0.000353, tokens/sec=1366717.13, grad_norm=0.3187, duration=0.38s
Step 12208: loss=2.9716, lr=0.000353, tokens/sec=1375612.97, grad_norm=0.3056, duration=0.38s
Step 12209: loss=2.8971, lr=0.000353, tokens/sec=1377042.92, grad_norm=0.3201, duration=0.38s
Step 12210: loss=2.9196, lr=0.000353, tokens/sec=1375290.35, grad_norm=0.3273, duration=0.38s
Step 12211: loss=2.9297, lr=0.000353, tokens/sec=1375837.60, grad_norm=0.3325, duration=0.38s
Step 12212: loss=2.9428, lr=0.000353, tokens/sec=1371124.54, grad_norm=0.3108, duration=0.38s
Step 12213: loss=2.9557, lr=0.000353, tokens/sec=1371154.47, grad_norm=0.3308, duration=0.38s
Step 12214: loss=2.9188, lr=0.000353, tokens/sec=1370968.97, grad_norm=0.3330, duration=0.38s
Step 12215: loss=2.8991, lr=0.000353, tokens/sec=1372960.80, grad_norm=0.3268, duration=0.38s
Step 12216: loss=2.9483, lr=0.000353, tokens/sec=1372915.37, grad_norm=0.3394, duration=0.38s
Step 12217: loss=2.8898, lr=0.000353, tokens/sec=1372281.37, grad_norm=0.3125, duration=0.38s
Step 12218: loss=2.8911, lr=0.000353, tokens/sec=1370486.22, grad_norm=0.3415, duration=0.38s
Step 12219: loss=2.8994, lr=0.000353, tokens/sec=1377343.07, grad_norm=0.3426, duration=0.38s
Step 12220: loss=2.8957, lr=0.000353, tokens/sec=1376194.93, grad_norm=0.3193, duration=0.38s
Step 12221: loss=2.7989, lr=0.000353, tokens/sec=1374480.59, grad_norm=0.3147, duration=0.38s
Step 12222: loss=2.8743, lr=0.000353, tokens/sec=1375847.07, grad_norm=0.3467, duration=0.38s
Step 12223: loss=2.9274, lr=0.000353, tokens/sec=1374294.19, grad_norm=0.3334, duration=0.38s
Step 12224: loss=2.8903, lr=0.000353, tokens/sec=1374313.09, grad_norm=0.3019, duration=0.38s
Step 12225: loss=2.9254, lr=0.000353, tokens/sec=1375495.95, grad_norm=0.3250, duration=0.38s
Step 12226: loss=2.8807, lr=0.000353, tokens/sec=1374875.90, grad_norm=0.3098, duration=0.38s
Step 12227: loss=2.9521, lr=0.000353, tokens/sec=1376467.14, grad_norm=0.3134, duration=0.38s
Step 12228: loss=2.9814, lr=0.000353, tokens/sec=1373331.21, grad_norm=0.3270, duration=0.38s
Step 12229: loss=2.9552, lr=0.000352, tokens/sec=1375012.59, grad_norm=0.3262, duration=0.38s
Step 12230: loss=2.9145, lr=0.000352, tokens/sec=1373733.58, grad_norm=0.2952, duration=0.38s
Step 12231: loss=2.9257, lr=0.000352, tokens/sec=1379435.68, grad_norm=0.3003, duration=0.38s
Step 12232: loss=2.8677, lr=0.000352, tokens/sec=1375865.15, grad_norm=0.3255, duration=0.38s
Step 12233: loss=2.9314, lr=0.000352, tokens/sec=1374105.27, grad_norm=0.3136, duration=0.38s
Step 12234: loss=2.9467, lr=0.000352, tokens/sec=1373895.79, grad_norm=0.2974, duration=0.38s
Step 12235: loss=2.9275, lr=0.000352, tokens/sec=1374954.98, grad_norm=0.3113, duration=0.38s
Step 12236: loss=2.9635, lr=0.000352, tokens/sec=1374868.16, grad_norm=0.3140, duration=0.38s
Step 12237: loss=2.9046, lr=0.000352, tokens/sec=1374171.39, grad_norm=0.2937, duration=0.38s
Step 12238: loss=2.9101, lr=0.000352, tokens/sec=1374141.33, grad_norm=0.3094, duration=0.38s
Step 12239: loss=2.9369, lr=0.000352, tokens/sec=1376548.13, grad_norm=0.3127, duration=0.38s
Step 12240: loss=2.9143, lr=0.000352, tokens/sec=1376715.32, grad_norm=0.2896, duration=0.38s
Step 12241: loss=2.8381, lr=0.000352, tokens/sec=1376732.56, grad_norm=0.3078, duration=0.38s
Step 12242: loss=2.9283, lr=0.000352, tokens/sec=1377664.93, grad_norm=0.3173, duration=0.38s
Step 12243: loss=2.8607, lr=0.000352, tokens/sec=1374426.47, grad_norm=0.2967, duration=0.38s
Step 12244: loss=2.8660, lr=0.000352, tokens/sec=1378720.44, grad_norm=0.3084, duration=0.38s
Step 12245: loss=2.8013, lr=0.000352, tokens/sec=1376139.81, grad_norm=0.3110, duration=0.38s
Step 12246: loss=2.8312, lr=0.000352, tokens/sec=1374056.33, grad_norm=0.3005, duration=0.38s
Step 12247: loss=2.8764, lr=0.000352, tokens/sec=1377817.72, grad_norm=0.3194, duration=0.38s
Step 12248: loss=2.8255, lr=0.000352, tokens/sec=1377923.05, grad_norm=0.3196, duration=0.38s
Step 12249: loss=2.8675, lr=0.000352, tokens/sec=1376812.73, grad_norm=0.3045, duration=0.38s
Validation loss at step 12250: 3.793288230895996
Step 12250: loss=2.8180, lr=0.000352, tokens/sec=156591.33, grad_norm=0.2995, duration=3.35s
Step 12251: loss=2.8686, lr=0.000352, tokens/sec=1376657.58, grad_norm=0.3093, duration=0.38s
Step 12252: loss=2.8638, lr=0.000352, tokens/sec=1380824.18, grad_norm=0.2952, duration=0.38s
Step 12253: loss=2.8787, lr=0.000352, tokens/sec=1379005.76, grad_norm=0.3178, duration=0.38s
Step 12254: loss=2.9422, lr=0.000352, tokens/sec=1376948.08, grad_norm=0.3133, duration=0.38s
Step 12255: loss=2.9384, lr=0.000352, tokens/sec=1374435.06, grad_norm=0.3248, duration=0.38s
Step 12256: loss=2.9685, lr=0.000352, tokens/sec=1378260.72, grad_norm=0.3198, duration=0.38s
Step 12257: loss=2.9480, lr=0.000352, tokens/sec=1378681.54, grad_norm=0.3005, duration=0.38s
Step 12258: loss=2.8787, lr=0.000352, tokens/sec=1377662.34, grad_norm=0.3047, duration=0.38s
Step 12259: loss=2.9257, lr=0.000352, tokens/sec=1372673.69, grad_norm=0.3240, duration=0.38s
Step 12260: loss=2.8989, lr=0.000352, tokens/sec=1375575.97, grad_norm=0.3298, duration=0.38s
Step 12261: loss=2.9434, lr=0.000352, tokens/sec=1373373.24, grad_norm=0.2874, duration=0.38s
Step 12262: loss=2.9407, lr=0.000352, tokens/sec=1372658.27, grad_norm=0.3089, duration=0.38s
Step 12263: loss=2.9349, lr=0.000351, tokens/sec=1373006.23, grad_norm=0.3278, duration=0.38s
Step 12264: loss=2.9366, lr=0.000351, tokens/sec=1373673.51, grad_norm=0.3190, duration=0.38s
Step 12265: loss=2.8910, lr=0.000351, tokens/sec=1375767.88, grad_norm=0.3129, duration=0.38s
Step 12266: loss=2.8965, lr=0.000351, tokens/sec=1374465.99, grad_norm=0.3236, duration=0.38s
Step 12267: loss=2.9306, lr=0.000351, tokens/sec=1377671.84, grad_norm=0.3240, duration=0.38s
Step 12268: loss=2.9341, lr=0.000351, tokens/sec=1379749.86, grad_norm=0.3166, duration=0.38s
Step 12269: loss=2.8549, lr=0.000351, tokens/sec=1375978.79, grad_norm=0.3108, duration=0.38s
Step 12270: loss=2.8978, lr=0.000351, tokens/sec=1376413.72, grad_norm=0.3207, duration=0.38s
Step 12271: loss=2.8965, lr=0.000351, tokens/sec=1375847.93, grad_norm=0.3059, duration=0.38s
Step 12272: loss=2.9050, lr=0.000351, tokens/sec=1375754.97, grad_norm=0.3093, duration=0.38s
Step 12273: loss=2.8649, lr=0.000351, tokens/sec=1379652.91, grad_norm=0.3062, duration=0.38s
Step 12274: loss=2.9156, lr=0.000351, tokens/sec=1376882.55, grad_norm=0.3026, duration=0.38s
Step 12275: loss=2.8426, lr=0.000351, tokens/sec=1375877.20, grad_norm=0.3086, duration=0.38s
Step 12276: loss=2.9132, lr=0.000351, tokens/sec=1375317.87, grad_norm=0.3033, duration=0.38s
Step 12277: loss=2.8813, lr=0.000351, tokens/sec=1372247.97, grad_norm=0.3002, duration=0.38s
Step 12278: loss=2.9072, lr=0.000351, tokens/sec=1376458.52, grad_norm=0.3096, duration=0.38s
Step 12279: loss=2.8897, lr=0.000351, tokens/sec=1374020.27, grad_norm=0.3038, duration=0.38s
Step 12280: loss=2.8712, lr=0.000351, tokens/sec=1372177.76, grad_norm=0.2949, duration=0.38s
Step 12281: loss=2.8881, lr=0.000351, tokens/sec=1377582.94, grad_norm=0.3210, duration=0.38s
Step 12282: loss=2.9378, lr=0.000351, tokens/sec=1376203.54, grad_norm=0.3333, duration=0.38s
Step 12283: loss=2.9242, lr=0.000351, tokens/sec=1375625.88, grad_norm=0.3089, duration=0.38s
Step 12284: loss=2.9257, lr=0.000351, tokens/sec=1371556.41, grad_norm=0.3117, duration=0.38s
Step 12285: loss=2.9134, lr=0.000351, tokens/sec=1379313.68, grad_norm=0.3360, duration=0.38s
Step 12286: loss=2.9192, lr=0.000351, tokens/sec=1378044.80, grad_norm=0.3136, duration=0.38s
Step 12287: loss=2.8924, lr=0.000351, tokens/sec=1371726.67, grad_norm=0.3054, duration=0.38s
Step 12288: loss=2.9124, lr=0.000351, tokens/sec=1377679.61, grad_norm=0.3393, duration=0.38s
Step 12289: loss=2.8144, lr=0.000351, tokens/sec=1374086.38, grad_norm=0.3231, duration=0.38s
Step 12290: loss=2.8772, lr=0.000351, tokens/sec=1375414.22, grad_norm=0.3071, duration=0.38s
Step 12291: loss=2.7916, lr=0.000351, tokens/sec=1377969.67, grad_norm=0.3064, duration=0.38s
Step 12292: loss=2.8679, lr=0.000351, tokens/sec=1372693.40, grad_norm=0.3076, duration=0.38s
Step 12293: loss=2.8750, lr=0.000351, tokens/sec=1373160.56, grad_norm=0.3124, duration=0.38s
Step 12294: loss=2.9170, lr=0.000351, tokens/sec=1377079.14, grad_norm=0.3161, duration=0.38s
Step 12295: loss=2.8797, lr=0.000351, tokens/sec=1375977.93, grad_norm=0.2983, duration=0.38s
Step 12296: loss=2.8849, lr=0.000351, tokens/sec=1372839.08, grad_norm=0.3142, duration=0.38s
Step 12297: loss=2.8040, lr=0.000351, tokens/sec=1373311.49, grad_norm=0.3205, duration=0.38s
Step 12298: loss=2.8792, lr=0.000350, tokens/sec=1376365.48, grad_norm=0.2950, duration=0.38s
Step 12299: loss=2.9134, lr=0.000350, tokens/sec=1374562.21, grad_norm=0.3163, duration=0.38s
Step 12300/19073 (64.5%), Elapsed time: 4874.69s, Steps per hour: 9083.66, Estimated hours remaining: 0.75
Step 12300: loss=2.9483, lr=0.000350, tokens/sec=1377672.70, grad_norm=0.3347, duration=0.38s
Step 12301: loss=2.9245, lr=0.000350, tokens/sec=1375371.21, grad_norm=0.3019, duration=0.38s
Step 12302: loss=2.9496, lr=0.000350, tokens/sec=1377672.70, grad_norm=0.3157, duration=0.38s
Step 12303: loss=2.8724, lr=0.000350, tokens/sec=1376670.51, grad_norm=0.3524, duration=0.38s
Step 12304: loss=2.9586, lr=0.000350, tokens/sec=1375785.96, grad_norm=0.3156, duration=0.38s
Step 12305: loss=2.9031, lr=0.000350, tokens/sec=1375552.74, grad_norm=0.3183, duration=0.38s
Step 12306: loss=2.9137, lr=0.000350, tokens/sec=1372887.08, grad_norm=0.3437, duration=0.38s
Step 12307: loss=2.9747, lr=0.000350, tokens/sec=1377767.65, grad_norm=0.3247, duration=0.38s
Step 12308: loss=3.0146, lr=0.000350, tokens/sec=1373589.42, grad_norm=0.3009, duration=0.38s
Step 12309: loss=2.9634, lr=0.000350, tokens/sec=1374552.76, grad_norm=0.3396, duration=0.38s
Step 12310: loss=2.9440, lr=0.000350, tokens/sec=1375722.27, grad_norm=0.3294, duration=0.38s
Step 12311: loss=2.9024, lr=0.000350, tokens/sec=1377727.08, grad_norm=0.2881, duration=0.38s
Step 12312: loss=2.9598, lr=0.000350, tokens/sec=1374961.00, grad_norm=0.3065, duration=0.38s
Step 12313: loss=2.8843, lr=0.000350, tokens/sec=1375495.09, grad_norm=0.3291, duration=0.38s
Step 12314: loss=2.8474, lr=0.000350, tokens/sec=1375886.67, grad_norm=0.3156, duration=0.38s
Step 12315: loss=2.9492, lr=0.000350, tokens/sec=1377228.35, grad_norm=0.2902, duration=0.38s
Step 12316: loss=2.8778, lr=0.000350, tokens/sec=1372666.84, grad_norm=0.3139, duration=0.38s
Step 12317: loss=2.9785, lr=0.000350, tokens/sec=1374090.67, grad_norm=0.3225, duration=0.38s
Step 12318: loss=2.9673, lr=0.000350, tokens/sec=1374108.70, grad_norm=0.3052, duration=0.38s
Step 12319: loss=2.9297, lr=0.000350, tokens/sec=1372773.09, grad_norm=0.3133, duration=0.38s
Step 12320: loss=2.8517, lr=0.000350, tokens/sec=1376157.04, grad_norm=0.3084, duration=0.38s
Step 12321: loss=2.9268, lr=0.000350, tokens/sec=1377821.17, grad_norm=0.3145, duration=0.38s
Step 12322: loss=2.9254, lr=0.000350, tokens/sec=1373634.89, grad_norm=0.2995, duration=0.38s
Step 12323: loss=2.9212, lr=0.000350, tokens/sec=1370598.12, grad_norm=0.3199, duration=0.38s
Step 12324: loss=2.8401, lr=0.000350, tokens/sec=1371688.17, grad_norm=0.2986, duration=0.38s
Step 12325: loss=2.8748, lr=0.000350, tokens/sec=1372871.65, grad_norm=0.3124, duration=0.38s
Step 12326: loss=2.9177, lr=0.000350, tokens/sec=1372235.98, grad_norm=0.3335, duration=0.38s
Step 12327: loss=2.9278, lr=0.000350, tokens/sec=1371663.35, grad_norm=0.3106, duration=0.38s
Step 12328: loss=2.8914, lr=0.000350, tokens/sec=1373387.82, grad_norm=0.3292, duration=0.38s
Step 12329: loss=2.8638, lr=0.000350, tokens/sec=1375309.27, grad_norm=0.3304, duration=0.38s
Step 12330: loss=2.8762, lr=0.000350, tokens/sec=1377550.15, grad_norm=0.3035, duration=0.38s
Step 12331: loss=2.9077, lr=0.000350, tokens/sec=1379177.00, grad_norm=0.2994, duration=0.38s
Step 12332: loss=2.8575, lr=0.000350, tokens/sec=1375663.74, grad_norm=0.3026, duration=0.38s
Step 12333: loss=2.8960, lr=0.000349, tokens/sec=1377301.67, grad_norm=0.3044, duration=0.38s
Step 12334: loss=2.8406, lr=0.000349, tokens/sec=1379211.60, grad_norm=0.2990, duration=0.38s
Step 12335: loss=2.8381, lr=0.000349, tokens/sec=1373336.36, grad_norm=0.2887, duration=0.38s
Step 12336: loss=2.8753, lr=0.000349, tokens/sec=1371543.58, grad_norm=0.2807, duration=0.38s
Step 12337: loss=2.9282, lr=0.000349, tokens/sec=1377566.55, grad_norm=0.3076, duration=0.38s
Step 12338: loss=2.8651, lr=0.000349, tokens/sec=1378697.10, grad_norm=0.2828, duration=0.38s
Step 12339: loss=2.8446, lr=0.000349, tokens/sec=1371928.64, grad_norm=0.2909, duration=0.38s
Step 12340: loss=2.8116, lr=0.000349, tokens/sec=1376147.56, grad_norm=0.2794, duration=0.38s
Step 12341: loss=2.8438, lr=0.000349, tokens/sec=1375480.46, grad_norm=0.2798, duration=0.38s
Step 12342: loss=2.8863, lr=0.000349, tokens/sec=1377813.40, grad_norm=0.2862, duration=0.38s
Step 12343: loss=2.8462, lr=0.000349, tokens/sec=1377452.65, grad_norm=0.2967, duration=0.38s
Step 12344: loss=2.8744, lr=0.000349, tokens/sec=1371591.49, grad_norm=0.2838, duration=0.38s
Step 12345: loss=2.9185, lr=0.000349, tokens/sec=1371863.59, grad_norm=0.2906, duration=0.38s
Step 12346: loss=2.9141, lr=0.000349, tokens/sec=1370058.44, grad_norm=0.3172, duration=0.38s
Step 12347: loss=2.9489, lr=0.000349, tokens/sec=1371854.18, grad_norm=0.2987, duration=0.38s
Step 12348: loss=2.8926, lr=0.000349, tokens/sec=1377209.37, grad_norm=0.2866, duration=0.38s
Step 12349: loss=2.9310, lr=0.000349, tokens/sec=1375682.68, grad_norm=0.3259, duration=0.38s
Step 12350: loss=2.9237, lr=0.000349, tokens/sec=1374407.57, grad_norm=0.3221, duration=0.38s
Step 12351: loss=2.9178, lr=0.000349, tokens/sec=1375440.03, grad_norm=0.3049, duration=0.38s
Step 12352: loss=2.9570, lr=0.000349, tokens/sec=1379177.87, grad_norm=0.3131, duration=0.38s
Step 12353: loss=3.0114, lr=0.000349, tokens/sec=1374321.68, grad_norm=0.3376, duration=0.38s
Step 12354: loss=2.9324, lr=0.000349, tokens/sec=1371086.93, grad_norm=0.3139, duration=0.38s
Step 12355: loss=2.9697, lr=0.000349, tokens/sec=1375108.89, grad_norm=0.3264, duration=0.38s
Step 12356: loss=3.0061, lr=0.000349, tokens/sec=1370757.88, grad_norm=0.3369, duration=0.38s
Step 12357: loss=2.9567, lr=0.000349, tokens/sec=1373562.82, grad_norm=0.3246, duration=0.38s
Step 12358: loss=2.9700, lr=0.000349, tokens/sec=1372893.94, grad_norm=0.3255, duration=0.38s
Step 12359: loss=2.9411, lr=0.000349, tokens/sec=1374172.24, grad_norm=0.3052, duration=0.38s
Step 12360: loss=2.9457, lr=0.000349, tokens/sec=1374828.62, grad_norm=0.3117, duration=0.38s
Step 12361: loss=2.9180, lr=0.000349, tokens/sec=1376024.42, grad_norm=0.3350, duration=0.38s
Step 12362: loss=2.9550, lr=0.000349, tokens/sec=1374606.04, grad_norm=0.3104, duration=0.38s
Step 12363: loss=2.8617, lr=0.000349, tokens/sec=1376361.17, grad_norm=0.2993, duration=0.38s
Step 12364: loss=2.9348, lr=0.000349, tokens/sec=1375944.35, grad_norm=0.3041, duration=0.38s
Step 12365: loss=2.9121, lr=0.000349, tokens/sec=1376626.55, grad_norm=0.3381, duration=0.38s
Step 12366: loss=2.8796, lr=0.000349, tokens/sec=1368453.01, grad_norm=0.3317, duration=0.38s
Step 12367: loss=2.9498, lr=0.000349, tokens/sec=1371926.93, grad_norm=0.3392, duration=0.38s
Step 12368: loss=2.8369, lr=0.000348, tokens/sec=1377455.23, grad_norm=0.3177, duration=0.38s
Step 12369: loss=2.8195, lr=0.000348, tokens/sec=1374459.98, grad_norm=0.3528, duration=0.38s
Step 12370: loss=2.8613, lr=0.000348, tokens/sec=1373588.56, grad_norm=0.3263, duration=0.38s
Step 12371: loss=2.8975, lr=0.000348, tokens/sec=1375186.28, grad_norm=0.3199, duration=0.38s
Step 12372: loss=2.9790, lr=0.000348, tokens/sec=1375988.26, grad_norm=0.3214, duration=0.38s
Step 12373: loss=2.8344, lr=0.000348, tokens/sec=1378344.52, grad_norm=0.3187, duration=0.38s
Step 12374: loss=2.8736, lr=0.000348, tokens/sec=1377815.13, grad_norm=0.3205, duration=0.38s
Step 12375: loss=2.9210, lr=0.000348, tokens/sec=1374258.98, grad_norm=0.3115, duration=0.38s
Step 12376: loss=2.8592, lr=0.000348, tokens/sec=1375377.23, grad_norm=0.3058, duration=0.38s
Step 12377: loss=2.9304, lr=0.000348, tokens/sec=1375876.34, grad_norm=0.2946, duration=0.38s
Step 12378: loss=2.9512, lr=0.000348, tokens/sec=1377397.43, grad_norm=0.3305, duration=0.38s
Step 12379: loss=2.9259, lr=0.000348, tokens/sec=1373122.83, grad_norm=0.2977, duration=0.38s
Step 12380: loss=2.8843, lr=0.000348, tokens/sec=1367704.03, grad_norm=0.2940, duration=0.38s
Step 12381: loss=2.9183, lr=0.000348, tokens/sec=1376402.52, grad_norm=0.3161, duration=0.38s
Step 12382: loss=2.9203, lr=0.000348, tokens/sec=1375707.63, grad_norm=0.3666, duration=0.38s
Step 12383: loss=2.9194, lr=0.000348, tokens/sec=1372962.51, grad_norm=0.2876, duration=0.38s
Step 12384: loss=2.9095, lr=0.000348, tokens/sec=1378783.55, grad_norm=0.3067, duration=0.38s
Step 12385: loss=2.8553, lr=0.000348, tokens/sec=1376492.99, grad_norm=0.3053, duration=0.38s
Step 12386: loss=2.8913, lr=0.000348, tokens/sec=1377906.64, grad_norm=0.2948, duration=0.38s
Step 12387: loss=2.8704, lr=0.000348, tokens/sec=1374166.23, grad_norm=0.3090, duration=0.38s
Step 12388: loss=2.8303, lr=0.000348, tokens/sec=1372758.53, grad_norm=0.3197, duration=0.38s
Step 12389: loss=2.8657, lr=0.000348, tokens/sec=1373555.96, grad_norm=0.2837, duration=0.38s
Step 12390: loss=2.8681, lr=0.000348, tokens/sec=1379283.40, grad_norm=0.2961, duration=0.38s
Step 12391: loss=2.8644, lr=0.000348, tokens/sec=1376514.53, grad_norm=0.3216, duration=0.38s
Step 12392: loss=2.8824, lr=0.000348, tokens/sec=1376096.75, grad_norm=0.2941, duration=0.38s
Step 12393: loss=2.8455, lr=0.000348, tokens/sec=1375807.48, grad_norm=0.2882, duration=0.38s
Step 12394: loss=2.8483, lr=0.000348, tokens/sec=1375886.67, grad_norm=0.3027, duration=0.38s
Step 12395: loss=2.8168, lr=0.000348, tokens/sec=1375574.25, grad_norm=0.3035, duration=0.38s
Step 12396: loss=2.8657, lr=0.000348, tokens/sec=1376704.12, grad_norm=0.3009, duration=0.38s
Step 12397: loss=2.9289, lr=0.000348, tokens/sec=1371693.30, grad_norm=0.3156, duration=0.38s
Step 12398: loss=2.9357, lr=0.000348, tokens/sec=1371789.99, grad_norm=0.3159, duration=0.38s
Step 12399: loss=2.9189, lr=0.000348, tokens/sec=1375822.97, grad_norm=0.3071, duration=0.38s
Step 12400/19073 (65.0%), Elapsed time: 4912.92s, Steps per hour: 9086.25, Estimated hours remaining: 0.73
Step 12400: loss=2.9016, lr=0.000348, tokens/sec=1374017.69, grad_norm=0.3258, duration=0.38s
Step 12401: loss=2.9449, lr=0.000348, tokens/sec=1375285.19, grad_norm=0.3474, duration=0.38s
Step 12402: loss=2.9533, lr=0.000348, tokens/sec=1379764.58, grad_norm=0.3220, duration=0.38s
Step 12403: loss=2.9511, lr=0.000347, tokens/sec=1375505.41, grad_norm=0.3183, duration=0.38s
Step 12404: loss=2.9006, lr=0.000347, tokens/sec=1377726.22, grad_norm=0.3280, duration=0.38s
Step 12405: loss=2.9182, lr=0.000347, tokens/sec=1377330.13, grad_norm=0.3420, duration=0.38s
Step 12406: loss=2.9009, lr=0.000347, tokens/sec=1373541.37, grad_norm=0.3559, duration=0.38s
Step 12407: loss=2.8920, lr=0.000347, tokens/sec=1375810.06, grad_norm=0.3192, duration=0.38s
Step 12408: loss=2.8982, lr=0.000347, tokens/sec=1378213.22, grad_norm=0.3216, duration=0.38s
Step 12409: loss=2.8906, lr=0.000347, tokens/sec=1377621.78, grad_norm=0.3429, duration=0.38s
Step 12410: loss=2.8741, lr=0.000347, tokens/sec=1380038.20, grad_norm=0.3473, duration=0.38s
Step 12411: loss=2.8156, lr=0.000347, tokens/sec=1375563.92, grad_norm=0.3145, duration=0.38s
Step 12412: loss=2.8787, lr=0.000347, tokens/sec=1376593.81, grad_norm=0.3286, duration=0.38s
Step 12413: loss=2.8967, lr=0.000347, tokens/sec=1375959.85, grad_norm=0.3388, duration=0.38s
Step 12414: loss=2.8786, lr=0.000347, tokens/sec=1373889.78, grad_norm=0.3139, duration=0.38s
Step 12415: loss=2.9328, lr=0.000347, tokens/sec=1378640.05, grad_norm=0.3315, duration=0.38s
Step 12416: loss=2.8701, lr=0.000347, tokens/sec=1377898.87, grad_norm=0.3283, duration=0.38s
Step 12417: loss=3.0129, lr=0.000347, tokens/sec=1376832.55, grad_norm=0.3195, duration=0.38s
Step 12418: loss=2.9650, lr=0.000347, tokens/sec=1378255.54, grad_norm=0.3232, duration=0.38s
Step 12419: loss=2.9103, lr=0.000347, tokens/sec=1378850.98, grad_norm=0.3293, duration=0.38s
Step 12420: loss=2.9156, lr=0.000347, tokens/sec=1374363.77, grad_norm=0.3177, duration=0.38s
Step 12421: loss=2.8877, lr=0.000347, tokens/sec=1372507.48, grad_norm=0.2999, duration=0.38s
Step 12422: loss=2.9063, lr=0.000347, tokens/sec=1372938.51, grad_norm=0.3236, duration=0.38s
Step 12423: loss=2.9376, lr=0.000347, tokens/sec=1374155.93, grad_norm=0.3399, duration=0.38s
Step 12424: loss=2.9275, lr=0.000347, tokens/sec=1373455.58, grad_norm=0.3151, duration=0.38s
Step 12425: loss=2.9545, lr=0.000347, tokens/sec=1376979.98, grad_norm=0.3096, duration=0.38s
Step 12426: loss=2.9429, lr=0.000347, tokens/sec=1373954.16, grad_norm=0.3356, duration=0.38s
Step 12427: loss=2.8678, lr=0.000347, tokens/sec=1374826.90, grad_norm=0.2987, duration=0.38s
Step 12428: loss=2.9416, lr=0.000347, tokens/sec=1375632.76, grad_norm=0.3059, duration=0.38s
Step 12429: loss=2.8972, lr=0.000347, tokens/sec=1376309.49, grad_norm=0.3280, duration=0.38s
Step 12430: loss=2.9070, lr=0.000347, tokens/sec=1380570.17, grad_norm=0.3096, duration=0.38s
Step 12431: loss=2.8631, lr=0.000347, tokens/sec=1373405.83, grad_norm=0.3123, duration=0.38s
Step 12432: loss=2.9077, lr=0.000347, tokens/sec=1375572.53, grad_norm=0.3179, duration=0.38s
Step 12433: loss=2.8509, lr=0.000347, tokens/sec=1380345.73, grad_norm=0.2990, duration=0.38s
Step 12434: loss=2.8456, lr=0.000347, tokens/sec=1378420.55, grad_norm=0.3157, duration=0.38s
Step 12435: loss=2.7943, lr=0.000347, tokens/sec=1375547.57, grad_norm=0.3224, duration=0.38s
Step 12436: loss=2.8483, lr=0.000347, tokens/sec=1375909.05, grad_norm=0.3098, duration=0.38s
Step 12437: loss=2.8668, lr=0.000347, tokens/sec=1375294.65, grad_norm=0.3056, duration=0.38s
Step 12438: loss=2.8412, lr=0.000346, tokens/sec=1373688.95, grad_norm=0.3402, duration=0.38s
Step 12439: loss=2.8338, lr=0.000346, tokens/sec=1370709.18, grad_norm=0.3108, duration=0.38s
Step 12440: loss=2.8105, lr=0.000346, tokens/sec=1374789.94, grad_norm=0.3020, duration=0.38s
Step 12441: loss=2.8790, lr=0.000346, tokens/sec=1375261.97, grad_norm=0.3134, duration=0.38s
Step 12442: loss=2.8488, lr=0.000346, tokens/sec=1372028.79, grad_norm=0.3062, duration=0.38s
Step 12443: loss=2.8930, lr=0.000346, tokens/sec=1375816.94, grad_norm=0.3223, duration=0.38s
Step 12444: loss=2.9306, lr=0.000346, tokens/sec=1380573.64, grad_norm=0.3208, duration=0.38s
Step 12445: loss=2.9602, lr=0.000346, tokens/sec=1377570.86, grad_norm=0.3032, duration=0.38s
Step 12446: loss=2.9441, lr=0.000346, tokens/sec=1372853.65, grad_norm=0.3389, duration=0.38s
Step 12447: loss=2.9396, lr=0.000346, tokens/sec=1373220.58, grad_norm=0.3109, duration=0.38s
Step 12448: loss=2.8724, lr=0.000346, tokens/sec=1375183.70, grad_norm=0.3057, duration=0.38s
Step 12449: loss=2.9292, lr=0.000346, tokens/sec=1376135.51, grad_norm=0.3214, duration=0.38s
Step 12450: loss=2.8788, lr=0.000346, tokens/sec=1378513.01, grad_norm=0.3536, duration=0.38s
Step 12451: loss=2.9452, lr=0.000346, tokens/sec=1374477.16, grad_norm=0.3104, duration=0.38s
Step 12452: loss=2.9408, lr=0.000346, tokens/sec=1376655.85, grad_norm=0.3058, duration=0.38s
Step 12453: loss=2.9295, lr=0.000346, tokens/sec=1372387.56, grad_norm=0.3293, duration=0.38s
Step 12454: loss=2.9200, lr=0.000346, tokens/sec=1376778.24, grad_norm=0.3316, duration=0.38s
Step 12455: loss=2.8790, lr=0.000346, tokens/sec=1374006.53, grad_norm=0.3194, duration=0.38s
Step 12456: loss=2.9208, lr=0.000346, tokens/sec=1378406.73, grad_norm=0.3142, duration=0.38s
Step 12457: loss=2.9219, lr=0.000346, tokens/sec=1378068.12, grad_norm=0.3059, duration=0.38s
Step 12458: loss=2.9045, lr=0.000346, tokens/sec=1377733.98, grad_norm=0.3240, duration=0.38s
Step 12459: loss=2.8431, lr=0.000346, tokens/sec=1379867.61, grad_norm=0.3006, duration=0.38s
Step 12460: loss=2.9029, lr=0.000346, tokens/sec=1376016.67, grad_norm=0.3129, duration=0.38s
Step 12461: loss=2.8844, lr=0.000346, tokens/sec=1374054.61, grad_norm=0.3003, duration=0.38s
Step 12462: loss=2.9186, lr=0.000346, tokens/sec=1372634.28, grad_norm=0.2895, duration=0.38s
Step 12463: loss=2.8419, lr=0.000346, tokens/sec=1370788.64, grad_norm=0.3017, duration=0.38s
Step 12464: loss=2.8935, lr=0.000346, tokens/sec=1375509.72, grad_norm=0.3221, duration=0.38s
Step 12465: loss=2.8544, lr=0.000346, tokens/sec=1377845.34, grad_norm=0.2908, duration=0.38s
Step 12466: loss=2.9057, lr=0.000346, tokens/sec=1375298.95, grad_norm=0.2968, duration=0.38s
Step 12467: loss=2.8827, lr=0.000346, tokens/sec=1375517.46, grad_norm=0.3194, duration=0.38s
Step 12468: loss=2.9036, lr=0.000346, tokens/sec=1373052.52, grad_norm=0.2964, duration=0.38s
Step 12469: loss=2.8559, lr=0.000346, tokens/sec=1376399.08, grad_norm=0.3108, duration=0.38s
Step 12470: loss=2.8847, lr=0.000346, tokens/sec=1376549.00, grad_norm=0.3068, duration=0.38s
Step 12471: loss=2.9269, lr=0.000346, tokens/sec=1375409.92, grad_norm=0.3161, duration=0.38s
Step 12472: loss=2.9368, lr=0.000346, tokens/sec=1376723.94, grad_norm=0.3252, duration=0.38s
Step 12473: loss=2.9024, lr=0.000345, tokens/sec=1375274.87, grad_norm=0.3392, duration=0.38s
Step 12474: loss=2.9061, lr=0.000345, tokens/sec=1374850.97, grad_norm=0.3056, duration=0.38s
Step 12475: loss=2.9249, lr=0.000345, tokens/sec=1375206.92, grad_norm=0.3238, duration=0.38s
Step 12476: loss=2.8974, lr=0.000345, tokens/sec=1372983.94, grad_norm=0.3353, duration=0.38s
Step 12477: loss=2.9114, lr=0.000345, tokens/sec=1375612.11, grad_norm=0.3129, duration=0.38s
Step 12478: loss=2.8641, lr=0.000345, tokens/sec=1378736.00, grad_norm=0.3193, duration=0.38s
Step 12479: loss=2.8368, lr=0.000345, tokens/sec=1376002.04, grad_norm=0.3218, duration=0.38s
Step 12480: loss=2.8658, lr=0.000345, tokens/sec=1377738.30, grad_norm=0.3147, duration=0.38s
Step 12481: loss=2.8147, lr=0.000345, tokens/sec=1376175.12, grad_norm=0.3098, duration=0.38s
Step 12482: loss=2.8701, lr=0.000345, tokens/sec=1372569.16, grad_norm=0.3110, duration=0.38s
Step 12483: loss=2.8481, lr=0.000345, tokens/sec=1377614.01, grad_norm=0.3003, duration=0.38s
Step 12484: loss=2.9083, lr=0.000345, tokens/sec=1373168.27, grad_norm=0.3083, duration=0.38s
Step 12485: loss=2.9040, lr=0.000345, tokens/sec=1379116.46, grad_norm=0.3092, duration=0.38s
Step 12486: loss=2.8144, lr=0.000345, tokens/sec=1377693.42, grad_norm=0.3129, duration=0.38s
Step 12487: loss=2.8439, lr=0.000345, tokens/sec=1372565.74, grad_norm=0.3059, duration=0.38s
Step 12488: loss=2.8747, lr=0.000345, tokens/sec=1375977.07, grad_norm=0.3124, duration=0.38s
Step 12489: loss=2.9278, lr=0.000345, tokens/sec=1372712.25, grad_norm=0.3002, duration=0.38s
Step 12490: loss=2.8991, lr=0.000345, tokens/sec=1372378.14, grad_norm=0.3239, duration=0.38s
Step 12491: loss=2.9377, lr=0.000345, tokens/sec=1369607.04, grad_norm=0.3133, duration=0.38s
Step 12492: loss=2.8873, lr=0.000345, tokens/sec=1377495.79, grad_norm=0.3209, duration=0.38s
Step 12493: loss=2.9295, lr=0.000345, tokens/sec=1374306.22, grad_norm=0.3438, duration=0.38s
Step 12494: loss=2.9561, lr=0.000345, tokens/sec=1379064.56, grad_norm=0.3607, duration=0.38s
Step 12495: loss=2.8811, lr=0.000345, tokens/sec=1376723.08, grad_norm=0.3063, duration=0.38s
Step 12496: loss=2.9235, lr=0.000345, tokens/sec=1374172.24, grad_norm=0.3488, duration=0.38s
Step 12497: loss=2.9488, lr=0.000345, tokens/sec=1375195.74, grad_norm=0.3380, duration=0.38s
Step 12498: loss=3.0144, lr=0.000345, tokens/sec=1377248.19, grad_norm=0.3050, duration=0.38s
Step 12499: loss=2.9503, lr=0.000345, tokens/sec=1371521.34, grad_norm=0.3404, duration=0.38s
Step 12500/19073 (65.5%), Elapsed time: 4951.11s, Steps per hour: 9088.87, Estimated hours remaining: 0.72
Validation loss at step 12500: 3.780332565307617
Step 12500: loss=2.9511, lr=0.000345, tokens/sec=152822.20, grad_norm=0.3621, duration=3.43s
Step 12501: loss=2.9273, lr=0.000345, tokens/sec=1377368.96, grad_norm=0.3072, duration=0.38s
Step 12502: loss=2.9178, lr=0.000345, tokens/sec=1373406.69, grad_norm=0.3001, duration=0.38s
Step 12503: loss=2.8764, lr=0.000345, tokens/sec=1372140.08, grad_norm=0.3387, duration=0.38s
Step 12504: loss=2.8538, lr=0.000345, tokens/sec=1378672.90, grad_norm=0.3347, duration=0.38s
Step 12505: loss=2.9601, lr=0.000345, tokens/sec=1374494.34, grad_norm=0.3122, duration=0.38s
Step 12506: loss=2.8877, lr=0.000345, tokens/sec=1373401.54, grad_norm=0.3225, duration=0.38s
Step 12507: loss=2.9414, lr=0.000345, tokens/sec=1374478.02, grad_norm=0.3120, duration=0.38s
Step 12508: loss=2.9741, lr=0.000344, tokens/sec=1375711.08, grad_norm=0.3145, duration=0.38s
Step 12509: loss=2.8931, lr=0.000344, tokens/sec=1373938.71, grad_norm=0.3188, duration=0.38s
Step 12510: loss=2.8696, lr=0.000344, tokens/sec=1374862.14, grad_norm=0.3040, duration=0.38s
Step 12511: loss=2.8953, lr=0.000344, tokens/sec=1373501.91, grad_norm=0.3197, duration=0.38s
Step 12512: loss=2.9489, lr=0.000344, tokens/sec=1371850.75, grad_norm=0.2973, duration=0.38s
Step 12513: loss=2.9103, lr=0.000344, tokens/sec=1374649.00, grad_norm=0.3031, duration=0.38s
Step 12514: loss=2.8154, lr=0.000344, tokens/sec=1375047.84, grad_norm=0.3088, duration=0.38s
Step 12515: loss=2.9099, lr=0.000344, tokens/sec=1376573.12, grad_norm=0.3084, duration=0.38s
Step 12516: loss=2.9024, lr=0.000344, tokens/sec=1373984.21, grad_norm=0.2980, duration=0.38s
Step 12517: loss=2.8569, lr=0.000344, tokens/sec=1373314.06, grad_norm=0.3131, duration=0.38s
Step 12518: loss=2.9388, lr=0.000344, tokens/sec=1375798.87, grad_norm=0.3209, duration=0.38s
Step 12519: loss=2.8589, lr=0.000344, tokens/sec=1374642.13, grad_norm=0.3061, duration=0.38s
Step 12520: loss=2.8998, lr=0.000344, tokens/sec=1375461.54, grad_norm=0.2928, duration=0.38s
Step 12521: loss=2.8922, lr=0.000344, tokens/sec=1372092.14, grad_norm=0.3109, duration=0.38s
Step 12522: loss=2.8555, lr=0.000344, tokens/sec=1372669.41, grad_norm=0.2914, duration=0.38s
Step 12523: loss=2.8732, lr=0.000344, tokens/sec=1374680.80, grad_norm=0.2866, duration=0.38s
Step 12524: loss=2.8515, lr=0.000344, tokens/sec=1374736.66, grad_norm=0.3064, duration=0.38s
Step 12525: loss=2.8667, lr=0.000344, tokens/sec=1373350.94, grad_norm=0.3038, duration=0.38s
Step 12526: loss=2.8611, lr=0.000344, tokens/sec=1374408.43, grad_norm=0.2907, duration=0.38s
Step 12527: loss=2.9063, lr=0.000344, tokens/sec=1378056.03, grad_norm=0.2954, duration=0.38s
Step 12528: loss=2.8621, lr=0.000344, tokens/sec=1373500.19, grad_norm=0.3123, duration=0.38s
Step 12529: loss=2.8361, lr=0.000344, tokens/sec=1372953.08, grad_norm=0.2920, duration=0.38s
Step 12530: loss=2.8131, lr=0.000344, tokens/sec=1376923.94, grad_norm=0.2793, duration=0.38s
Step 12531: loss=2.8469, lr=0.000344, tokens/sec=1373531.08, grad_norm=0.2869, duration=0.38s
Step 12532: loss=2.8602, lr=0.000344, tokens/sec=1365378.04, grad_norm=0.2967, duration=0.38s
Step 12533: loss=2.8497, lr=0.000344, tokens/sec=1375656.00, grad_norm=0.2926, duration=0.38s
Step 12534: loss=2.8713, lr=0.000344, tokens/sec=1373489.90, grad_norm=0.3062, duration=0.38s
Step 12535: loss=2.9126, lr=0.000344, tokens/sec=1375014.31, grad_norm=0.3000, duration=0.38s
Step 12536: loss=2.9411, lr=0.000344, tokens/sec=1376499.02, grad_norm=0.3092, duration=0.38s
Step 12537: loss=2.9173, lr=0.000344, tokens/sec=1377787.50, grad_norm=0.3253, duration=0.38s
Step 12538: loss=2.8961, lr=0.000344, tokens/sec=1371836.20, grad_norm=0.3074, duration=0.38s
Step 12539: loss=2.9439, lr=0.000344, tokens/sec=1376656.72, grad_norm=0.3093, duration=0.38s
Step 12540: loss=2.8783, lr=0.000344, tokens/sec=1375235.30, grad_norm=0.3393, duration=0.38s
Step 12541: loss=2.9465, lr=0.000344, tokens/sec=1373771.34, grad_norm=0.3338, duration=0.38s
Step 12542: loss=2.9386, lr=0.000344, tokens/sec=1376473.17, grad_norm=0.3125, duration=0.38s
Step 12543: loss=2.9921, lr=0.000344, tokens/sec=1373335.50, grad_norm=0.3121, duration=0.38s
Step 12544: loss=2.9459, lr=0.000343, tokens/sec=1376266.42, grad_norm=0.3211, duration=0.38s
Step 12545: loss=2.9564, lr=0.000343, tokens/sec=1377555.33, grad_norm=0.3331, duration=0.38s
Step 12546: loss=3.0126, lr=0.000343, tokens/sec=1377136.06, grad_norm=0.3148, duration=0.38s
Step 12547: loss=2.9488, lr=0.000343, tokens/sec=1377003.26, grad_norm=0.3173, duration=0.38s
Step 12548: loss=2.9589, lr=0.000343, tokens/sec=1376215.60, grad_norm=0.3340, duration=0.38s
Step 12549: loss=2.9261, lr=0.000343, tokens/sec=1374464.27, grad_norm=0.3054, duration=0.38s
Step 12550: loss=2.9486, lr=0.000343, tokens/sec=1376252.64, grad_norm=0.3194, duration=0.38s
Step 12551: loss=2.9405, lr=0.000343, tokens/sec=1377582.08, grad_norm=0.3184, duration=0.38s
Step 12552: loss=2.9145, lr=0.000343, tokens/sec=1378483.63, grad_norm=0.3094, duration=0.38s
Step 12553: loss=2.8596, lr=0.000343, tokens/sec=1372218.00, grad_norm=0.2997, duration=0.38s
Step 12554: loss=2.9268, lr=0.000343, tokens/sec=1369687.23, grad_norm=0.2997, duration=0.38s
Step 12555: loss=2.9168, lr=0.000343, tokens/sec=1373630.60, grad_norm=0.3379, duration=0.38s
Step 12556: loss=2.8782, lr=0.000343, tokens/sec=1379694.46, grad_norm=0.3274, duration=0.38s
Step 12557: loss=2.9168, lr=0.000343, tokens/sec=1377220.58, grad_norm=0.3181, duration=0.38s
Step 12558: loss=2.8453, lr=0.000343, tokens/sec=1374612.91, grad_norm=0.3183, duration=0.38s
Step 12559: loss=2.8074, lr=0.000343, tokens/sec=1373273.75, grad_norm=0.3412, duration=0.38s
Step 12560: loss=2.8513, lr=0.000343, tokens/sec=1375064.18, grad_norm=0.3278, duration=0.38s
Step 12561: loss=2.9016, lr=0.000343, tokens/sec=1374944.67, grad_norm=0.3081, duration=0.38s
Step 12562: loss=2.9814, lr=0.000343, tokens/sec=1375380.67, grad_norm=0.3251, duration=0.38s
Step 12563: loss=2.7630, lr=0.000343, tokens/sec=1372086.15, grad_norm=0.3207, duration=0.38s
Step 12564: loss=2.9297, lr=0.000343, tokens/sec=1377352.56, grad_norm=0.3194, duration=0.38s
Step 12565: loss=2.9150, lr=0.000343, tokens/sec=1374539.02, grad_norm=0.3159, duration=0.38s
Step 12566: loss=2.8909, lr=0.000343, tokens/sec=1372993.37, grad_norm=0.3128, duration=0.38s
Step 12567: loss=2.9213, lr=0.000343, tokens/sec=1376569.68, grad_norm=0.2920, duration=0.38s
Step 12568: loss=2.9450, lr=0.000343, tokens/sec=1376999.81, grad_norm=0.3125, duration=0.38s
Step 12569: loss=2.9166, lr=0.000343, tokens/sec=1372975.37, grad_norm=0.3227, duration=0.38s
Step 12570: loss=2.8800, lr=0.000343, tokens/sec=1372796.23, grad_norm=0.2902, duration=0.38s
Step 12571: loss=2.8950, lr=0.000343, tokens/sec=1369294.05, grad_norm=0.2916, duration=0.38s
Step 12572: loss=2.8801, lr=0.000343, tokens/sec=1371032.22, grad_norm=0.3399, duration=0.38s
Step 12573: loss=2.9600, lr=0.000343, tokens/sec=1372512.62, grad_norm=0.3127, duration=0.38s
Step 12574: loss=2.9036, lr=0.000343, tokens/sec=1373052.52, grad_norm=0.3049, duration=0.38s
Step 12575: loss=2.8412, lr=0.000343, tokens/sec=1374361.19, grad_norm=0.3040, duration=0.38s
Step 12576: loss=2.9123, lr=0.000343, tokens/sec=1376853.24, grad_norm=0.2976, duration=0.38s
Step 12577: loss=2.8310, lr=0.000343, tokens/sec=1374391.25, grad_norm=0.3040, duration=0.38s
Step 12578: loss=2.8474, lr=0.000343, tokens/sec=1375500.25, grad_norm=0.3252, duration=0.38s
Step 12579: loss=2.8499, lr=0.000342, tokens/sec=1377627.82, grad_norm=0.2938, duration=0.38s
Step 12580: loss=2.8556, lr=0.000342, tokens/sec=1375334.22, grad_norm=0.2829, duration=0.38s
Step 12581: loss=2.8784, lr=0.000342, tokens/sec=1374993.67, grad_norm=0.3274, duration=0.38s
Step 12582: loss=2.8230, lr=0.000342, tokens/sec=1373392.11, grad_norm=0.3113, duration=0.38s
Step 12583: loss=2.8700, lr=0.000342, tokens/sec=1376679.99, grad_norm=0.3008, duration=0.38s
Step 12584: loss=2.8129, lr=0.000342, tokens/sec=1376570.54, grad_norm=0.3172, duration=0.38s
Step 12585: loss=2.8465, lr=0.000342, tokens/sec=1376064.03, grad_norm=0.2943, duration=0.38s
Step 12586: loss=2.8557, lr=0.000342, tokens/sec=1374620.64, grad_norm=0.3061, duration=0.38s
Step 12587: loss=2.8918, lr=0.000342, tokens/sec=1377337.90, grad_norm=0.3351, duration=0.38s
Step 12588: loss=2.9563, lr=0.000342, tokens/sec=1376745.49, grad_norm=0.3364, duration=0.38s
Step 12589: loss=2.9024, lr=0.000342, tokens/sec=1375212.08, grad_norm=0.2995, duration=0.38s
Step 12590: loss=2.9143, lr=0.000342, tokens/sec=1374292.48, grad_norm=0.3161, duration=0.38s
Step 12591: loss=2.9543, lr=0.000342, tokens/sec=1377518.22, grad_norm=0.3447, duration=0.38s
Step 12592: loss=2.9469, lr=0.000342, tokens/sec=1376367.20, grad_norm=0.3202, duration=0.38s
Step 12593: loss=2.9311, lr=0.000342, tokens/sec=1374525.27, grad_norm=0.3117, duration=0.38s
Step 12594: loss=2.9195, lr=0.000342, tokens/sec=1374820.88, grad_norm=0.3132, duration=0.38s
Step 12595: loss=2.8711, lr=0.000342, tokens/sec=1381611.91, grad_norm=0.3319, duration=0.38s
Step 12596: loss=2.9034, lr=0.000342, tokens/sec=1376231.97, grad_norm=0.3359, duration=0.38s
Step 12597: loss=2.8949, lr=0.000342, tokens/sec=1372570.02, grad_norm=0.3149, duration=0.38s
Step 12598: loss=2.8883, lr=0.000342, tokens/sec=1375540.69, grad_norm=0.3121, duration=0.38s
Step 12599: loss=2.8679, lr=0.000342, tokens/sec=1374712.59, grad_norm=0.3247, duration=0.38s
Step 12600/19073 (66.1%), Elapsed time: 4992.38s, Steps per hour: 9085.85, Estimated hours remaining: 0.71
Step 12600: loss=2.8868, lr=0.000342, tokens/sec=1378649.56, grad_norm=0.3137, duration=0.38s
Step 12601: loss=2.8205, lr=0.000342, tokens/sec=1374518.40, grad_norm=0.3042, duration=0.38s
Step 12602: loss=2.8491, lr=0.000342, tokens/sec=1378341.07, grad_norm=0.3269, duration=0.38s
Step 12603: loss=2.8828, lr=0.000342, tokens/sec=1378398.09, grad_norm=0.3293, duration=0.38s
Step 12604: loss=2.8885, lr=0.000342, tokens/sec=1373424.70, grad_norm=0.2956, duration=0.38s
Step 12605: loss=2.9227, lr=0.000342, tokens/sec=1377607.11, grad_norm=0.3220, duration=0.38s
Step 12606: loss=2.9292, lr=0.000342, tokens/sec=1375982.23, grad_norm=0.3371, duration=0.38s
Step 12607: loss=2.9941, lr=0.000342, tokens/sec=1373039.67, grad_norm=0.3201, duration=0.38s
Step 12608: loss=2.9205, lr=0.000342, tokens/sec=1378117.34, grad_norm=0.3241, duration=0.38s
Step 12609: loss=2.9111, lr=0.000342, tokens/sec=1378043.07, grad_norm=0.3306, duration=0.38s
Step 12610: loss=2.8779, lr=0.000342, tokens/sec=1378190.76, grad_norm=0.3159, duration=0.38s
Step 12611: loss=2.9267, lr=0.000342, tokens/sec=1377402.60, grad_norm=0.3208, duration=0.38s
Step 12612: loss=2.9142, lr=0.000342, tokens/sec=1377656.30, grad_norm=0.3363, duration=0.38s
Step 12613: loss=2.9210, lr=0.000342, tokens/sec=1376364.62, grad_norm=0.3278, duration=0.38s
Step 12614: loss=2.9561, lr=0.000342, tokens/sec=1373696.68, grad_norm=0.3155, duration=0.38s
Step 12615: loss=2.9373, lr=0.000341, tokens/sec=1376184.60, grad_norm=0.3297, duration=0.38s
Step 12616: loss=2.9083, lr=0.000341, tokens/sec=1378286.64, grad_norm=0.3274, duration=0.38s
Step 12617: loss=2.9021, lr=0.000341, tokens/sec=1375362.60, grad_norm=0.3130, duration=0.38s
Step 12618: loss=2.9042, lr=0.000341, tokens/sec=1379503.18, grad_norm=0.3036, duration=0.38s
Step 12619: loss=2.8895, lr=0.000341, tokens/sec=1377673.56, grad_norm=0.3274, duration=0.38s
Step 12620: loss=2.9285, lr=0.000341, tokens/sec=1374276.16, grad_norm=0.3217, duration=0.38s
Step 12621: loss=2.8419, lr=0.000341, tokens/sec=1366378.29, grad_norm=0.3146, duration=0.38s
Step 12622: loss=2.9016, lr=0.000341, tokens/sec=1375594.04, grad_norm=0.3173, duration=0.38s
Step 12623: loss=2.8298, lr=0.000341, tokens/sec=1377022.23, grad_norm=0.3116, duration=0.38s
Step 12624: loss=2.8374, lr=0.000341, tokens/sec=1371489.69, grad_norm=0.3047, duration=0.38s
Step 12625: loss=2.8125, lr=0.000341, tokens/sec=1376373.23, grad_norm=0.3241, duration=0.38s
Step 12626: loss=2.8405, lr=0.000341, tokens/sec=1376993.77, grad_norm=0.3129, duration=0.38s
Step 12627: loss=2.8829, lr=0.000341, tokens/sec=1375184.56, grad_norm=0.3003, duration=0.38s
Step 12628: loss=2.8067, lr=0.000341, tokens/sec=1375098.57, grad_norm=0.3383, duration=0.38s
Step 12629: loss=2.8244, lr=0.000341, tokens/sec=1375526.92, grad_norm=0.3176, duration=0.38s
Step 12630: loss=2.8160, lr=0.000341, tokens/sec=1373751.60, grad_norm=0.2938, duration=0.38s
Step 12631: loss=2.8621, lr=0.000341, tokens/sec=1380256.49, grad_norm=0.3081, duration=0.38s
Step 12632: loss=2.8642, lr=0.000341, tokens/sec=1372638.56, grad_norm=0.3197, duration=0.38s
Step 12633: loss=2.8818, lr=0.000341, tokens/sec=1373278.90, grad_norm=0.3276, duration=0.38s
Step 12634: loss=2.9519, lr=0.000341, tokens/sec=1371846.47, grad_norm=0.3231, duration=0.38s
Step 12635: loss=2.9334, lr=0.000341, tokens/sec=1376561.06, grad_norm=0.3182, duration=0.38s
Step 12636: loss=2.9324, lr=0.000341, tokens/sec=1374223.77, grad_norm=0.3334, duration=0.38s
Step 12637: loss=2.9333, lr=0.000341, tokens/sec=1374040.87, grad_norm=0.3204, duration=0.38s
Step 12638: loss=2.8721, lr=0.000341, tokens/sec=1372221.42, grad_norm=0.3452, duration=0.38s
Step 12639: loss=2.9028, lr=0.000341, tokens/sec=1377673.56, grad_norm=0.3150, duration=0.38s
Step 12640: loss=2.8787, lr=0.000341, tokens/sec=1373554.24, grad_norm=0.3417, duration=0.38s
Step 12641: loss=2.9468, lr=0.000341, tokens/sec=1378771.44, grad_norm=0.3216, duration=0.38s
Step 12642: loss=2.9373, lr=0.000341, tokens/sec=1378737.73, grad_norm=0.3117, duration=0.38s
Step 12643: loss=2.9109, lr=0.000341, tokens/sec=1375544.99, grad_norm=0.3198, duration=0.38s
Step 12644: loss=2.9087, lr=0.000341, tokens/sec=1375608.67, grad_norm=0.3111, duration=0.38s
Step 12645: loss=2.9035, lr=0.000341, tokens/sec=1377238.70, grad_norm=0.3084, duration=0.38s
Step 12646: loss=2.9143, lr=0.000341, tokens/sec=1376859.28, grad_norm=0.3089, duration=0.38s
Step 12647: loss=2.8922, lr=0.000341, tokens/sec=1376368.06, grad_norm=0.3144, duration=0.38s
Step 12648: loss=2.8888, lr=0.000341, tokens/sec=1374997.97, grad_norm=0.3002, duration=0.38s
Step 12649: loss=2.8481, lr=0.000341, tokens/sec=1374723.76, grad_norm=0.2998, duration=0.38s
Step 12650: loss=2.8906, lr=0.000340, tokens/sec=1378182.12, grad_norm=0.2943, duration=0.38s
Step 12651: loss=2.8975, lr=0.000340, tokens/sec=1376614.49, grad_norm=0.3074, duration=0.38s
Step 12652: loss=2.8967, lr=0.000340, tokens/sec=1370164.29, grad_norm=0.2988, duration=0.38s
Step 12653: loss=2.8184, lr=0.000340, tokens/sec=1376065.75, grad_norm=0.2952, duration=0.38s
Step 12654: loss=2.9046, lr=0.000340, tokens/sec=1374267.57, grad_norm=0.3135, duration=0.38s
Step 12655: loss=2.8477, lr=0.000340, tokens/sec=1371236.55, grad_norm=0.2925, duration=0.38s
Step 12656: loss=2.9081, lr=0.000340, tokens/sec=1379956.80, grad_norm=0.3047, duration=0.38s
Step 12657: loss=2.8784, lr=0.000340, tokens/sec=1377429.35, grad_norm=0.3004, duration=0.38s
Step 12658: loss=2.8680, lr=0.000340, tokens/sec=1373778.20, grad_norm=0.2972, duration=0.38s
Step 12659: loss=2.8705, lr=0.000340, tokens/sec=1375563.06, grad_norm=0.3027, duration=0.38s
Step 12660: loss=2.9238, lr=0.000340, tokens/sec=1376451.63, grad_norm=0.3085, duration=0.38s
Step 12661: loss=2.9256, lr=0.000340, tokens/sec=1374072.64, grad_norm=0.2976, duration=0.38s
Step 12662: loss=2.9162, lr=0.000340, tokens/sec=1378789.60, grad_norm=0.3148, duration=0.38s
Step 12663: loss=2.8846, lr=0.000340, tokens/sec=1374875.90, grad_norm=0.3291, duration=0.38s
Step 12664: loss=2.9165, lr=0.000340, tokens/sec=1376827.38, grad_norm=0.3024, duration=0.38s
Step 12665: loss=2.9040, lr=0.000340, tokens/sec=1373620.31, grad_norm=0.3054, duration=0.38s
Step 12666: loss=2.9155, lr=0.000340, tokens/sec=1374010.82, grad_norm=0.3323, duration=0.38s
Step 12667: loss=2.8625, lr=0.000340, tokens/sec=1371804.54, grad_norm=0.3050, duration=0.38s
Step 12668: loss=2.8854, lr=0.000340, tokens/sec=1377384.49, grad_norm=0.3088, duration=0.38s
Step 12669: loss=2.8270, lr=0.000340, tokens/sec=1373589.42, grad_norm=0.3317, duration=0.38s
Step 12670: loss=2.8866, lr=0.000340, tokens/sec=1377956.72, grad_norm=0.3134, duration=0.38s
Step 12671: loss=2.8168, lr=0.000340, tokens/sec=1376599.84, grad_norm=0.3006, duration=0.38s
Step 12672: loss=2.8434, lr=0.000340, tokens/sec=1377431.07, grad_norm=0.3206, duration=0.38s
Step 12673: loss=2.8394, lr=0.000340, tokens/sec=1372015.09, grad_norm=0.3159, duration=0.38s
Step 12674: loss=2.9314, lr=0.000340, tokens/sec=1373012.23, grad_norm=0.3107, duration=0.38s
Step 12675: loss=2.8326, lr=0.000340, tokens/sec=1374734.94, grad_norm=0.3090, duration=0.38s
Step 12676: loss=2.8552, lr=0.000340, tokens/sec=1373350.94, grad_norm=0.3122, duration=0.38s
Step 12677: loss=2.8416, lr=0.000340, tokens/sec=1376465.42, grad_norm=0.3171, duration=0.38s
Step 12678: loss=2.8884, lr=0.000340, tokens/sec=1376397.35, grad_norm=0.3253, duration=0.38s
Step 12679: loss=2.8767, lr=0.000340, tokens/sec=1375219.82, grad_norm=0.3040, duration=0.38s
Step 12680: loss=2.9130, lr=0.000340, tokens/sec=1376406.83, grad_norm=0.3328, duration=0.38s
Step 12681: loss=2.8790, lr=0.000340, tokens/sec=1376658.44, grad_norm=0.3575, duration=0.38s
Step 12682: loss=2.9433, lr=0.000340, tokens/sec=1375184.56, grad_norm=0.3138, duration=0.38s
Step 12683: loss=2.9242, lr=0.000340, tokens/sec=1372174.33, grad_norm=0.3404, duration=0.38s
Step 12684: loss=2.9314, lr=0.000340, tokens/sec=1373102.25, grad_norm=0.3628, duration=0.38s
Step 12685: loss=2.8930, lr=0.000340, tokens/sec=1370516.11, grad_norm=0.3191, duration=0.38s
Step 12686: loss=2.8980, lr=0.000339, tokens/sec=1365539.98, grad_norm=0.3350, duration=0.38s
Step 12687: loss=2.9469, lr=0.000339, tokens/sec=1372140.08, grad_norm=0.3687, duration=0.38s
Step 12688: loss=3.0003, lr=0.000339, tokens/sec=1376466.28, grad_norm=0.3464, duration=0.38s
Step 12689: loss=2.9570, lr=0.000339, tokens/sec=1371366.53, grad_norm=0.2930, duration=0.38s
Step 12690: loss=2.9721, lr=0.000339, tokens/sec=1376619.66, grad_norm=0.3478, duration=0.38s
Step 12691: loss=2.8874, lr=0.000339, tokens/sec=1378194.21, grad_norm=0.3671, duration=0.38s
Step 12692: loss=2.9104, lr=0.000339, tokens/sec=1374908.56, grad_norm=0.3151, duration=0.38s
Step 12693: loss=2.8827, lr=0.000339, tokens/sec=1374924.04, grad_norm=0.3157, duration=0.38s
Step 12694: loss=2.8634, lr=0.000339, tokens/sec=1372695.97, grad_norm=0.3460, duration=0.38s
Step 12695: loss=2.9693, lr=0.000339, tokens/sec=1371613.73, grad_norm=0.3376, duration=0.38s
Step 12696: loss=2.8516, lr=0.000339, tokens/sec=1376474.89, grad_norm=0.3240, duration=0.38s
Step 12697: loss=2.9475, lr=0.000339, tokens/sec=1372015.95, grad_norm=0.3539, duration=0.38s
Step 12698: loss=2.9391, lr=0.000339, tokens/sec=1375793.70, grad_norm=0.3375, duration=0.38s
Step 12699: loss=2.9125, lr=0.000339, tokens/sec=1373776.49, grad_norm=0.3309, duration=0.38s
Step 12700/19073 (66.6%), Elapsed time: 5030.59s, Steps per hour: 9088.40, Estimated hours remaining: 0.70
Step 12700: loss=2.8377, lr=0.000339, tokens/sec=1372410.69, grad_norm=0.3089, duration=0.38s
Step 12701: loss=2.9220, lr=0.000339, tokens/sec=1372248.83, grad_norm=0.3540, duration=0.38s
Step 12702: loss=2.9392, lr=0.000339, tokens/sec=1378448.20, grad_norm=0.3255, duration=0.38s
Step 12703: loss=2.8837, lr=0.000339, tokens/sec=1372032.21, grad_norm=0.2974, duration=0.38s
Step 12704: loss=2.8493, lr=0.000339, tokens/sec=1375026.34, grad_norm=0.3120, duration=0.38s
Step 12705: loss=2.8953, lr=0.000339, tokens/sec=1374942.09, grad_norm=0.3468, duration=0.38s
Step 12706: loss=2.8322, lr=0.000339, tokens/sec=1371015.12, grad_norm=0.3093, duration=0.38s
Step 12707: loss=2.9037, lr=0.000339, tokens/sec=1372650.56, grad_norm=0.3053, duration=0.38s
Step 12708: loss=2.9328, lr=0.000339, tokens/sec=1372128.10, grad_norm=0.3247, duration=0.38s
Step 12709: loss=2.8891, lr=0.000339, tokens/sec=1375088.25, grad_norm=0.3438, duration=0.38s
Step 12710: loss=2.8874, lr=0.000339, tokens/sec=1379079.27, grad_norm=0.3101, duration=0.38s
Step 12711: loss=2.8915, lr=0.000339, tokens/sec=1377629.55, grad_norm=0.2932, duration=0.38s
Step 12712: loss=2.8303, lr=0.000339, tokens/sec=1374202.30, grad_norm=0.3110, duration=0.38s
Step 12713: loss=2.8842, lr=0.000339, tokens/sec=1376700.67, grad_norm=0.3167, duration=0.38s
Step 12714: loss=2.8789, lr=0.000339, tokens/sec=1379565.49, grad_norm=0.2996, duration=0.38s
Step 12715: loss=2.8542, lr=0.000339, tokens/sec=1375903.89, grad_norm=0.3191, duration=0.38s
Step 12716: loss=2.8424, lr=0.000339, tokens/sec=1376813.59, grad_norm=0.3171, duration=0.38s
Step 12717: loss=2.9033, lr=0.000339, tokens/sec=1375893.56, grad_norm=0.2969, duration=0.38s
Step 12718: loss=2.8498, lr=0.000339, tokens/sec=1377083.45, grad_norm=0.3026, duration=0.38s
Step 12719: loss=2.8355, lr=0.000339, tokens/sec=1376885.14, grad_norm=0.3166, duration=0.38s
Step 12720: loss=2.8170, lr=0.000339, tokens/sec=1375686.98, grad_norm=0.3044, duration=0.38s
Step 12721: loss=2.8187, lr=0.000339, tokens/sec=1375132.11, grad_norm=0.2903, duration=0.38s
Step 12722: loss=2.8627, lr=0.000338, tokens/sec=1377925.64, grad_norm=0.3018, duration=0.38s
Step 12723: loss=2.8466, lr=0.000338, tokens/sec=1373079.96, grad_norm=0.3131, duration=0.38s
Step 12724: loss=2.8638, lr=0.000338, tokens/sec=1371851.61, grad_norm=0.3074, duration=0.38s
Step 12725: loss=2.9382, lr=0.000338, tokens/sec=1375786.82, grad_norm=0.3247, duration=0.38s
Step 12726: loss=2.9117, lr=0.000338, tokens/sec=1376892.90, grad_norm=0.3102, duration=0.38s
Step 12727: loss=2.9229, lr=0.000338, tokens/sec=1371807.11, grad_norm=0.3072, duration=0.38s
Step 12728: loss=2.9111, lr=0.000338, tokens/sec=1378995.38, grad_norm=0.3392, duration=0.38s
Step 12729: loss=2.8981, lr=0.000338, tokens/sec=1376242.30, grad_norm=0.3247, duration=0.38s
Step 12730: loss=2.9068, lr=0.000338, tokens/sec=1373055.10, grad_norm=0.3153, duration=0.38s
Step 12731: loss=2.9283, lr=0.000338, tokens/sec=1374034.00, grad_norm=0.3229, duration=0.38s
Step 12732: loss=2.9185, lr=0.000338, tokens/sec=1373405.83, grad_norm=0.3274, duration=0.38s
Step 12733: loss=3.0039, lr=0.000338, tokens/sec=1372376.43, grad_norm=0.3170, duration=0.38s
Step 12734: loss=2.9305, lr=0.000338, tokens/sec=1368499.85, grad_norm=0.3227, duration=0.38s
Step 12735: loss=2.9642, lr=0.000338, tokens/sec=1376045.95, grad_norm=0.3130, duration=0.38s
Step 12736: loss=3.0055, lr=0.000338, tokens/sec=1378208.90, grad_norm=0.3240, duration=0.38s
Step 12737: loss=2.9337, lr=0.000338, tokens/sec=1371828.50, grad_norm=0.3247, duration=0.38s
Step 12738: loss=2.9430, lr=0.000338, tokens/sec=1372701.97, grad_norm=0.3266, duration=0.38s
Step 12739: loss=2.9292, lr=0.000338, tokens/sec=1376591.22, grad_norm=0.3178, duration=0.38s
Step 12740: loss=2.9680, lr=0.000338, tokens/sec=1376738.59, grad_norm=0.3299, duration=0.38s
Step 12741: loss=2.9026, lr=0.000338, tokens/sec=1377282.69, grad_norm=0.3345, duration=0.38s
Step 12742: loss=2.9159, lr=0.000338, tokens/sec=1376642.93, grad_norm=0.3086, duration=0.38s
Step 12743: loss=2.8497, lr=0.000338, tokens/sec=1371209.19, grad_norm=0.3069, duration=0.38s
Step 12744: loss=2.9315, lr=0.000338, tokens/sec=1372937.65, grad_norm=0.3362, duration=0.38s
Step 12745: loss=2.9181, lr=0.000338, tokens/sec=1377840.16, grad_norm=0.3599, duration=0.38s
Step 12746: loss=2.8452, lr=0.000338, tokens/sec=1376823.93, grad_norm=0.3295, duration=0.38s
Step 12747: loss=2.9283, lr=0.000338, tokens/sec=1372715.68, grad_norm=0.3433, duration=0.38s
Step 12748: loss=2.8364, lr=0.000338, tokens/sec=1371008.29, grad_norm=0.3501, duration=0.38s
Step 12749: loss=2.8004, lr=0.000338, tokens/sec=1375047.84, grad_norm=0.3586, duration=0.38s
Validation loss at step 12750: 3.7984530925750732
Step 12750: loss=2.8539, lr=0.000338, tokens/sec=152760.70, grad_norm=0.3364, duration=3.43s
Step 12751: loss=2.9012, lr=0.000338, tokens/sec=1377650.26, grad_norm=0.3405, duration=0.38s
Step 12752: loss=2.9106, lr=0.000338, tokens/sec=1373757.61, grad_norm=0.3282, duration=0.38s
Step 12753: loss=2.8174, lr=0.000338, tokens/sec=1375504.55, grad_norm=0.3191, duration=0.38s
Step 12754: loss=2.9230, lr=0.000338, tokens/sec=1371635.98, grad_norm=0.3309, duration=0.38s
Step 12755: loss=2.9475, lr=0.000338, tokens/sec=1376761.87, grad_norm=0.3231, duration=0.38s
Step 12756: loss=2.8830, lr=0.000338, tokens/sec=1371977.43, grad_norm=0.3234, duration=0.38s
Step 12757: loss=2.9137, lr=0.000338, tokens/sec=1376465.42, grad_norm=0.3153, duration=0.38s
Step 12758: loss=2.9311, lr=0.000337, tokens/sec=1375847.07, grad_norm=0.3107, duration=0.38s
Step 12759: loss=2.9135, lr=0.000337, tokens/sec=1374102.69, grad_norm=0.3245, duration=0.38s
Step 12760: loss=2.8609, lr=0.000337, tokens/sec=1374707.44, grad_norm=0.3217, duration=0.38s
Step 12761: loss=2.8660, lr=0.000337, tokens/sec=1372123.82, grad_norm=0.3038, duration=0.38s
Step 12762: loss=2.9248, lr=0.000337, tokens/sec=1374155.93, grad_norm=0.3276, duration=0.38s
Step 12763: loss=2.9556, lr=0.000337, tokens/sec=1373044.81, grad_norm=0.3184, duration=0.38s
Step 12764: loss=2.8934, lr=0.000337, tokens/sec=1371001.45, grad_norm=0.3253, duration=0.38s
Step 12765: loss=2.8642, lr=0.000337, tokens/sec=1372839.08, grad_norm=0.3035, duration=0.38s
Step 12766: loss=2.8735, lr=0.000337, tokens/sec=1374247.82, grad_norm=0.2935, duration=0.38s
Step 12767: loss=2.8461, lr=0.000337, tokens/sec=1378004.21, grad_norm=0.3024, duration=0.38s
Step 12768: loss=2.8306, lr=0.000337, tokens/sec=1371595.77, grad_norm=0.3209, duration=0.38s
Step 12769: loss=2.8348, lr=0.000337, tokens/sec=1375313.57, grad_norm=0.3061, duration=0.38s
Step 12770: loss=2.8680, lr=0.000337, tokens/sec=1377385.35, grad_norm=0.2955, duration=0.38s
Step 12771: loss=2.8188, lr=0.000337, tokens/sec=1376211.29, grad_norm=0.3039, duration=0.38s
Step 12772: loss=2.8478, lr=0.000337, tokens/sec=1377039.48, grad_norm=0.3103, duration=0.38s
Step 12773: loss=2.8326, lr=0.000337, tokens/sec=1372685.69, grad_norm=0.3105, duration=0.38s
Step 12774: loss=2.8453, lr=0.000337, tokens/sec=1374930.91, grad_norm=0.3095, duration=0.38s
Step 12775: loss=2.8374, lr=0.000337, tokens/sec=1374078.65, grad_norm=0.2940, duration=0.38s
Step 12776: loss=2.8188, lr=0.000337, tokens/sec=1377911.82, grad_norm=0.2952, duration=0.38s
Step 12777: loss=2.9130, lr=0.000337, tokens/sec=1374726.34, grad_norm=0.3256, duration=0.38s
Step 12778: loss=2.9406, lr=0.000337, tokens/sec=1375440.03, grad_norm=0.3430, duration=0.38s
Step 12779: loss=2.9148, lr=0.000337, tokens/sec=1373956.74, grad_norm=0.3185, duration=0.38s
Step 12780: loss=2.9243, lr=0.000337, tokens/sec=1373748.17, grad_norm=0.3056, duration=0.38s
Step 12781: loss=2.9497, lr=0.000337, tokens/sec=1375464.12, grad_norm=0.3388, duration=0.38s
Step 12782: loss=2.9290, lr=0.000337, tokens/sec=1375010.87, grad_norm=0.3415, duration=0.38s
Step 12783: loss=2.9530, lr=0.000337, tokens/sec=1373224.87, grad_norm=0.3198, duration=0.38s
Step 12784: loss=2.8737, lr=0.000337, tokens/sec=1374625.80, grad_norm=0.3216, duration=0.38s
Step 12785: loss=2.8710, lr=0.000337, tokens/sec=1377994.71, grad_norm=0.3243, duration=0.38s
Step 12786: loss=2.9085, lr=0.000337, tokens/sec=1375790.26, grad_norm=0.3406, duration=0.38s
Step 12787: loss=2.8886, lr=0.000337, tokens/sec=1376891.17, grad_norm=0.3496, duration=0.38s
Step 12788: loss=2.8641, lr=0.000337, tokens/sec=1373236.87, grad_norm=0.3093, duration=0.38s
Step 12789: loss=2.8820, lr=0.000337, tokens/sec=1377614.88, grad_norm=0.3072, duration=0.38s
Step 12790: loss=2.8943, lr=0.000337, tokens/sec=1375957.27, grad_norm=0.3496, duration=0.38s
Step 12791: loss=2.7906, lr=0.000337, tokens/sec=1376440.43, grad_norm=0.3504, duration=0.38s
Step 12792: loss=2.8359, lr=0.000337, tokens/sec=1377401.74, grad_norm=0.3194, duration=0.38s
Step 12793: loss=2.8933, lr=0.000337, tokens/sec=1376427.51, grad_norm=0.3459, duration=0.38s
Step 12794: loss=2.8753, lr=0.000336, tokens/sec=1376126.89, grad_norm=0.3402, duration=0.38s
Step 12795: loss=2.9833, lr=0.000336, tokens/sec=1374395.55, grad_norm=0.3469, duration=0.38s
Step 12796: loss=2.9121, lr=0.000336, tokens/sec=1377349.98, grad_norm=0.3402, duration=0.38s
Step 12797: loss=2.9488, lr=0.000336, tokens/sec=1371220.30, grad_norm=0.3419, duration=0.38s
Step 12798: loss=2.9210, lr=0.000336, tokens/sec=1374185.12, grad_norm=0.3303, duration=0.38s
Step 12799: loss=2.8740, lr=0.000336, tokens/sec=1372339.60, grad_norm=0.3414, duration=0.38s
Step 12800/19073 (67.1%), Elapsed time: 5071.86s, Steps per hour: 9085.43, Estimated hours remaining: 0.69
Step 12800: loss=2.9166, lr=0.000336, tokens/sec=1374110.42, grad_norm=0.3389, duration=0.38s
Step 12801: loss=2.9337, lr=0.000336, tokens/sec=1376764.45, grad_norm=0.3271, duration=0.38s
Step 12802: loss=2.8973, lr=0.000336, tokens/sec=1374003.10, grad_norm=0.3372, duration=0.38s
Step 12803: loss=2.9477, lr=0.000336, tokens/sec=1373445.29, grad_norm=0.3303, duration=0.38s
Step 12804: loss=2.9336, lr=0.000336, tokens/sec=1374533.86, grad_norm=0.3230, duration=0.38s
Step 12805: loss=2.8980, lr=0.000336, tokens/sec=1373277.18, grad_norm=0.3247, duration=0.38s
Step 12806: loss=2.9369, lr=0.000336, tokens/sec=1374960.14, grad_norm=0.3282, duration=0.38s
Step 12807: loss=2.8625, lr=0.000336, tokens/sec=1377803.04, grad_norm=0.3182, duration=0.38s
Step 12808: loss=2.8952, lr=0.000336, tokens/sec=1373021.66, grad_norm=0.3078, duration=0.38s
Step 12809: loss=2.9100, lr=0.000336, tokens/sec=1373815.11, grad_norm=0.3161, duration=0.38s
Step 12810: loss=2.9065, lr=0.000336, tokens/sec=1375850.52, grad_norm=0.3253, duration=0.38s
Step 12811: loss=2.8339, lr=0.000336, tokens/sec=1374537.30, grad_norm=0.3044, duration=0.38s
Step 12812: loss=2.8772, lr=0.000336, tokens/sec=1374720.33, grad_norm=0.3185, duration=0.38s
Step 12813: loss=2.8229, lr=0.000336, tokens/sec=1371556.41, grad_norm=0.3172, duration=0.38s
Step 12814: loss=2.8537, lr=0.000336, tokens/sec=1374314.81, grad_norm=0.2983, duration=0.38s
Step 12815: loss=2.8060, lr=0.000336, tokens/sec=1374190.28, grad_norm=0.3166, duration=0.38s
Step 12816: loss=2.8596, lr=0.000336, tokens/sec=1371470.02, grad_norm=0.3376, duration=0.38s
Step 12817: loss=2.8492, lr=0.000336, tokens/sec=1375717.96, grad_norm=0.3171, duration=0.38s
Step 12818: loss=2.7998, lr=0.000336, tokens/sec=1375197.46, grad_norm=0.3342, duration=0.38s
Step 12819: loss=2.8339, lr=0.000336, tokens/sec=1376227.66, grad_norm=0.3395, duration=0.38s
Step 12820: loss=2.8013, lr=0.000336, tokens/sec=1376679.99, grad_norm=0.3171, duration=0.38s
Step 12821: loss=2.8774, lr=0.000336, tokens/sec=1374036.58, grad_norm=0.3130, duration=0.38s
Step 12822: loss=2.8542, lr=0.000336, tokens/sec=1373023.38, grad_norm=0.3267, duration=0.38s
Step 12823: loss=2.9019, lr=0.000336, tokens/sec=1371920.93, grad_norm=0.3362, duration=0.38s
Step 12824: loss=2.9227, lr=0.000336, tokens/sec=1370317.12, grad_norm=0.3207, duration=0.38s
Step 12825: loss=2.9221, lr=0.000336, tokens/sec=1374222.91, grad_norm=0.3239, duration=0.38s
Step 12826: loss=2.9286, lr=0.000336, tokens/sec=1375014.31, grad_norm=0.3533, duration=0.38s
Step 12827: loss=2.9343, lr=0.000336, tokens/sec=1374366.34, grad_norm=0.3185, duration=0.38s
Step 12828: loss=2.8509, lr=0.000336, tokens/sec=1378030.12, grad_norm=0.3459, duration=0.38s
Step 12829: loss=2.9063, lr=0.000336, tokens/sec=1373567.97, grad_norm=0.3460, duration=0.38s
Step 12830: loss=2.8769, lr=0.000335, tokens/sec=1369468.01, grad_norm=0.3123, duration=0.38s
Step 12831: loss=2.9424, lr=0.000335, tokens/sec=1373105.68, grad_norm=0.3193, duration=0.38s
Step 12832: loss=2.9214, lr=0.000335, tokens/sec=1371551.28, grad_norm=0.3207, duration=0.38s
Step 12833: loss=2.8981, lr=0.000335, tokens/sec=1376786.00, grad_norm=0.3282, duration=0.38s
Step 12834: loss=2.9307, lr=0.000335, tokens/sec=1372473.22, grad_norm=0.3008, duration=0.38s
Step 12835: loss=2.8958, lr=0.000335, tokens/sec=1369630.07, grad_norm=0.3071, duration=0.38s
Step 12836: loss=2.8807, lr=0.000335, tokens/sec=1376544.69, grad_norm=0.2936, duration=0.38s
Step 12837: loss=2.8801, lr=0.000335, tokens/sec=1367931.19, grad_norm=0.2822, duration=0.38s
Step 12838: loss=2.8957, lr=0.000335, tokens/sec=1374797.68, grad_norm=0.2956, duration=0.38s
Step 12839: loss=2.8345, lr=0.000335, tokens/sec=1376448.19, grad_norm=0.2925, duration=0.38s
Step 12840: loss=2.9030, lr=0.000335, tokens/sec=1378200.26, grad_norm=0.2898, duration=0.38s
Step 12841: loss=2.8750, lr=0.000335, tokens/sec=1376387.88, grad_norm=0.2933, duration=0.38s
Step 12842: loss=2.8729, lr=0.000335, tokens/sec=1374979.92, grad_norm=0.2866, duration=0.38s
Step 12843: loss=2.8289, lr=0.000335, tokens/sec=1375435.73, grad_norm=0.3023, duration=0.38s
Step 12844: loss=2.8994, lr=0.000335, tokens/sec=1372364.44, grad_norm=0.3209, duration=0.38s
Step 12845: loss=2.8496, lr=0.000335, tokens/sec=1376147.56, grad_norm=0.2892, duration=0.38s
Step 12846: loss=2.9037, lr=0.000335, tokens/sec=1376849.79, grad_norm=0.2837, duration=0.38s
Step 12847: loss=2.8419, lr=0.000335, tokens/sec=1380034.74, grad_norm=0.3065, duration=0.38s
Step 12848: loss=2.8825, lr=0.000335, tokens/sec=1376042.51, grad_norm=0.3091, duration=0.38s
Step 12849: loss=2.9054, lr=0.000335, tokens/sec=1378880.38, grad_norm=0.2890, duration=0.38s
Step 12850: loss=2.9230, lr=0.000335, tokens/sec=1373462.45, grad_norm=0.3077, duration=0.38s
Step 12851: loss=2.9044, lr=0.000335, tokens/sec=1375048.70, grad_norm=0.3083, duration=0.38s
Step 12852: loss=2.8939, lr=0.000335, tokens/sec=1373303.77, grad_norm=0.2927, duration=0.38s
Step 12853: loss=2.8934, lr=0.000335, tokens/sec=1376682.57, grad_norm=0.3268, duration=0.38s
Step 12854: loss=2.8975, lr=0.000335, tokens/sec=1375435.73, grad_norm=0.3251, duration=0.38s
Step 12855: loss=2.9212, lr=0.000335, tokens/sec=1377795.27, grad_norm=0.2865, duration=0.38s
Step 12856: loss=2.8682, lr=0.000335, tokens/sec=1374143.05, grad_norm=0.3177, duration=0.38s
Step 12857: loss=2.8872, lr=0.000335, tokens/sec=1380988.07, grad_norm=0.3364, duration=0.38s
Step 12858: loss=2.8742, lr=0.000335, tokens/sec=1376104.50, grad_norm=0.2877, duration=0.38s
Step 12859: loss=2.8479, lr=0.000335, tokens/sec=1373055.10, grad_norm=0.2982, duration=0.38s
Step 12860: loss=2.8925, lr=0.000335, tokens/sec=1377262.85, grad_norm=0.3640, duration=0.38s
Step 12861: loss=2.7912, lr=0.000335, tokens/sec=1375540.69, grad_norm=0.3094, duration=0.38s
Step 12862: loss=2.8322, lr=0.000335, tokens/sec=1377293.04, grad_norm=0.3004, duration=0.38s
Step 12863: loss=2.8618, lr=0.000335, tokens/sec=1372379.86, grad_norm=0.3190, duration=0.38s
Step 12864: loss=2.8601, lr=0.000335, tokens/sec=1374404.14, grad_norm=0.3230, duration=0.38s
Step 12865: loss=2.8724, lr=0.000335, tokens/sec=1375835.88, grad_norm=0.3368, duration=0.38s
Step 12866: loss=2.8515, lr=0.000335, tokens/sec=1379030.84, grad_norm=0.3113, duration=0.38s
Step 12867: loss=2.8533, lr=0.000334, tokens/sec=1377740.03, grad_norm=0.3143, duration=0.38s
Step 12868: loss=2.8409, lr=0.000334, tokens/sec=1375529.50, grad_norm=0.3345, duration=0.38s
Step 12869: loss=2.8933, lr=0.000334, tokens/sec=1376694.64, grad_norm=0.3066, duration=0.38s
Step 12870: loss=2.8517, lr=0.000334, tokens/sec=1376924.80, grad_norm=0.3654, duration=0.38s
Step 12871: loss=2.9357, lr=0.000334, tokens/sec=1375084.81, grad_norm=0.3561, duration=0.38s
Step 12872: loss=2.9404, lr=0.000334, tokens/sec=1378413.64, grad_norm=0.3342, duration=0.38s
Step 12873: loss=2.8988, lr=0.000334, tokens/sec=1377325.82, grad_norm=0.3275, duration=0.38s
Step 12874: loss=2.9455, lr=0.000334, tokens/sec=1378068.12, grad_norm=0.3618, duration=0.38s
Step 12875: loss=2.8698, lr=0.000334, tokens/sec=1370388.86, grad_norm=0.3409, duration=0.38s
Step 12876: loss=2.8957, lr=0.000334, tokens/sec=1375686.12, grad_norm=0.3224, duration=0.38s
Step 12877: loss=2.9339, lr=0.000334, tokens/sec=1376255.22, grad_norm=0.3314, duration=0.38s
Step 12878: loss=3.0078, lr=0.000334, tokens/sec=1377489.75, grad_norm=0.3681, duration=0.38s
Step 12879: loss=2.9784, lr=0.000334, tokens/sec=1373839.14, grad_norm=0.3336, duration=0.38s
Step 12880: loss=2.9306, lr=0.000334, tokens/sec=1379835.57, grad_norm=0.3344, duration=0.38s
Step 12881: loss=2.8790, lr=0.000334, tokens/sec=1379907.44, grad_norm=0.3506, duration=0.38s
Step 12882: loss=2.9193, lr=0.000334, tokens/sec=1376107.09, grad_norm=0.3408, duration=0.38s
Step 12883: loss=2.8923, lr=0.000334, tokens/sec=1376137.23, grad_norm=0.3246, duration=0.38s
Step 12884: loss=2.8721, lr=0.000334, tokens/sec=1374153.35, grad_norm=0.3288, duration=0.38s
Step 12885: loss=2.9352, lr=0.000334, tokens/sec=1376070.92, grad_norm=0.3450, duration=0.38s
Step 12886: loss=2.8576, lr=0.000334, tokens/sec=1376967.04, grad_norm=0.3406, duration=0.38s
Step 12887: loss=2.9123, lr=0.000334, tokens/sec=1376447.32, grad_norm=0.3294, duration=0.38s
Step 12888: loss=2.9569, lr=0.000334, tokens/sec=1378316.88, grad_norm=0.3475, duration=0.38s
Step 12889: loss=2.8808, lr=0.000334, tokens/sec=1379949.87, grad_norm=0.3495, duration=0.38s
Step 12890: loss=2.8622, lr=0.000334, tokens/sec=1377318.92, grad_norm=0.3307, duration=0.38s
Step 12891: loss=2.9121, lr=0.000334, tokens/sec=1375841.91, grad_norm=0.3347, duration=0.38s
Step 12892: loss=2.9145, lr=0.000334, tokens/sec=1377361.19, grad_norm=0.3322, duration=0.38s
Step 12893: loss=2.9201, lr=0.000334, tokens/sec=1380527.71, grad_norm=0.3470, duration=0.38s
Step 12894: loss=2.8326, lr=0.000334, tokens/sec=1374554.48, grad_norm=0.3017, duration=0.38s
Step 12895: loss=2.8261, lr=0.000334, tokens/sec=1376244.88, grad_norm=0.3310, duration=0.38s
Step 12896: loss=2.8803, lr=0.000334, tokens/sec=1379230.63, grad_norm=0.3394, duration=0.38s
Step 12897: loss=2.8974, lr=0.000334, tokens/sec=1375943.49, grad_norm=0.3212, duration=0.38s
Step 12898: loss=2.9572, lr=0.000334, tokens/sec=1377255.95, grad_norm=0.2959, duration=0.38s
Step 12899: loss=2.8748, lr=0.000334, tokens/sec=1374798.54, grad_norm=0.3395, duration=0.38s
Step 12900/19073 (67.6%), Elapsed time: 5110.07s, Steps per hour: 9087.94, Estimated hours remaining: 0.68
Step 12900: loss=2.8881, lr=0.000334, tokens/sec=1375449.49, grad_norm=0.3553, duration=0.38s
Step 12901: loss=2.8664, lr=0.000334, tokens/sec=1374125.87, grad_norm=0.3103, duration=0.38s
Step 12902: loss=2.8366, lr=0.000334, tokens/sec=1376101.92, grad_norm=0.2904, duration=0.38s
Step 12903: loss=2.9110, lr=0.000333, tokens/sec=1378093.16, grad_norm=0.3270, duration=0.38s
Step 12904: loss=2.8656, lr=0.000333, tokens/sec=1374837.22, grad_norm=0.3277, duration=0.38s
Step 12905: loss=2.8319, lr=0.000333, tokens/sec=1378444.75, grad_norm=0.2977, duration=0.38s
Step 12906: loss=2.8387, lr=0.000333, tokens/sec=1378307.37, grad_norm=0.3077, duration=0.38s
Step 12907: loss=2.8922, lr=0.000333, tokens/sec=1374453.10, grad_norm=0.3200, duration=0.38s
Step 12908: loss=2.8518, lr=0.000333, tokens/sec=1374654.16, grad_norm=0.3160, duration=0.38s
Step 12909: loss=2.8385, lr=0.000333, tokens/sec=1376610.18, grad_norm=0.2949, duration=0.38s
Step 12910: loss=2.7870, lr=0.000333, tokens/sec=1377947.22, grad_norm=0.3060, duration=0.38s
Step 12911: loss=2.8250, lr=0.000333, tokens/sec=1376194.93, grad_norm=0.3171, duration=0.38s
Step 12912: loss=2.8594, lr=0.000333, tokens/sec=1376524.01, grad_norm=0.2989, duration=0.38s
Step 12913: loss=2.8384, lr=0.000333, tokens/sec=1377423.31, grad_norm=0.3088, duration=0.38s
Step 12914: loss=2.8890, lr=0.000333, tokens/sec=1373840.86, grad_norm=0.3253, duration=0.38s
Step 12915: loss=2.9079, lr=0.000333, tokens/sec=1378633.14, grad_norm=0.3149, duration=0.38s
Step 12916: loss=2.9156, lr=0.000333, tokens/sec=1375698.17, grad_norm=0.3030, duration=0.38s
Step 12917: loss=2.9369, lr=0.000333, tokens/sec=1378507.83, grad_norm=0.3329, duration=0.38s
Step 12918: loss=2.8644, lr=0.000333, tokens/sec=1377504.42, grad_norm=0.3176, duration=0.38s
Step 12919: loss=2.9263, lr=0.000333, tokens/sec=1373756.75, grad_norm=0.3132, duration=0.38s
Step 12920: loss=2.8892, lr=0.000333, tokens/sec=1379310.22, grad_norm=0.3441, duration=0.38s
Step 12921: loss=2.9086, lr=0.000333, tokens/sec=1375835.88, grad_norm=0.3468, duration=0.38s
Step 12922: loss=2.9314, lr=0.000333, tokens/sec=1367103.73, grad_norm=0.3199, duration=0.38s
Step 12923: loss=2.9889, lr=0.000333, tokens/sec=1374118.15, grad_norm=0.3161, duration=0.38s
Step 12924: loss=2.9390, lr=0.000333, tokens/sec=1373804.81, grad_norm=0.3488, duration=0.38s
Step 12925: loss=2.9596, lr=0.000333, tokens/sec=1374538.16, grad_norm=0.3535, duration=0.38s
Step 12926: loss=2.9940, lr=0.000333, tokens/sec=1376365.48, grad_norm=0.3227, duration=0.38s
Step 12927: loss=2.9209, lr=0.000333, tokens/sec=1370356.40, grad_norm=0.3163, duration=0.38s
Step 12928: loss=2.9445, lr=0.000333, tokens/sec=1375359.16, grad_norm=0.3503, duration=0.38s
Step 12929: loss=2.9506, lr=0.000333, tokens/sec=1372776.52, grad_norm=0.3427, duration=0.38s
Step 12930: loss=2.9306, lr=0.000333, tokens/sec=1373704.40, grad_norm=0.3071, duration=0.38s
Step 12931: loss=2.8995, lr=0.000333, tokens/sec=1372945.37, grad_norm=0.3234, duration=0.38s
Step 12932: loss=2.9060, lr=0.000333, tokens/sec=1376082.12, grad_norm=0.3297, duration=0.38s
Step 12933: loss=2.8543, lr=0.000333, tokens/sec=1374975.62, grad_norm=0.3019, duration=0.38s
Step 12934: loss=2.9280, lr=0.000333, tokens/sec=1373892.36, grad_norm=0.3066, duration=0.38s
Step 12935: loss=2.8899, lr=0.000333, tokens/sec=1371760.04, grad_norm=0.3782, duration=0.38s
Step 12936: loss=2.8576, lr=0.000333, tokens/sec=1375081.37, grad_norm=0.3491, duration=0.38s
Step 12937: loss=2.9172, lr=0.000333, tokens/sec=1375632.76, grad_norm=0.3159, duration=0.38s
Step 12938: loss=2.8293, lr=0.000333, tokens/sec=1372617.14, grad_norm=0.3249, duration=0.38s
Step 12939: loss=2.8062, lr=0.000333, tokens/sec=1374634.39, grad_norm=0.3956, duration=0.38s
Step 12940: loss=2.8541, lr=0.000332, tokens/sec=1379221.98, grad_norm=0.3490, duration=0.38s
Step 12941: loss=2.8290, lr=0.000332, tokens/sec=1376687.74, grad_norm=0.3147, duration=0.38s
Step 12942: loss=2.9656, lr=0.000332, tokens/sec=1374990.23, grad_norm=0.3259, duration=0.38s
Step 12943: loss=2.8102, lr=0.000332, tokens/sec=1377237.83, grad_norm=0.3179, duration=0.38s
Step 12944: loss=2.9548, lr=0.000332, tokens/sec=1375783.38, grad_norm=0.3382, duration=0.38s
Step 12945: loss=2.9357, lr=0.000332, tokens/sec=1376289.67, grad_norm=0.3193, duration=0.38s
Step 12946: loss=2.8761, lr=0.000332, tokens/sec=1376980.84, grad_norm=0.3072, duration=0.38s
Step 12947: loss=2.9049, lr=0.000332, tokens/sec=1373887.21, grad_norm=0.3211, duration=0.38s
Step 12948: loss=2.9299, lr=0.000332, tokens/sec=1375728.29, grad_norm=0.3341, duration=0.38s
Step 12949: loss=2.8920, lr=0.000332, tokens/sec=1379311.95, grad_norm=0.3244, duration=0.38s
Step 12950: loss=2.8304, lr=0.000332, tokens/sec=1377381.03, grad_norm=0.3255, duration=0.38s
Step 12951: loss=2.9103, lr=0.000332, tokens/sec=1374879.34, grad_norm=0.3338, duration=0.38s
Step 12952: loss=2.9237, lr=0.000332, tokens/sec=1373312.34, grad_norm=0.3447, duration=0.38s
Step 12953: loss=2.9413, lr=0.000332, tokens/sec=1377875.56, grad_norm=0.3044, duration=0.38s
Step 12954: loss=2.9150, lr=0.000332, tokens/sec=1377341.35, grad_norm=0.3168, duration=0.38s
Step 12955: loss=2.8291, lr=0.000332, tokens/sec=1377010.16, grad_norm=0.3337, duration=0.38s
Step 12956: loss=2.8900, lr=0.000332, tokens/sec=1376732.56, grad_norm=0.3115, duration=0.38s
Step 12957: loss=2.8289, lr=0.000332, tokens/sec=1378521.65, grad_norm=0.2854, duration=0.38s
Step 12958: loss=2.8172, lr=0.000332, tokens/sec=1376643.79, grad_norm=0.3191, duration=0.38s
Step 12959: loss=2.8508, lr=0.000332, tokens/sec=1373652.06, grad_norm=0.3129, duration=0.38s
Step 12960: loss=2.8107, lr=0.000332, tokens/sec=1372786.81, grad_norm=0.2906, duration=0.38s
Step 12961: loss=2.8456, lr=0.000332, tokens/sec=1375492.51, grad_norm=0.3067, duration=0.38s
Step 12962: loss=2.8141, lr=0.000332, tokens/sec=1374520.11, grad_norm=0.3051, duration=0.38s
Step 12963: loss=2.8643, lr=0.000332, tokens/sec=1374206.59, grad_norm=0.2911, duration=0.38s
Step 12964: loss=2.8355, lr=0.000332, tokens/sec=1378691.92, grad_norm=0.3024, duration=0.38s
Step 12965: loss=2.8001, lr=0.000332, tokens/sec=1374562.21, grad_norm=0.3032, duration=0.38s
Step 12966: loss=2.8389, lr=0.000332, tokens/sec=1372652.27, grad_norm=0.2907, duration=0.38s
Step 12967: loss=2.8943, lr=0.000332, tokens/sec=1371951.75, grad_norm=0.2991, duration=0.38s
Step 12968: loss=2.9550, lr=0.000332, tokens/sec=1372268.52, grad_norm=0.3330, duration=0.38s
Step 12969: loss=2.9259, lr=0.000332, tokens/sec=1375171.66, grad_norm=0.3284, duration=0.38s
Step 12970: loss=2.9169, lr=0.000332, tokens/sec=1374493.48, grad_norm=0.2936, duration=0.38s
Step 12971: loss=2.9281, lr=0.000332, tokens/sec=1372968.51, grad_norm=0.3113, duration=0.38s
Step 12972: loss=2.9509, lr=0.000332, tokens/sec=1371687.31, grad_norm=0.3442, duration=0.38s
Step 12973: loss=2.9061, lr=0.000332, tokens/sec=1376930.83, grad_norm=0.2998, duration=0.38s
Step 12974: loss=2.8735, lr=0.000332, tokens/sec=1374289.04, grad_norm=0.3107, duration=0.38s
Step 12975: loss=2.8766, lr=0.000332, tokens/sec=1369869.82, grad_norm=0.3237, duration=0.38s
Step 12976: loss=2.8989, lr=0.000331, tokens/sec=1372519.48, grad_norm=0.3168, duration=0.38s
Step 12977: loss=2.8620, lr=0.000331, tokens/sec=1372770.52, grad_norm=0.3304, duration=0.38s
Step 12978: loss=2.8773, lr=0.000331, tokens/sec=1373550.81, grad_norm=0.3425, duration=0.38s
Step 12979: loss=2.8867, lr=0.000331, tokens/sec=1378805.16, grad_norm=0.3095, duration=0.38s
Step 12980: loss=2.8606, lr=0.000331, tokens/sec=1374071.78, grad_norm=0.3102, duration=0.38s
Step 12981: loss=2.7779, lr=0.000331, tokens/sec=1374667.05, grad_norm=0.3496, duration=0.38s
Step 12982: loss=2.8467, lr=0.000331, tokens/sec=1376463.69, grad_norm=0.3417, duration=0.38s
Step 12983: loss=2.8803, lr=0.000331, tokens/sec=1372675.41, grad_norm=0.3110, duration=0.38s
Step 12984: loss=2.9407, lr=0.000331, tokens/sec=1377167.11, grad_norm=0.3200, duration=0.38s
Step 12985: loss=2.9667, lr=0.000331, tokens/sec=1376835.14, grad_norm=0.3437, duration=0.38s
Step 12986: loss=2.8671, lr=0.000331, tokens/sec=1374540.73, grad_norm=0.3332, duration=0.38s
Step 12987: loss=2.9498, lr=0.000331, tokens/sec=1376293.98, grad_norm=0.3193, duration=0.38s
Step 12988: loss=2.8834, lr=0.000331, tokens/sec=1372743.10, grad_norm=0.3218, duration=0.38s
Step 12989: loss=2.9128, lr=0.000331, tokens/sec=1374904.26, grad_norm=0.3322, duration=0.38s
Step 12990: loss=2.9244, lr=0.000331, tokens/sec=1371284.43, grad_norm=0.3288, duration=0.38s
Step 12991: loss=2.9139, lr=0.000331, tokens/sec=1374496.06, grad_norm=0.3036, duration=0.38s
Step 12992: loss=2.9239, lr=0.000331, tokens/sec=1375841.91, grad_norm=0.3246, duration=0.38s
Step 12993: loss=2.9276, lr=0.000331, tokens/sec=1374677.36, grad_norm=0.3441, duration=0.38s
Step 12994: loss=2.8962, lr=0.000331, tokens/sec=1373476.17, grad_norm=0.3073, duration=0.38s
Step 12995: loss=2.9282, lr=0.000331, tokens/sec=1370821.12, grad_norm=0.3116, duration=0.38s
Step 12996: loss=2.9022, lr=0.000331, tokens/sec=1374360.33, grad_norm=0.3455, duration=0.38s
Step 12997: loss=2.8531, lr=0.000331, tokens/sec=1372753.38, grad_norm=0.3333, duration=0.38s
Step 12998: loss=2.9163, lr=0.000331, tokens/sec=1378199.39, grad_norm=0.3027, duration=0.38s
Step 12999: loss=2.8895, lr=0.000331, tokens/sec=1373083.39, grad_norm=0.3074, duration=0.38s
Step 13000/19073 (68.2%), Elapsed time: 5148.28s, Steps per hour: 9090.41, Estimated hours remaining: 0.67
Validation loss at step 13000: 3.800591468811035
Step 13000: loss=2.9006, lr=0.000331, tokens/sec=153253.78, grad_norm=0.3237, duration=3.42s
Step 13001: loss=2.8100, lr=0.000331, tokens/sec=1372432.10, grad_norm=0.3183, duration=0.38s
Step 13002: loss=2.8710, lr=0.000331, tokens/sec=1375296.37, grad_norm=0.3044, duration=0.38s
Step 13003: loss=2.8414, lr=0.000331, tokens/sec=1373136.55, grad_norm=0.3080, duration=0.38s
Step 13004: loss=2.8464, lr=0.000331, tokens/sec=1371908.95, grad_norm=0.3073, duration=0.38s
Step 13005: loss=2.8217, lr=0.000331, tokens/sec=1376392.19, grad_norm=0.3211, duration=0.38s
Step 13006: loss=2.8260, lr=0.000331, tokens/sec=1375415.94, grad_norm=0.3096, duration=0.38s
Step 13007: loss=2.8411, lr=0.000331, tokens/sec=1376628.28, grad_norm=0.3080, duration=0.38s
Step 13008: loss=2.8080, lr=0.000331, tokens/sec=1374119.01, grad_norm=0.3027, duration=0.38s
Step 13009: loss=2.8189, lr=0.000331, tokens/sec=1369972.23, grad_norm=0.3327, duration=0.38s
Step 13010: loss=2.8172, lr=0.000331, tokens/sec=1370717.72, grad_norm=0.3172, duration=0.38s
Step 13011: loss=2.8663, lr=0.000331, tokens/sec=1371699.29, grad_norm=0.2960, duration=0.38s
Step 13012: loss=2.8737, lr=0.000331, tokens/sec=1370869.83, grad_norm=0.3140, duration=0.38s
Step 13013: loss=2.8746, lr=0.000330, tokens/sec=1374206.59, grad_norm=0.3502, duration=0.38s
Step 13014: loss=2.9139, lr=0.000330, tokens/sec=1372809.95, grad_norm=0.3043, duration=0.38s
Step 13015: loss=2.9182, lr=0.000330, tokens/sec=1376804.97, grad_norm=0.2973, duration=0.38s
Step 13016: loss=2.9272, lr=0.000330, tokens/sec=1376892.90, grad_norm=0.3619, duration=0.38s
Step 13017: loss=2.9115, lr=0.000330, tokens/sec=1370630.58, grad_norm=0.3209, duration=0.38s
Step 13018: loss=2.8530, lr=0.000330, tokens/sec=1373784.21, grad_norm=0.3109, duration=0.38s
Step 13019: loss=2.9058, lr=0.000330, tokens/sec=1372546.89, grad_norm=0.3289, duration=0.38s
Step 13020: loss=2.8742, lr=0.000330, tokens/sec=1371326.33, grad_norm=0.3215, duration=0.38s
Step 13021: loss=2.9247, lr=0.000330, tokens/sec=1369781.93, grad_norm=0.3009, duration=0.38s
Step 13022: loss=2.9070, lr=0.000330, tokens/sec=1371849.90, grad_norm=0.2980, duration=0.38s
Step 13023: loss=2.9248, lr=0.000330, tokens/sec=1370211.25, grad_norm=0.3117, duration=0.38s
Step 13024: loss=2.9245, lr=0.000330, tokens/sec=1364643.42, grad_norm=0.3155, duration=0.38s
Step 13025: loss=2.8651, lr=0.000330, tokens/sec=1372749.10, grad_norm=0.3042, duration=0.38s
Step 13026: loss=2.8703, lr=0.000330, tokens/sec=1373633.18, grad_norm=0.3042, duration=0.38s
Step 13027: loss=2.8851, lr=0.000330, tokens/sec=1372028.79, grad_norm=0.2922, duration=0.38s
Step 13028: loss=2.8828, lr=0.000330, tokens/sec=1372124.67, grad_norm=0.3076, duration=0.38s
Step 13029: loss=2.8469, lr=0.000330, tokens/sec=1372122.10, grad_norm=0.2973, duration=0.38s
Step 13030: loss=2.8817, lr=0.000330, tokens/sec=1374423.04, grad_norm=0.2971, duration=0.38s
Step 13031: loss=2.8511, lr=0.000330, tokens/sec=1379070.62, grad_norm=0.3019, duration=0.38s
Step 13032: loss=2.8819, lr=0.000330, tokens/sec=1372051.05, grad_norm=0.2993, duration=0.38s
Step 13033: loss=2.8218, lr=0.000330, tokens/sec=1376326.71, grad_norm=0.2814, duration=0.38s
Step 13034: loss=2.8996, lr=0.000330, tokens/sec=1374355.18, grad_norm=0.3214, duration=0.38s
Step 13035: loss=2.8458, lr=0.000330, tokens/sec=1371950.04, grad_norm=0.3108, duration=0.38s
Step 13036: loss=2.8702, lr=0.000330, tokens/sec=1374608.61, grad_norm=0.2962, duration=0.38s
Step 13037: loss=2.8566, lr=0.000330, tokens/sec=1375483.90, grad_norm=0.2958, duration=0.38s
Step 13038: loss=2.9216, lr=0.000330, tokens/sec=1373844.29, grad_norm=0.3276, duration=0.38s
Step 13039: loss=2.9065, lr=0.000330, tokens/sec=1377120.54, grad_norm=0.3089, duration=0.38s
Step 13040: loss=2.9020, lr=0.000330, tokens/sec=1376108.81, grad_norm=0.3118, duration=0.38s
Step 13041: loss=2.8849, lr=0.000330, tokens/sec=1375975.35, grad_norm=0.3212, duration=0.38s
Step 13042: loss=2.9078, lr=0.000330, tokens/sec=1379728.22, grad_norm=0.3110, duration=0.38s
Step 13043: loss=2.8716, lr=0.000330, tokens/sec=1375588.88, grad_norm=0.3266, duration=0.38s
Step 13044: loss=2.9156, lr=0.000330, tokens/sec=1374761.58, grad_norm=0.3396, duration=0.38s
Step 13045: loss=2.8744, lr=0.000330, tokens/sec=1373574.83, grad_norm=0.3004, duration=0.38s
Step 13046: loss=2.8914, lr=0.000330, tokens/sec=1377357.74, grad_norm=0.3125, duration=0.38s
Step 13047: loss=2.8764, lr=0.000330, tokens/sec=1377706.36, grad_norm=0.3356, duration=0.38s
Step 13048: loss=2.9001, lr=0.000330, tokens/sec=1376636.03, grad_norm=0.3263, duration=0.38s
Step 13049: loss=2.8524, lr=0.000330, tokens/sec=1377970.54, grad_norm=0.3091, duration=0.38s
Step 13050: loss=2.8661, lr=0.000329, tokens/sec=1375590.60, grad_norm=0.3332, duration=0.38s
Step 13051: loss=2.7842, lr=0.000329, tokens/sec=1375912.50, grad_norm=0.3386, duration=0.38s
Step 13052: loss=2.8566, lr=0.000329, tokens/sec=1373894.93, grad_norm=0.3154, duration=0.38s
Step 13053: loss=2.7936, lr=0.000329, tokens/sec=1372526.33, grad_norm=0.3108, duration=0.38s
Step 13054: loss=2.9055, lr=0.000329, tokens/sec=1375144.15, grad_norm=0.3397, duration=0.38s
Step 13055: loss=2.8705, lr=0.000329, tokens/sec=1378949.55, grad_norm=0.3260, duration=0.38s
Step 13056: loss=2.8650, lr=0.000329, tokens/sec=1375019.47, grad_norm=0.3064, duration=0.38s
Step 13057: loss=2.8061, lr=0.000329, tokens/sec=1377942.04, grad_norm=0.3071, duration=0.38s
Step 13058: loss=2.8558, lr=0.000329, tokens/sec=1377530.30, grad_norm=0.3338, duration=0.38s
Step 13059: loss=2.8295, lr=0.000329, tokens/sec=1378358.35, grad_norm=0.3358, duration=0.38s
Step 13060: loss=2.9057, lr=0.000329, tokens/sec=1376646.37, grad_norm=0.3187, duration=0.38s
Step 13061: loss=2.9292, lr=0.000329, tokens/sec=1373121.11, grad_norm=0.3317, duration=0.38s
Step 13062: loss=2.9200, lr=0.000329, tokens/sec=1373059.38, grad_norm=0.3519, duration=0.38s
Step 13063: loss=2.9117, lr=0.000329, tokens/sec=1378048.25, grad_norm=0.3295, duration=0.38s
Step 13064: loss=2.9203, lr=0.000329, tokens/sec=1380288.54, grad_norm=0.3271, duration=0.38s
Step 13065: loss=2.8718, lr=0.000329, tokens/sec=1373632.32, grad_norm=0.3386, duration=0.38s
Step 13066: loss=2.8840, lr=0.000329, tokens/sec=1376575.71, grad_norm=0.3154, duration=0.38s
Step 13067: loss=2.9371, lr=0.000329, tokens/sec=1374501.21, grad_norm=0.3317, duration=0.38s
Step 13068: loss=3.0307, lr=0.000329, tokens/sec=1375126.95, grad_norm=0.3685, duration=0.38s
Step 13069: loss=2.9381, lr=0.000329, tokens/sec=1374654.16, grad_norm=0.3373, duration=0.38s
Step 13070: loss=2.9239, lr=0.000329, tokens/sec=1372047.62, grad_norm=0.3259, duration=0.38s
Step 13071: loss=2.8854, lr=0.000329, tokens/sec=1372964.23, grad_norm=0.3355, duration=0.38s
Step 13072: loss=2.9281, lr=0.000329, tokens/sec=1378921.88, grad_norm=0.3524, duration=0.38s
Step 13073: loss=2.9026, lr=0.000329, tokens/sec=1373940.43, grad_norm=0.3442, duration=0.38s
Step 13074: loss=2.8370, lr=0.000329, tokens/sec=1375682.68, grad_norm=0.3381, duration=0.38s
Step 13075: loss=2.9404, lr=0.000329, tokens/sec=1377519.95, grad_norm=0.3508, duration=0.38s
Step 13076: loss=2.8252, lr=0.000329, tokens/sec=1377357.74, grad_norm=0.3395, duration=0.38s
Step 13077: loss=2.9309, lr=0.000329, tokens/sec=1376959.28, grad_norm=0.3454, duration=0.38s
Step 13078: loss=2.9244, lr=0.000329, tokens/sec=1377055.00, grad_norm=0.3185, duration=0.38s
Step 13079: loss=2.9058, lr=0.000329, tokens/sec=1372995.94, grad_norm=0.3577, duration=0.38s
Step 13080: loss=2.8516, lr=0.000329, tokens/sec=1374014.26, grad_norm=0.3458, duration=0.38s
Step 13081: loss=2.8833, lr=0.000329, tokens/sec=1377973.13, grad_norm=0.3313, duration=0.38s
Step 13082: loss=2.9471, lr=0.000329, tokens/sec=1375733.45, grad_norm=0.3379, duration=0.38s
Step 13083: loss=2.9046, lr=0.000329, tokens/sec=1377138.65, grad_norm=0.3489, duration=0.38s
Step 13084: loss=2.7643, lr=0.000329, tokens/sec=1375261.97, grad_norm=0.3365, duration=0.38s
Step 13085: loss=2.8690, lr=0.000329, tokens/sec=1378628.82, grad_norm=0.3183, duration=0.38s
Step 13086: loss=2.8776, lr=0.000329, tokens/sec=1377067.93, grad_norm=0.3356, duration=0.38s
Step 13087: loss=2.9246, lr=0.000328, tokens/sec=1374324.25, grad_norm=0.3320, duration=0.38s
Step 13088: loss=2.9465, lr=0.000328, tokens/sec=1373673.51, grad_norm=0.3253, duration=0.38s
Step 13089: loss=2.8710, lr=0.000328, tokens/sec=1376477.48, grad_norm=0.3194, duration=0.38s
Step 13090: loss=2.8629, lr=0.000328, tokens/sec=1376787.73, grad_norm=0.3353, duration=0.38s
Step 13091: loss=2.8768, lr=0.000328, tokens/sec=1377876.42, grad_norm=0.3250, duration=0.38s
Step 13092: loss=2.8674, lr=0.000328, tokens/sec=1376568.82, grad_norm=0.3146, duration=0.38s
Step 13093: loss=2.8956, lr=0.000328, tokens/sec=1378612.40, grad_norm=0.3022, duration=0.38s
Step 13094: loss=2.8448, lr=0.000328, tokens/sec=1372651.41, grad_norm=0.3115, duration=0.38s
Step 13095: loss=2.8300, lr=0.000328, tokens/sec=1374100.11, grad_norm=0.3307, duration=0.38s
Step 13096: loss=2.8282, lr=0.000328, tokens/sec=1374316.52, grad_norm=0.3066, duration=0.38s
Step 13097: loss=2.8932, lr=0.000328, tokens/sec=1378047.39, grad_norm=0.3000, duration=0.38s
Step 13098: loss=2.8546, lr=0.000328, tokens/sec=1375664.60, grad_norm=0.3261, duration=0.38s
Step 13099: loss=2.8114, lr=0.000328, tokens/sec=1378341.93, grad_norm=0.3096, duration=0.38s
Step 13100/19073 (68.7%), Elapsed time: 5189.54s, Steps per hour: 9087.51, Estimated hours remaining: 0.66
Step 13100: loss=2.7921, lr=0.000328, tokens/sec=1377383.62, grad_norm=0.2815, duration=0.38s
Step 13101: loss=2.8185, lr=0.000328, tokens/sec=1377279.24, grad_norm=0.3045, duration=0.38s
Step 13102: loss=2.8534, lr=0.000328, tokens/sec=1375506.27, grad_norm=0.3005, duration=0.38s
Step 13103: loss=2.8621, lr=0.000328, tokens/sec=1375449.49, grad_norm=0.2780, duration=0.38s
Step 13104: loss=2.8575, lr=0.000328, tokens/sec=1378689.32, grad_norm=0.2928, duration=0.38s
Step 13105: loss=2.9102, lr=0.000328, tokens/sec=1375531.23, grad_norm=0.3026, duration=0.38s
Step 13106: loss=2.9257, lr=0.000328, tokens/sec=1376224.21, grad_norm=0.3124, duration=0.38s
Step 13107: loss=2.8912, lr=0.000328, tokens/sec=1377689.10, grad_norm=0.3067, duration=0.38s
Step 13108: loss=2.8932, lr=0.000328, tokens/sec=1376613.63, grad_norm=0.3077, duration=0.38s
Step 13109: loss=2.9085, lr=0.000328, tokens/sec=1374765.88, grad_norm=0.3368, duration=0.38s
Step 13110: loss=2.8682, lr=0.000328, tokens/sec=1374641.27, grad_norm=0.3033, duration=0.38s
Step 13111: loss=2.9200, lr=0.000328, tokens/sec=1370710.03, grad_norm=0.3150, duration=0.38s
Step 13112: loss=2.9169, lr=0.000328, tokens/sec=1370111.36, grad_norm=0.3462, duration=0.38s
Step 13113: loss=2.9987, lr=0.000328, tokens/sec=1375585.43, grad_norm=0.3257, duration=0.38s
Step 13114: loss=2.9345, lr=0.000328, tokens/sec=1376528.32, grad_norm=0.2969, duration=0.38s
Step 13115: loss=2.9468, lr=0.000328, tokens/sec=1374303.64, grad_norm=0.3426, duration=0.38s
Step 13116: loss=2.9799, lr=0.000328, tokens/sec=1373923.26, grad_norm=0.3548, duration=0.38s
Step 13117: loss=2.9229, lr=0.000328, tokens/sec=1375370.35, grad_norm=0.3094, duration=0.38s
Step 13118: loss=2.9636, lr=0.000328, tokens/sec=1373896.65, grad_norm=0.3186, duration=0.38s
Step 13119: loss=2.9101, lr=0.000328, tokens/sec=1375885.81, grad_norm=0.3281, duration=0.38s
Step 13120: loss=2.9307, lr=0.000328, tokens/sec=1372701.11, grad_norm=0.3142, duration=0.38s
Step 13121: loss=2.8905, lr=0.000328, tokens/sec=1375304.11, grad_norm=0.3039, duration=0.38s
Step 13122: loss=2.9109, lr=0.000328, tokens/sec=1373428.99, grad_norm=0.3163, duration=0.38s
Step 13123: loss=2.8537, lr=0.000328, tokens/sec=1374289.04, grad_norm=0.3035, duration=0.38s
Step 13124: loss=2.8960, lr=0.000328, tokens/sec=1377211.10, grad_norm=0.3001, duration=0.38s
Step 13125: loss=2.8927, lr=0.000327, tokens/sec=1374112.14, grad_norm=0.3215, duration=0.38s
Step 13126: loss=2.8454, lr=0.000327, tokens/sec=1368561.17, grad_norm=0.3211, duration=0.38s
Step 13127: loss=2.9080, lr=0.000327, tokens/sec=1376829.97, grad_norm=0.3048, duration=0.38s
Step 13128: loss=2.8340, lr=0.000327, tokens/sec=1379018.73, grad_norm=0.3129, duration=0.38s
Step 13129: loss=2.8076, lr=0.000327, tokens/sec=1374385.24, grad_norm=0.3865, duration=0.38s
Step 13130: loss=2.7824, lr=0.000327, tokens/sec=1378972.90, grad_norm=0.3219, duration=0.38s
Step 13131: loss=2.8829, lr=0.000327, tokens/sec=1375353.14, grad_norm=0.3034, duration=0.38s
Step 13132: loss=2.9570, lr=0.000327, tokens/sec=1372983.09, grad_norm=0.3173, duration=0.38s
Step 13133: loss=2.8406, lr=0.000327, tokens/sec=1378007.67, grad_norm=0.3243, duration=0.38s
Step 13134: loss=2.9418, lr=0.000327, tokens/sec=1375624.16, grad_norm=0.3138, duration=0.38s
Step 13135: loss=2.9293, lr=0.000327, tokens/sec=1376612.76, grad_norm=0.3173, duration=0.38s
Step 13136: loss=2.8648, lr=0.000327, tokens/sec=1376168.23, grad_norm=0.3196, duration=0.38s
Step 13137: loss=2.9015, lr=0.000327, tokens/sec=1375822.11, grad_norm=0.3034, duration=0.38s
Step 13138: loss=2.9070, lr=0.000327, tokens/sec=1374876.76, grad_norm=0.3122, duration=0.38s
Step 13139: loss=2.8600, lr=0.000327, tokens/sec=1376109.67, grad_norm=0.3114, duration=0.38s
Step 13140: loss=2.8706, lr=0.000327, tokens/sec=1377059.31, grad_norm=0.3163, duration=0.38s
Step 13141: loss=2.9026, lr=0.000327, tokens/sec=1375288.63, grad_norm=0.3262, duration=0.38s
Step 13142: loss=2.9042, lr=0.000327, tokens/sec=1378691.92, grad_norm=0.3255, duration=0.38s
Step 13143: loss=2.9633, lr=0.000327, tokens/sec=1376188.04, grad_norm=0.3158, duration=0.38s
Step 13144: loss=2.8759, lr=0.000327, tokens/sec=1375373.79, grad_norm=0.3213, duration=0.38s
Step 13145: loss=2.8431, lr=0.000327, tokens/sec=1371586.36, grad_norm=0.3252, duration=0.38s
Step 13146: loss=2.8769, lr=0.000327, tokens/sec=1376763.59, grad_norm=0.3306, duration=0.38s
Step 13147: loss=2.8166, lr=0.000327, tokens/sec=1377946.36, grad_norm=0.3041, duration=0.38s
Step 13148: loss=2.8277, lr=0.000327, tokens/sec=1373940.43, grad_norm=0.3088, duration=0.38s
Step 13149: loss=2.7921, lr=0.000327, tokens/sec=1377642.49, grad_norm=0.3209, duration=0.38s
Step 13150: loss=2.8362, lr=0.000327, tokens/sec=1374039.15, grad_norm=0.3078, duration=0.38s
Step 13151: loss=2.8079, lr=0.000327, tokens/sec=1375271.43, grad_norm=0.3006, duration=0.38s
Step 13152: loss=2.8428, lr=0.000327, tokens/sec=1373646.91, grad_norm=0.3221, duration=0.38s
Step 13153: loss=2.8572, lr=0.000327, tokens/sec=1375001.41, grad_norm=0.3052, duration=0.38s
Step 13154: loss=2.7980, lr=0.000327, tokens/sec=1373879.48, grad_norm=0.2807, duration=0.38s
Step 13155: loss=2.8205, lr=0.000327, tokens/sec=1374113.85, grad_norm=0.2949, duration=0.38s
Step 13156: loss=2.8233, lr=0.000327, tokens/sec=1375868.59, grad_norm=0.3256, duration=0.38s
Step 13157: loss=2.9059, lr=0.000327, tokens/sec=1375450.35, grad_norm=0.2966, duration=0.38s
Step 13158: loss=2.9617, lr=0.000327, tokens/sec=1377576.90, grad_norm=0.2979, duration=0.38s
Step 13159: loss=2.9194, lr=0.000327, tokens/sec=1372478.36, grad_norm=0.3483, duration=0.38s
Step 13160: loss=2.9008, lr=0.000327, tokens/sec=1376776.52, grad_norm=0.3270, duration=0.38s
Step 13161: loss=2.9464, lr=0.000327, tokens/sec=1376104.50, grad_norm=0.3280, duration=0.38s
Step 13162: loss=2.8998, lr=0.000326, tokens/sec=1376920.49, grad_norm=0.3360, duration=0.38s
Step 13163: loss=2.9077, lr=0.000326, tokens/sec=1373453.01, grad_norm=0.3209, duration=0.38s
Step 13164: loss=2.8803, lr=0.000326, tokens/sec=1371992.83, grad_norm=0.3117, duration=0.38s
Step 13165: loss=2.8693, lr=0.000326, tokens/sec=1374080.37, grad_norm=0.3445, duration=0.38s
Step 13166: loss=2.8747, lr=0.000326, tokens/sec=1373515.64, grad_norm=0.3117, duration=0.38s
Step 13167: loss=2.8780, lr=0.000326, tokens/sec=1376268.14, grad_norm=0.3295, duration=0.38s
Step 13168: loss=2.8844, lr=0.000326, tokens/sec=1374985.93, grad_norm=0.3509, duration=0.38s
Step 13169: loss=2.8554, lr=0.000326, tokens/sec=1372019.37, grad_norm=0.3160, duration=0.38s
Step 13170: loss=2.8475, lr=0.000326, tokens/sec=1372160.63, grad_norm=0.3016, duration=0.38s
Step 13171: loss=2.7877, lr=0.000326, tokens/sec=1379987.97, grad_norm=0.3343, duration=0.38s
Step 13172: loss=2.8382, lr=0.000326, tokens/sec=1372059.61, grad_norm=0.3474, duration=0.38s
Step 13173: loss=2.9442, lr=0.000326, tokens/sec=1374166.23, grad_norm=0.3404, duration=0.38s
Step 13174: loss=2.9185, lr=0.000326, tokens/sec=1374532.14, grad_norm=0.3143, duration=0.38s
Step 13175: loss=2.9194, lr=0.000326, tokens/sec=1377286.14, grad_norm=0.3328, duration=0.38s
Step 13176: loss=2.8715, lr=0.000326, tokens/sec=1375258.53, grad_norm=0.3485, duration=0.38s
Step 13177: loss=2.9122, lr=0.000326, tokens/sec=1375798.87, grad_norm=0.3339, duration=0.38s
Step 13178: loss=2.9190, lr=0.000326, tokens/sec=1376753.25, grad_norm=0.2970, duration=0.38s
Step 13179: loss=2.9179, lr=0.000326, tokens/sec=1376261.25, grad_norm=0.3227, duration=0.38s
Step 13180: loss=2.9081, lr=0.000326, tokens/sec=1376493.85, grad_norm=0.3523, duration=0.38s
Step 13181: loss=2.9422, lr=0.000326, tokens/sec=1374297.63, grad_norm=0.3264, duration=0.38s
Step 13182: loss=2.9052, lr=0.000326, tokens/sec=1376239.72, grad_norm=0.3029, duration=0.38s
Step 13183: loss=2.8928, lr=0.000326, tokens/sec=1375471.86, grad_norm=0.3456, duration=0.38s
Step 13184: loss=2.9304, lr=0.000326, tokens/sec=1377318.92, grad_norm=0.3458, duration=0.38s
Step 13185: loss=2.8908, lr=0.000326, tokens/sec=1375226.70, grad_norm=0.3089, duration=0.38s
Step 13186: loss=2.8906, lr=0.000326, tokens/sec=1377749.52, grad_norm=0.3213, duration=0.38s
Step 13187: loss=2.8786, lr=0.000326, tokens/sec=1376219.91, grad_norm=0.3674, duration=0.38s
Step 13188: loss=2.8974, lr=0.000326, tokens/sec=1372588.87, grad_norm=0.3389, duration=0.38s
Step 13189: loss=2.8806, lr=0.000326, tokens/sec=1375748.95, grad_norm=0.2955, duration=0.38s
Step 13190: loss=2.8754, lr=0.000326, tokens/sec=1375411.64, grad_norm=0.3247, duration=0.38s
Step 13191: loss=2.8056, lr=0.000326, tokens/sec=1375757.55, grad_norm=0.3402, duration=0.38s
Step 13192: loss=2.8887, lr=0.000326, tokens/sec=1372234.27, grad_norm=0.3316, duration=0.38s
Step 13193: loss=2.8323, lr=0.000326, tokens/sec=1377389.66, grad_norm=0.2973, duration=0.38s
Step 13194: loss=2.8627, lr=0.000326, tokens/sec=1380781.69, grad_norm=0.2987, duration=0.38s
Step 13195: loss=2.7899, lr=0.000326, tokens/sec=1377186.95, grad_norm=0.3125, duration=0.38s
Step 13196: loss=2.8173, lr=0.000326, tokens/sec=1378024.07, grad_norm=0.3037, duration=0.38s
Step 13197: loss=2.8494, lr=0.000326, tokens/sec=1375544.99, grad_norm=0.3061, duration=0.38s
Step 13198: loss=2.7903, lr=0.000326, tokens/sec=1377942.04, grad_norm=0.2935, duration=0.38s
Step 13199: loss=2.8351, lr=0.000326, tokens/sec=1376748.08, grad_norm=0.2980, duration=0.38s
Step 13200/19073 (69.2%), Elapsed time: 5227.75s, Steps per hour: 9089.95, Estimated hours remaining: 0.65
Step 13200: loss=2.8086, lr=0.000325, tokens/sec=1379860.68, grad_norm=0.3126, duration=0.38s
Step 13201: loss=2.8864, lr=0.000325, tokens/sec=1377893.69, grad_norm=0.2906, duration=0.38s
Step 13202: loss=2.8481, lr=0.000325, tokens/sec=1373912.10, grad_norm=0.2878, duration=0.38s
Step 13203: loss=2.8631, lr=0.000325, tokens/sec=1377984.35, grad_norm=0.3344, duration=0.38s
Step 13204: loss=2.9095, lr=0.000325, tokens/sec=1379980.18, grad_norm=0.3100, duration=0.38s
Step 13205: loss=2.9160, lr=0.000325, tokens/sec=1375180.26, grad_norm=0.2829, duration=0.38s
Step 13206: loss=2.9053, lr=0.000325, tokens/sec=1377063.62, grad_norm=0.3173, duration=0.38s
Step 13207: loss=2.9171, lr=0.000325, tokens/sec=1377218.00, grad_norm=0.3494, duration=0.38s
Step 13208: loss=2.8543, lr=0.000325, tokens/sec=1376250.91, grad_norm=0.3117, duration=0.38s
Step 13209: loss=2.9023, lr=0.000325, tokens/sec=1378979.81, grad_norm=0.2954, duration=0.38s
Step 13210: loss=2.8566, lr=0.000325, tokens/sec=1375463.26, grad_norm=0.3241, duration=0.38s
Step 13211: loss=2.9125, lr=0.000325, tokens/sec=1375907.33, grad_norm=0.3174, duration=0.38s
Step 13212: loss=2.9311, lr=0.000325, tokens/sec=1380510.37, grad_norm=0.2933, duration=0.38s
Step 13213: loss=2.9186, lr=0.000325, tokens/sec=1379055.05, grad_norm=0.3115, duration=0.38s
Step 13214: loss=2.8949, lr=0.000325, tokens/sec=1375111.47, grad_norm=0.3201, duration=0.38s
Step 13215: loss=2.8511, lr=0.000325, tokens/sec=1376366.34, grad_norm=0.3067, duration=0.38s
Step 13216: loss=2.8753, lr=0.000325, tokens/sec=1377072.24, grad_norm=0.3035, duration=0.38s
Step 13217: loss=2.8743, lr=0.000325, tokens/sec=1374260.70, grad_norm=0.2960, duration=0.38s
Step 13218: loss=2.8974, lr=0.000325, tokens/sec=1375917.66, grad_norm=0.3042, duration=0.38s
Step 13219: loss=2.8260, lr=0.000325, tokens/sec=1375750.67, grad_norm=0.3073, duration=0.38s
Step 13220: loss=2.8577, lr=0.000325, tokens/sec=1376402.52, grad_norm=0.2958, duration=0.38s
Step 13221: loss=2.8630, lr=0.000325, tokens/sec=1377653.71, grad_norm=0.2903, duration=0.38s
Step 13222: loss=2.8795, lr=0.000325, tokens/sec=1374918.88, grad_norm=0.3124, duration=0.38s
Step 13223: loss=2.8229, lr=0.000325, tokens/sec=1377148.13, grad_norm=0.2964, duration=0.38s
Step 13224: loss=2.8940, lr=0.000325, tokens/sec=1378021.48, grad_norm=0.3004, duration=0.38s
Step 13225: loss=2.8127, lr=0.000325, tokens/sec=1378051.71, grad_norm=0.3141, duration=0.38s
Step 13226: loss=2.8852, lr=0.000325, tokens/sec=1377382.76, grad_norm=0.3203, duration=0.38s
Step 13227: loss=2.8948, lr=0.000325, tokens/sec=1376677.40, grad_norm=0.2861, duration=0.38s
Step 13228: loss=2.9231, lr=0.000325, tokens/sec=1378367.85, grad_norm=0.3120, duration=0.38s
Step 13229: loss=2.8864, lr=0.000325, tokens/sec=1375490.79, grad_norm=0.3101, duration=0.38s
Step 13230: loss=2.8815, lr=0.000325, tokens/sec=1375586.29, grad_norm=0.2955, duration=0.38s
Step 13231: loss=2.8981, lr=0.000325, tokens/sec=1375872.90, grad_norm=0.3153, duration=0.38s
Step 13232: loss=2.8898, lr=0.000325, tokens/sec=1376538.66, grad_norm=0.3207, duration=0.38s
Step 13233: loss=2.8921, lr=0.000325, tokens/sec=1377165.38, grad_norm=0.3328, duration=0.38s
Step 13234: loss=2.8700, lr=0.000325, tokens/sec=1380639.52, grad_norm=0.3209, duration=0.38s
Step 13235: loss=2.8972, lr=0.000325, tokens/sec=1379227.17, grad_norm=0.3344, duration=0.38s
Step 13236: loss=2.8817, lr=0.000325, tokens/sec=1372479.22, grad_norm=0.3088, duration=0.38s
Step 13237: loss=2.9003, lr=0.000324, tokens/sec=1378398.95, grad_norm=0.3234, duration=0.38s
Step 13238: loss=2.9046, lr=0.000324, tokens/sec=1378246.90, grad_norm=0.3414, duration=0.38s
Step 13239: loss=2.8257, lr=0.000324, tokens/sec=1375696.45, grad_norm=0.3186, duration=0.38s
Step 13240: loss=2.8557, lr=0.000324, tokens/sec=1375284.33, grad_norm=0.3266, duration=0.38s
Step 13241: loss=2.8073, lr=0.000324, tokens/sec=1376948.94, grad_norm=0.3394, duration=0.38s
Step 13242: loss=2.7883, lr=0.000324, tokens/sec=1378688.46, grad_norm=0.3158, duration=0.38s
Step 13243: loss=2.8346, lr=0.000324, tokens/sec=1378663.39, grad_norm=0.3060, duration=0.38s
Step 13244: loss=2.8968, lr=0.000324, tokens/sec=1373755.89, grad_norm=0.3251, duration=0.38s
Step 13245: loss=2.8851, lr=0.000324, tokens/sec=1374234.07, grad_norm=0.3279, duration=0.38s
Step 13246: loss=2.8173, lr=0.000324, tokens/sec=1374250.39, grad_norm=0.3158, duration=0.38s
Step 13247: loss=2.8226, lr=0.000324, tokens/sec=1377023.95, grad_norm=0.3156, duration=0.38s
Step 13248: loss=2.7946, lr=0.000324, tokens/sec=1377336.17, grad_norm=0.3310, duration=0.38s
Step 13249: loss=2.8905, lr=0.000324, tokens/sec=1378075.89, grad_norm=0.3200, duration=0.38s
Validation loss at step 13250: 3.817736864089966
Step 13250: loss=2.9013, lr=0.000324, tokens/sec=156311.38, grad_norm=0.3517, duration=3.35s
Step 13251: loss=2.9045, lr=0.000324, tokens/sec=1379785.36, grad_norm=0.3292, duration=0.38s
Step 13252: loss=2.9296, lr=0.000324, tokens/sec=1374880.20, grad_norm=0.3342, duration=0.38s
Step 13253: loss=2.8880, lr=0.000324, tokens/sec=1376166.51, grad_norm=0.3362, duration=0.38s
Step 13254: loss=2.9229, lr=0.000324, tokens/sec=1379735.15, grad_norm=0.3442, duration=0.38s
Step 13255: loss=2.8578, lr=0.000324, tokens/sec=1376718.77, grad_norm=0.3261, duration=0.38s
Step 13256: loss=2.8917, lr=0.000324, tokens/sec=1377091.22, grad_norm=0.3081, duration=0.38s
Step 13257: loss=2.9623, lr=0.000324, tokens/sec=1376480.06, grad_norm=0.3346, duration=0.38s
Step 13258: loss=2.9880, lr=0.000324, tokens/sec=1374768.46, grad_norm=0.3470, duration=0.38s
Step 13259: loss=2.9286, lr=0.000324, tokens/sec=1375114.91, grad_norm=0.3436, duration=0.38s
Step 13260: loss=2.9293, lr=0.000324, tokens/sec=1375820.39, grad_norm=0.3284, duration=0.38s
Step 13261: loss=2.8935, lr=0.000324, tokens/sec=1374070.06, grad_norm=0.3183, duration=0.38s
Step 13262: loss=2.9389, lr=0.000324, tokens/sec=1372187.17, grad_norm=0.3318, duration=0.38s
Step 13263: loss=2.8677, lr=0.000324, tokens/sec=1373526.79, grad_norm=0.3291, duration=0.38s
Step 13264: loss=2.8417, lr=0.000324, tokens/sec=1369208.79, grad_norm=0.3239, duration=0.38s
Step 13265: loss=2.9016, lr=0.000324, tokens/sec=1372425.25, grad_norm=0.3040, duration=0.38s
Step 13266: loss=2.8424, lr=0.000324, tokens/sec=1371134.80, grad_norm=0.3359, duration=0.38s
Step 13267: loss=2.8996, lr=0.000324, tokens/sec=1371075.82, grad_norm=0.3416, duration=0.38s
Step 13268: loss=2.9493, lr=0.000324, tokens/sec=1372406.41, grad_norm=0.3112, duration=0.38s
Step 13269: loss=2.8944, lr=0.000324, tokens/sec=1377383.62, grad_norm=0.3171, duration=0.38s
Step 13270: loss=2.8298, lr=0.000324, tokens/sec=1374576.82, grad_norm=0.3563, duration=0.38s
Step 13271: loss=2.9171, lr=0.000324, tokens/sec=1372210.29, grad_norm=0.3358, duration=0.38s
Step 13272: loss=2.9309, lr=0.000324, tokens/sec=1372756.81, grad_norm=0.3025, duration=0.38s
Step 13273: loss=2.8335, lr=0.000324, tokens/sec=1372483.50, grad_norm=0.3382, duration=0.38s
Step 13274: loss=2.8106, lr=0.000324, tokens/sec=1375287.77, grad_norm=0.3288, duration=0.38s
Step 13275: loss=2.8661, lr=0.000323, tokens/sec=1369485.07, grad_norm=0.3246, duration=0.38s
Step 13276: loss=2.9000, lr=0.000323, tokens/sec=1370069.53, grad_norm=0.3333, duration=0.38s
Step 13277: loss=2.9118, lr=0.000323, tokens/sec=1372011.67, grad_norm=0.3239, duration=0.38s
Step 13278: loss=2.9430, lr=0.000323, tokens/sec=1373004.52, grad_norm=0.3257, duration=0.38s
Step 13279: loss=2.8460, lr=0.000323, tokens/sec=1374186.84, grad_norm=0.3136, duration=0.38s
Step 13280: loss=2.8714, lr=0.000323, tokens/sec=1374600.02, grad_norm=0.3126, duration=0.38s
Step 13281: loss=2.9035, lr=0.000323, tokens/sec=1373519.92, grad_norm=0.3382, duration=0.38s
Step 13282: loss=2.8519, lr=0.000323, tokens/sec=1375308.41, grad_norm=0.3094, duration=0.38s
Step 13283: loss=2.8756, lr=0.000323, tokens/sec=1375102.01, grad_norm=0.2813, duration=0.38s
Step 13284: loss=2.8406, lr=0.000323, tokens/sec=1373519.92, grad_norm=0.2876, duration=0.38s
Step 13285: loss=2.8204, lr=0.000323, tokens/sec=1370300.05, grad_norm=0.3134, duration=0.38s
Step 13286: loss=2.8300, lr=0.000323, tokens/sec=1370627.16, grad_norm=0.3157, duration=0.38s
Step 13287: loss=2.8956, lr=0.000323, tokens/sec=1374227.20, grad_norm=0.2950, duration=0.38s
Step 13288: loss=2.8251, lr=0.000323, tokens/sec=1374565.65, grad_norm=0.2912, duration=0.38s
Step 13289: loss=2.8141, lr=0.000323, tokens/sec=1370513.55, grad_norm=0.3008, duration=0.38s
Step 13290: loss=2.7887, lr=0.000323, tokens/sec=1371672.77, grad_norm=0.2892, duration=0.38s
Step 13291: loss=2.8123, lr=0.000323, tokens/sec=1376107.09, grad_norm=0.3013, duration=0.38s
Step 13292: loss=2.8785, lr=0.000323, tokens/sec=1376006.34, grad_norm=0.2956, duration=0.38s
Step 13293: loss=2.8335, lr=0.000323, tokens/sec=1374660.17, grad_norm=0.2981, duration=0.38s
Step 13294: loss=2.8605, lr=0.000323, tokens/sec=1373265.17, grad_norm=0.2951, duration=0.38s
Step 13295: loss=2.9217, lr=0.000323, tokens/sec=1379237.55, grad_norm=0.2936, duration=0.38s
Step 13296: loss=2.8811, lr=0.000323, tokens/sec=1373393.82, grad_norm=0.3199, duration=0.38s
Step 13297: loss=2.9212, lr=0.000323, tokens/sec=1374788.22, grad_norm=0.3234, duration=0.38s
Step 13298: loss=2.8759, lr=0.000323, tokens/sec=1375372.93, grad_norm=0.2937, duration=0.38s
Step 13299: loss=2.8890, lr=0.000323, tokens/sec=1375013.45, grad_norm=0.3140, duration=0.38s
Step 13300/19073 (69.7%), Elapsed time: 5268.92s, Steps per hour: 9087.25, Estimated hours remaining: 0.64
Step 13300: loss=2.8810, lr=0.000323, tokens/sec=1378765.39, grad_norm=0.3254, duration=0.38s
Step 13301: loss=2.9077, lr=0.000323, tokens/sec=1375421.10, grad_norm=0.3077, duration=0.38s
Step 13302: loss=2.9245, lr=0.000323, tokens/sec=1377813.40, grad_norm=0.3069, duration=0.38s
Step 13303: loss=2.9923, lr=0.000323, tokens/sec=1378279.73, grad_norm=0.3375, duration=0.38s
Step 13304: loss=2.9212, lr=0.000323, tokens/sec=1374255.54, grad_norm=0.3070, duration=0.38s
Step 13305: loss=2.9305, lr=0.000323, tokens/sec=1373702.68, grad_norm=0.2938, duration=0.38s
Step 13306: loss=2.9794, lr=0.000323, tokens/sec=1378169.16, grad_norm=0.3385, duration=0.38s
Step 13307: loss=2.9414, lr=0.000323, tokens/sec=1377006.71, grad_norm=0.3267, duration=0.38s
Step 13308: loss=2.9241, lr=0.000323, tokens/sec=1371200.64, grad_norm=0.2990, duration=0.38s
Step 13309: loss=2.9102, lr=0.000323, tokens/sec=1376642.93, grad_norm=0.2986, duration=0.38s
Step 13310: loss=2.9182, lr=0.000323, tokens/sec=1377520.81, grad_norm=0.3209, duration=0.38s
Step 13311: loss=2.8945, lr=0.000323, tokens/sec=1374482.31, grad_norm=0.3014, duration=0.38s
Step 13312: loss=2.9083, lr=0.000323, tokens/sec=1371358.83, grad_norm=0.3029, duration=0.38s
Step 13313: loss=2.8210, lr=0.000322, tokens/sec=1372830.51, grad_norm=0.2985, duration=0.38s
Step 13314: loss=2.9079, lr=0.000322, tokens/sec=1377718.45, grad_norm=0.3041, duration=0.38s
Step 13315: loss=2.8849, lr=0.000322, tokens/sec=1374520.11, grad_norm=0.3338, duration=0.38s
Step 13316: loss=2.8362, lr=0.000322, tokens/sec=1378046.53, grad_norm=0.3239, duration=0.38s
Step 13317: loss=2.9108, lr=0.000322, tokens/sec=1374191.99, grad_norm=0.3250, duration=0.38s
Step 13318: loss=2.8314, lr=0.000322, tokens/sec=1376849.79, grad_norm=0.2999, duration=0.38s
Step 13319: loss=2.7326, lr=0.000322, tokens/sec=1374172.24, grad_norm=0.3325, duration=0.38s
Step 13320: loss=2.8364, lr=0.000322, tokens/sec=1377294.77, grad_norm=0.3267, duration=0.38s
Step 13321: loss=2.8763, lr=0.000322, tokens/sec=1376618.80, grad_norm=0.3091, duration=0.38s
Step 13322: loss=2.9876, lr=0.000322, tokens/sec=1376499.02, grad_norm=0.3013, duration=0.38s
Step 13323: loss=2.8294, lr=0.000322, tokens/sec=1377161.93, grad_norm=0.3195, duration=0.38s
Step 13324: loss=2.9373, lr=0.000322, tokens/sec=1374908.56, grad_norm=0.3187, duration=0.38s
Step 13325: loss=2.9171, lr=0.000322, tokens/sec=1374718.61, grad_norm=0.3064, duration=0.38s
Step 13326: loss=2.8618, lr=0.000322, tokens/sec=1373281.47, grad_norm=0.3146, duration=0.38s
Step 13327: loss=2.8805, lr=0.000322, tokens/sec=1375854.82, grad_norm=0.3088, duration=0.38s
Step 13328: loss=2.8775, lr=0.000322, tokens/sec=1373414.41, grad_norm=0.2977, duration=0.38s
Step 13329: loss=2.9014, lr=0.000322, tokens/sec=1378221.85, grad_norm=0.3053, duration=0.38s
Step 13330: loss=2.8646, lr=0.000322, tokens/sec=1377502.69, grad_norm=0.3133, duration=0.38s
Step 13331: loss=2.8882, lr=0.000322, tokens/sec=1376605.01, grad_norm=0.2967, duration=0.38s
Step 13332: loss=2.9242, lr=0.000322, tokens/sec=1376736.87, grad_norm=0.3023, duration=0.38s
Step 13333: loss=2.9274, lr=0.000322, tokens/sec=1376072.64, grad_norm=0.3126, duration=0.38s
Step 13334: loss=2.8893, lr=0.000322, tokens/sec=1374812.29, grad_norm=0.3121, duration=0.38s
Step 13335: loss=2.8268, lr=0.000322, tokens/sec=1375850.52, grad_norm=0.3010, duration=0.38s
Step 13336: loss=2.8616, lr=0.000322, tokens/sec=1373946.44, grad_norm=0.3178, duration=0.38s
Step 13337: loss=2.8317, lr=0.000322, tokens/sec=1377037.75, grad_norm=0.3032, duration=0.38s
Step 13338: loss=2.7698, lr=0.000322, tokens/sec=1374437.64, grad_norm=0.2987, duration=0.38s
Step 13339: loss=2.8172, lr=0.000322, tokens/sec=1373798.80, grad_norm=0.3146, duration=0.38s
Step 13340: loss=2.7982, lr=0.000322, tokens/sec=1376866.17, grad_norm=0.3072, duration=0.38s
Step 13341: loss=2.8376, lr=0.000322, tokens/sec=1372658.27, grad_norm=0.3036, duration=0.38s
Step 13342: loss=2.8342, lr=0.000322, tokens/sec=1375792.84, grad_norm=0.2910, duration=0.38s
Step 13343: loss=2.8193, lr=0.000322, tokens/sec=1376015.81, grad_norm=0.3186, duration=0.38s
Step 13344: loss=2.8216, lr=0.000322, tokens/sec=1375174.24, grad_norm=0.3025, duration=0.38s
Step 13345: loss=2.8039, lr=0.000322, tokens/sec=1378215.81, grad_norm=0.2806, duration=0.38s
Step 13346: loss=2.8367, lr=0.000322, tokens/sec=1378123.39, grad_norm=0.3016, duration=0.38s
Step 13347: loss=2.9165, lr=0.000322, tokens/sec=1375872.90, grad_norm=0.3319, duration=0.38s
Step 13348: loss=2.9579, lr=0.000322, tokens/sec=1371979.99, grad_norm=0.3063, duration=0.38s
Step 13349: loss=2.9013, lr=0.000322, tokens/sec=1377436.25, grad_norm=0.3053, duration=0.38s
Step 13350: loss=2.9212, lr=0.000322, tokens/sec=1377001.53, grad_norm=0.3266, duration=0.38s
Step 13351: loss=2.9019, lr=0.000321, tokens/sec=1376160.48, grad_norm=0.3115, duration=0.38s
Step 13352: loss=2.9022, lr=0.000321, tokens/sec=1377389.66, grad_norm=0.3076, duration=0.38s
Step 13353: loss=2.9124, lr=0.000321, tokens/sec=1377463.86, grad_norm=0.3108, duration=0.38s
Step 13354: loss=2.8746, lr=0.000321, tokens/sec=1374242.66, grad_norm=0.3062, duration=0.38s
Step 13355: loss=2.8459, lr=0.000321, tokens/sec=1374532.14, grad_norm=0.3172, duration=0.38s
Step 13356: loss=2.8902, lr=0.000321, tokens/sec=1375359.16, grad_norm=0.3156, duration=0.38s
Step 13357: loss=2.8847, lr=0.000321, tokens/sec=1377182.63, grad_norm=0.3153, duration=0.38s
Step 13358: loss=2.8523, lr=0.000321, tokens/sec=1375526.06, grad_norm=0.3193, duration=0.38s
Step 13359: loss=2.8432, lr=0.000321, tokens/sec=1376490.40, grad_norm=0.3217, duration=0.38s
Step 13360: loss=2.8576, lr=0.000321, tokens/sec=1379673.68, grad_norm=0.2924, duration=0.38s
Step 13361: loss=2.7755, lr=0.000321, tokens/sec=1376229.38, grad_norm=0.3027, duration=0.38s
Step 13362: loss=2.8996, lr=0.000321, tokens/sec=1376991.19, grad_norm=0.3477, duration=0.38s
Step 13363: loss=2.9237, lr=0.000321, tokens/sec=1377144.68, grad_norm=0.3260, duration=0.38s
Step 13364: loss=2.8731, lr=0.000321, tokens/sec=1374881.05, grad_norm=0.2901, duration=0.38s
Step 13365: loss=2.9210, lr=0.000321, tokens/sec=1370939.05, grad_norm=0.3003, duration=0.38s
Step 13366: loss=2.8290, lr=0.000321, tokens/sec=1376634.31, grad_norm=0.3275, duration=0.38s
Step 13367: loss=2.9479, lr=0.000321, tokens/sec=1372214.57, grad_norm=0.3212, duration=0.38s
Step 13368: loss=2.9275, lr=0.000321, tokens/sec=1371788.28, grad_norm=0.2865, duration=0.38s
Step 13369: loss=2.8992, lr=0.000321, tokens/sec=1377515.63, grad_norm=0.3053, duration=0.38s
Step 13370: loss=2.9318, lr=0.000321, tokens/sec=1375411.64, grad_norm=0.3365, duration=0.38s
Step 13371: loss=2.9221, lr=0.000321, tokens/sec=1377582.94, grad_norm=0.3031, duration=0.38s
Step 13372: loss=2.8658, lr=0.000321, tokens/sec=1375901.31, grad_norm=0.2941, duration=0.38s
Step 13373: loss=2.9245, lr=0.000321, tokens/sec=1372636.85, grad_norm=0.3250, duration=0.38s
Step 13374: loss=2.8927, lr=0.000321, tokens/sec=1375246.49, grad_norm=0.3102, duration=0.38s
Step 13375: loss=2.8782, lr=0.000321, tokens/sec=1376915.31, grad_norm=0.2911, duration=0.38s
Step 13376: loss=2.9111, lr=0.000321, tokens/sec=1378574.37, grad_norm=0.3023, duration=0.38s
Step 13377: loss=2.8549, lr=0.000321, tokens/sec=1374584.55, grad_norm=0.3238, duration=0.38s
Step 13378: loss=2.8883, lr=0.000321, tokens/sec=1374591.43, grad_norm=0.3120, duration=0.38s
Step 13379: loss=2.8586, lr=0.000321, tokens/sec=1376770.49, grad_norm=0.3064, duration=0.38s
Step 13380: loss=2.8670, lr=0.000321, tokens/sec=1377530.30, grad_norm=0.2856, duration=0.38s
Step 13381: loss=2.8198, lr=0.000321, tokens/sec=1373460.73, grad_norm=0.3050, duration=0.38s
Step 13382: loss=2.8807, lr=0.000321, tokens/sec=1376884.28, grad_norm=0.3111, duration=0.38s
Step 13383: loss=2.8485, lr=0.000321, tokens/sec=1373362.09, grad_norm=0.2886, duration=0.38s
Step 13384: loss=2.8313, lr=0.000321, tokens/sec=1373763.61, grad_norm=0.2747, duration=0.38s
Step 13385: loss=2.7812, lr=0.000321, tokens/sec=1377174.01, grad_norm=0.2916, duration=0.38s
Step 13386: loss=2.8245, lr=0.000321, tokens/sec=1375836.74, grad_norm=0.3065, duration=0.38s
Step 13387: loss=2.8329, lr=0.000321, tokens/sec=1371867.01, grad_norm=0.2980, duration=0.38s
Step 13388: loss=2.8065, lr=0.000321, tokens/sec=1373437.57, grad_norm=0.2825, duration=0.38s
Step 13389: loss=2.8242, lr=0.000321, tokens/sec=1375730.01, grad_norm=0.2935, duration=0.38s
Step 13390: loss=2.8277, lr=0.000320, tokens/sec=1376992.05, grad_norm=0.3107, duration=0.38s
Step 13391: loss=2.8598, lr=0.000320, tokens/sec=1373861.46, grad_norm=0.2943, duration=0.38s
Step 13392: loss=2.8395, lr=0.000320, tokens/sec=1376447.32, grad_norm=0.2818, duration=0.38s
Step 13393: loss=2.8588, lr=0.000320, tokens/sec=1375208.64, grad_norm=0.3080, duration=0.38s
Step 13394: loss=2.9101, lr=0.000320, tokens/sec=1373133.12, grad_norm=0.3211, duration=0.38s
Step 13395: loss=2.8960, lr=0.000320, tokens/sec=1378176.07, grad_norm=0.3131, duration=0.38s
Step 13396: loss=2.9088, lr=0.000320, tokens/sec=1375666.33, grad_norm=0.3162, duration=0.38s
Step 13397: loss=2.9156, lr=0.000320, tokens/sec=1374475.44, grad_norm=0.3155, duration=0.38s
Step 13398: loss=2.8509, lr=0.000320, tokens/sec=1374235.79, grad_norm=0.3390, duration=0.38s
Step 13399: loss=2.8882, lr=0.000320, tokens/sec=1371131.38, grad_norm=0.3211, duration=0.38s
Step 13400/19073 (70.3%), Elapsed time: 5307.12s, Steps per hour: 9089.67, Estimated hours remaining: 0.62
Step 13400: loss=2.8463, lr=0.000320, tokens/sec=1373259.17, grad_norm=0.3172, duration=0.38s
Step 13401: loss=2.9356, lr=0.000320, tokens/sec=1375299.81, grad_norm=0.3246, duration=0.38s
Step 13402: loss=2.9284, lr=0.000320, tokens/sec=1374048.60, grad_norm=0.3260, duration=0.38s
Step 13403: loss=2.8874, lr=0.000320, tokens/sec=1377068.79, grad_norm=0.3129, duration=0.38s
Step 13404: loss=2.8811, lr=0.000320, tokens/sec=1375909.05, grad_norm=0.3226, duration=0.38s
Step 13405: loss=2.8592, lr=0.000320, tokens/sec=1374141.33, grad_norm=0.3277, duration=0.38s
Step 13406: loss=2.8648, lr=0.000320, tokens/sec=1373953.30, grad_norm=0.3234, duration=0.38s
Step 13407: loss=2.8875, lr=0.000320, tokens/sec=1377221.45, grad_norm=0.3114, duration=0.38s
Step 13408: loss=2.8767, lr=0.000320, tokens/sec=1377914.41, grad_norm=0.2982, duration=0.38s
Step 13409: loss=2.8061, lr=0.000320, tokens/sec=1376822.21, grad_norm=0.3179, duration=0.38s
Step 13410: loss=2.8703, lr=0.000320, tokens/sec=1378158.80, grad_norm=0.3087, duration=0.38s
Step 13411: loss=2.8585, lr=0.000320, tokens/sec=1375650.83, grad_norm=0.2935, duration=0.38s
Step 13412: loss=2.8811, lr=0.000320, tokens/sec=1374792.52, grad_norm=0.3180, duration=0.38s
Step 13413: loss=2.8218, lr=0.000320, tokens/sec=1375322.18, grad_norm=0.3194, duration=0.38s
Step 13414: loss=2.8630, lr=0.000320, tokens/sec=1375941.77, grad_norm=0.3054, duration=0.38s
Step 13415: loss=2.8249, lr=0.000320, tokens/sec=1375089.11, grad_norm=0.3004, duration=0.38s
Step 13416: loss=2.9229, lr=0.000320, tokens/sec=1378141.53, grad_norm=0.3120, duration=0.38s
Step 13417: loss=2.8973, lr=0.000320, tokens/sec=1378177.80, grad_norm=0.3020, duration=0.38s
Step 13418: loss=2.9025, lr=0.000320, tokens/sec=1375495.09, grad_norm=0.3119, duration=0.38s
Step 13419: loss=2.8681, lr=0.000320, tokens/sec=1374051.17, grad_norm=0.3083, duration=0.38s
Step 13420: loss=2.8947, lr=0.000320, tokens/sec=1376948.94, grad_norm=0.2877, duration=0.38s
Step 13421: loss=2.8800, lr=0.000320, tokens/sec=1376288.81, grad_norm=0.3008, duration=0.38s
Step 13422: loss=2.9077, lr=0.000320, tokens/sec=1373618.59, grad_norm=0.3344, duration=0.38s
Step 13423: loss=2.8484, lr=0.000320, tokens/sec=1377555.33, grad_norm=0.3066, duration=0.38s
Step 13424: loss=2.8920, lr=0.000320, tokens/sec=1375616.41, grad_norm=0.2831, duration=0.38s
Step 13425: loss=2.8897, lr=0.000320, tokens/sec=1370992.05, grad_norm=0.3331, duration=0.38s
Step 13426: loss=2.9059, lr=0.000320, tokens/sec=1373333.78, grad_norm=0.3284, duration=0.38s
Step 13427: loss=2.9052, lr=0.000320, tokens/sec=1377162.80, grad_norm=0.3171, duration=0.38s
Step 13428: loss=2.8789, lr=0.000319, tokens/sec=1376704.98, grad_norm=0.3148, duration=0.38s
Step 13429: loss=2.8189, lr=0.000319, tokens/sec=1378474.13, grad_norm=0.3205, duration=0.38s
Step 13430: loss=2.8793, lr=0.000319, tokens/sec=1373703.54, grad_norm=0.3057, duration=0.38s
Step 13431: loss=2.7371, lr=0.000319, tokens/sec=1375873.76, grad_norm=0.3172, duration=0.38s
Step 13432: loss=2.8276, lr=0.000319, tokens/sec=1377866.06, grad_norm=0.3267, duration=0.38s
Step 13433: loss=2.8317, lr=0.000319, tokens/sec=1375605.23, grad_norm=0.3136, duration=0.38s
Step 13434: loss=2.9118, lr=0.000319, tokens/sec=1371903.82, grad_norm=0.3041, duration=0.38s
Step 13435: loss=2.8357, lr=0.000319, tokens/sec=1372883.65, grad_norm=0.3256, duration=0.38s
Step 13436: loss=2.8325, lr=0.000319, tokens/sec=1376106.23, grad_norm=0.3189, duration=0.38s
Step 13437: loss=2.7603, lr=0.000319, tokens/sec=1377808.22, grad_norm=0.3317, duration=0.38s
Step 13438: loss=2.8524, lr=0.000319, tokens/sec=1372663.41, grad_norm=0.3245, duration=0.38s
Step 13439: loss=2.8818, lr=0.000319, tokens/sec=1376248.33, grad_norm=0.3210, duration=0.38s
Step 13440: loss=2.8781, lr=0.000319, tokens/sec=1376576.57, grad_norm=0.3297, duration=0.38s
Step 13441: loss=2.9165, lr=0.000319, tokens/sec=1376966.18, grad_norm=0.3373, duration=0.38s
Step 13442: loss=2.9057, lr=0.000319, tokens/sec=1376380.12, grad_norm=0.3411, duration=0.38s
Step 13443: loss=2.8865, lr=0.000319, tokens/sec=1374056.33, grad_norm=0.3281, duration=0.38s
Step 13444: loss=2.9088, lr=0.000319, tokens/sec=1371533.32, grad_norm=0.3575, duration=0.38s
Step 13445: loss=2.8639, lr=0.000319, tokens/sec=1372128.10, grad_norm=0.3278, duration=0.38s
Step 13446: loss=2.9155, lr=0.000319, tokens/sec=1378665.98, grad_norm=0.3217, duration=0.38s
Step 13447: loss=2.9199, lr=0.000319, tokens/sec=1377012.74, grad_norm=0.3329, duration=0.38s
Step 13448: loss=2.9774, lr=0.000319, tokens/sec=1374120.72, grad_norm=0.3445, duration=0.38s
Step 13449: loss=2.9338, lr=0.000319, tokens/sec=1375179.40, grad_norm=0.3312, duration=0.38s
Step 13450: loss=2.9378, lr=0.000319, tokens/sec=1375622.44, grad_norm=0.3579, duration=0.38s
Step 13451: loss=2.9018, lr=0.000319, tokens/sec=1377036.89, grad_norm=0.3424, duration=0.38s
Step 13452: loss=2.9015, lr=0.000319, tokens/sec=1379313.68, grad_norm=0.3206, duration=0.38s
Step 13453: loss=2.8720, lr=0.000319, tokens/sec=1372115.25, grad_norm=0.3395, duration=0.38s
Step 13454: loss=2.8092, lr=0.000319, tokens/sec=1374684.23, grad_norm=0.3269, duration=0.38s
Step 13455: loss=2.9223, lr=0.000319, tokens/sec=1376362.03, grad_norm=0.3181, duration=0.38s
Step 13456: loss=2.8096, lr=0.000319, tokens/sec=1372635.99, grad_norm=0.3311, duration=0.38s
Step 13457: loss=2.9233, lr=0.000319, tokens/sec=1373039.67, grad_norm=0.3568, duration=0.38s
Step 13458: loss=2.9370, lr=0.000319, tokens/sec=1373526.79, grad_norm=0.3224, duration=0.38s
Step 13459: loss=2.8698, lr=0.000319, tokens/sec=1375437.45, grad_norm=0.3118, duration=0.38s
Step 13460: loss=2.8626, lr=0.000319, tokens/sec=1372189.74, grad_norm=0.3400, duration=0.38s
Step 13461: loss=2.8988, lr=0.000319, tokens/sec=1374875.04, grad_norm=0.3383, duration=0.38s
Step 13462: loss=2.8601, lr=0.000319, tokens/sec=1374849.25, grad_norm=0.3245, duration=0.38s
Step 13463: loss=2.8786, lr=0.000319, tokens/sec=1373458.16, grad_norm=0.3114, duration=0.38s
Step 13464: loss=2.8059, lr=0.000319, tokens/sec=1376746.35, grad_norm=0.3180, duration=0.38s
Step 13465: loss=2.8908, lr=0.000319, tokens/sec=1374070.92, grad_norm=0.3405, duration=0.38s
Step 13466: loss=2.8868, lr=0.000319, tokens/sec=1375558.76, grad_norm=0.3395, duration=0.38s
Step 13467: loss=2.9102, lr=0.000318, tokens/sec=1376208.71, grad_norm=0.3265, duration=0.38s
Step 13468: loss=2.9176, lr=0.000318, tokens/sec=1376164.79, grad_norm=0.3184, duration=0.38s
Step 13469: loss=2.8525, lr=0.000318, tokens/sec=1374942.95, grad_norm=0.3345, duration=0.38s
Step 13470: loss=2.8976, lr=0.000318, tokens/sec=1379457.32, grad_norm=0.3207, duration=0.38s
Step 13471: loss=2.8894, lr=0.000318, tokens/sec=1379331.85, grad_norm=0.3304, duration=0.38s
Step 13472: loss=2.8346, lr=0.000318, tokens/sec=1377749.52, grad_norm=0.3276, duration=0.38s
Step 13473: loss=2.8698, lr=0.000318, tokens/sec=1377544.97, grad_norm=0.3085, duration=0.38s
Step 13474: loss=2.8287, lr=0.000318, tokens/sec=1375051.28, grad_norm=0.2817, duration=0.38s
Step 13475: loss=2.8236, lr=0.000318, tokens/sec=1378686.73, grad_norm=0.3069, duration=0.38s
Step 13476: loss=2.8325, lr=0.000318, tokens/sec=1376416.31, grad_norm=0.3165, duration=0.38s
Step 13477: loss=2.8696, lr=0.000318, tokens/sec=1376711.88, grad_norm=0.3059, duration=0.38s
Step 13478: loss=2.8307, lr=0.000318, tokens/sec=1376446.46, grad_norm=0.3000, duration=0.38s
Step 13479: loss=2.8122, lr=0.000318, tokens/sec=1375400.45, grad_norm=0.3071, duration=0.38s
Step 13480: loss=2.7799, lr=0.000318, tokens/sec=1374289.04, grad_norm=0.2953, duration=0.38s
Step 13481: loss=2.8386, lr=0.000318, tokens/sec=1378245.18, grad_norm=0.3049, duration=0.38s
Step 13482: loss=2.8473, lr=0.000318, tokens/sec=1375982.23, grad_norm=0.3071, duration=0.38s
Step 13483: loss=2.8365, lr=0.000318, tokens/sec=1377438.84, grad_norm=0.3150, duration=0.38s
Step 13484: loss=2.8738, lr=0.000318, tokens/sec=1374878.48, grad_norm=0.3086, duration=0.38s
Step 13485: loss=2.8771, lr=0.000318, tokens/sec=1371985.99, grad_norm=0.2972, duration=0.38s
Step 13486: loss=2.9092, lr=0.000318, tokens/sec=1377443.15, grad_norm=0.3211, duration=0.38s
Step 13487: loss=2.9022, lr=0.000318, tokens/sec=1375867.73, grad_norm=0.3390, duration=0.38s
Step 13488: loss=2.8582, lr=0.000318, tokens/sec=1372343.03, grad_norm=0.3055, duration=0.38s
Step 13489: loss=2.9011, lr=0.000318, tokens/sec=1373385.25, grad_norm=0.3055, duration=0.38s
Step 13490: loss=2.8653, lr=0.000318, tokens/sec=1372404.69, grad_norm=0.3285, duration=0.38s
Step 13491: loss=2.9155, lr=0.000318, tokens/sec=1373830.56, grad_norm=0.3178, duration=0.38s
Step 13492: loss=2.9187, lr=0.000318, tokens/sec=1372830.51, grad_norm=0.3069, duration=0.38s
Step 13493: loss=2.9794, lr=0.000318, tokens/sec=1376781.69, grad_norm=0.3155, duration=0.38s
Step 13494: loss=2.9039, lr=0.000318, tokens/sec=1371274.17, grad_norm=0.3273, duration=0.38s
Step 13495: loss=2.9332, lr=0.000318, tokens/sec=1376464.56, grad_norm=0.3059, duration=0.38s
Step 13496: loss=2.9983, lr=0.000318, tokens/sec=1377307.71, grad_norm=0.3011, duration=0.38s
Step 13497: loss=2.9051, lr=0.000318, tokens/sec=1373228.30, grad_norm=0.3216, duration=0.38s
Step 13498: loss=2.9248, lr=0.000318, tokens/sec=1376912.73, grad_norm=0.3213, duration=0.38s
Step 13499: loss=2.8989, lr=0.000318, tokens/sec=1375048.70, grad_norm=0.2915, duration=0.38s
Step 13500/19073 (70.8%), Elapsed time: 5345.33s, Steps per hour: 9092.05, Estimated hours remaining: 0.61
Validation loss at step 13500: 3.803896903991699
Step 13500: loss=2.9218, lr=0.000318, tokens/sec=156433.01, grad_norm=0.3092, duration=3.35s
Step 13501: loss=2.8942, lr=0.000318, tokens/sec=1378133.75, grad_norm=0.3083, duration=0.38s
Step 13502: loss=2.8756, lr=0.000318, tokens/sec=1377377.58, grad_norm=0.3014, duration=0.38s
Step 13503: loss=2.8297, lr=0.000318, tokens/sec=1375316.15, grad_norm=0.2945, duration=0.38s
Step 13504: loss=2.8971, lr=0.000318, tokens/sec=1379150.19, grad_norm=0.3123, duration=0.38s
Step 13505: loss=2.8757, lr=0.000318, tokens/sec=1378406.73, grad_norm=0.3157, duration=0.38s
Step 13506: loss=2.8404, lr=0.000317, tokens/sec=1375592.32, grad_norm=0.3091, duration=0.38s
Step 13507: loss=2.9120, lr=0.000317, tokens/sec=1379646.85, grad_norm=0.3235, duration=0.38s
Step 13508: loss=2.7593, lr=0.000317, tokens/sec=1377307.71, grad_norm=0.3190, duration=0.38s
Step 13509: loss=2.7894, lr=0.000317, tokens/sec=1372327.61, grad_norm=0.3460, duration=0.38s
Step 13510: loss=2.8323, lr=0.000317, tokens/sec=1372750.81, grad_norm=0.3331, duration=0.38s
Step 13511: loss=2.9093, lr=0.000317, tokens/sec=1371444.36, grad_norm=0.3142, duration=0.38s
Step 13512: loss=2.9755, lr=0.000317, tokens/sec=1373874.33, grad_norm=0.2990, duration=0.38s
Step 13513: loss=2.8239, lr=0.000317, tokens/sec=1372875.08, grad_norm=0.3187, duration=0.38s
Step 13514: loss=2.9248, lr=0.000317, tokens/sec=1375393.57, grad_norm=0.3315, duration=0.38s
Step 13515: loss=2.9159, lr=0.000317, tokens/sec=1377249.91, grad_norm=0.3092, duration=0.38s
Step 13516: loss=2.8409, lr=0.000317, tokens/sec=1375271.43, grad_norm=0.3116, duration=0.38s
Step 13517: loss=2.8527, lr=0.000317, tokens/sec=1374255.54, grad_norm=0.3348, duration=0.38s
Step 13518: loss=2.9201, lr=0.000317, tokens/sec=1374704.86, grad_norm=0.3406, duration=0.38s
Step 13519: loss=2.8960, lr=0.000317, tokens/sec=1372110.12, grad_norm=0.2991, duration=0.38s
Step 13520: loss=2.8545, lr=0.000317, tokens/sec=1372857.94, grad_norm=0.3191, duration=0.38s
Step 13521: loss=2.9132, lr=0.000317, tokens/sec=1374412.73, grad_norm=0.3357, duration=0.38s
Step 13522: loss=2.8865, lr=0.000317, tokens/sec=1372492.92, grad_norm=0.3290, duration=0.38s
Step 13523: loss=2.9399, lr=0.000317, tokens/sec=1376636.03, grad_norm=0.2987, duration=0.38s
Step 13524: loss=2.8768, lr=0.000317, tokens/sec=1373575.69, grad_norm=0.3222, duration=0.38s
Step 13525: loss=2.8135, lr=0.000317, tokens/sec=1374875.04, grad_norm=0.3318, duration=0.38s
Step 13526: loss=2.8758, lr=0.000317, tokens/sec=1377350.84, grad_norm=0.3288, duration=0.38s
Step 13527: loss=2.7714, lr=0.000317, tokens/sec=1373829.70, grad_norm=0.3128, duration=0.38s
Step 13528: loss=2.7971, lr=0.000317, tokens/sec=1374740.95, grad_norm=0.3047, duration=0.38s
Step 13529: loss=2.7797, lr=0.000317, tokens/sec=1372408.98, grad_norm=0.3143, duration=0.38s
Step 13530: loss=2.8297, lr=0.000317, tokens/sec=1372829.66, grad_norm=0.3168, duration=0.38s
Step 13531: loss=2.8311, lr=0.000317, tokens/sec=1374343.15, grad_norm=0.3274, duration=0.38s
Step 13532: loss=2.7982, lr=0.000317, tokens/sec=1372705.40, grad_norm=0.3046, duration=0.38s
Step 13533: loss=2.8421, lr=0.000317, tokens/sec=1375415.08, grad_norm=0.3107, duration=0.38s
Step 13534: loss=2.8059, lr=0.000317, tokens/sec=1376243.16, grad_norm=0.3030, duration=0.38s
Step 13535: loss=2.8204, lr=0.000317, tokens/sec=1378614.13, grad_norm=0.3107, duration=0.38s
Step 13536: loss=2.8444, lr=0.000317, tokens/sec=1377561.37, grad_norm=0.3019, duration=0.38s
Step 13537: loss=2.9113, lr=0.000317, tokens/sec=1376623.11, grad_norm=0.3093, duration=0.38s
Step 13538: loss=2.9396, lr=0.000317, tokens/sec=1376615.35, grad_norm=0.3189, duration=0.38s
Step 13539: loss=2.9214, lr=0.000317, tokens/sec=1376258.67, grad_norm=0.3284, duration=0.38s
Step 13540: loss=2.8719, lr=0.000317, tokens/sec=1374179.11, grad_norm=0.3139, duration=0.38s
Step 13541: loss=2.9016, lr=0.000317, tokens/sec=1373659.78, grad_norm=0.3028, duration=0.38s
Step 13542: loss=2.9099, lr=0.000317, tokens/sec=1374807.13, grad_norm=0.3232, duration=0.38s
Step 13543: loss=2.9055, lr=0.000317, tokens/sec=1376330.16, grad_norm=0.3257, duration=0.38s
Step 13544: loss=2.8471, lr=0.000317, tokens/sec=1376157.04, grad_norm=0.3050, duration=0.38s
Step 13545: loss=2.8576, lr=0.000316, tokens/sec=1375419.38, grad_norm=0.3046, duration=0.38s
Step 13546: loss=2.8966, lr=0.000316, tokens/sec=1376648.96, grad_norm=0.3203, duration=0.38s
Step 13547: loss=2.8537, lr=0.000316, tokens/sec=1377760.74, grad_norm=0.3276, duration=0.38s
Step 13548: loss=2.8400, lr=0.000316, tokens/sec=1373559.39, grad_norm=0.2988, duration=0.38s
Step 13549: loss=2.8535, lr=0.000316, tokens/sec=1377373.27, grad_norm=0.3135, duration=0.38s
Step 13550: loss=2.8492, lr=0.000316, tokens/sec=1376776.52, grad_norm=0.3244, duration=0.38s
Step 13551: loss=2.8355, lr=0.000316, tokens/sec=1379876.27, grad_norm=0.3134, duration=0.38s
Step 13552: loss=2.8819, lr=0.000316, tokens/sec=1374102.69, grad_norm=0.3280, duration=0.38s
Step 13553: loss=2.8814, lr=0.000316, tokens/sec=1375287.77, grad_norm=0.3323, duration=0.38s
Step 13554: loss=2.8782, lr=0.000316, tokens/sec=1376703.26, grad_norm=0.3140, duration=0.38s
Step 13555: loss=2.8804, lr=0.000316, tokens/sec=1377407.78, grad_norm=0.3010, duration=0.38s
Step 13556: loss=2.8694, lr=0.000316, tokens/sec=1371363.96, grad_norm=0.3190, duration=0.38s
Step 13557: loss=2.9566, lr=0.000316, tokens/sec=1375192.30, grad_norm=0.3404, duration=0.38s
Step 13558: loss=2.9097, lr=0.000316, tokens/sec=1378925.34, grad_norm=0.3046, duration=0.38s
Step 13559: loss=2.9267, lr=0.000316, tokens/sec=1376762.73, grad_norm=0.3032, duration=0.38s
Step 13560: loss=2.9149, lr=0.000316, tokens/sec=1376198.38, grad_norm=0.3406, duration=0.38s
Step 13561: loss=2.8855, lr=0.000316, tokens/sec=1379478.95, grad_norm=0.3195, duration=0.38s
Step 13562: loss=2.8966, lr=0.000316, tokens/sec=1373322.63, grad_norm=0.2998, duration=0.38s
Step 13563: loss=2.8859, lr=0.000316, tokens/sec=1378408.46, grad_norm=0.3193, duration=0.38s
Step 13564: loss=2.8836, lr=0.000316, tokens/sec=1369776.81, grad_norm=0.3397, duration=0.38s
Step 13565: loss=2.9012, lr=0.000316, tokens/sec=1373490.76, grad_norm=0.3027, duration=0.38s
Step 13566: loss=2.8948, lr=0.000316, tokens/sec=1378636.60, grad_norm=0.2936, duration=0.38s
Step 13567: loss=2.8480, lr=0.000316, tokens/sec=1375531.23, grad_norm=0.3228, duration=0.38s
Step 13568: loss=2.8643, lr=0.000316, tokens/sec=1375918.52, grad_norm=0.3298, duration=0.38s
Step 13569: loss=2.8511, lr=0.000316, tokens/sec=1377875.56, grad_norm=0.2997, duration=0.38s
Step 13570: loss=2.8861, lr=0.000316, tokens/sec=1377462.14, grad_norm=0.2852, duration=0.38s
Step 13571: loss=2.8117, lr=0.000316, tokens/sec=1374494.34, grad_norm=0.2972, duration=0.38s
Step 13572: loss=2.8975, lr=0.000316, tokens/sec=1374975.62, grad_norm=0.3104, duration=0.38s
Step 13573: loss=2.8163, lr=0.000316, tokens/sec=1376950.66, grad_norm=0.3036, duration=0.38s
Step 13574: loss=2.8223, lr=0.000316, tokens/sec=1373770.48, grad_norm=0.2760, duration=0.38s
Step 13575: loss=2.7858, lr=0.000316, tokens/sec=1375740.34, grad_norm=0.2653, duration=0.38s
Step 13576: loss=2.8111, lr=0.000316, tokens/sec=1381848.06, grad_norm=0.2946, duration=0.38s
Step 13577: loss=2.8501, lr=0.000316, tokens/sec=1375103.73, grad_norm=0.2971, duration=0.38s
Step 13578: loss=2.7965, lr=0.000316, tokens/sec=1374633.53, grad_norm=0.3011, duration=0.38s
Step 13579: loss=2.8436, lr=0.000316, tokens/sec=1373565.40, grad_norm=0.2815, duration=0.38s
Step 13580: loss=2.8024, lr=0.000316, tokens/sec=1371082.65, grad_norm=0.2981, duration=0.38s
Step 13581: loss=2.8507, lr=0.000316, tokens/sec=1373816.82, grad_norm=0.2941, duration=0.38s
Step 13582: loss=2.8324, lr=0.000316, tokens/sec=1376441.29, grad_norm=0.2937, duration=0.38s
Step 13583: loss=2.8620, lr=0.000316, tokens/sec=1380368.25, grad_norm=0.3098, duration=0.38s
Step 13584: loss=2.8872, lr=0.000315, tokens/sec=1374234.93, grad_norm=0.3001, duration=0.38s
Step 13585: loss=2.8994, lr=0.000315, tokens/sec=1375118.35, grad_norm=0.3044, duration=0.38s
Step 13586: loss=2.9106, lr=0.000315, tokens/sec=1377400.88, grad_norm=0.3133, duration=0.38s
Step 13587: loss=2.9103, lr=0.000315, tokens/sec=1375217.24, grad_norm=0.3056, duration=0.38s
Step 13588: loss=2.8345, lr=0.000315, tokens/sec=1374789.94, grad_norm=0.3094, duration=0.38s
Step 13589: loss=2.8767, lr=0.000315, tokens/sec=1376854.10, grad_norm=0.3033, duration=0.38s
Step 13590: loss=2.8698, lr=0.000315, tokens/sec=1373247.16, grad_norm=0.3215, duration=0.38s
Step 13591: loss=2.9310, lr=0.000315, tokens/sec=1373048.24, grad_norm=0.3192, duration=0.38s
Step 13592: loss=2.8964, lr=0.000315, tokens/sec=1376507.64, grad_norm=0.3007, duration=0.38s
Step 13593: loss=2.8762, lr=0.000315, tokens/sec=1375022.90, grad_norm=0.3114, duration=0.38s
Step 13594: loss=2.8874, lr=0.000315, tokens/sec=1372566.59, grad_norm=0.3297, duration=0.38s
Step 13595: loss=2.8468, lr=0.000315, tokens/sec=1374726.34, grad_norm=0.3155, duration=0.38s
Step 13596: loss=2.8769, lr=0.000315, tokens/sec=1377613.15, grad_norm=0.2993, duration=0.38s
Step 13597: loss=2.8663, lr=0.000315, tokens/sec=1377595.89, grad_norm=0.3044, duration=0.38s
Step 13598: loss=2.8537, lr=0.000315, tokens/sec=1378272.82, grad_norm=0.3092, duration=0.38s
Step 13599: loss=2.8201, lr=0.000315, tokens/sec=1374849.25, grad_norm=0.3122, duration=0.38s
Step 13600/19073 (71.3%), Elapsed time: 5386.51s, Steps per hour: 9089.37, Estimated hours remaining: 0.60
Step 13600: loss=2.8687, lr=0.000315, tokens/sec=1378214.94, grad_norm=0.3054, duration=0.38s
Step 13601: loss=2.8598, lr=0.000315, tokens/sec=1376038.20, grad_norm=0.3001, duration=0.38s
Step 13602: loss=2.8764, lr=0.000315, tokens/sec=1375749.81, grad_norm=0.3073, duration=0.38s
Step 13603: loss=2.7871, lr=0.000315, tokens/sec=1377752.11, grad_norm=0.3092, duration=0.38s
Step 13604: loss=2.8758, lr=0.000315, tokens/sec=1373852.02, grad_norm=0.3077, duration=0.38s
Step 13605: loss=2.8657, lr=0.000315, tokens/sec=1376422.34, grad_norm=0.3033, duration=0.38s
Step 13606: loss=2.9244, lr=0.000315, tokens/sec=1378489.68, grad_norm=0.3121, duration=0.38s
Step 13607: loss=2.8782, lr=0.000315, tokens/sec=1372995.09, grad_norm=0.3078, duration=0.38s
Step 13608: loss=2.8837, lr=0.000315, tokens/sec=1374826.04, grad_norm=0.3051, duration=0.38s
Step 13609: loss=2.8812, lr=0.000315, tokens/sec=1379942.94, grad_norm=0.3030, duration=0.38s
Step 13610: loss=2.8761, lr=0.000315, tokens/sec=1376558.47, grad_norm=0.2947, duration=0.38s
Step 13611: loss=2.8969, lr=0.000315, tokens/sec=1368728.98, grad_norm=0.2955, duration=0.38s
Step 13612: loss=2.8616, lr=0.000315, tokens/sec=1375244.77, grad_norm=0.3263, duration=0.38s
Step 13613: loss=2.8686, lr=0.000315, tokens/sec=1379811.33, grad_norm=0.3207, duration=0.38s
Step 13614: loss=2.8816, lr=0.000315, tokens/sec=1374094.96, grad_norm=0.2798, duration=0.38s
Step 13615: loss=2.9122, lr=0.000315, tokens/sec=1373808.24, grad_norm=0.3023, duration=0.38s
Step 13616: loss=2.9091, lr=0.000315, tokens/sec=1376112.26, grad_norm=0.3244, duration=0.38s
Step 13617: loss=2.8784, lr=0.000315, tokens/sec=1375702.47, grad_norm=0.3035, duration=0.38s
Step 13618: loss=2.8698, lr=0.000315, tokens/sec=1375261.97, grad_norm=0.2959, duration=0.38s
Step 13619: loss=2.8436, lr=0.000315, tokens/sec=1374412.73, grad_norm=0.3215, duration=0.38s
Step 13620: loss=2.8093, lr=0.000315, tokens/sec=1373337.22, grad_norm=0.3128, duration=0.38s
Step 13621: loss=2.7744, lr=0.000315, tokens/sec=1377031.71, grad_norm=0.2976, duration=0.38s
Step 13622: loss=2.8250, lr=0.000315, tokens/sec=1373853.73, grad_norm=0.3120, duration=0.38s
Step 13623: loss=2.8451, lr=0.000315, tokens/sec=1377281.83, grad_norm=0.3319, duration=0.38s
Step 13624: loss=2.8630, lr=0.000314, tokens/sec=1376174.26, grad_norm=0.3041, duration=0.38s
Step 13625: loss=2.8510, lr=0.000314, tokens/sec=1375271.43, grad_norm=0.2981, duration=0.38s
Step 13626: loss=2.7699, lr=0.000314, tokens/sec=1374667.90, grad_norm=0.3257, duration=0.38s
Step 13627: loss=2.8199, lr=0.000314, tokens/sec=1374476.30, grad_norm=0.3263, duration=0.38s
Step 13628: loss=2.8486, lr=0.000314, tokens/sec=1378593.38, grad_norm=0.3244, duration=0.38s
Step 13629: loss=2.8588, lr=0.000314, tokens/sec=1377872.11, grad_norm=0.3006, duration=0.38s
Step 13630: loss=2.8893, lr=0.000314, tokens/sec=1375122.65, grad_norm=0.3154, duration=0.38s
Step 13631: loss=2.8910, lr=0.000314, tokens/sec=1377526.85, grad_norm=0.3269, duration=0.38s
Step 13632: loss=2.9025, lr=0.000314, tokens/sec=1376533.49, grad_norm=0.3371, duration=0.38s
Step 13633: loss=2.8743, lr=0.000314, tokens/sec=1374530.42, grad_norm=0.3134, duration=0.38s
Step 13634: loss=2.9131, lr=0.000314, tokens/sec=1376019.26, grad_norm=0.3641, duration=0.38s
Step 13635: loss=2.8883, lr=0.000314, tokens/sec=1377844.48, grad_norm=0.3461, duration=0.38s
Step 13636: loss=2.8742, lr=0.000314, tokens/sec=1375300.67, grad_norm=0.3025, duration=0.38s
Step 13637: loss=2.9128, lr=0.000314, tokens/sec=1374706.58, grad_norm=0.3328, duration=0.38s
Step 13638: loss=2.9842, lr=0.000314, tokens/sec=1375029.78, grad_norm=0.3518, duration=0.38s
Step 13639: loss=2.9438, lr=0.000314, tokens/sec=1375144.15, grad_norm=0.3202, duration=0.38s
Step 13640: loss=2.9474, lr=0.000314, tokens/sec=1374996.25, grad_norm=0.3178, duration=0.38s
Step 13641: loss=2.8680, lr=0.000314, tokens/sec=1370431.56, grad_norm=0.3201, duration=0.38s
Step 13642: loss=2.9085, lr=0.000314, tokens/sec=1372509.20, grad_norm=0.3423, duration=0.38s
Step 13643: loss=2.8377, lr=0.000314, tokens/sec=1377105.88, grad_norm=0.3130, duration=0.38s
Step 13644: loss=2.8258, lr=0.000314, tokens/sec=1375816.94, grad_norm=0.3101, duration=0.38s
Step 13645: loss=2.8913, lr=0.000314, tokens/sec=1377061.90, grad_norm=0.3116, duration=0.38s
Step 13646: loss=2.8321, lr=0.000314, tokens/sec=1370783.52, grad_norm=0.3211, duration=0.38s
Step 13647: loss=2.9122, lr=0.000314, tokens/sec=1378945.23, grad_norm=0.3390, duration=0.38s
Step 13648: loss=2.9148, lr=0.000314, tokens/sec=1379973.25, grad_norm=0.3401, duration=0.38s
Step 13649: loss=2.9014, lr=0.000314, tokens/sec=1373429.85, grad_norm=0.3185, duration=0.38s
Step 13650: loss=2.8439, lr=0.000314, tokens/sec=1376064.89, grad_norm=0.3255, duration=0.38s
Step 13651: loss=2.8289, lr=0.000314, tokens/sec=1376667.06, grad_norm=0.3438, duration=0.38s
Step 13652: loss=2.9081, lr=0.000314, tokens/sec=1372827.09, grad_norm=0.3250, duration=0.38s
Step 13653: loss=2.8758, lr=0.000314, tokens/sec=1376606.73, grad_norm=0.3146, duration=0.38s
Step 13654: loss=2.8311, lr=0.000314, tokens/sec=1373793.65, grad_norm=0.3092, duration=0.38s
Step 13655: loss=2.8768, lr=0.000314, tokens/sec=1375254.23, grad_norm=0.3270, duration=0.38s
Step 13656: loss=2.8884, lr=0.000314, tokens/sec=1378597.70, grad_norm=0.3425, duration=0.38s
Step 13657: loss=2.8836, lr=0.000314, tokens/sec=1376070.92, grad_norm=0.3374, duration=0.38s
Step 13658: loss=2.9256, lr=0.000314, tokens/sec=1376610.18, grad_norm=0.3324, duration=0.38s
Step 13659: loss=2.8826, lr=0.000314, tokens/sec=1375231.86, grad_norm=0.3279, duration=0.38s
Step 13660: loss=2.8853, lr=0.000314, tokens/sec=1378536.34, grad_norm=0.3166, duration=0.38s
Step 13661: loss=2.8716, lr=0.000314, tokens/sec=1377263.71, grad_norm=0.3391, duration=0.38s
Step 13662: loss=2.8312, lr=0.000314, tokens/sec=1374947.25, grad_norm=0.3528, duration=0.38s
Step 13663: loss=2.8626, lr=0.000313, tokens/sec=1377639.04, grad_norm=0.3257, duration=0.38s
Step 13664: loss=2.8323, lr=0.000313, tokens/sec=1375551.02, grad_norm=0.2956, duration=0.38s
Step 13665: loss=2.8255, lr=0.000313, tokens/sec=1376301.73, grad_norm=0.3024, duration=0.38s
Step 13666: loss=2.8061, lr=0.000313, tokens/sec=1376344.80, grad_norm=0.3038, duration=0.38s
Step 13667: loss=2.8741, lr=0.000313, tokens/sec=1373857.17, grad_norm=0.3095, duration=0.38s
Step 13668: loss=2.8265, lr=0.000313, tokens/sec=1378100.07, grad_norm=0.3073, duration=0.38s
Step 13669: loss=2.8036, lr=0.000313, tokens/sec=1376778.24, grad_norm=0.3122, duration=0.38s
Step 13670: loss=2.8061, lr=0.000313, tokens/sec=1372408.12, grad_norm=0.2892, duration=0.38s
Step 13671: loss=2.8082, lr=0.000313, tokens/sec=1374708.30, grad_norm=0.2964, duration=0.38s
Step 13672: loss=2.8522, lr=0.000313, tokens/sec=1371631.70, grad_norm=0.3039, duration=0.38s
Step 13673: loss=2.8508, lr=0.000313, tokens/sec=1376926.52, grad_norm=0.3011, duration=0.38s
Step 13674: loss=2.8291, lr=0.000313, tokens/sec=1376654.13, grad_norm=0.3008, duration=0.38s
Step 13675: loss=2.9070, lr=0.000313, tokens/sec=1370640.83, grad_norm=0.2955, duration=0.38s
Step 13676: loss=2.8915, lr=0.000313, tokens/sec=1373546.52, grad_norm=0.2929, duration=0.38s
Step 13677: loss=2.8830, lr=0.000313, tokens/sec=1378731.68, grad_norm=0.3024, duration=0.38s
Step 13678: loss=2.8717, lr=0.000313, tokens/sec=1372836.51, grad_norm=0.3158, duration=0.38s
Step 13679: loss=2.8880, lr=0.000313, tokens/sec=1372209.44, grad_norm=0.3009, duration=0.38s
Step 13680: loss=2.8749, lr=0.000313, tokens/sec=1378167.44, grad_norm=0.2907, duration=0.38s
Step 13681: loss=2.9080, lr=0.000313, tokens/sec=1377670.97, grad_norm=0.3102, duration=0.38s
Step 13682: loss=2.9081, lr=0.000313, tokens/sec=1374930.05, grad_norm=0.3156, duration=0.38s
Step 13683: loss=2.9647, lr=0.000313, tokens/sec=1374626.66, grad_norm=0.3165, duration=0.38s
Step 13684: loss=2.9087, lr=0.000313, tokens/sec=1374642.98, grad_norm=0.3124, duration=0.38s
Step 13685: loss=2.9545, lr=0.000313, tokens/sec=1375866.01, grad_norm=0.3172, duration=0.38s
Step 13686: loss=2.9638, lr=0.000313, tokens/sec=1374227.20, grad_norm=0.3103, duration=0.38s
Step 13687: loss=2.9057, lr=0.000313, tokens/sec=1376836.86, grad_norm=0.3031, duration=0.38s
Step 13688: loss=2.9145, lr=0.000313, tokens/sec=1378584.74, grad_norm=0.3165, duration=0.38s
Step 13689: loss=2.9023, lr=0.000313, tokens/sec=1374101.83, grad_norm=0.3069, duration=0.38s
Step 13690: loss=2.9206, lr=0.000313, tokens/sec=1376106.23, grad_norm=0.3067, duration=0.38s
Step 13691: loss=2.8607, lr=0.000313, tokens/sec=1377468.18, grad_norm=0.2909, duration=0.38s
Step 13692: loss=2.8852, lr=0.000313, tokens/sec=1375207.78, grad_norm=0.3081, duration=0.38s
Step 13693: loss=2.8229, lr=0.000313, tokens/sec=1376701.53, grad_norm=0.2972, duration=0.38s
Step 13694: loss=2.8879, lr=0.000313, tokens/sec=1376218.18, grad_norm=0.2949, duration=0.38s
Step 13695: loss=2.8789, lr=0.000313, tokens/sec=1375167.36, grad_norm=0.3175, duration=0.38s
Step 13696: loss=2.8410, lr=0.000313, tokens/sec=1376850.66, grad_norm=0.3049, duration=0.38s
Step 13697: loss=2.8397, lr=0.000313, tokens/sec=1375409.06, grad_norm=0.3164, duration=0.38s
Step 13698: loss=2.8145, lr=0.000313, tokens/sec=1373660.64, grad_norm=0.3090, duration=0.38s
Step 13699: loss=2.7803, lr=0.000313, tokens/sec=1374203.16, grad_norm=0.3521, duration=0.38s
Step 13700/19073 (71.8%), Elapsed time: 5424.71s, Steps per hour: 9091.72, Estimated hours remaining: 0.59
Step 13700: loss=2.8645, lr=0.000313, tokens/sec=1373288.33, grad_norm=0.3296, duration=0.38s
Step 13701: loss=2.8980, lr=0.000313, tokens/sec=1375211.22, grad_norm=0.3202, duration=0.38s
Step 13702: loss=2.9728, lr=0.000313, tokens/sec=1378718.71, grad_norm=0.3058, duration=0.38s
Step 13703: loss=2.8149, lr=0.000312, tokens/sec=1378233.95, grad_norm=0.3124, duration=0.38s
Step 13704: loss=2.9244, lr=0.000312, tokens/sec=1374712.59, grad_norm=0.3481, duration=0.38s
Step 13705: loss=2.8942, lr=0.000312, tokens/sec=1376045.09, grad_norm=0.3230, duration=0.38s
Step 13706: loss=2.8089, lr=0.000312, tokens/sec=1378840.60, grad_norm=0.2973, duration=0.38s
Step 13707: loss=2.8953, lr=0.000312, tokens/sec=1372109.26, grad_norm=0.3344, duration=0.38s
Step 13708: loss=2.9128, lr=0.000312, tokens/sec=1376372.37, grad_norm=0.3419, duration=0.38s
Step 13709: loss=2.8855, lr=0.000312, tokens/sec=1376077.81, grad_norm=0.2979, duration=0.38s
Step 13710: loss=2.8770, lr=0.000312, tokens/sec=1379174.41, grad_norm=0.3115, duration=0.38s
Step 13711: loss=2.8753, lr=0.000312, tokens/sec=1374916.30, grad_norm=0.3165, duration=0.38s
Step 13712: loss=2.9041, lr=0.000312, tokens/sec=1373923.26, grad_norm=0.3474, duration=0.38s
Step 13713: loss=2.9246, lr=0.000312, tokens/sec=1377855.70, grad_norm=0.2896, duration=0.38s
Step 13714: loss=2.8622, lr=0.000312, tokens/sec=1374253.83, grad_norm=0.3070, duration=0.38s
Step 13715: loss=2.8282, lr=0.000312, tokens/sec=1373055.95, grad_norm=0.3148, duration=0.38s
Step 13716: loss=2.8172, lr=0.000312, tokens/sec=1372244.54, grad_norm=0.3177, duration=0.38s
Step 13717: loss=2.7989, lr=0.000312, tokens/sec=1375009.15, grad_norm=0.2903, duration=0.38s
Step 13718: loss=2.7602, lr=0.000312, tokens/sec=1374207.45, grad_norm=0.3016, duration=0.38s
Step 13719: loss=2.8104, lr=0.000312, tokens/sec=1376314.65, grad_norm=0.3150, duration=0.38s
Step 13720: loss=2.8226, lr=0.000312, tokens/sec=1376779.11, grad_norm=0.3234, duration=0.38s
Step 13721: loss=2.7941, lr=0.000312, tokens/sec=1374828.62, grad_norm=0.3137, duration=0.38s
Step 13722: loss=2.8232, lr=0.000312, tokens/sec=1374973.04, grad_norm=0.3124, duration=0.38s
Step 13723: loss=2.8274, lr=0.000312, tokens/sec=1372261.67, grad_norm=0.3205, duration=0.38s
Step 13724: loss=2.8200, lr=0.000312, tokens/sec=1372635.99, grad_norm=0.3252, duration=0.38s
Step 13725: loss=2.8269, lr=0.000312, tokens/sec=1373238.59, grad_norm=0.3125, duration=0.38s
Step 13726: loss=2.8405, lr=0.000312, tokens/sec=1372161.49, grad_norm=0.2967, duration=0.38s
Step 13727: loss=2.8947, lr=0.000312, tokens/sec=1376651.55, grad_norm=0.3399, duration=0.38s
Step 13728: loss=2.9592, lr=0.000312, tokens/sec=1375767.02, grad_norm=0.3280, duration=0.38s
Step 13729: loss=2.8722, lr=0.000312, tokens/sec=1373604.86, grad_norm=0.2935, duration=0.38s
Step 13730: loss=2.8757, lr=0.000312, tokens/sec=1377583.81, grad_norm=0.3482, duration=0.38s
Step 13731: loss=2.9093, lr=0.000312, tokens/sec=1376210.43, grad_norm=0.3340, duration=0.38s
Step 13732: loss=2.9003, lr=0.000312, tokens/sec=1376101.92, grad_norm=0.2950, duration=0.38s
Step 13733: loss=2.8796, lr=0.000312, tokens/sec=1372337.89, grad_norm=0.3138, duration=0.38s
Step 13734: loss=2.8618, lr=0.000312, tokens/sec=1369387.85, grad_norm=0.3301, duration=0.38s
Step 13735: loss=2.8645, lr=0.000312, tokens/sec=1376681.71, grad_norm=0.3226, duration=0.38s
Step 13736: loss=2.8643, lr=0.000312, tokens/sec=1377989.53, grad_norm=0.2857, duration=0.38s
Step 13737: loss=2.8407, lr=0.000312, tokens/sec=1372687.40, grad_norm=0.3032, duration=0.38s
Step 13738: loss=2.8494, lr=0.000312, tokens/sec=1373699.25, grad_norm=0.3408, duration=0.38s
Step 13739: loss=2.8437, lr=0.000312, tokens/sec=1378455.98, grad_norm=0.3191, duration=0.38s
Step 13740: loss=2.9081, lr=0.000312, tokens/sec=1374389.53, grad_norm=0.2942, duration=0.38s
Step 13741: loss=2.8197, lr=0.000312, tokens/sec=1375184.56, grad_norm=0.3161, duration=0.38s
Step 13742: loss=2.8377, lr=0.000312, tokens/sec=1374423.90, grad_norm=0.3239, duration=0.38s
Step 13743: loss=2.8850, lr=0.000311, tokens/sec=1374045.16, grad_norm=0.3197, duration=0.38s
Step 13744: loss=2.8404, lr=0.000311, tokens/sec=1372038.20, grad_norm=0.3106, duration=0.38s
Step 13745: loss=2.9207, lr=0.000311, tokens/sec=1374901.69, grad_norm=0.3141, duration=0.38s
Step 13746: loss=2.8771, lr=0.000311, tokens/sec=1377826.35, grad_norm=0.3053, duration=0.38s
Step 13747: loss=2.9404, lr=0.000311, tokens/sec=1378127.71, grad_norm=0.3335, duration=0.38s
Step 13748: loss=2.9370, lr=0.000311, tokens/sec=1375031.50, grad_norm=0.3049, duration=0.38s
Step 13749: loss=2.9090, lr=0.000311, tokens/sec=1373787.64, grad_norm=0.3017, duration=0.38s
Validation loss at step 13750: 3.801547050476074
Step 13750: loss=2.8760, lr=0.000311, tokens/sec=156465.91, grad_norm=0.3231, duration=3.35s
Step 13751: loss=2.9176, lr=0.000311, tokens/sec=1380437.58, grad_norm=0.3427, duration=0.38s
Step 13752: loss=2.8600, lr=0.000311, tokens/sec=1376449.05, grad_norm=0.3014, duration=0.38s
Step 13753: loss=2.8762, lr=0.000311, tokens/sec=1378182.12, grad_norm=0.3137, duration=0.38s
Step 13754: loss=2.9042, lr=0.000311, tokens/sec=1379686.67, grad_norm=0.3374, duration=0.38s
Step 13755: loss=2.8843, lr=0.000311, tokens/sec=1373860.60, grad_norm=0.3263, duration=0.38s
Step 13756: loss=2.8866, lr=0.000311, tokens/sec=1375225.84, grad_norm=0.3128, duration=0.38s
Step 13757: loss=2.8228, lr=0.000311, tokens/sec=1375446.05, grad_norm=0.3207, duration=0.38s
Step 13758: loss=2.8584, lr=0.000311, tokens/sec=1375664.60, grad_norm=0.3226, duration=0.38s
Step 13759: loss=2.8689, lr=0.000311, tokens/sec=1377342.21, grad_norm=0.3221, duration=0.38s
Step 13760: loss=2.8813, lr=0.000311, tokens/sec=1375707.63, grad_norm=0.3101, duration=0.38s
Step 13761: loss=2.8297, lr=0.000311, tokens/sec=1376423.20, grad_norm=0.2947, duration=0.38s
Step 13762: loss=2.8665, lr=0.000311, tokens/sec=1379073.21, grad_norm=0.3024, duration=0.38s
Step 13763: loss=2.8114, lr=0.000311, tokens/sec=1375831.58, grad_norm=0.3154, duration=0.38s
Step 13764: loss=2.8304, lr=0.000311, tokens/sec=1377918.73, grad_norm=0.3149, duration=0.38s
Step 13765: loss=2.7726, lr=0.000311, tokens/sec=1373660.64, grad_norm=0.2878, duration=0.38s
Step 13766: loss=2.8261, lr=0.000311, tokens/sec=1375405.62, grad_norm=0.2913, duration=0.38s
Step 13767: loss=2.8398, lr=0.000311, tokens/sec=1375940.05, grad_norm=0.3115, duration=0.38s
Step 13768: loss=2.8179, lr=0.000311, tokens/sec=1378101.80, grad_norm=0.3103, duration=0.38s
Step 13769: loss=2.8188, lr=0.000311, tokens/sec=1376289.67, grad_norm=0.2976, duration=0.38s
Step 13770: loss=2.7923, lr=0.000311, tokens/sec=1374661.03, grad_norm=0.2915, duration=0.38s
Step 13771: loss=2.8476, lr=0.000311, tokens/sec=1374305.36, grad_norm=0.2926, duration=0.38s
Step 13772: loss=2.8347, lr=0.000311, tokens/sec=1377687.37, grad_norm=0.3003, duration=0.38s
Step 13773: loss=2.8385, lr=0.000311, tokens/sec=1377910.96, grad_norm=0.3140, duration=0.38s
Step 13774: loss=2.8910, lr=0.000311, tokens/sec=1376174.26, grad_norm=0.3133, duration=0.38s
Step 13775: loss=2.8991, lr=0.000311, tokens/sec=1375742.06, grad_norm=0.3006, duration=0.38s
Step 13776: loss=2.9074, lr=0.000311, tokens/sec=1375638.79, grad_norm=0.3225, duration=0.38s
Step 13777: loss=2.8955, lr=0.000311, tokens/sec=1373309.77, grad_norm=0.3391, duration=0.38s
Step 13778: loss=2.8242, lr=0.000311, tokens/sec=1376832.55, grad_norm=0.3391, duration=0.38s
Step 13779: loss=2.9009, lr=0.000311, tokens/sec=1377381.90, grad_norm=0.3088, duration=0.38s
Step 13780: loss=2.8640, lr=0.000311, tokens/sec=1378906.31, grad_norm=0.3351, duration=0.38s
Step 13781: loss=2.9002, lr=0.000311, tokens/sec=1375083.09, grad_norm=0.3372, duration=0.38s
Step 13782: loss=2.8842, lr=0.000311, tokens/sec=1377687.37, grad_norm=0.3289, duration=0.38s
Step 13783: loss=2.8826, lr=0.000311, tokens/sec=1375419.38, grad_norm=0.3041, duration=0.38s
Step 13784: loss=2.8764, lr=0.000310, tokens/sec=1375940.05, grad_norm=0.3091, duration=0.38s
Step 13785: loss=2.8592, lr=0.000310, tokens/sec=1376103.64, grad_norm=0.3176, duration=0.38s
Step 13786: loss=2.8552, lr=0.000310, tokens/sec=1379038.62, grad_norm=0.3315, duration=0.38s
Step 13787: loss=2.8443, lr=0.000310, tokens/sec=1377267.16, grad_norm=0.3082, duration=0.38s
Step 13788: loss=2.8638, lr=0.000310, tokens/sec=1374332.84, grad_norm=0.2925, duration=0.38s
Step 13789: loss=2.8124, lr=0.000310, tokens/sec=1375391.85, grad_norm=0.3052, duration=0.38s
Step 13790: loss=2.8689, lr=0.000310, tokens/sec=1377349.11, grad_norm=0.3161, duration=0.38s
Step 13791: loss=2.8582, lr=0.000310, tokens/sec=1372340.46, grad_norm=0.3094, duration=0.38s
Step 13792: loss=2.8421, lr=0.000310, tokens/sec=1375637.93, grad_norm=0.3049, duration=0.38s
Step 13793: loss=2.8021, lr=0.000310, tokens/sec=1376870.48, grad_norm=0.3104, duration=0.38s
Step 13794: loss=2.9139, lr=0.000310, tokens/sec=1378530.30, grad_norm=0.3222, duration=0.38s
Step 13795: loss=2.8660, lr=0.000310, tokens/sec=1374904.26, grad_norm=0.3121, duration=0.38s
Step 13796: loss=2.9039, lr=0.000310, tokens/sec=1375450.35, grad_norm=0.3144, duration=0.38s
Step 13797: loss=2.8599, lr=0.000310, tokens/sec=1377606.25, grad_norm=0.3096, duration=0.38s
Step 13798: loss=2.8941, lr=0.000310, tokens/sec=1376111.39, grad_norm=0.3098, duration=0.38s
Step 13799: loss=2.8633, lr=0.000310, tokens/sec=1374696.26, grad_norm=0.3052, duration=0.38s
Step 13800/19073 (72.4%), Elapsed time: 5465.88s, Steps per hour: 9089.12, Estimated hours remaining: 0.58
Step 13800: loss=2.8937, lr=0.000310, tokens/sec=1375972.76, grad_norm=0.2957, duration=0.38s
Step 13801: loss=2.8508, lr=0.000310, tokens/sec=1373766.19, grad_norm=0.2872, duration=0.38s
Step 13802: loss=2.8830, lr=0.000310, tokens/sec=1377244.74, grad_norm=0.3149, duration=0.38s
Step 13803: loss=2.8585, lr=0.000310, tokens/sec=1378285.78, grad_norm=0.3265, duration=0.38s
Step 13804: loss=2.9050, lr=0.000310, tokens/sec=1376676.54, grad_norm=0.2902, duration=0.38s
Step 13805: loss=2.9155, lr=0.000310, tokens/sec=1374452.24, grad_norm=0.2870, duration=0.38s
Step 13806: loss=2.8828, lr=0.000310, tokens/sec=1376507.64, grad_norm=0.3157, duration=0.38s
Step 13807: loss=2.8706, lr=0.000310, tokens/sec=1375081.37, grad_norm=0.3147, duration=0.38s
Step 13808: loss=2.8947, lr=0.000310, tokens/sec=1375167.36, grad_norm=0.2862, duration=0.38s
Step 13809: loss=2.7735, lr=0.000310, tokens/sec=1377368.09, grad_norm=0.2924, duration=0.38s
Step 13810: loss=2.8522, lr=0.000310, tokens/sec=1376337.91, grad_norm=0.3197, duration=0.38s
Step 13811: loss=2.7734, lr=0.000310, tokens/sec=1375036.66, grad_norm=0.3087, duration=0.38s
Step 13812: loss=2.8370, lr=0.000310, tokens/sec=1376822.21, grad_norm=0.3009, duration=0.38s
Step 13813: loss=2.7948, lr=0.000310, tokens/sec=1375872.90, grad_norm=0.3084, duration=0.38s
Step 13814: loss=2.8790, lr=0.000310, tokens/sec=1378430.06, grad_norm=0.3148, duration=0.38s
Step 13815: loss=2.7883, lr=0.000310, tokens/sec=1372432.10, grad_norm=0.3358, duration=0.38s
Step 13816: loss=2.8284, lr=0.000310, tokens/sec=1375215.52, grad_norm=0.3049, duration=0.38s
Step 13817: loss=2.8130, lr=0.000310, tokens/sec=1372213.72, grad_norm=0.3052, duration=0.38s
Step 13818: loss=2.8243, lr=0.000310, tokens/sec=1376698.95, grad_norm=0.3326, duration=0.38s
Step 13819: loss=2.8740, lr=0.000310, tokens/sec=1373188.00, grad_norm=0.3144, duration=0.38s
Step 13820: loss=2.8665, lr=0.000310, tokens/sec=1373420.41, grad_norm=0.3155, duration=0.38s
Step 13821: loss=2.8918, lr=0.000310, tokens/sec=1374040.01, grad_norm=0.3078, duration=0.38s
Step 13822: loss=2.8900, lr=0.000310, tokens/sec=1375538.11, grad_norm=0.3297, duration=0.38s
Step 13823: loss=2.8814, lr=0.000310, tokens/sec=1376632.59, grad_norm=0.3107, duration=0.38s
Step 13824: loss=2.9361, lr=0.000309, tokens/sec=1378619.31, grad_norm=0.3319, duration=0.38s
Step 13825: loss=2.8474, lr=0.000309, tokens/sec=1376055.42, grad_norm=0.3263, duration=0.38s
Step 13826: loss=2.8676, lr=0.000309, tokens/sec=1372131.52, grad_norm=0.3081, duration=0.38s
Step 13827: loss=2.9219, lr=0.000309, tokens/sec=1375640.51, grad_norm=0.3114, duration=0.38s
Step 13828: loss=2.9941, lr=0.000309, tokens/sec=1373388.68, grad_norm=0.3355, duration=0.38s
Step 13829: loss=2.9537, lr=0.000309, tokens/sec=1374347.45, grad_norm=0.3502, duration=0.38s
Step 13830: loss=2.9122, lr=0.000309, tokens/sec=1371730.95, grad_norm=0.3136, duration=0.38s
Step 13831: loss=2.8745, lr=0.000309, tokens/sec=1375021.19, grad_norm=0.3169, duration=0.38s
Step 13832: loss=2.8736, lr=0.000309, tokens/sec=1373615.16, grad_norm=0.3458, duration=0.38s
Step 13833: loss=2.8569, lr=0.000309, tokens/sec=1377642.49, grad_norm=0.3440, duration=0.38s
Step 13834: loss=2.7972, lr=0.000309, tokens/sec=1372277.94, grad_norm=0.3133, duration=0.38s
Step 13835: loss=2.9149, lr=0.000309, tokens/sec=1377505.28, grad_norm=0.3056, duration=0.38s
Step 13836: loss=2.8240, lr=0.000309, tokens/sec=1371812.24, grad_norm=0.3225, duration=0.38s
Step 13837: loss=2.8876, lr=0.000309, tokens/sec=1375915.94, grad_norm=0.3434, duration=0.38s
Step 13838: loss=2.9462, lr=0.000309, tokens/sec=1376904.11, grad_norm=0.3295, duration=0.38s
Step 13839: loss=2.8872, lr=0.000309, tokens/sec=1376225.94, grad_norm=0.3160, duration=0.38s
Step 13840: loss=2.7765, lr=0.000309, tokens/sec=1376742.04, grad_norm=0.3318, duration=0.38s
Step 13841: loss=2.8770, lr=0.000309, tokens/sec=1372566.59, grad_norm=0.3318, duration=0.38s
Step 13842: loss=2.9041, lr=0.000309, tokens/sec=1379510.10, grad_norm=0.3168, duration=0.38s
Step 13843: loss=2.9002, lr=0.000309, tokens/sec=1373841.72, grad_norm=0.3150, duration=0.38s
Step 13844: loss=2.8183, lr=0.000309, tokens/sec=1372221.42, grad_norm=0.3141, duration=0.38s
Step 13845: loss=2.8792, lr=0.000309, tokens/sec=1375396.15, grad_norm=0.3182, duration=0.38s
Step 13846: loss=2.8638, lr=0.000309, tokens/sec=1374051.17, grad_norm=0.3242, duration=0.38s
Step 13847: loss=2.8942, lr=0.000309, tokens/sec=1374338.86, grad_norm=0.3176, duration=0.38s
Step 13848: loss=2.9569, lr=0.000309, tokens/sec=1368815.04, grad_norm=0.3260, duration=0.38s
Step 13849: loss=2.8681, lr=0.000309, tokens/sec=1375897.86, grad_norm=0.3151, duration=0.38s
Step 13850: loss=2.8635, lr=0.000309, tokens/sec=1375214.66, grad_norm=0.3040, duration=0.38s
Step 13851: loss=2.8664, lr=0.000309, tokens/sec=1376949.80, grad_norm=0.3076, duration=0.38s
Step 13852: loss=2.8184, lr=0.000309, tokens/sec=1374932.63, grad_norm=0.3194, duration=0.38s
Step 13853: loss=2.8647, lr=0.000309, tokens/sec=1374564.79, grad_norm=0.3176, duration=0.38s
Step 13854: loss=2.8356, lr=0.000309, tokens/sec=1377757.29, grad_norm=0.3038, duration=0.38s
Step 13855: loss=2.7978, lr=0.000309, tokens/sec=1376701.53, grad_norm=0.2974, duration=0.38s
Step 13856: loss=2.8110, lr=0.000309, tokens/sec=1374930.05, grad_norm=0.2961, duration=0.38s
Step 13857: loss=2.8711, lr=0.000309, tokens/sec=1374208.31, grad_norm=0.2966, duration=0.38s
Step 13858: loss=2.8211, lr=0.000309, tokens/sec=1376987.74, grad_norm=0.2974, duration=0.38s
Step 13859: loss=2.8316, lr=0.000309, tokens/sec=1377546.70, grad_norm=0.3019, duration=0.38s
Step 13860: loss=2.7753, lr=0.000309, tokens/sec=1371675.33, grad_norm=0.2976, duration=0.38s
Step 13861: loss=2.8114, lr=0.000309, tokens/sec=1374802.83, grad_norm=0.2879, duration=0.38s
Step 13862: loss=2.8665, lr=0.000309, tokens/sec=1375301.53, grad_norm=0.3026, duration=0.38s
Step 13863: loss=2.8061, lr=0.000309, tokens/sec=1374957.56, grad_norm=0.2911, duration=0.38s
Step 13864: loss=2.8587, lr=0.000309, tokens/sec=1373819.40, grad_norm=0.2935, duration=0.38s
Step 13865: loss=2.8908, lr=0.000308, tokens/sec=1379962.86, grad_norm=0.3017, duration=0.38s
Step 13866: loss=2.8743, lr=0.000308, tokens/sec=1377821.17, grad_norm=0.2948, duration=0.38s
Step 13867: loss=2.8951, lr=0.000308, tokens/sec=1375123.51, grad_norm=0.3008, duration=0.38s
Step 13868: loss=2.8581, lr=0.000308, tokens/sec=1374231.50, grad_norm=0.3085, duration=0.38s
Step 13869: loss=2.8977, lr=0.000308, tokens/sec=1375791.98, grad_norm=0.3064, duration=0.38s
Step 13870: loss=2.8695, lr=0.000308, tokens/sec=1372609.43, grad_norm=0.2927, duration=0.38s
Step 13871: loss=2.8953, lr=0.000308, tokens/sec=1369831.42, grad_norm=0.2982, duration=0.38s
Step 13872: loss=2.8935, lr=0.000308, tokens/sec=1375145.01, grad_norm=0.3249, duration=0.38s
Step 13873: loss=2.9685, lr=0.000308, tokens/sec=1373257.46, grad_norm=0.3197, duration=0.38s
Step 13874: loss=2.9291, lr=0.000308, tokens/sec=1375097.71, grad_norm=0.2893, duration=0.38s
Step 13875: loss=2.9185, lr=0.000308, tokens/sec=1375403.03, grad_norm=0.3056, duration=0.38s
Step 13876: loss=2.9642, lr=0.000308, tokens/sec=1375230.14, grad_norm=0.3236, duration=0.38s
Step 13877: loss=2.8948, lr=0.000308, tokens/sec=1378195.08, grad_norm=0.2989, duration=0.38s
Step 13878: loss=2.9170, lr=0.000308, tokens/sec=1373212.86, grad_norm=0.3027, duration=0.38s
Step 13879: loss=2.9034, lr=0.000308, tokens/sec=1371727.53, grad_norm=0.3129, duration=0.38s
Step 13880: loss=2.8888, lr=0.000308, tokens/sec=1372236.84, grad_norm=0.2937, duration=0.38s
Step 13881: loss=2.8732, lr=0.000308, tokens/sec=1370880.94, grad_norm=0.2963, duration=0.38s
Step 13882: loss=2.8794, lr=0.000308, tokens/sec=1375536.39, grad_norm=0.3048, duration=0.38s
Step 13883: loss=2.8124, lr=0.000308, tokens/sec=1374799.40, grad_norm=0.2921, duration=0.38s
Step 13884: loss=2.8933, lr=0.000308, tokens/sec=1375305.83, grad_norm=0.3030, duration=0.38s
Step 13885: loss=2.8779, lr=0.000308, tokens/sec=1377479.39, grad_norm=0.3222, duration=0.38s
Step 13886: loss=2.7689, lr=0.000308, tokens/sec=1377712.40, grad_norm=0.3093, duration=0.38s
Step 13887: loss=2.8954, lr=0.000308, tokens/sec=1377417.27, grad_norm=0.3224, duration=0.38s
Step 13888: loss=2.8108, lr=0.000308, tokens/sec=1376875.66, grad_norm=0.3213, duration=0.38s
Step 13889: loss=2.8184, lr=0.000308, tokens/sec=1376985.15, grad_norm=0.4552, duration=0.38s
Step 13890: loss=2.8527, lr=0.000308, tokens/sec=1375175.10, grad_norm=0.3323, duration=0.38s
Step 13891: loss=2.8930, lr=0.000308, tokens/sec=1374418.74, grad_norm=0.3404, duration=0.38s
Step 13892: loss=2.9607, lr=0.000308, tokens/sec=1374195.43, grad_norm=0.3075, duration=0.38s
Step 13893: loss=2.8115, lr=0.000308, tokens/sec=1378002.48, grad_norm=0.2960, duration=0.38s
Step 13894: loss=2.9035, lr=0.000308, tokens/sec=1378339.34, grad_norm=0.3466, duration=0.38s
Step 13895: loss=2.8672, lr=0.000308, tokens/sec=1375024.62, grad_norm=0.3323, duration=0.38s
Step 13896: loss=2.8531, lr=0.000308, tokens/sec=1378574.37, grad_norm=0.3073, duration=0.38s
Step 13897: loss=2.8901, lr=0.000308, tokens/sec=1374760.72, grad_norm=0.3122, duration=0.38s
Step 13898: loss=2.9041, lr=0.000308, tokens/sec=1377037.75, grad_norm=0.3181, duration=0.38s
Step 13899: loss=2.9098, lr=0.000308, tokens/sec=1375151.02, grad_norm=0.3070, duration=0.38s
Step 13900/19073 (72.9%), Elapsed time: 5504.10s, Steps per hour: 9091.41, Estimated hours remaining: 0.57
Step 13900: loss=2.8392, lr=0.000308, tokens/sec=1374453.96, grad_norm=0.3040, duration=0.38s
Step 13901: loss=2.8911, lr=0.000308, tokens/sec=1375816.08, grad_norm=0.3046, duration=0.38s
Step 13902: loss=2.8933, lr=0.000308, tokens/sec=1378441.29, grad_norm=0.3408, duration=0.38s
Step 13903: loss=2.9126, lr=0.000308, tokens/sec=1379179.60, grad_norm=0.3192, duration=0.38s
Step 13904: loss=2.8738, lr=0.000308, tokens/sec=1377776.28, grad_norm=0.2832, duration=0.38s
Step 13905: loss=2.7702, lr=0.000308, tokens/sec=1376987.74, grad_norm=0.3035, duration=0.38s
Step 13906: loss=2.8438, lr=0.000307, tokens/sec=1376845.48, grad_norm=0.3323, duration=0.38s
Step 13907: loss=2.7618, lr=0.000307, tokens/sec=1372653.13, grad_norm=0.2945, duration=0.38s
Step 13908: loss=2.7917, lr=0.000307, tokens/sec=1376586.91, grad_norm=0.2871, duration=0.38s
Step 13909: loss=2.8045, lr=0.000307, tokens/sec=1377665.80, grad_norm=0.3083, duration=0.38s
Step 13910: loss=2.7844, lr=0.000307, tokens/sec=1371649.66, grad_norm=0.3214, duration=0.38s
Step 13911: loss=2.8148, lr=0.000307, tokens/sec=1367913.32, grad_norm=0.3085, duration=0.38s
Step 13912: loss=2.8049, lr=0.000307, tokens/sec=1376725.67, grad_norm=0.2993, duration=0.38s
Step 13913: loss=2.8424, lr=0.000307, tokens/sec=1374374.07, grad_norm=0.3093, duration=0.38s
Step 13914: loss=2.8292, lr=0.000307, tokens/sec=1377647.67, grad_norm=0.3199, duration=0.38s
Step 13915: loss=2.8239, lr=0.000307, tokens/sec=1380229.63, grad_norm=0.3222, duration=0.38s
Step 13916: loss=2.8230, lr=0.000307, tokens/sec=1373982.49, grad_norm=0.2878, duration=0.38s
Step 13917: loss=2.9136, lr=0.000307, tokens/sec=1376755.83, grad_norm=0.3349, duration=0.38s
Step 13918: loss=2.9149, lr=0.000307, tokens/sec=1377980.03, grad_norm=0.3477, duration=0.38s
Step 13919: loss=2.8753, lr=0.000307, tokens/sec=1372456.09, grad_norm=0.3142, duration=0.38s
Step 13920: loss=2.8823, lr=0.000307, tokens/sec=1374401.56, grad_norm=0.3050, duration=0.38s
Step 13921: loss=2.9019, lr=0.000307, tokens/sec=1378981.54, grad_norm=0.3385, duration=0.38s
Step 13922: loss=2.8782, lr=0.000307, tokens/sec=1374173.96, grad_norm=0.3403, duration=0.38s
Step 13923: loss=2.8979, lr=0.000307, tokens/sec=1371721.54, grad_norm=0.2941, duration=0.38s
Step 13924: loss=2.8695, lr=0.000307, tokens/sec=1378312.56, grad_norm=0.3009, duration=0.38s
Step 13925: loss=2.8359, lr=0.000307, tokens/sec=1373052.52, grad_norm=0.3372, duration=0.38s
Step 13926: loss=2.8550, lr=0.000307, tokens/sec=1376452.49, grad_norm=0.3154, duration=0.38s
Step 13927: loss=2.8521, lr=0.000307, tokens/sec=1375646.53, grad_norm=0.2904, duration=0.38s
Step 13928: loss=2.8383, lr=0.000307, tokens/sec=1374751.27, grad_norm=0.3067, duration=0.38s
Step 13929: loss=2.9052, lr=0.000307, tokens/sec=1374815.73, grad_norm=0.3219, duration=0.38s
Step 13930: loss=2.8912, lr=0.000307, tokens/sec=1373604.00, grad_norm=0.3052, duration=0.38s
Step 13931: loss=2.7770, lr=0.000307, tokens/sec=1374844.09, grad_norm=0.3036, duration=0.38s
Step 13932: loss=2.8390, lr=0.000307, tokens/sec=1374343.15, grad_norm=0.3143, duration=0.38s
Step 13933: loss=2.8468, lr=0.000307, tokens/sec=1376215.60, grad_norm=0.3203, duration=0.38s
Step 13934: loss=2.8812, lr=0.000307, tokens/sec=1376371.51, grad_norm=0.3132, duration=0.38s
Step 13935: loss=2.9307, lr=0.000307, tokens/sec=1376606.73, grad_norm=0.3084, duration=0.38s
Step 13936: loss=2.8603, lr=0.000307, tokens/sec=1374935.21, grad_norm=0.3169, duration=0.38s
Step 13937: loss=2.9653, lr=0.000307, tokens/sec=1373354.37, grad_norm=0.3221, duration=0.38s
Step 13938: loss=2.9159, lr=0.000307, tokens/sec=1376235.41, grad_norm=0.2964, duration=0.38s
Step 13939: loss=2.8726, lr=0.000307, tokens/sec=1377152.45, grad_norm=0.2911, duration=0.38s
Step 13940: loss=2.9095, lr=0.000307, tokens/sec=1372152.93, grad_norm=0.3133, duration=0.38s
Step 13941: loss=2.8824, lr=0.000307, tokens/sec=1377474.22, grad_norm=0.3112, duration=0.38s
Step 13942: loss=2.8522, lr=0.000307, tokens/sec=1376215.60, grad_norm=0.2958, duration=0.38s
Step 13943: loss=2.8970, lr=0.000307, tokens/sec=1375585.43, grad_norm=0.2940, duration=0.38s
Step 13944: loss=2.8852, lr=0.000307, tokens/sec=1376466.28, grad_norm=0.3048, duration=0.38s
Step 13945: loss=2.8769, lr=0.000307, tokens/sec=1376954.97, grad_norm=0.3131, duration=0.38s
Step 13946: loss=2.8616, lr=0.000307, tokens/sec=1375846.21, grad_norm=0.3060, duration=0.38s
Step 13947: loss=2.8164, lr=0.000306, tokens/sec=1376943.77, grad_norm=0.3073, duration=0.38s
Step 13948: loss=2.8759, lr=0.000306, tokens/sec=1374402.42, grad_norm=0.2977, duration=0.38s
Step 13949: loss=2.8638, lr=0.000306, tokens/sec=1376555.03, grad_norm=0.2923, duration=0.38s
Step 13950: loss=2.8963, lr=0.000306, tokens/sec=1374198.01, grad_norm=0.2980, duration=0.38s
Step 13951: loss=2.7991, lr=0.000306, tokens/sec=1377875.56, grad_norm=0.2958, duration=0.38s
Step 13952: loss=2.8582, lr=0.000306, tokens/sec=1376443.88, grad_norm=0.2890, duration=0.38s
Step 13953: loss=2.8182, lr=0.000306, tokens/sec=1373236.02, grad_norm=0.2894, duration=0.38s
Step 13954: loss=2.8162, lr=0.000306, tokens/sec=1376704.98, grad_norm=0.2999, duration=0.38s
Step 13955: loss=2.7910, lr=0.000306, tokens/sec=1376861.00, grad_norm=0.2926, duration=0.38s
Step 13956: loss=2.8156, lr=0.000306, tokens/sec=1372772.24, grad_norm=0.2899, duration=0.38s
Step 13957: loss=2.8614, lr=0.000306, tokens/sec=1376252.64, grad_norm=0.2916, duration=0.38s
Step 13958: loss=2.7927, lr=0.000306, tokens/sec=1377810.81, grad_norm=0.3150, duration=0.38s
Step 13959: loss=2.8103, lr=0.000306, tokens/sec=1377647.67, grad_norm=0.3015, duration=0.38s
Step 13960: loss=2.7896, lr=0.000306, tokens/sec=1377267.16, grad_norm=0.2962, duration=0.38s
Step 13961: loss=2.8491, lr=0.000306, tokens/sec=1373313.20, grad_norm=0.2955, duration=0.38s
Step 13962: loss=2.8148, lr=0.000306, tokens/sec=1377295.63, grad_norm=0.2969, duration=0.38s
Step 13963: loss=2.8422, lr=0.000306, tokens/sec=1377832.39, grad_norm=0.3382, duration=0.38s
Step 13964: loss=2.8915, lr=0.000306, tokens/sec=1374758.14, grad_norm=0.3223, duration=0.38s
Step 13965: loss=2.8968, lr=0.000306, tokens/sec=1375181.98, grad_norm=0.3034, duration=0.38s
Step 13966: loss=2.8913, lr=0.000306, tokens/sec=1376064.89, grad_norm=0.3268, duration=0.38s
Step 13967: loss=2.8837, lr=0.000306, tokens/sec=1376785.14, grad_norm=0.3250, duration=0.38s
Step 13968: loss=2.8470, lr=0.000306, tokens/sec=1375736.04, grad_norm=0.3311, duration=0.38s
Step 13969: loss=2.8950, lr=0.000306, tokens/sec=1374736.66, grad_norm=0.3072, duration=0.38s
Step 13970: loss=2.8334, lr=0.000306, tokens/sec=1378947.82, grad_norm=0.3195, duration=0.38s
Step 13971: loss=2.8896, lr=0.000306, tokens/sec=1373188.00, grad_norm=0.3173, duration=0.38s
Step 13972: loss=2.8907, lr=0.000306, tokens/sec=1375593.18, grad_norm=0.3151, duration=0.38s
Step 13973: loss=2.8712, lr=0.000306, tokens/sec=1377302.53, grad_norm=0.3132, duration=0.38s
Step 13974: loss=2.8905, lr=0.000306, tokens/sec=1379210.74, grad_norm=0.3121, duration=0.38s
Step 13975: loss=2.8380, lr=0.000306, tokens/sec=1376529.18, grad_norm=0.3091, duration=0.38s
Step 13976: loss=2.8333, lr=0.000306, tokens/sec=1375342.82, grad_norm=0.3221, duration=0.38s
Step 13977: loss=2.8565, lr=0.000306, tokens/sec=1374556.20, grad_norm=0.3115, duration=0.38s
Step 13978: loss=2.8604, lr=0.000306, tokens/sec=1374604.32, grad_norm=0.3005, duration=0.38s
Step 13979: loss=2.8162, lr=0.000306, tokens/sec=1374107.84, grad_norm=0.2973, duration=0.38s
Step 13980: loss=2.8673, lr=0.000306, tokens/sec=1371968.01, grad_norm=0.3098, duration=0.38s
Step 13981: loss=2.8243, lr=0.000306, tokens/sec=1371068.12, grad_norm=0.3036, duration=0.38s
Step 13982: loss=2.8576, lr=0.000306, tokens/sec=1370938.20, grad_norm=0.2982, duration=0.38s
Step 13983: loss=2.8403, lr=0.000306, tokens/sec=1375059.88, grad_norm=0.2973, duration=0.38s
Step 13984: loss=2.9140, lr=0.000306, tokens/sec=1372017.66, grad_norm=0.3127, duration=0.38s
Step 13985: loss=2.8438, lr=0.000306, tokens/sec=1379817.39, grad_norm=0.3067, duration=0.38s
Step 13986: loss=2.8856, lr=0.000306, tokens/sec=1373249.74, grad_norm=0.3030, duration=0.38s
Step 13987: loss=2.8709, lr=0.000306, tokens/sec=1375992.57, grad_norm=0.2960, duration=0.38s
Step 13988: loss=2.8768, lr=0.000305, tokens/sec=1378908.91, grad_norm=0.3007, duration=0.38s
Step 13989: loss=2.8818, lr=0.000305, tokens/sec=1375074.49, grad_norm=0.2996, duration=0.38s
Step 13990: loss=2.8492, lr=0.000305, tokens/sec=1376133.78, grad_norm=0.2974, duration=0.38s
Step 13991: loss=2.8719, lr=0.000305, tokens/sec=1378430.06, grad_norm=0.2888, duration=0.38s
Step 13992: loss=2.8724, lr=0.000305, tokens/sec=1373604.86, grad_norm=0.2886, duration=0.38s
Step 13993: loss=2.8823, lr=0.000305, tokens/sec=1373115.97, grad_norm=0.3142, duration=0.38s
Step 13994: loss=2.9095, lr=0.000305, tokens/sec=1375696.45, grad_norm=0.3031, duration=0.38s
Step 13995: loss=2.8873, lr=0.000305, tokens/sec=1373096.25, grad_norm=0.2833, duration=0.38s
Step 13996: loss=2.8738, lr=0.000305, tokens/sec=1371452.06, grad_norm=0.2915, duration=0.38s
Step 13997: loss=2.8941, lr=0.000305, tokens/sec=1377233.52, grad_norm=0.3064, duration=0.38s
Step 13998: loss=2.8243, lr=0.000305, tokens/sec=1373260.03, grad_norm=0.2910, duration=0.38s
Step 13999: loss=2.8147, lr=0.000305, tokens/sec=1372345.60, grad_norm=0.2833, duration=0.38s
Step 14000/19073 (73.4%), Elapsed time: 5542.30s, Steps per hour: 9093.70, Estimated hours remaining: 0.56
Validation loss at step 14000: 3.812068223953247
Step 14000: loss=2.8491, lr=0.000305, tokens/sec=156351.88, grad_norm=0.2934, duration=3.35s
Step 14001: loss=2.7877, lr=0.000305, tokens/sec=1376799.79, grad_norm=0.3025, duration=0.38s
Step 14002: loss=2.7892, lr=0.000305, tokens/sec=1377428.49, grad_norm=0.2932, duration=0.38s
Step 14003: loss=2.8103, lr=0.000305, tokens/sec=1378262.45, grad_norm=0.2906, duration=0.38s
Step 14004: loss=2.8148, lr=0.000305, tokens/sec=1370915.98, grad_norm=0.3265, duration=0.38s
Step 14005: loss=2.8451, lr=0.000305, tokens/sec=1374308.79, grad_norm=0.2979, duration=0.38s
Step 14006: loss=2.8241, lr=0.000305, tokens/sec=1372337.03, grad_norm=0.2993, duration=0.38s
Step 14007: loss=2.7909, lr=0.000305, tokens/sec=1377174.87, grad_norm=0.2912, duration=0.38s
Step 14008: loss=2.8385, lr=0.000305, tokens/sec=1375170.80, grad_norm=0.3129, duration=0.38s
Step 14009: loss=2.8508, lr=0.000305, tokens/sec=1375218.96, grad_norm=0.3099, duration=0.38s
Step 14010: loss=2.8676, lr=0.000305, tokens/sec=1378230.49, grad_norm=0.3030, duration=0.38s
Step 14011: loss=2.8762, lr=0.000305, tokens/sec=1375211.22, grad_norm=0.2917, duration=0.38s
Step 14012: loss=2.8958, lr=0.000305, tokens/sec=1372670.27, grad_norm=0.3195, duration=0.38s
Step 14013: loss=2.9036, lr=0.000305, tokens/sec=1380045.13, grad_norm=0.3087, duration=0.38s
Step 14014: loss=2.8957, lr=0.000305, tokens/sec=1378130.30, grad_norm=0.3161, duration=0.38s
Step 14015: loss=2.8382, lr=0.000305, tokens/sec=1374667.05, grad_norm=0.3147, duration=0.38s
Step 14016: loss=2.8751, lr=0.000305, tokens/sec=1374536.44, grad_norm=0.3108, duration=0.38s
Step 14017: loss=2.9302, lr=0.000305, tokens/sec=1374707.44, grad_norm=0.3072, duration=0.38s
Step 14018: loss=3.0027, lr=0.000305, tokens/sec=1373808.24, grad_norm=0.3021, duration=0.38s
Step 14019: loss=2.9193, lr=0.000305, tokens/sec=1376149.29, grad_norm=0.3185, duration=0.38s
Step 14020: loss=2.9192, lr=0.000305, tokens/sec=1380526.84, grad_norm=0.3231, duration=0.38s
Step 14021: loss=2.8389, lr=0.000305, tokens/sec=1376767.04, grad_norm=0.3078, duration=0.38s
Step 14022: loss=2.8924, lr=0.000305, tokens/sec=1373166.56, grad_norm=0.3276, duration=0.38s
Step 14023: loss=2.8266, lr=0.000305, tokens/sec=1373715.56, grad_norm=0.3251, duration=0.38s
Step 14024: loss=2.8228, lr=0.000305, tokens/sec=1375258.53, grad_norm=0.3441, duration=0.38s
Step 14025: loss=2.9047, lr=0.000305, tokens/sec=1377522.54, grad_norm=0.3148, duration=0.38s
Step 14026: loss=2.7999, lr=0.000305, tokens/sec=1377601.93, grad_norm=0.3005, duration=0.38s
Step 14027: loss=2.9216, lr=0.000305, tokens/sec=1378602.89, grad_norm=0.3516, duration=0.38s
Step 14028: loss=2.9304, lr=0.000305, tokens/sec=1378041.34, grad_norm=0.3425, duration=0.38s
Step 14029: loss=2.8178, lr=0.000305, tokens/sec=1376030.45, grad_norm=0.3249, duration=0.38s
Step 14030: loss=2.8201, lr=0.000304, tokens/sec=1373585.13, grad_norm=0.3034, duration=0.38s
Step 14031: loss=2.8737, lr=0.000304, tokens/sec=1373585.13, grad_norm=0.3509, duration=0.38s
Step 14032: loss=2.9301, lr=0.000304, tokens/sec=1377872.97, grad_norm=0.3330, duration=0.38s
Step 14033: loss=2.8856, lr=0.000304, tokens/sec=1376273.31, grad_norm=0.3114, duration=0.38s
Step 14034: loss=2.8175, lr=0.000304, tokens/sec=1377819.44, grad_norm=0.3068, duration=0.38s
Step 14035: loss=2.8520, lr=0.000304, tokens/sec=1377229.21, grad_norm=0.3230, duration=0.38s
Step 14036: loss=2.8726, lr=0.000304, tokens/sec=1375153.60, grad_norm=0.3319, duration=0.38s
Step 14037: loss=2.9249, lr=0.000304, tokens/sec=1373670.93, grad_norm=0.3155, duration=0.38s
Step 14038: loss=2.9412, lr=0.000304, tokens/sec=1377642.49, grad_norm=0.3272, duration=0.38s
Step 14039: loss=2.8499, lr=0.000304, tokens/sec=1378105.25, grad_norm=0.3295, duration=0.38s
Step 14040: loss=2.8584, lr=0.000304, tokens/sec=1376184.60, grad_norm=0.3190, duration=0.38s
Step 14041: loss=2.8534, lr=0.000304, tokens/sec=1376424.06, grad_norm=0.3122, duration=0.38s
Step 14042: loss=2.8213, lr=0.000304, tokens/sec=1380431.51, grad_norm=0.3224, duration=0.38s
Step 14043: loss=2.8648, lr=0.000304, tokens/sec=1373519.92, grad_norm=0.3210, duration=0.38s
Step 14044: loss=2.8111, lr=0.000304, tokens/sec=1379335.31, grad_norm=0.3049, duration=0.38s
Step 14045: loss=2.8015, lr=0.000304, tokens/sec=1375794.56, grad_norm=0.2956, duration=0.38s
Step 14046: loss=2.8085, lr=0.000304, tokens/sec=1379398.48, grad_norm=0.3109, duration=0.38s
Step 14047: loss=2.8628, lr=0.000304, tokens/sec=1376596.39, grad_norm=0.3008, duration=0.38s
Step 14048: loss=2.8485, lr=0.000304, tokens/sec=1377912.69, grad_norm=0.2871, duration=0.38s
Step 14049: loss=2.8007, lr=0.000304, tokens/sec=1376306.04, grad_norm=0.2950, duration=0.38s
Step 14050: loss=2.7812, lr=0.000304, tokens/sec=1372955.65, grad_norm=0.3057, duration=0.38s
Step 14051: loss=2.8242, lr=0.000304, tokens/sec=1374600.02, grad_norm=0.3188, duration=0.38s
Step 14052: loss=2.8198, lr=0.000304, tokens/sec=1377921.32, grad_norm=0.2909, duration=0.38s
Step 14053: loss=2.8325, lr=0.000304, tokens/sec=1376820.48, grad_norm=0.2860, duration=0.38s
Step 14054: loss=2.8420, lr=0.000304, tokens/sec=1378178.66, grad_norm=0.3064, duration=0.38s
Step 14055: loss=2.8712, lr=0.000304, tokens/sec=1372420.11, grad_norm=0.3085, duration=0.38s
Step 14056: loss=2.8879, lr=0.000304, tokens/sec=1376161.34, grad_norm=0.2946, duration=0.38s
Step 14057: loss=2.8806, lr=0.000304, tokens/sec=1378742.05, grad_norm=0.2917, duration=0.38s
Step 14058: loss=2.8651, lr=0.000304, tokens/sec=1374539.02, grad_norm=0.2994, duration=0.38s
Step 14059: loss=2.8927, lr=0.000304, tokens/sec=1374570.81, grad_norm=0.3044, duration=0.38s
Step 14060: loss=2.8596, lr=0.000304, tokens/sec=1373814.25, grad_norm=0.2913, duration=0.38s
Step 14061: loss=2.8801, lr=0.000304, tokens/sec=1372915.37, grad_norm=0.3107, duration=0.38s
Step 14062: loss=2.8924, lr=0.000304, tokens/sec=1378928.80, grad_norm=0.3161, duration=0.38s
Step 14063: loss=2.9864, lr=0.000304, tokens/sec=1376751.52, grad_norm=0.3209, duration=0.38s
Step 14064: loss=2.8910, lr=0.000304, tokens/sec=1376355.14, grad_norm=0.2939, duration=0.38s
Step 14065: loss=2.9185, lr=0.000304, tokens/sec=1376974.80, grad_norm=0.3185, duration=0.38s
Step 14066: loss=2.9562, lr=0.000304, tokens/sec=1377255.95, grad_norm=0.3197, duration=0.38s
Step 14067: loss=2.8992, lr=0.000304, tokens/sec=1375198.32, grad_norm=0.3087, duration=0.38s
Step 14068: loss=2.9197, lr=0.000304, tokens/sec=1376760.14, grad_norm=0.3017, duration=0.38s
Step 14069: loss=2.8698, lr=0.000304, tokens/sec=1374101.83, grad_norm=0.3167, duration=0.38s
Step 14070: loss=2.8964, lr=0.000304, tokens/sec=1378950.41, grad_norm=0.3103, duration=0.38s
Step 14071: loss=2.8622, lr=0.000304, tokens/sec=1378521.65, grad_norm=0.2985, duration=0.38s
Step 14072: loss=2.8672, lr=0.000303, tokens/sec=1376524.01, grad_norm=0.3042, duration=0.38s
Step 14073: loss=2.8181, lr=0.000303, tokens/sec=1376881.69, grad_norm=0.3098, duration=0.38s
Step 14074: loss=2.8933, lr=0.000303, tokens/sec=1378360.94, grad_norm=0.2976, duration=0.38s
Step 14075: loss=2.8097, lr=0.000303, tokens/sec=1376842.03, grad_norm=0.3261, duration=0.38s
Step 14076: loss=2.8244, lr=0.000303, tokens/sec=1376173.40, grad_norm=0.3141, duration=0.38s
Step 14077: loss=2.8892, lr=0.000303, tokens/sec=1379979.31, grad_norm=0.3195, duration=0.38s
Step 14078: loss=2.8440, lr=0.000303, tokens/sec=1377846.21, grad_norm=0.3215, duration=0.38s
Step 14079: loss=2.8032, lr=0.000303, tokens/sec=1378458.57, grad_norm=0.3240, duration=0.38s
Step 14080: loss=2.8468, lr=0.000303, tokens/sec=1379406.26, grad_norm=0.3175, duration=0.38s
Step 14081: loss=2.8843, lr=0.000303, tokens/sec=1379391.55, grad_norm=0.3244, duration=0.38s
Step 14082: loss=2.9597, lr=0.000303, tokens/sec=1374716.89, grad_norm=0.3301, duration=0.38s
Step 14083: loss=2.7904, lr=0.000303, tokens/sec=1373301.19, grad_norm=0.2990, duration=0.38s
Step 14084: loss=2.8741, lr=0.000303, tokens/sec=1375874.62, grad_norm=0.3181, duration=0.38s
Step 14085: loss=2.9097, lr=0.000303, tokens/sec=1375597.48, grad_norm=0.3433, duration=0.38s
Step 14086: loss=2.8482, lr=0.000303, tokens/sec=1378218.40, grad_norm=0.3293, duration=0.38s
Step 14087: loss=2.8778, lr=0.000303, tokens/sec=1374612.05, grad_norm=0.2977, duration=0.38s
Step 14088: loss=2.9254, lr=0.000303, tokens/sec=1376772.21, grad_norm=0.3284, duration=0.38s
Step 14089: loss=2.8706, lr=0.000303, tokens/sec=1379546.45, grad_norm=0.3449, duration=0.38s
Step 14090: loss=2.8534, lr=0.000303, tokens/sec=1376404.25, grad_norm=0.3126, duration=0.38s
Step 14091: loss=2.8758, lr=0.000303, tokens/sec=1375236.16, grad_norm=0.2964, duration=0.38s
Step 14092: loss=2.8736, lr=0.000303, tokens/sec=1377294.77, grad_norm=0.3658, duration=0.38s
Step 14093: loss=2.9273, lr=0.000303, tokens/sec=1374440.22, grad_norm=0.3415, duration=0.38s
Step 14094: loss=2.8187, lr=0.000303, tokens/sec=1373494.19, grad_norm=0.2893, duration=0.38s
Step 14095: loss=2.7955, lr=0.000303, tokens/sec=1374402.42, grad_norm=0.2896, duration=0.38s
Step 14096: loss=2.8070, lr=0.000303, tokens/sec=1376256.94, grad_norm=0.3322, duration=0.38s
Step 14097: loss=2.7952, lr=0.000303, tokens/sec=1376442.15, grad_norm=0.3247, duration=0.38s
Step 14098: loss=2.7857, lr=0.000303, tokens/sec=1378287.50, grad_norm=0.3051, duration=0.38s
Step 14099: loss=2.7642, lr=0.000303, tokens/sec=1376357.73, grad_norm=0.2885, duration=0.38s
Step 14100/19073 (73.9%), Elapsed time: 5583.45s, Steps per hour: 9091.15, Estimated hours remaining: 0.55
Step 14100: loss=2.8102, lr=0.000303, tokens/sec=1371647.10, grad_norm=0.3247, duration=0.38s
Step 14101: loss=2.8013, lr=0.000303, tokens/sec=1373361.23, grad_norm=0.3380, duration=0.38s
Step 14102: loss=2.8200, lr=0.000303, tokens/sec=1376766.18, grad_norm=0.2991, duration=0.38s
Step 14103: loss=2.8531, lr=0.000303, tokens/sec=1375446.91, grad_norm=0.2993, duration=0.38s
Step 14104: loss=2.8293, lr=0.000303, tokens/sec=1376847.21, grad_norm=0.3293, duration=0.38s
Step 14105: loss=2.8099, lr=0.000303, tokens/sec=1379177.87, grad_norm=0.3349, duration=0.38s
Step 14106: loss=2.8430, lr=0.000303, tokens/sec=1378488.82, grad_norm=0.2957, duration=0.38s
Step 14107: loss=2.8691, lr=0.000303, tokens/sec=1375850.52, grad_norm=0.3032, duration=0.38s
Step 14108: loss=2.9164, lr=0.000303, tokens/sec=1377705.50, grad_norm=0.3473, duration=0.38s
Step 14109: loss=2.8873, lr=0.000303, tokens/sec=1372846.80, grad_norm=0.3537, duration=0.38s
Step 14110: loss=2.8744, lr=0.000303, tokens/sec=1374771.03, grad_norm=0.3127, duration=0.38s
Step 14111: loss=2.8797, lr=0.000303, tokens/sec=1372990.80, grad_norm=0.3117, duration=0.38s
Step 14112: loss=2.8939, lr=0.000303, tokens/sec=1378831.96, grad_norm=0.3489, duration=0.38s
Step 14113: loss=2.9039, lr=0.000303, tokens/sec=1375841.91, grad_norm=0.3393, duration=0.38s
Step 14114: loss=2.8416, lr=0.000302, tokens/sec=1377347.39, grad_norm=0.3066, duration=0.38s
Step 14115: loss=2.8236, lr=0.000302, tokens/sec=1379830.38, grad_norm=0.3143, duration=0.38s
Step 14116: loss=2.8644, lr=0.000302, tokens/sec=1376414.58, grad_norm=0.3176, duration=0.38s
Step 14117: loss=2.8442, lr=0.000302, tokens/sec=1374308.79, grad_norm=0.3266, duration=0.38s
Step 14118: loss=2.9016, lr=0.000302, tokens/sec=1373768.76, grad_norm=0.3213, duration=0.38s
Step 14119: loss=2.8891, lr=0.000302, tokens/sec=1377466.45, grad_norm=0.3213, duration=0.38s
Step 14120: loss=2.8502, lr=0.000302, tokens/sec=1376835.14, grad_norm=0.3068, duration=0.38s
Step 14121: loss=2.7796, lr=0.000302, tokens/sec=1376432.68, grad_norm=0.3159, duration=0.38s
Step 14122: loss=2.8027, lr=0.000302, tokens/sec=1379338.77, grad_norm=0.3235, duration=0.38s
Step 14123: loss=2.8850, lr=0.000302, tokens/sec=1374327.69, grad_norm=0.3163, duration=0.38s
Step 14124: loss=2.8879, lr=0.000302, tokens/sec=1378063.80, grad_norm=0.3131, duration=0.38s
Step 14125: loss=2.9143, lr=0.000302, tokens/sec=1375199.18, grad_norm=0.3164, duration=0.38s
Step 14126: loss=2.8888, lr=0.000302, tokens/sec=1373678.66, grad_norm=0.3129, duration=0.38s
Step 14127: loss=2.9504, lr=0.000302, tokens/sec=1379074.94, grad_norm=0.3464, duration=0.38s
Step 14128: loss=2.8819, lr=0.000302, tokens/sec=1374709.15, grad_norm=0.3141, duration=0.38s
Step 14129: loss=2.9046, lr=0.000302, tokens/sec=1377268.89, grad_norm=0.3013, duration=0.38s
Step 14130: loss=2.8754, lr=0.000302, tokens/sec=1379890.99, grad_norm=0.3013, duration=0.38s
Step 14131: loss=2.8737, lr=0.000302, tokens/sec=1378474.13, grad_norm=0.3141, duration=0.38s
Step 14132: loss=2.8759, lr=0.000302, tokens/sec=1378199.39, grad_norm=0.3178, duration=0.38s
Step 14133: loss=2.8800, lr=0.000302, tokens/sec=1372848.51, grad_norm=0.3043, duration=0.38s
Step 14134: loss=2.8768, lr=0.000302, tokens/sec=1374663.61, grad_norm=0.2985, duration=0.38s
Step 14135: loss=2.8540, lr=0.000302, tokens/sec=1376359.45, grad_norm=0.3138, duration=0.38s
Step 14136: loss=2.8591, lr=0.000302, tokens/sec=1377042.92, grad_norm=0.3129, duration=0.38s
Step 14137: loss=2.8366, lr=0.000302, tokens/sec=1373314.92, grad_norm=0.3230, duration=0.38s
Step 14138: loss=2.8715, lr=0.000302, tokens/sec=1373129.69, grad_norm=0.3220, duration=0.38s
Step 14139: loss=2.8796, lr=0.000302, tokens/sec=1377354.29, grad_norm=0.3039, duration=0.38s
Step 14140: loss=2.8658, lr=0.000302, tokens/sec=1377318.92, grad_norm=0.2940, duration=0.38s
Step 14141: loss=2.7945, lr=0.000302, tokens/sec=1374738.37, grad_norm=0.3088, duration=0.38s
Step 14142: loss=2.8679, lr=0.000302, tokens/sec=1374527.85, grad_norm=0.3046, duration=0.38s
Step 14143: loss=2.8050, lr=0.000302, tokens/sec=1374771.89, grad_norm=0.3115, duration=0.38s
Step 14144: loss=2.8334, lr=0.000302, tokens/sec=1378330.70, grad_norm=0.3033, duration=0.38s
Step 14145: loss=2.7790, lr=0.000302, tokens/sec=1376636.03, grad_norm=0.3058, duration=0.38s
Step 14146: loss=2.8374, lr=0.000302, tokens/sec=1373809.96, grad_norm=0.2941, duration=0.38s
Step 14147: loss=2.8361, lr=0.000302, tokens/sec=1376714.46, grad_norm=0.3028, duration=0.38s
Step 14148: loss=2.7836, lr=0.000302, tokens/sec=1378221.85, grad_norm=0.3169, duration=0.38s
Step 14149: loss=2.8057, lr=0.000302, tokens/sec=1374595.72, grad_norm=0.2991, duration=0.38s
Step 14150: loss=2.7918, lr=0.000302, tokens/sec=1373205.15, grad_norm=0.3101, duration=0.38s
Step 14151: loss=2.8296, lr=0.000302, tokens/sec=1374854.41, grad_norm=0.3135, duration=0.38s
Step 14152: loss=2.8193, lr=0.000302, tokens/sec=1372911.08, grad_norm=0.3016, duration=0.38s
Step 14153: loss=2.8438, lr=0.000302, tokens/sec=1375682.68, grad_norm=0.3187, duration=0.38s
Step 14154: loss=2.8889, lr=0.000302, tokens/sec=1378824.18, grad_norm=0.3500, duration=0.38s
Step 14155: loss=2.8807, lr=0.000302, tokens/sec=1375276.59, grad_norm=0.3329, duration=0.38s
Step 14156: loss=2.8804, lr=0.000301, tokens/sec=1376180.29, grad_norm=0.3259, duration=0.38s
Step 14157: loss=2.9095, lr=0.000301, tokens/sec=1374560.50, grad_norm=0.3495, duration=0.38s
Step 14158: loss=2.8433, lr=0.000301, tokens/sec=1378391.18, grad_norm=0.3846, duration=0.38s
Step 14159: loss=2.8638, lr=0.000301, tokens/sec=1376038.20, grad_norm=0.3234, duration=0.38s
Step 14160: loss=2.8229, lr=0.000301, tokens/sec=1375575.97, grad_norm=0.3221, duration=0.38s
Step 14161: loss=2.8957, lr=0.000301, tokens/sec=1378145.84, grad_norm=0.3400, duration=0.38s
Step 14162: loss=2.8801, lr=0.000301, tokens/sec=1376696.36, grad_norm=0.3388, duration=0.38s
Step 14163: loss=2.8860, lr=0.000301, tokens/sec=1376611.04, grad_norm=0.3373, duration=0.38s
Step 14164: loss=2.8698, lr=0.000301, tokens/sec=1373955.88, grad_norm=0.3246, duration=0.38s
Step 14165: loss=2.8190, lr=0.000301, tokens/sec=1376474.03, grad_norm=0.3165, duration=0.38s
Step 14166: loss=2.8466, lr=0.000301, tokens/sec=1372921.37, grad_norm=0.3327, duration=0.38s
Step 14167: loss=2.8524, lr=0.000301, tokens/sec=1374414.45, grad_norm=0.3165, duration=0.38s
Step 14168: loss=2.8628, lr=0.000301, tokens/sec=1378271.95, grad_norm=0.3048, duration=0.38s
Step 14169: loss=2.8119, lr=0.000301, tokens/sec=1373723.28, grad_norm=0.3110, duration=0.38s
Step 14170: loss=2.8312, lr=0.000301, tokens/sec=1375391.85, grad_norm=0.3152, duration=0.38s
Step 14171: loss=2.8401, lr=0.000301, tokens/sec=1375853.10, grad_norm=0.3067, duration=0.38s
Step 14172: loss=2.8993, lr=0.000301, tokens/sec=1375268.85, grad_norm=0.2959, duration=0.38s
Step 14173: loss=2.8447, lr=0.000301, tokens/sec=1377934.27, grad_norm=0.3068, duration=0.38s
Step 14174: loss=2.8956, lr=0.000301, tokens/sec=1374238.37, grad_norm=0.3201, duration=0.38s
Step 14175: loss=2.8267, lr=0.000301, tokens/sec=1375720.54, grad_norm=0.2971, duration=0.38s
Step 14176: loss=2.8985, lr=0.000301, tokens/sec=1372819.37, grad_norm=0.3085, duration=0.38s
Step 14177: loss=2.8534, lr=0.000301, tokens/sec=1371314.36, grad_norm=0.3062, duration=0.38s
Step 14178: loss=2.8986, lr=0.000301, tokens/sec=1375141.57, grad_norm=0.3058, duration=0.38s
Step 14179: loss=2.8365, lr=0.000301, tokens/sec=1372302.78, grad_norm=0.2993, duration=0.38s
Step 14180: loss=2.8703, lr=0.000301, tokens/sec=1377324.96, grad_norm=0.2981, duration=0.38s
Step 14181: loss=2.8654, lr=0.000301, tokens/sec=1374116.43, grad_norm=0.3002, duration=0.38s
Step 14182: loss=2.8969, lr=0.000301, tokens/sec=1371230.56, grad_norm=0.2991, duration=0.38s
Step 14183: loss=2.8864, lr=0.000301, tokens/sec=1374253.83, grad_norm=0.3102, duration=0.38s
Step 14184: loss=2.8851, lr=0.000301, tokens/sec=1373099.68, grad_norm=0.3029, duration=0.38s
Step 14185: loss=2.8821, lr=0.000301, tokens/sec=1370940.76, grad_norm=0.2894, duration=0.38s
Step 14186: loss=2.8984, lr=0.000301, tokens/sec=1375528.64, grad_norm=0.2885, duration=0.38s
Step 14187: loss=2.8257, lr=0.000301, tokens/sec=1375603.50, grad_norm=0.3038, duration=0.38s
Step 14188: loss=2.8633, lr=0.000301, tokens/sec=1373881.20, grad_norm=0.2908, duration=0.38s
Step 14189: loss=2.8121, lr=0.000301, tokens/sec=1373769.62, grad_norm=0.2789, duration=0.38s
Step 14190: loss=2.8614, lr=0.000301, tokens/sec=1377081.73, grad_norm=0.2900, duration=0.38s
Step 14191: loss=2.7388, lr=0.000301, tokens/sec=1375948.66, grad_norm=0.2923, duration=0.38s
Step 14192: loss=2.8049, lr=0.000301, tokens/sec=1373840.00, grad_norm=0.2903, duration=0.38s
Step 14193: loss=2.7485, lr=0.000301, tokens/sec=1377024.82, grad_norm=0.3295, duration=0.38s
Step 14194: loss=2.8754, lr=0.000301, tokens/sec=1370751.05, grad_norm=0.2944, duration=0.38s
Step 14195: loss=2.8383, lr=0.000301, tokens/sec=1375741.20, grad_norm=0.2911, duration=0.38s
Step 14196: loss=2.7981, lr=0.000301, tokens/sec=1375872.90, grad_norm=0.2842, duration=0.38s
Step 14197: loss=2.8056, lr=0.000301, tokens/sec=1375401.31, grad_norm=0.2950, duration=0.38s
Step 14198: loss=2.8138, lr=0.000301, tokens/sec=1378093.16, grad_norm=0.2950, duration=0.38s
Step 14199: loss=2.8502, lr=0.000300, tokens/sec=1368195.03, grad_norm=0.2975, duration=0.38s
Step 14200/19073 (74.5%), Elapsed time: 5621.66s, Steps per hour: 9093.41, Estimated hours remaining: 0.54
Step 14200: loss=2.8540, lr=0.000300, tokens/sec=1370423.87, grad_norm=0.3054, duration=0.38s
Step 14201: loss=2.8846, lr=0.000300, tokens/sec=1369478.24, grad_norm=0.2912, duration=0.38s
Step 14202: loss=2.9190, lr=0.000300, tokens/sec=1369822.03, grad_norm=0.3086, duration=0.38s
Step 14203: loss=2.8643, lr=0.000300, tokens/sec=1377499.24, grad_norm=0.2940, duration=0.38s
Step 14204: loss=2.8861, lr=0.000300, tokens/sec=1374462.55, grad_norm=0.3028, duration=0.38s
Step 14205: loss=2.8450, lr=0.000300, tokens/sec=1373810.82, grad_norm=0.3159, duration=0.38s
Step 14206: loss=2.8827, lr=0.000300, tokens/sec=1373870.04, grad_norm=0.2965, duration=0.38s
Step 14207: loss=2.9410, lr=0.000300, tokens/sec=1375161.34, grad_norm=0.2992, duration=0.38s
Step 14208: loss=2.9671, lr=0.000300, tokens/sec=1371146.77, grad_norm=0.3149, duration=0.38s
Step 14209: loss=2.9236, lr=0.000300, tokens/sec=1369200.27, grad_norm=0.3169, duration=0.38s
Step 14210: loss=2.8846, lr=0.000300, tokens/sec=1373619.45, grad_norm=0.2997, duration=0.38s
Step 14211: loss=2.8585, lr=0.000300, tokens/sec=1376908.42, grad_norm=0.3108, duration=0.38s
Step 14212: loss=2.8619, lr=0.000300, tokens/sec=1373494.19, grad_norm=0.3240, duration=0.38s
Step 14213: loss=2.8489, lr=0.000300, tokens/sec=1374528.71, grad_norm=0.3253, duration=0.38s
Step 14214: loss=2.8132, lr=0.000300, tokens/sec=1371426.40, grad_norm=0.3209, duration=0.38s
Step 14215: loss=2.8818, lr=0.000300, tokens/sec=1376305.18, grad_norm=0.3152, duration=0.38s
Step 14216: loss=2.8346, lr=0.000300, tokens/sec=1372662.55, grad_norm=0.3146, duration=0.38s
Step 14217: loss=2.9065, lr=0.000300, tokens/sec=1372325.90, grad_norm=0.3386, duration=0.38s
Step 14218: loss=2.8613, lr=0.000300, tokens/sec=1372380.71, grad_norm=0.3479, duration=0.38s
Step 14219: loss=2.8623, lr=0.000300, tokens/sec=1374786.50, grad_norm=0.3216, duration=0.38s
Step 14220: loss=2.8168, lr=0.000300, tokens/sec=1372420.97, grad_norm=0.3130, duration=0.38s
Step 14221: loss=2.8970, lr=0.000300, tokens/sec=1371432.38, grad_norm=0.3479, duration=0.38s
Step 14222: loss=2.9154, lr=0.000300, tokens/sec=1371479.43, grad_norm=0.3232, duration=0.38s
Step 14223: loss=2.8871, lr=0.000300, tokens/sec=1375868.59, grad_norm=0.3116, duration=0.38s
Step 14224: loss=2.7931, lr=0.000300, tokens/sec=1375455.51, grad_norm=0.3064, duration=0.38s
Step 14225: loss=2.8633, lr=0.000300, tokens/sec=1375281.75, grad_norm=0.3270, duration=0.38s
Step 14226: loss=2.9033, lr=0.000300, tokens/sec=1376961.87, grad_norm=0.3488, duration=0.38s
Step 14227: loss=2.9085, lr=0.000300, tokens/sec=1371902.10, grad_norm=0.3289, duration=0.38s
Step 14228: loss=2.9243, lr=0.000300, tokens/sec=1375043.54, grad_norm=0.3097, duration=0.38s
Step 14229: loss=2.8464, lr=0.000300, tokens/sec=1374035.72, grad_norm=0.3311, duration=0.38s
Step 14230: loss=2.8514, lr=0.000300, tokens/sec=1372512.62, grad_norm=0.3413, duration=0.38s
Step 14231: loss=2.8577, lr=0.000300, tokens/sec=1371365.67, grad_norm=0.3181, duration=0.38s
Step 14232: loss=2.8236, lr=0.000300, tokens/sec=1373157.98, grad_norm=0.3089, duration=0.38s
Step 14233: loss=2.8402, lr=0.000300, tokens/sec=1373949.87, grad_norm=0.3170, duration=0.38s
Step 14234: loss=2.8144, lr=0.000300, tokens/sec=1372241.12, grad_norm=0.3169, duration=0.38s
Step 14235: loss=2.8000, lr=0.000300, tokens/sec=1377736.57, grad_norm=0.3111, duration=0.38s
Step 14236: loss=2.7999, lr=0.000300, tokens/sec=1374461.69, grad_norm=0.3067, duration=0.38s
Step 14237: loss=2.8889, lr=0.000300, tokens/sec=1375572.53, grad_norm=0.2997, duration=0.38s
Step 14238: loss=2.8172, lr=0.000300, tokens/sec=1376936.01, grad_norm=0.2916, duration=0.38s
Step 14239: loss=2.8052, lr=0.000300, tokens/sec=1373657.20, grad_norm=0.3011, duration=0.38s
Step 14240: loss=2.7958, lr=0.000300, tokens/sec=1371570.10, grad_norm=0.3115, duration=0.38s
Step 14241: loss=2.7800, lr=0.000300, tokens/sec=1371549.57, grad_norm=0.3047, duration=0.38s
Step 14242: loss=2.8504, lr=0.000299, tokens/sec=1372483.50, grad_norm=0.2957, duration=0.38s
Step 14243: loss=2.8192, lr=0.000299, tokens/sec=1373033.66, grad_norm=0.2865, duration=0.38s
Step 14244: loss=2.8239, lr=0.000299, tokens/sec=1371837.06, grad_norm=0.2922, duration=0.38s
Step 14245: loss=2.8846, lr=0.000299, tokens/sec=1373805.67, grad_norm=0.3033, duration=0.38s
Step 14246: loss=2.8756, lr=0.000299, tokens/sec=1376663.61, grad_norm=0.3084, duration=0.38s
Step 14247: loss=2.8912, lr=0.000299, tokens/sec=1375765.30, grad_norm=0.2884, duration=0.38s
Step 14248: loss=2.8612, lr=0.000299, tokens/sec=1376633.45, grad_norm=0.3073, duration=0.38s
Step 14249: loss=2.8804, lr=0.000299, tokens/sec=1372312.20, grad_norm=0.2944, duration=0.38s
Validation loss at step 14250: 3.8160219192504883
Step 14250: loss=2.8428, lr=0.000299, tokens/sec=152649.43, grad_norm=0.3020, duration=3.43s
Step 14251: loss=2.8826, lr=0.000299, tokens/sec=1377930.82, grad_norm=0.2987, duration=0.38s
Step 14252: loss=2.9126, lr=0.000299, tokens/sec=1369271.88, grad_norm=0.3181, duration=0.38s
Step 14253: loss=2.9506, lr=0.000299, tokens/sec=1375347.98, grad_norm=0.3072, duration=0.38s
Step 14254: loss=2.8917, lr=0.000299, tokens/sec=1373908.67, grad_norm=0.2985, duration=0.38s
Step 14255: loss=2.9089, lr=0.000299, tokens/sec=1375839.33, grad_norm=0.3005, duration=0.38s
Step 14256: loss=2.9584, lr=0.000299, tokens/sec=1372883.65, grad_norm=0.3208, duration=0.38s
Step 14257: loss=2.9011, lr=0.000299, tokens/sec=1379207.28, grad_norm=0.3139, duration=0.38s
Step 14258: loss=2.8866, lr=0.000299, tokens/sec=1374101.83, grad_norm=0.3128, duration=0.38s
Step 14259: loss=2.8810, lr=0.000299, tokens/sec=1372554.60, grad_norm=0.3210, duration=0.38s
Step 14260: loss=2.8889, lr=0.000299, tokens/sec=1376319.82, grad_norm=0.3046, duration=0.38s
Step 14261: loss=2.8543, lr=0.000299, tokens/sec=1372171.76, grad_norm=0.3088, duration=0.38s
Step 14262: loss=2.8732, lr=0.000299, tokens/sec=1374277.02, grad_norm=0.3183, duration=0.38s
Step 14263: loss=2.8183, lr=0.000299, tokens/sec=1372301.06, grad_norm=0.3055, duration=0.38s
Step 14264: loss=2.8219, lr=0.000299, tokens/sec=1376009.79, grad_norm=0.2910, duration=0.38s
Step 14265: loss=2.8652, lr=0.000299, tokens/sec=1373889.78, grad_norm=0.3444, duration=0.38s
Step 14266: loss=2.8190, lr=0.000299, tokens/sec=1370325.66, grad_norm=0.3238, duration=0.38s
Step 14267: loss=2.9237, lr=0.000299, tokens/sec=1372582.87, grad_norm=0.3095, duration=0.38s
Step 14268: loss=2.8311, lr=0.000299, tokens/sec=1373825.41, grad_norm=0.3223, duration=0.38s
Step 14269: loss=2.7985, lr=0.000299, tokens/sec=1377070.52, grad_norm=0.3451, duration=0.38s
Step 14270: loss=2.8359, lr=0.000299, tokens/sec=1375874.62, grad_norm=0.2997, duration=0.38s
Step 14271: loss=2.8821, lr=0.000299, tokens/sec=1375286.05, grad_norm=0.3138, duration=0.38s
Step 14272: loss=2.9382, lr=0.000299, tokens/sec=1375632.76, grad_norm=0.3235, duration=0.38s
Step 14273: loss=2.7610, lr=0.000299, tokens/sec=1377091.22, grad_norm=0.3063, duration=0.38s
Step 14274: loss=2.9171, lr=0.000299, tokens/sec=1375213.80, grad_norm=0.2907, duration=0.38s
Step 14275: loss=2.9031, lr=0.000299, tokens/sec=1371925.21, grad_norm=0.3076, duration=0.38s
Step 14276: loss=2.8374, lr=0.000299, tokens/sec=1375336.80, grad_norm=0.3063, duration=0.38s
Step 14277: loss=2.9005, lr=0.000299, tokens/sec=1377431.94, grad_norm=0.3060, duration=0.38s
Step 14278: loss=2.8901, lr=0.000299, tokens/sec=1377633.00, grad_norm=0.3073, duration=0.38s
Step 14279: loss=2.8869, lr=0.000299, tokens/sec=1375614.69, grad_norm=0.3065, duration=0.38s
Step 14280: loss=2.8392, lr=0.000299, tokens/sec=1380641.25, grad_norm=0.3104, duration=0.38s
Step 14281: loss=2.8645, lr=0.000299, tokens/sec=1373549.95, grad_norm=0.2934, duration=0.38s
Step 14282: loss=2.8887, lr=0.000299, tokens/sec=1374472.86, grad_norm=0.3134, duration=0.38s
Step 14283: loss=2.8720, lr=0.000299, tokens/sec=1369738.42, grad_norm=0.3186, duration=0.38s
Step 14284: loss=2.8446, lr=0.000299, tokens/sec=1371547.00, grad_norm=0.3053, duration=0.38s
Step 14285: loss=2.7599, lr=0.000298, tokens/sec=1375977.07, grad_norm=0.2948, duration=0.38s
Step 14286: loss=2.8395, lr=0.000298, tokens/sec=1374464.27, grad_norm=0.2891, duration=0.38s
Step 14287: loss=2.7862, lr=0.000298, tokens/sec=1375683.54, grad_norm=0.3032, duration=0.38s
Step 14288: loss=2.7482, lr=0.000298, tokens/sec=1377557.92, grad_norm=0.3236, duration=0.38s
Step 14289: loss=2.7894, lr=0.000298, tokens/sec=1376685.16, grad_norm=0.2900, duration=0.38s
Step 14290: loss=2.7911, lr=0.000298, tokens/sec=1378081.07, grad_norm=0.2995, duration=0.38s
Step 14291: loss=2.8156, lr=0.000298, tokens/sec=1374506.37, grad_norm=0.3140, duration=0.38s
Step 14292: loss=2.8305, lr=0.000298, tokens/sec=1374204.02, grad_norm=0.3130, duration=0.38s
Step 14293: loss=2.8467, lr=0.000298, tokens/sec=1377390.52, grad_norm=0.2952, duration=0.38s
Step 14294: loss=2.8070, lr=0.000298, tokens/sec=1375230.14, grad_norm=0.3074, duration=0.38s
Step 14295: loss=2.8297, lr=0.000298, tokens/sec=1376157.90, grad_norm=0.3220, duration=0.38s
Step 14296: loss=2.8006, lr=0.000298, tokens/sec=1375025.48, grad_norm=0.3143, duration=0.38s
Step 14297: loss=2.8695, lr=0.000298, tokens/sec=1375686.98, grad_norm=0.3043, duration=0.38s
Step 14298: loss=2.9214, lr=0.000298, tokens/sec=1376872.21, grad_norm=0.3110, duration=0.38s
Step 14299: loss=2.8772, lr=0.000298, tokens/sec=1376704.98, grad_norm=0.3431, duration=0.38s
Step 14300/19073 (75.0%), Elapsed time: 5662.95s, Steps per hour: 9090.67, Estimated hours remaining: 0.53
Step 14300: loss=2.8516, lr=0.000298, tokens/sec=1374342.29, grad_norm=0.3347, duration=0.38s
Step 14301: loss=2.8928, lr=0.000298, tokens/sec=1375187.14, grad_norm=0.3043, duration=0.38s
Step 14302: loss=2.9008, lr=0.000298, tokens/sec=1378293.55, grad_norm=0.3258, duration=0.38s
Step 14303: loss=2.8749, lr=0.000298, tokens/sec=1373991.94, grad_norm=0.3388, duration=0.38s
Step 14304: loss=2.8297, lr=0.000298, tokens/sec=1376424.92, grad_norm=0.3219, duration=0.38s
Step 14305: loss=2.8332, lr=0.000298, tokens/sec=1375529.50, grad_norm=0.3116, duration=0.38s
Step 14306: loss=2.8547, lr=0.000298, tokens/sec=1376769.63, grad_norm=0.3248, duration=0.38s
Step 14307: loss=2.9056, lr=0.000298, tokens/sec=1377865.20, grad_norm=0.3468, duration=0.38s
Step 14308: loss=2.8868, lr=0.000298, tokens/sec=1375206.06, grad_norm=0.3225, duration=0.38s
Step 14309: loss=2.8463, lr=0.000298, tokens/sec=1375864.29, grad_norm=0.3259, duration=0.38s
Step 14310: loss=2.8530, lr=0.000298, tokens/sec=1374312.23, grad_norm=0.3280, duration=0.38s
Step 14311: loss=2.7407, lr=0.000298, tokens/sec=1378011.12, grad_norm=0.3337, duration=0.38s
Step 14312: loss=2.8402, lr=0.000298, tokens/sec=1375717.10, grad_norm=0.3232, duration=0.38s
Step 14313: loss=2.8956, lr=0.000298, tokens/sec=1375536.39, grad_norm=0.3175, duration=0.38s
Step 14314: loss=2.8736, lr=0.000298, tokens/sec=1378874.32, grad_norm=0.3408, duration=0.38s
Step 14315: loss=2.9405, lr=0.000298, tokens/sec=1377213.68, grad_norm=0.3442, duration=0.38s
Step 14316: loss=2.8680, lr=0.000298, tokens/sec=1373088.53, grad_norm=0.3206, duration=0.38s
Step 14317: loss=2.9119, lr=0.000298, tokens/sec=1377776.28, grad_norm=0.3259, duration=0.38s
Step 14318: loss=2.9137, lr=0.000298, tokens/sec=1377018.78, grad_norm=0.3358, duration=0.38s
Step 14319: loss=2.8686, lr=0.000298, tokens/sec=1376817.90, grad_norm=0.3305, duration=0.38s
Step 14320: loss=2.8645, lr=0.000298, tokens/sec=1375409.92, grad_norm=0.2964, duration=0.38s
Step 14321: loss=2.8957, lr=0.000298, tokens/sec=1375383.25, grad_norm=0.3065, duration=0.38s
Step 14322: loss=2.8583, lr=0.000298, tokens/sec=1378385.13, grad_norm=0.3127, duration=0.38s
Step 14323: loss=2.8711, lr=0.000298, tokens/sec=1375087.39, grad_norm=0.3271, duration=0.38s
Step 14324: loss=2.8544, lr=0.000298, tokens/sec=1376047.67, grad_norm=0.3194, duration=0.38s
Step 14325: loss=2.8489, lr=0.000298, tokens/sec=1376989.46, grad_norm=0.3156, duration=0.38s
Step 14326: loss=2.8750, lr=0.000298, tokens/sec=1375095.13, grad_norm=0.3096, duration=0.38s
Step 14327: loss=2.8305, lr=0.000298, tokens/sec=1373589.42, grad_norm=0.3352, duration=0.38s
Step 14328: loss=2.8874, lr=0.000298, tokens/sec=1375434.87, grad_norm=0.3257, duration=0.38s
Step 14329: loss=2.8497, lr=0.000297, tokens/sec=1374341.43, grad_norm=0.3283, duration=0.38s
Step 14330: loss=2.8595, lr=0.000297, tokens/sec=1378022.35, grad_norm=0.3216, duration=0.38s
Step 14331: loss=2.7987, lr=0.000297, tokens/sec=1376416.31, grad_norm=0.3157, duration=0.38s
Step 14332: loss=2.8508, lr=0.000297, tokens/sec=1376967.91, grad_norm=0.3092, duration=0.38s
Step 14333: loss=2.8211, lr=0.000297, tokens/sec=1374540.73, grad_norm=0.3109, duration=0.38s
Step 14334: loss=2.8231, lr=0.000297, tokens/sec=1378522.52, grad_norm=0.3151, duration=0.38s
Step 14335: loss=2.7995, lr=0.000297, tokens/sec=1375000.55, grad_norm=0.2991, duration=0.38s
Step 14336: loss=2.8113, lr=0.000297, tokens/sec=1375763.58, grad_norm=0.3011, duration=0.38s
Step 14337: loss=2.8241, lr=0.000297, tokens/sec=1375702.47, grad_norm=0.2870, duration=0.38s
Step 14338: loss=2.7762, lr=0.000297, tokens/sec=1376487.82, grad_norm=0.3055, duration=0.38s
Step 14339: loss=2.8074, lr=0.000297, tokens/sec=1375647.39, grad_norm=0.3159, duration=0.38s
Step 14340: loss=2.7707, lr=0.000297, tokens/sec=1374952.41, grad_norm=0.3216, duration=0.38s
Step 14341: loss=2.8299, lr=0.000297, tokens/sec=1375169.94, grad_norm=0.2951, duration=0.38s
Step 14342: loss=2.8196, lr=0.000297, tokens/sec=1373095.39, grad_norm=0.2847, duration=0.38s
Step 14343: loss=2.8430, lr=0.000297, tokens/sec=1377817.72, grad_norm=0.3046, duration=0.38s
Step 14344: loss=2.8749, lr=0.000297, tokens/sec=1375364.32, grad_norm=0.3279, duration=0.38s
Step 14345: loss=2.8680, lr=0.000297, tokens/sec=1370958.71, grad_norm=0.3209, duration=0.38s
Step 14346: loss=2.9045, lr=0.000297, tokens/sec=1376747.21, grad_norm=0.3157, duration=0.38s
Step 14347: loss=2.9026, lr=0.000297, tokens/sec=1377002.40, grad_norm=0.2978, duration=0.38s
Step 14348: loss=2.8146, lr=0.000297, tokens/sec=1379160.57, grad_norm=0.3219, duration=0.38s
Step 14349: loss=2.8547, lr=0.000297, tokens/sec=1373648.62, grad_norm=0.3404, duration=0.38s
Step 14350: loss=2.8313, lr=0.000297, tokens/sec=1375782.51, grad_norm=0.3252, duration=0.38s
Step 14351: loss=2.8862, lr=0.000297, tokens/sec=1374299.35, grad_norm=0.3000, duration=0.38s
Step 14352: loss=2.8944, lr=0.000297, tokens/sec=1377643.36, grad_norm=0.3202, duration=0.38s
Step 14353: loss=2.8632, lr=0.000297, tokens/sec=1376777.38, grad_norm=0.3184, duration=0.38s
Step 14354: loss=2.8469, lr=0.000297, tokens/sec=1376793.76, grad_norm=0.3173, duration=0.38s
Step 14355: loss=2.8309, lr=0.000297, tokens/sec=1381420.97, grad_norm=0.3187, duration=0.38s
Step 14356: loss=2.8427, lr=0.000297, tokens/sec=1376067.48, grad_norm=0.3367, duration=0.38s
Step 14357: loss=2.8535, lr=0.000297, tokens/sec=1377303.39, grad_norm=0.3186, duration=0.38s
Step 14358: loss=2.8578, lr=0.000297, tokens/sec=1378180.39, grad_norm=0.2942, duration=0.38s
Step 14359: loss=2.7780, lr=0.000297, tokens/sec=1377954.99, grad_norm=0.3066, duration=0.38s
Step 14360: loss=2.8519, lr=0.000297, tokens/sec=1374679.94, grad_norm=0.3304, duration=0.38s
Step 14361: loss=2.8810, lr=0.000297, tokens/sec=1376426.65, grad_norm=0.3285, duration=0.38s
Step 14362: loss=2.8984, lr=0.000297, tokens/sec=1378452.52, grad_norm=0.3022, duration=0.38s
Step 14363: loss=2.8222, lr=0.000297, tokens/sec=1377488.88, grad_norm=0.3082, duration=0.38s
Step 14364: loss=2.8772, lr=0.000297, tokens/sec=1375125.23, grad_norm=0.3376, duration=0.38s
Step 14365: loss=2.8399, lr=0.000297, tokens/sec=1374563.07, grad_norm=0.3294, duration=0.38s
Step 14366: loss=2.8789, lr=0.000297, tokens/sec=1376937.73, grad_norm=0.3219, duration=0.38s
Step 14367: loss=2.8720, lr=0.000297, tokens/sec=1378296.14, grad_norm=0.3083, duration=0.38s
Step 14368: loss=2.8515, lr=0.000297, tokens/sec=1377273.20, grad_norm=0.3108, duration=0.38s
Step 14369: loss=2.8593, lr=0.000297, tokens/sec=1373103.11, grad_norm=0.3239, duration=0.38s
Step 14370: loss=2.8628, lr=0.000297, tokens/sec=1376474.03, grad_norm=0.3145, duration=0.38s
Step 14371: loss=2.8887, lr=0.000297, tokens/sec=1376405.97, grad_norm=0.3178, duration=0.38s
Step 14372: loss=2.8999, lr=0.000297, tokens/sec=1379904.84, grad_norm=0.3104, duration=0.38s
Step 14373: loss=2.8593, lr=0.000296, tokens/sec=1374648.14, grad_norm=0.3176, duration=0.38s
Step 14374: loss=2.8782, lr=0.000296, tokens/sec=1380707.13, grad_norm=0.3380, duration=0.38s
Step 14375: loss=2.9052, lr=0.000296, tokens/sec=1368014.59, grad_norm=0.3097, duration=0.38s
Step 14376: loss=2.8279, lr=0.000296, tokens/sec=1375578.55, grad_norm=0.3044, duration=0.38s
Step 14377: loss=2.8673, lr=0.000296, tokens/sec=1375225.84, grad_norm=0.3196, duration=0.38s
Step 14378: loss=2.8635, lr=0.000296, tokens/sec=1376701.53, grad_norm=0.3032, duration=0.38s
Step 14379: loss=2.8260, lr=0.000296, tokens/sec=1370278.70, grad_norm=0.3033, duration=0.38s
Step 14380: loss=2.8141, lr=0.000296, tokens/sec=1373687.24, grad_norm=0.3103, duration=0.38s
Step 14381: loss=2.7551, lr=0.000296, tokens/sec=1376480.06, grad_norm=0.2966, duration=0.38s
Step 14382: loss=2.7421, lr=0.000296, tokens/sec=1373478.75, grad_norm=0.3249, duration=0.38s
Step 14383: loss=2.8072, lr=0.000296, tokens/sec=1374902.55, grad_norm=0.3231, duration=0.38s
Step 14384: loss=2.8707, lr=0.000296, tokens/sec=1375103.73, grad_norm=0.3114, duration=0.38s
Step 14385: loss=2.8172, lr=0.000296, tokens/sec=1379540.39, grad_norm=0.3091, duration=0.38s
Step 14386: loss=2.8152, lr=0.000296, tokens/sec=1380736.61, grad_norm=0.3084, duration=0.38s
Step 14387: loss=2.7817, lr=0.000296, tokens/sec=1378629.68, grad_norm=0.3198, duration=0.38s
Step 14388: loss=2.8174, lr=0.000296, tokens/sec=1374885.35, grad_norm=0.3251, duration=0.38s
Step 14389: loss=2.8387, lr=0.000296, tokens/sec=1377112.78, grad_norm=0.3120, duration=0.38s
Step 14390: loss=2.8614, lr=0.000296, tokens/sec=1375723.13, grad_norm=0.3149, duration=0.38s
Step 14391: loss=2.9093, lr=0.000296, tokens/sec=1373972.19, grad_norm=0.3217, duration=0.38s
Step 14392: loss=2.8795, lr=0.000296, tokens/sec=1371385.34, grad_norm=0.3416, duration=0.38s
Step 14393: loss=2.8552, lr=0.000296, tokens/sec=1374349.16, grad_norm=0.3186, duration=0.38s
Step 14394: loss=2.8965, lr=0.000296, tokens/sec=1375270.57, grad_norm=0.3246, duration=0.38s
Step 14395: loss=2.8564, lr=0.000296, tokens/sec=1375030.64, grad_norm=0.3711, duration=0.38s
Step 14396: loss=2.8940, lr=0.000296, tokens/sec=1375331.64, grad_norm=0.3475, duration=0.38s
Step 14397: loss=2.9070, lr=0.000296, tokens/sec=1378077.61, grad_norm=0.3054, duration=0.38s
Step 14398: loss=2.9754, lr=0.000296, tokens/sec=1376423.20, grad_norm=0.3322, duration=0.38s
Step 14399: loss=2.8901, lr=0.000296, tokens/sec=1375549.29, grad_norm=0.3913, duration=0.38s
Step 14400/19073 (75.5%), Elapsed time: 5701.13s, Steps per hour: 9092.93, Estimated hours remaining: 0.51
Step 14400: loss=2.9030, lr=0.000296, tokens/sec=1380551.97, grad_norm=0.3740, duration=0.38s
Step 14401: loss=2.8274, lr=0.000296, tokens/sec=1371489.69, grad_norm=0.3074, duration=0.38s
Step 14402: loss=2.8848, lr=0.000296, tokens/sec=1374746.97, grad_norm=0.3164, duration=0.38s
Step 14403: loss=2.8403, lr=0.000296, tokens/sec=1378424.01, grad_norm=0.3879, duration=0.38s
Step 14404: loss=2.7880, lr=0.000296, tokens/sec=1378345.39, grad_norm=0.3799, duration=0.38s
Step 14405: loss=2.9168, lr=0.000296, tokens/sec=1374092.39, grad_norm=0.3385, duration=0.38s
Step 14406: loss=2.8156, lr=0.000296, tokens/sec=1376555.03, grad_norm=0.2953, duration=0.38s
Step 14407: loss=2.8351, lr=0.000296, tokens/sec=1378685.86, grad_norm=0.3400, duration=0.38s
Step 14408: loss=2.9110, lr=0.000296, tokens/sec=1375935.74, grad_norm=0.3828, duration=0.38s
Step 14409: loss=2.8596, lr=0.000296, tokens/sec=1375872.90, grad_norm=0.3552, duration=0.38s
Step 14410: loss=2.8442, lr=0.000296, tokens/sec=1378265.04, grad_norm=0.2969, duration=0.38s
Step 14411: loss=2.8849, lr=0.000296, tokens/sec=1376084.70, grad_norm=0.3235, duration=0.38s
Step 14412: loss=2.9175, lr=0.000296, tokens/sec=1375398.73, grad_norm=0.3670, duration=0.38s
Step 14413: loss=2.8650, lr=0.000296, tokens/sec=1368378.93, grad_norm=0.3600, duration=0.38s
Step 14414: loss=2.8031, lr=0.000296, tokens/sec=1374786.50, grad_norm=0.3212, duration=0.38s
Step 14415: loss=2.8911, lr=0.000296, tokens/sec=1373223.15, grad_norm=0.3108, duration=0.38s
Step 14416: loss=2.8863, lr=0.000296, tokens/sec=1376877.38, grad_norm=0.3335, duration=0.38s
Step 14417: loss=2.8907, lr=0.000295, tokens/sec=1378705.75, grad_norm=0.3416, duration=0.38s
Step 14418: loss=2.9181, lr=0.000295, tokens/sec=1377817.72, grad_norm=0.3510, duration=0.38s
Step 14419: loss=2.8370, lr=0.000295, tokens/sec=1378976.35, grad_norm=0.3363, duration=0.38s
Step 14420: loss=2.8522, lr=0.000295, tokens/sec=1374539.02, grad_norm=0.3292, duration=0.38s
Step 14421: loss=2.8581, lr=0.000295, tokens/sec=1378963.38, grad_norm=0.3257, duration=0.38s
Step 14422: loss=2.7972, lr=0.000295, tokens/sec=1373368.95, grad_norm=0.3426, duration=0.38s
Step 14423: loss=2.8437, lr=0.000295, tokens/sec=1376196.65, grad_norm=0.3701, duration=0.38s
Step 14424: loss=2.8131, lr=0.000295, tokens/sec=1374357.75, grad_norm=0.3254, duration=0.38s
Step 14425: loss=2.7936, lr=0.000295, tokens/sec=1373157.13, grad_norm=0.3017, duration=0.38s
Step 14426: loss=2.8276, lr=0.000295, tokens/sec=1373338.93, grad_norm=0.3159, duration=0.38s
Step 14427: loss=2.8624, lr=0.000295, tokens/sec=1372226.56, grad_norm=0.3409, duration=0.38s
Step 14428: loss=2.8218, lr=0.000295, tokens/sec=1376298.29, grad_norm=0.3338, duration=0.38s
Step 14429: loss=2.8199, lr=0.000295, tokens/sec=1375475.30, grad_norm=0.2984, duration=0.38s
Step 14430: loss=2.7524, lr=0.000295, tokens/sec=1376293.98, grad_norm=0.2944, duration=0.38s
Step 14431: loss=2.8105, lr=0.000295, tokens/sec=1377232.66, grad_norm=0.3190, duration=0.38s
Step 14432: loss=2.8344, lr=0.000295, tokens/sec=1375138.99, grad_norm=0.3344, duration=0.38s
Step 14433: loss=2.7997, lr=0.000295, tokens/sec=1371680.47, grad_norm=0.3071, duration=0.38s
Step 14434: loss=2.8354, lr=0.000295, tokens/sec=1376817.04, grad_norm=0.2902, duration=0.38s
Step 14435: loss=2.8700, lr=0.000295, tokens/sec=1376047.67, grad_norm=0.3030, duration=0.38s
Step 14436: loss=2.8854, lr=0.000295, tokens/sec=1374231.50, grad_norm=0.3297, duration=0.38s
Step 14437: loss=2.8872, lr=0.000295, tokens/sec=1377882.47, grad_norm=0.3346, duration=0.38s
Step 14438: loss=2.8504, lr=0.000295, tokens/sec=1376665.33, grad_norm=0.3061, duration=0.38s
Step 14439: loss=2.8661, lr=0.000295, tokens/sec=1376113.98, grad_norm=0.2999, duration=0.38s
Step 14440: loss=2.8450, lr=0.000295, tokens/sec=1377203.33, grad_norm=0.3063, duration=0.38s
Step 14441: loss=2.9057, lr=0.000295, tokens/sec=1372664.27, grad_norm=0.3256, duration=0.38s
Step 14442: loss=2.8781, lr=0.000295, tokens/sec=1372143.51, grad_norm=0.3295, duration=0.38s
Step 14443: loss=2.9519, lr=0.000295, tokens/sec=1375313.57, grad_norm=0.3185, duration=0.38s
Step 14444: loss=2.8841, lr=0.000295, tokens/sec=1372886.22, grad_norm=0.3104, duration=0.38s
Step 14445: loss=2.9101, lr=0.000295, tokens/sec=1376147.56, grad_norm=0.3161, duration=0.38s
Step 14446: loss=2.9603, lr=0.000295, tokens/sec=1373729.29, grad_norm=0.3329, duration=0.38s
Step 14447: loss=2.8710, lr=0.000295, tokens/sec=1374173.96, grad_norm=0.2982, duration=0.38s
Step 14448: loss=2.8965, lr=0.000295, tokens/sec=1374766.74, grad_norm=0.3067, duration=0.38s
Step 14449: loss=2.8738, lr=0.000295, tokens/sec=1377226.62, grad_norm=0.3256, duration=0.38s
Step 14450: loss=2.8805, lr=0.000295, tokens/sec=1378464.62, grad_norm=0.3144, duration=0.38s
Step 14451: loss=2.8577, lr=0.000295, tokens/sec=1379208.14, grad_norm=0.2948, duration=0.38s
Step 14452: loss=2.8720, lr=0.000295, tokens/sec=1374222.05, grad_norm=0.2999, duration=0.38s
Step 14453: loss=2.7494, lr=0.000295, tokens/sec=1376590.36, grad_norm=0.3094, duration=0.38s
Step 14454: loss=2.8789, lr=0.000295, tokens/sec=1376677.40, grad_norm=0.3098, duration=0.38s
Step 14455: loss=2.8616, lr=0.000295, tokens/sec=1371702.71, grad_norm=0.3252, duration=0.38s
Step 14456: loss=2.8534, lr=0.000295, tokens/sec=1373348.36, grad_norm=0.3139, duration=0.38s
Step 14457: loss=2.9105, lr=0.000295, tokens/sec=1375668.91, grad_norm=0.3099, duration=0.38s
Step 14458: loss=2.8266, lr=0.000295, tokens/sec=1378178.66, grad_norm=0.3161, duration=0.38s
Step 14459: loss=2.7863, lr=0.000295, tokens/sec=1376817.04, grad_norm=0.3224, duration=0.38s
Step 14460: loss=2.8360, lr=0.000295, tokens/sec=1378569.18, grad_norm=0.3194, duration=0.38s
Step 14461: loss=2.8614, lr=0.000295, tokens/sec=1377596.75, grad_norm=0.2935, duration=0.38s
Step 14462: loss=2.9073, lr=0.000294, tokens/sec=1375253.37, grad_norm=0.3143, duration=0.38s
Step 14463: loss=2.8045, lr=0.000294, tokens/sec=1378198.53, grad_norm=0.3342, duration=0.38s
Step 14464: loss=2.9124, lr=0.000294, tokens/sec=1377263.71, grad_norm=0.3076, duration=0.38s
Step 14465: loss=2.8917, lr=0.000294, tokens/sec=1376807.55, grad_norm=0.3051, duration=0.38s
Step 14466: loss=2.8610, lr=0.000294, tokens/sec=1378843.20, grad_norm=0.3170, duration=0.38s
Step 14467: loss=2.8650, lr=0.000294, tokens/sec=1380071.11, grad_norm=0.3133, duration=0.38s
Step 14468: loss=2.9044, lr=0.000294, tokens/sec=1372029.64, grad_norm=0.2994, duration=0.38s
Step 14469: loss=2.8723, lr=0.000294, tokens/sec=1374880.20, grad_norm=0.3037, duration=0.38s
Step 14470: loss=2.8262, lr=0.000294, tokens/sec=1375718.82, grad_norm=0.3368, duration=0.38s
Step 14471: loss=2.8777, lr=0.000294, tokens/sec=1373229.16, grad_norm=0.3215, duration=0.38s
Step 14472: loss=2.8262, lr=0.000294, tokens/sec=1374338.86, grad_norm=0.2984, duration=0.38s
Step 14473: loss=2.8959, lr=0.000294, tokens/sec=1379130.30, grad_norm=0.2944, duration=0.38s
Step 14474: loss=2.8092, lr=0.000294, tokens/sec=1375261.97, grad_norm=0.3110, duration=0.38s
Step 14475: loss=2.7932, lr=0.000294, tokens/sec=1375548.43, grad_norm=0.3278, duration=0.38s
Step 14476: loss=2.8326, lr=0.000294, tokens/sec=1374356.03, grad_norm=0.2965, duration=0.38s
Step 14477: loss=2.7495, lr=0.000294, tokens/sec=1375872.90, grad_norm=0.2896, duration=0.38s
Step 14478: loss=2.7708, lr=0.000294, tokens/sec=1375987.40, grad_norm=0.3244, duration=0.38s
Step 14479: loss=2.7740, lr=0.000294, tokens/sec=1374484.03, grad_norm=0.3049, duration=0.38s
Step 14480: loss=2.8080, lr=0.000294, tokens/sec=1376278.48, grad_norm=0.2979, duration=0.38s
Step 14481: loss=2.8244, lr=0.000294, tokens/sec=1374694.55, grad_norm=0.3050, duration=0.38s
Step 14482: loss=2.8260, lr=0.000294, tokens/sec=1374910.28, grad_norm=0.3074, duration=0.38s
Step 14483: loss=2.8302, lr=0.000294, tokens/sec=1379027.38, grad_norm=0.3040, duration=0.38s
Step 14484: loss=2.8296, lr=0.000294, tokens/sec=1376900.66, grad_norm=0.3072, duration=0.38s
Step 14485: loss=2.7831, lr=0.000294, tokens/sec=1376647.24, grad_norm=0.3138, duration=0.38s
Step 14486: loss=2.8007, lr=0.000294, tokens/sec=1373937.85, grad_norm=0.3193, duration=0.38s
Step 14487: loss=2.8782, lr=0.000294, tokens/sec=1375206.06, grad_norm=0.3159, duration=0.38s
Step 14488: loss=2.9123, lr=0.000294, tokens/sec=1374551.04, grad_norm=0.3045, duration=0.38s
Step 14489: loss=2.8536, lr=0.000294, tokens/sec=1376321.55, grad_norm=0.3205, duration=0.38s
Step 14490: loss=2.8668, lr=0.000294, tokens/sec=1375776.49, grad_norm=0.3398, duration=0.38s
Step 14491: loss=2.9023, lr=0.000294, tokens/sec=1377319.78, grad_norm=0.3348, duration=0.38s
Step 14492: loss=2.8694, lr=0.000294, tokens/sec=1379619.15, grad_norm=0.2934, duration=0.38s
Step 14493: loss=2.8632, lr=0.000294, tokens/sec=1377056.72, grad_norm=0.3238, duration=0.38s
Step 14494: loss=2.8432, lr=0.000294, tokens/sec=1377198.16, grad_norm=0.3563, duration=0.38s
Step 14495: loss=2.8263, lr=0.000294, tokens/sec=1374735.80, grad_norm=0.3493, duration=0.38s
Step 14496: loss=2.9167, lr=0.000294, tokens/sec=1376748.94, grad_norm=0.3088, duration=0.38s
Step 14497: loss=2.8861, lr=0.000294, tokens/sec=1378128.57, grad_norm=0.3315, duration=0.38s
Step 14498: loss=2.8426, lr=0.000294, tokens/sec=1378827.64, grad_norm=0.3734, duration=0.38s
Step 14499: loss=2.8478, lr=0.000294, tokens/sec=1378387.72, grad_norm=0.3649, duration=0.38s
Step 14500/19073 (76.0%), Elapsed time: 5739.32s, Steps per hour: 9095.15, Estimated hours remaining: 0.50
Validation loss at step 14500: 3.816091299057007
Step 14500: loss=2.8144, lr=0.000294, tokens/sec=155595.29, grad_norm=0.3162, duration=3.37s
Step 14501: loss=2.7788, lr=0.000294, tokens/sec=1379769.77, grad_norm=0.2998, duration=0.38s
Step 14502: loss=2.8491, lr=0.000294, tokens/sec=1375489.93, grad_norm=0.3488, duration=0.38s
Step 14503: loss=2.8774, lr=0.000294, tokens/sec=1375723.99, grad_norm=0.3685, duration=0.38s
Step 14504: loss=2.9003, lr=0.000294, tokens/sec=1377881.60, grad_norm=0.3439, duration=0.38s
Step 14505: loss=2.9221, lr=0.000294, tokens/sec=1373205.15, grad_norm=0.3200, duration=0.38s
Step 14506: loss=2.8316, lr=0.000293, tokens/sec=1375404.76, grad_norm=0.3220, duration=0.38s
Step 14507: loss=2.9427, lr=0.000293, tokens/sec=1380546.77, grad_norm=0.3636, duration=0.38s
Step 14508: loss=2.8795, lr=0.000293, tokens/sec=1381577.19, grad_norm=0.3491, duration=0.38s
Step 14509: loss=2.8605, lr=0.000293, tokens/sec=1368702.58, grad_norm=0.3246, duration=0.38s
Step 14510: loss=2.8876, lr=0.000293, tokens/sec=1374354.32, grad_norm=0.3032, duration=0.38s
Step 14511: loss=2.8766, lr=0.000293, tokens/sec=1373533.65, grad_norm=0.3338, duration=0.38s
Step 14512: loss=2.8507, lr=0.000293, tokens/sec=1372421.82, grad_norm=0.3395, duration=0.38s
Step 14513: loss=2.8517, lr=0.000293, tokens/sec=1373568.83, grad_norm=0.3387, duration=0.38s
Step 14514: loss=2.8484, lr=0.000293, tokens/sec=1377740.89, grad_norm=0.3180, duration=0.38s
Step 14515: loss=2.8675, lr=0.000293, tokens/sec=1377391.39, grad_norm=0.3183, duration=0.38s
Step 14516: loss=2.8693, lr=0.000293, tokens/sec=1370270.16, grad_norm=0.3140, duration=0.38s
Step 14517: loss=2.8486, lr=0.000293, tokens/sec=1372608.58, grad_norm=0.3361, duration=0.38s
Step 14518: loss=2.8585, lr=0.000293, tokens/sec=1378996.24, grad_norm=0.3134, duration=0.38s
Step 14519: loss=2.8424, lr=0.000293, tokens/sec=1376597.25, grad_norm=0.3026, duration=0.38s
Step 14520: loss=2.8671, lr=0.000293, tokens/sec=1375544.99, grad_norm=0.3232, duration=0.38s
Step 14521: loss=2.7849, lr=0.000293, tokens/sec=1376663.61, grad_norm=0.3394, duration=0.38s
Step 14522: loss=2.8692, lr=0.000293, tokens/sec=1371465.74, grad_norm=0.3135, duration=0.38s
Step 14523: loss=2.8096, lr=0.000293, tokens/sec=1372534.04, grad_norm=0.3003, duration=0.38s
Step 14524: loss=2.8451, lr=0.000293, tokens/sec=1373453.01, grad_norm=0.3107, duration=0.38s
Step 14525: loss=2.7777, lr=0.000293, tokens/sec=1372488.64, grad_norm=0.3136, duration=0.38s
Step 14526: loss=2.8036, lr=0.000293, tokens/sec=1373091.11, grad_norm=0.3148, duration=0.38s
Step 14527: loss=2.8216, lr=0.000293, tokens/sec=1376712.74, grad_norm=0.3077, duration=0.38s
Step 14528: loss=2.7775, lr=0.000293, tokens/sec=1375889.25, grad_norm=0.2985, duration=0.38s
Step 14529: loss=2.7858, lr=0.000293, tokens/sec=1374417.02, grad_norm=0.3032, duration=0.38s
Step 14530: loss=2.7730, lr=0.000293, tokens/sec=1374987.65, grad_norm=0.3195, duration=0.38s
Step 14531: loss=2.8346, lr=0.000293, tokens/sec=1375788.54, grad_norm=0.3193, duration=0.38s
Step 14532: loss=2.8175, lr=0.000293, tokens/sec=1375022.90, grad_norm=0.2964, duration=0.38s
Step 14533: loss=2.8261, lr=0.000293, tokens/sec=1369692.35, grad_norm=0.2958, duration=0.38s
Step 14534: loss=2.8627, lr=0.000293, tokens/sec=1374622.36, grad_norm=0.3133, duration=0.38s
Step 14535: loss=2.8951, lr=0.000293, tokens/sec=1374868.16, grad_norm=0.3347, duration=0.38s
Step 14536: loss=2.8989, lr=0.000293, tokens/sec=1376986.01, grad_norm=0.3445, duration=0.38s
Step 14537: loss=2.8739, lr=0.000293, tokens/sec=1375254.23, grad_norm=0.3001, duration=0.38s
Step 14538: loss=2.8018, lr=0.000293, tokens/sec=1370850.17, grad_norm=0.3319, duration=0.38s
Step 14539: loss=2.8604, lr=0.000293, tokens/sec=1371298.11, grad_norm=0.3333, duration=0.38s
Step 14540: loss=2.8201, lr=0.000293, tokens/sec=1374734.08, grad_norm=0.3294, duration=0.38s
Step 14541: loss=2.8989, lr=0.000293, tokens/sec=1371852.47, grad_norm=0.3120, duration=0.38s
Step 14542: loss=2.8741, lr=0.000293, tokens/sec=1374535.58, grad_norm=0.3046, duration=0.38s
Step 14543: loss=2.8408, lr=0.000293, tokens/sec=1373854.59, grad_norm=0.3280, duration=0.38s
Step 14544: loss=2.8584, lr=0.000293, tokens/sec=1375187.14, grad_norm=0.3192, duration=0.38s
Step 14545: loss=2.8266, lr=0.000293, tokens/sec=1377458.68, grad_norm=0.3131, duration=0.38s
Step 14546: loss=2.8441, lr=0.000293, tokens/sec=1378201.12, grad_norm=0.3131, duration=0.38s
Step 14547: loss=2.8531, lr=0.000293, tokens/sec=1377338.76, grad_norm=0.3160, duration=0.38s
Step 14548: loss=2.8285, lr=0.000293, tokens/sec=1377463.00, grad_norm=0.3116, duration=0.38s
Step 14549: loss=2.7929, lr=0.000293, tokens/sec=1379742.94, grad_norm=0.3046, duration=0.38s
Step 14550: loss=2.8862, lr=0.000293, tokens/sec=1375236.16, grad_norm=0.2940, duration=0.38s
Step 14551: loss=2.8809, lr=0.000293, tokens/sec=1373880.34, grad_norm=0.3249, duration=0.38s
Step 14552: loss=2.8810, lr=0.000292, tokens/sec=1380232.23, grad_norm=0.3253, duration=0.38s
Step 14553: loss=2.8043, lr=0.000292, tokens/sec=1377744.34, grad_norm=0.2979, duration=0.38s
Step 14554: loss=2.8882, lr=0.000292, tokens/sec=1374791.66, grad_norm=0.3140, duration=0.38s
Step 14555: loss=2.8222, lr=0.000292, tokens/sec=1373646.05, grad_norm=0.3234, duration=0.38s
Step 14556: loss=2.8982, lr=0.000292, tokens/sec=1377811.67, grad_norm=0.3268, duration=0.38s
Step 14557: loss=2.8288, lr=0.000292, tokens/sec=1378499.19, grad_norm=0.3078, duration=0.38s
Step 14558: loss=2.8740, lr=0.000292, tokens/sec=1375955.54, grad_norm=0.3180, duration=0.38s
Step 14559: loss=2.8497, lr=0.000292, tokens/sec=1374954.98, grad_norm=0.3152, duration=0.38s
Step 14560: loss=2.8864, lr=0.000292, tokens/sec=1378141.53, grad_norm=0.3241, duration=0.38s
Step 14561: loss=2.8951, lr=0.000292, tokens/sec=1379228.04, grad_norm=0.3383, duration=0.38s
Step 14562: loss=2.8752, lr=0.000292, tokens/sec=1375384.11, grad_norm=0.3265, duration=0.38s
Step 14563: loss=2.8527, lr=0.000292, tokens/sec=1375319.59, grad_norm=0.3205, duration=0.38s
Step 14564: loss=2.9000, lr=0.000292, tokens/sec=1373615.16, grad_norm=0.3353, duration=0.38s
Step 14565: loss=2.8357, lr=0.000292, tokens/sec=1377093.80, grad_norm=0.3261, duration=0.38s
Step 14566: loss=2.8683, lr=0.000292, tokens/sec=1377865.20, grad_norm=0.3107, duration=0.38s
Step 14567: loss=2.8634, lr=0.000292, tokens/sec=1376905.83, grad_norm=0.3103, duration=0.38s
Step 14568: loss=2.8773, lr=0.000292, tokens/sec=1376196.65, grad_norm=0.3189, duration=0.38s
Step 14569: loss=2.7804, lr=0.000292, tokens/sec=1374597.44, grad_norm=0.3344, duration=0.38s
Step 14570: loss=2.8308, lr=0.000292, tokens/sec=1379273.89, grad_norm=0.3226, duration=0.38s
Step 14571: loss=2.6923, lr=0.000292, tokens/sec=1376104.50, grad_norm=0.3305, duration=0.38s
Step 14572: loss=2.8031, lr=0.000292, tokens/sec=1375002.27, grad_norm=0.3096, duration=0.38s
Step 14573: loss=2.8027, lr=0.000292, tokens/sec=1378567.46, grad_norm=0.3321, duration=0.38s
Step 14574: loss=2.8461, lr=0.000292, tokens/sec=1379550.78, grad_norm=0.3332, duration=0.38s
Step 14575: loss=2.8315, lr=0.000292, tokens/sec=1376101.92, grad_norm=0.3293, duration=0.38s
Step 14576: loss=2.7914, lr=0.000292, tokens/sec=1375021.19, grad_norm=0.3179, duration=0.38s
Step 14577: loss=2.7828, lr=0.000292, tokens/sec=1374483.17, grad_norm=0.3136, duration=0.38s
Step 14578: loss=2.8033, lr=0.000292, tokens/sec=1374937.79, grad_norm=0.3593, duration=0.38s
Step 14579: loss=2.8455, lr=0.000292, tokens/sec=1375224.98, grad_norm=0.3479, duration=0.38s
Step 14580: loss=2.8856, lr=0.000292, tokens/sec=1377367.23, grad_norm=0.3371, duration=0.38s
Step 14581: loss=2.8705, lr=0.000292, tokens/sec=1375928.86, grad_norm=0.3162, duration=0.38s
Step 14582: loss=2.8717, lr=0.000292, tokens/sec=1375816.08, grad_norm=0.3509, duration=0.38s
Step 14583: loss=2.8636, lr=0.000292, tokens/sec=1375188.86, grad_norm=0.3501, duration=0.38s
Step 14584: loss=2.9033, lr=0.000292, tokens/sec=1377500.10, grad_norm=0.3322, duration=0.38s
Step 14585: loss=2.8670, lr=0.000292, tokens/sec=1376885.14, grad_norm=0.3429, duration=0.38s
Step 14586: loss=2.8614, lr=0.000292, tokens/sec=1374697.12, grad_norm=0.3641, duration=0.38s
Step 14587: loss=2.9121, lr=0.000292, tokens/sec=1373666.64, grad_norm=0.3581, duration=0.38s
Step 14588: loss=2.9404, lr=0.000292, tokens/sec=1381194.51, grad_norm=0.3277, duration=0.38s
Step 14589: loss=2.9108, lr=0.000292, tokens/sec=1377357.74, grad_norm=0.3402, duration=0.38s
Step 14590: loss=2.8725, lr=0.000292, tokens/sec=1375651.70, grad_norm=0.3646, duration=0.38s
Step 14591: loss=2.8530, lr=0.000292, tokens/sec=1373714.70, grad_norm=0.3702, duration=0.38s
Step 14592: loss=2.8754, lr=0.000292, tokens/sec=1375824.69, grad_norm=0.3198, duration=0.38s
Step 14593: loss=2.8172, lr=0.000292, tokens/sec=1376409.42, grad_norm=0.3130, duration=0.38s
Step 14594: loss=2.8224, lr=0.000292, tokens/sec=1377296.49, grad_norm=0.3604, duration=0.38s
Step 14595: loss=2.8994, lr=0.000292, tokens/sec=1377159.35, grad_norm=0.3707, duration=0.38s
Step 14596: loss=2.7482, lr=0.000292, tokens/sec=1371025.38, grad_norm=0.3354, duration=0.38s
Step 14597: loss=2.8829, lr=0.000291, tokens/sec=1373747.31, grad_norm=0.3053, duration=0.38s
Step 14598: loss=2.9034, lr=0.000291, tokens/sec=1375816.08, grad_norm=0.3434, duration=0.38s
Step 14599: loss=2.8845, lr=0.000291, tokens/sec=1375279.17, grad_norm=0.3818, duration=0.38s
Step 14600/19073 (76.5%), Elapsed time: 5780.52s, Steps per hour: 9092.61, Estimated hours remaining: 0.49
Step 14600: loss=2.8282, lr=0.000291, tokens/sec=1373086.82, grad_norm=0.3465, duration=0.38s
Step 14601: loss=2.8792, lr=0.000291, tokens/sec=1374959.28, grad_norm=0.2985, duration=0.38s
Step 14602: loss=2.8917, lr=0.000291, tokens/sec=1375609.53, grad_norm=0.3048, duration=0.38s
Step 14603: loss=2.8711, lr=0.000291, tokens/sec=1372862.23, grad_norm=0.3468, duration=0.38s
Step 14604: loss=2.8336, lr=0.000291, tokens/sec=1376134.65, grad_norm=0.3622, duration=0.38s
Step 14605: loss=2.8794, lr=0.000291, tokens/sec=1376540.38, grad_norm=0.3327, duration=0.38s
Step 14606: loss=2.8666, lr=0.000291, tokens/sec=1376163.93, grad_norm=0.3042, duration=0.38s
Step 14607: loss=2.8859, lr=0.000291, tokens/sec=1376551.58, grad_norm=0.3167, duration=0.38s
Step 14608: loss=2.9091, lr=0.000291, tokens/sec=1374787.36, grad_norm=0.3348, duration=0.38s
Step 14609: loss=2.8377, lr=0.000291, tokens/sec=1374948.97, grad_norm=0.3580, duration=0.38s
Step 14610: loss=2.8571, lr=0.000291, tokens/sec=1377277.51, grad_norm=0.3449, duration=0.38s
Step 14611: loss=2.8311, lr=0.000291, tokens/sec=1376237.99, grad_norm=0.3131, duration=0.38s
Step 14612: loss=2.8005, lr=0.000291, tokens/sec=1378139.80, grad_norm=0.2972, duration=0.38s
Step 14613: loss=2.8420, lr=0.000291, tokens/sec=1378247.77, grad_norm=0.3417, duration=0.38s
Step 14614: loss=2.8048, lr=0.000291, tokens/sec=1375358.30, grad_norm=0.3702, duration=0.38s
Step 14615: loss=2.8207, lr=0.000291, tokens/sec=1378604.62, grad_norm=0.3410, duration=0.38s
Step 14616: loss=2.7972, lr=0.000291, tokens/sec=1377744.34, grad_norm=0.2905, duration=0.38s
Step 14617: loss=2.8645, lr=0.000291, tokens/sec=1377017.92, grad_norm=0.2974, duration=0.38s
Step 14618: loss=2.8336, lr=0.000291, tokens/sec=1378474.13, grad_norm=0.3287, duration=0.38s
Step 14619: loss=2.7753, lr=0.000291, tokens/sec=1376343.08, grad_norm=0.3377, duration=0.38s
Step 14620: loss=2.7816, lr=0.000291, tokens/sec=1373253.17, grad_norm=0.3144, duration=0.38s
Step 14621: loss=2.7950, lr=0.000291, tokens/sec=1375157.90, grad_norm=0.2953, duration=0.38s
Step 14622: loss=2.8175, lr=0.000291, tokens/sec=1379416.65, grad_norm=0.3081, duration=0.38s
Step 14623: loss=2.8149, lr=0.000291, tokens/sec=1377123.99, grad_norm=0.3131, duration=0.38s
Step 14624: loss=2.8222, lr=0.000291, tokens/sec=1376012.37, grad_norm=0.3203, duration=0.38s
Step 14625: loss=2.8804, lr=0.000291, tokens/sec=1379181.33, grad_norm=0.3146, duration=0.38s
Step 14626: loss=2.8802, lr=0.000291, tokens/sec=1379095.70, grad_norm=0.3030, duration=0.38s
Step 14627: loss=2.8744, lr=0.000291, tokens/sec=1375892.70, grad_norm=0.3015, duration=0.38s
Step 14628: loss=2.8373, lr=0.000291, tokens/sec=1377315.47, grad_norm=0.3178, duration=0.38s
Step 14629: loss=2.8694, lr=0.000291, tokens/sec=1378464.62, grad_norm=0.3408, duration=0.38s
Step 14630: loss=2.8679, lr=0.000291, tokens/sec=1374268.43, grad_norm=0.3085, duration=0.38s
Step 14631: loss=2.8676, lr=0.000291, tokens/sec=1377509.59, grad_norm=0.3015, duration=0.38s
Step 14632: loss=2.8771, lr=0.000291, tokens/sec=1378432.65, grad_norm=0.3061, duration=0.38s
Step 14633: loss=2.9425, lr=0.000291, tokens/sec=1378595.11, grad_norm=0.3314, duration=0.38s
Step 14634: loss=2.8875, lr=0.000291, tokens/sec=1373444.43, grad_norm=0.3075, duration=0.38s
Step 14635: loss=2.9116, lr=0.000291, tokens/sec=1375633.62, grad_norm=0.2947, duration=0.38s
Step 14636: loss=2.9265, lr=0.000291, tokens/sec=1376282.78, grad_norm=0.3103, duration=0.38s
Step 14637: loss=2.8791, lr=0.000291, tokens/sec=1378206.30, grad_norm=0.3099, duration=0.38s
Step 14638: loss=2.8893, lr=0.000291, tokens/sec=1372881.94, grad_norm=0.2973, duration=0.38s
Step 14639: loss=2.8625, lr=0.000291, tokens/sec=1373876.05, grad_norm=0.2903, duration=0.38s
Step 14640: loss=2.8845, lr=0.000291, tokens/sec=1373898.37, grad_norm=0.3041, duration=0.38s
Step 14641: loss=2.8581, lr=0.000291, tokens/sec=1375964.15, grad_norm=0.2934, duration=0.38s
Step 14642: loss=2.8006, lr=0.000291, tokens/sec=1371962.02, grad_norm=0.2959, duration=0.38s
Step 14643: loss=2.8030, lr=0.000290, tokens/sec=1372834.80, grad_norm=0.2943, duration=0.38s
Step 14644: loss=2.8730, lr=0.000290, tokens/sec=1370926.23, grad_norm=0.2891, duration=0.38s
Step 14645: loss=2.8917, lr=0.000290, tokens/sec=1375996.87, grad_norm=0.3210, duration=0.38s
Step 14646: loss=2.8399, lr=0.000290, tokens/sec=1375902.17, grad_norm=0.3117, duration=0.38s
Step 14647: loss=2.9081, lr=0.000290, tokens/sec=1373542.23, grad_norm=0.3149, duration=0.38s
Step 14648: loss=2.8140, lr=0.000290, tokens/sec=1375383.25, grad_norm=0.2909, duration=0.38s
Step 14649: loss=2.7832, lr=0.000290, tokens/sec=1376236.27, grad_norm=0.3028, duration=0.38s
Step 14650: loss=2.8137, lr=0.000290, tokens/sec=1375403.03, grad_norm=0.3178, duration=0.38s
Step 14651: loss=2.8310, lr=0.000290, tokens/sec=1370756.17, grad_norm=0.3085, duration=0.38s
Step 14652: loss=2.9521, lr=0.000290, tokens/sec=1376520.56, grad_norm=0.2981, duration=0.38s
Step 14653: loss=2.7977, lr=0.000290, tokens/sec=1375977.93, grad_norm=0.3000, duration=0.38s
Step 14654: loss=2.8986, lr=0.000290, tokens/sec=1374283.03, grad_norm=0.3096, duration=0.38s
Step 14655: loss=2.9159, lr=0.000290, tokens/sec=1376842.90, grad_norm=0.2990, duration=0.38s
Step 14656: loss=2.8239, lr=0.000290, tokens/sec=1374989.37, grad_norm=0.2910, duration=0.38s
Step 14657: loss=2.8819, lr=0.000290, tokens/sec=1375735.18, grad_norm=0.2987, duration=0.38s
Step 14658: loss=2.8909, lr=0.000290, tokens/sec=1381110.36, grad_norm=0.3009, duration=0.38s
Step 14659: loss=2.8584, lr=0.000290, tokens/sec=1376993.77, grad_norm=0.2888, duration=0.38s
Step 14660: loss=2.8406, lr=0.000290, tokens/sec=1373271.18, grad_norm=0.3084, duration=0.38s
Step 14661: loss=2.8201, lr=0.000290, tokens/sec=1376620.52, grad_norm=0.3238, duration=0.38s
Step 14662: loss=2.8531, lr=0.000290, tokens/sec=1376348.25, grad_norm=0.3016, duration=0.38s
Step 14663: loss=2.8590, lr=0.000290, tokens/sec=1372478.36, grad_norm=0.2924, duration=0.38s
Step 14664: loss=2.8418, lr=0.000290, tokens/sec=1374508.09, grad_norm=0.3068, duration=0.38s
Step 14665: loss=2.7852, lr=0.000290, tokens/sec=1375484.76, grad_norm=0.3160, duration=0.38s
Step 14666: loss=2.7950, lr=0.000290, tokens/sec=1376729.11, grad_norm=0.2928, duration=0.38s
Step 14667: loss=2.7729, lr=0.000290, tokens/sec=1377155.90, grad_norm=0.2802, duration=0.38s
Step 14668: loss=2.7557, lr=0.000290, tokens/sec=1376132.92, grad_norm=0.2997, duration=0.38s
Step 14669: loss=2.7884, lr=0.000290, tokens/sec=1375280.03, grad_norm=0.3034, duration=0.38s
Step 14670: loss=2.8177, lr=0.000290, tokens/sec=1377255.09, grad_norm=0.3002, duration=0.38s
Step 14671: loss=2.8204, lr=0.000290, tokens/sec=1376467.14, grad_norm=0.2933, duration=0.38s
Step 14672: loss=2.8096, lr=0.000290, tokens/sec=1376131.20, grad_norm=0.3018, duration=0.38s
Step 14673: loss=2.8500, lr=0.000290, tokens/sec=1378466.35, grad_norm=0.2965, duration=0.38s
Step 14674: loss=2.7832, lr=0.000290, tokens/sec=1376326.71, grad_norm=0.2852, duration=0.38s
Step 14675: loss=2.7850, lr=0.000290, tokens/sec=1377237.83, grad_norm=0.2974, duration=0.38s
Step 14676: loss=2.8108, lr=0.000290, tokens/sec=1373459.02, grad_norm=0.3054, duration=0.38s
Step 14677: loss=2.8728, lr=0.000290, tokens/sec=1373982.49, grad_norm=0.3112, duration=0.38s
Step 14678: loss=2.8917, lr=0.000290, tokens/sec=1376804.97, grad_norm=0.2995, duration=0.38s
Step 14679: loss=2.8672, lr=0.000290, tokens/sec=1374645.56, grad_norm=0.3043, duration=0.38s
Step 14680: loss=2.8744, lr=0.000290, tokens/sec=1376282.78, grad_norm=0.3402, duration=0.38s
Step 14681: loss=2.8714, lr=0.000290, tokens/sec=1373852.87, grad_norm=0.3317, duration=0.38s
Step 14682: loss=2.8585, lr=0.000290, tokens/sec=1374727.20, grad_norm=0.3016, duration=0.38s
Step 14683: loss=2.8732, lr=0.000290, tokens/sec=1376324.13, grad_norm=0.3058, duration=0.38s
Step 14684: loss=2.8302, lr=0.000290, tokens/sec=1376896.35, grad_norm=0.3397, duration=0.38s
Step 14685: loss=2.8871, lr=0.000290, tokens/sec=1373525.07, grad_norm=0.3571, duration=0.38s
Step 14686: loss=2.9020, lr=0.000290, tokens/sec=1375041.82, grad_norm=0.3318, duration=0.38s
Step 14687: loss=2.8418, lr=0.000290, tokens/sec=1372923.94, grad_norm=0.3139, duration=0.38s
Step 14688: loss=2.8429, lr=0.000290, tokens/sec=1376022.70, grad_norm=0.3318, duration=0.38s
Step 14689: loss=2.8093, lr=0.000290, tokens/sec=1373194.86, grad_norm=0.3531, duration=0.38s
Step 14690: loss=2.8531, lr=0.000289, tokens/sec=1378157.07, grad_norm=0.3322, duration=0.38s
Step 14691: loss=2.7890, lr=0.000289, tokens/sec=1377296.49, grad_norm=0.3098, duration=0.38s
Step 14692: loss=2.8316, lr=0.000289, tokens/sec=1374104.41, grad_norm=0.3222, duration=0.38s
Step 14693: loss=2.9061, lr=0.000289, tokens/sec=1376213.02, grad_norm=0.3369, duration=0.38s
Step 14694: loss=2.8808, lr=0.000289, tokens/sec=1377577.77, grad_norm=0.3543, duration=0.38s
Step 14695: loss=2.8881, lr=0.000289, tokens/sec=1373610.01, grad_norm=0.3252, duration=0.38s
Step 14696: loss=2.8633, lr=0.000289, tokens/sec=1376072.64, grad_norm=0.3072, duration=0.38s
Step 14697: loss=2.9077, lr=0.000289, tokens/sec=1378265.91, grad_norm=0.3455, duration=0.38s
Step 14698: loss=2.8707, lr=0.000289, tokens/sec=1375853.96, grad_norm=0.3662, duration=0.38s
Step 14699: loss=2.8833, lr=0.000289, tokens/sec=1378087.11, grad_norm=0.3487, duration=0.38s
Step 14700/19073 (77.1%), Elapsed time: 5818.71s, Steps per hour: 9094.80, Estimated hours remaining: 0.48
Step 14700: loss=2.8712, lr=0.000289, tokens/sec=1373139.12, grad_norm=0.3137, duration=0.38s
Step 14701: loss=2.8701, lr=0.000289, tokens/sec=1377750.38, grad_norm=0.3102, duration=0.38s
Step 14702: loss=2.8279, lr=0.000289, tokens/sec=1376318.96, grad_norm=0.3411, duration=0.38s
Step 14703: loss=2.8456, lr=0.000289, tokens/sec=1374302.78, grad_norm=0.3458, duration=0.38s
Step 14704: loss=2.8674, lr=0.000289, tokens/sec=1379416.65, grad_norm=0.3219, duration=0.38s
Step 14705: loss=2.8589, lr=0.000289, tokens/sec=1377379.31, grad_norm=0.3121, duration=0.38s
Step 14706: loss=2.8849, lr=0.000289, tokens/sec=1377611.42, grad_norm=0.3124, duration=0.38s
Step 14707: loss=2.8176, lr=0.000289, tokens/sec=1374924.90, grad_norm=0.3287, duration=0.38s
Step 14708: loss=2.8523, lr=0.000289, tokens/sec=1376875.66, grad_norm=0.3357, duration=0.38s
Step 14709: loss=2.8524, lr=0.000289, tokens/sec=1376457.66, grad_norm=0.3285, duration=0.38s
Step 14710: loss=2.8506, lr=0.000289, tokens/sec=1374947.25, grad_norm=0.3151, duration=0.38s
Step 14711: loss=2.8030, lr=0.000289, tokens/sec=1377692.55, grad_norm=0.3058, duration=0.38s
Step 14712: loss=2.8592, lr=0.000289, tokens/sec=1376318.10, grad_norm=0.3243, duration=0.38s
Step 14713: loss=2.8343, lr=0.000289, tokens/sec=1377303.39, grad_norm=0.3250, duration=0.38s
Step 14714: loss=2.8202, lr=0.000289, tokens/sec=1376613.63, grad_norm=0.3048, duration=0.38s
Step 14715: loss=2.7673, lr=0.000289, tokens/sec=1373607.44, grad_norm=0.3032, duration=0.38s
Step 14716: loss=2.7987, lr=0.000289, tokens/sec=1380532.04, grad_norm=0.3068, duration=0.38s
Step 14717: loss=2.8222, lr=0.000289, tokens/sec=1374180.83, grad_norm=0.3169, duration=0.38s
Step 14718: loss=2.7589, lr=0.000289, tokens/sec=1378146.71, grad_norm=0.3238, duration=0.38s
Step 14719: loss=2.7877, lr=0.000289, tokens/sec=1379125.97, grad_norm=0.3131, duration=0.38s
Step 14720: loss=2.7779, lr=0.000289, tokens/sec=1373724.14, grad_norm=0.3248, duration=0.38s
Step 14721: loss=2.8335, lr=0.000289, tokens/sec=1371749.77, grad_norm=0.3201, duration=0.38s
Step 14722: loss=2.8054, lr=0.000289, tokens/sec=1377929.95, grad_norm=0.3118, duration=0.38s
Step 14723: loss=2.8185, lr=0.000289, tokens/sec=1375146.72, grad_norm=0.3186, duration=0.38s
Step 14724: loss=2.8885, lr=0.000289, tokens/sec=1376906.69, grad_norm=0.3314, duration=0.38s
Step 14725: loss=2.8901, lr=0.000289, tokens/sec=1380687.19, grad_norm=0.3314, duration=0.38s
Step 14726: loss=2.8683, lr=0.000289, tokens/sec=1375568.22, grad_norm=0.3266, duration=0.38s
Step 14727: loss=2.8635, lr=0.000289, tokens/sec=1376940.32, grad_norm=0.3205, duration=0.38s
Step 14728: loss=2.8105, lr=0.000289, tokens/sec=1375357.44, grad_norm=0.3259, duration=0.38s
Step 14729: loss=2.8493, lr=0.000289, tokens/sec=1377545.84, grad_norm=0.3293, duration=0.38s
Step 14730: loss=2.8344, lr=0.000289, tokens/sec=1376633.45, grad_norm=0.3246, duration=0.38s
Step 14731: loss=2.8794, lr=0.000289, tokens/sec=1377513.05, grad_norm=0.3070, duration=0.38s
Step 14732: loss=2.8526, lr=0.000289, tokens/sec=1379530.87, grad_norm=0.3036, duration=0.38s
Step 14733: loss=2.8560, lr=0.000289, tokens/sec=1377048.10, grad_norm=0.3095, duration=0.38s
Step 14734: loss=2.8560, lr=0.000289, tokens/sec=1374779.63, grad_norm=0.3025, duration=0.38s
Step 14735: loss=2.8299, lr=0.000289, tokens/sec=1375770.46, grad_norm=0.3140, duration=0.38s
Step 14736: loss=2.8413, lr=0.000288, tokens/sec=1376829.97, grad_norm=0.3069, duration=0.38s
Step 14737: loss=2.8192, lr=0.000288, tokens/sec=1373755.03, grad_norm=0.3002, duration=0.38s
Step 14738: loss=2.8427, lr=0.000288, tokens/sec=1373902.66, grad_norm=0.3171, duration=0.38s
Step 14739: loss=2.8331, lr=0.000288, tokens/sec=1376464.56, grad_norm=0.3114, duration=0.38s
Step 14740: loss=2.8876, lr=0.000288, tokens/sec=1374016.83, grad_norm=0.2890, duration=0.38s
Step 14741: loss=2.8581, lr=0.000288, tokens/sec=1379386.36, grad_norm=0.2995, duration=0.38s
Step 14742: loss=2.8618, lr=0.000288, tokens/sec=1376282.78, grad_norm=0.3282, duration=0.38s
Step 14743: loss=2.8173, lr=0.000288, tokens/sec=1372783.38, grad_norm=0.3091, duration=0.38s
Step 14744: loss=2.8691, lr=0.000288, tokens/sec=1375696.45, grad_norm=0.3007, duration=0.38s
Step 14745: loss=2.8399, lr=0.000288, tokens/sec=1377527.72, grad_norm=0.3024, duration=0.38s
Step 14746: loss=2.8533, lr=0.000288, tokens/sec=1374948.97, grad_norm=0.3240, duration=0.38s
Step 14747: loss=2.8510, lr=0.000288, tokens/sec=1375986.54, grad_norm=0.3141, duration=0.38s
Step 14748: loss=2.8659, lr=0.000288, tokens/sec=1375819.53, grad_norm=0.3171, duration=0.38s
Step 14749: loss=2.8725, lr=0.000288, tokens/sec=1376132.92, grad_norm=0.3149, duration=0.38s
Validation loss at step 14750: 3.820716142654419
Step 14750: loss=2.8903, lr=0.000288, tokens/sec=156236.88, grad_norm=0.3064, duration=3.36s
Step 14751: loss=2.8697, lr=0.000288, tokens/sec=1375235.30, grad_norm=0.3140, duration=0.38s
Step 14752: loss=2.8696, lr=0.000288, tokens/sec=1376838.59, grad_norm=0.3331, duration=0.38s
Step 14753: loss=2.8757, lr=0.000288, tokens/sec=1375189.72, grad_norm=0.3233, duration=0.38s
Step 14754: loss=2.8327, lr=0.000288, tokens/sec=1374354.32, grad_norm=0.3162, duration=0.38s
Step 14755: loss=2.8766, lr=0.000288, tokens/sec=1377718.45, grad_norm=0.3215, duration=0.38s
Step 14756: loss=2.8690, lr=0.000288, tokens/sec=1373254.88, grad_norm=0.3302, duration=0.38s
Step 14757: loss=2.8766, lr=0.000288, tokens/sec=1377407.78, grad_norm=0.3187, duration=0.38s
Step 14758: loss=2.8290, lr=0.000288, tokens/sec=1377114.50, grad_norm=0.3057, duration=0.38s
Step 14759: loss=2.7957, lr=0.000288, tokens/sec=1374375.79, grad_norm=0.3177, duration=0.38s
Step 14760: loss=2.7675, lr=0.000288, tokens/sec=1374862.14, grad_norm=0.3494, duration=0.38s
Step 14761: loss=2.7544, lr=0.000288, tokens/sec=1376764.45, grad_norm=0.3274, duration=0.38s
Step 14762: loss=2.7984, lr=0.000288, tokens/sec=1372631.71, grad_norm=0.3084, duration=0.38s
Step 14763: loss=2.7804, lr=0.000288, tokens/sec=1377386.21, grad_norm=0.3196, duration=0.38s
Step 14764: loss=2.8622, lr=0.000288, tokens/sec=1373767.05, grad_norm=0.3314, duration=0.38s
Step 14765: loss=2.8102, lr=0.000288, tokens/sec=1371909.81, grad_norm=0.3338, duration=0.38s
Step 14766: loss=2.7933, lr=0.000288, tokens/sec=1379517.03, grad_norm=0.3165, duration=0.38s
Step 14767: loss=2.7711, lr=0.000288, tokens/sec=1378032.71, grad_norm=0.3040, duration=0.38s
Step 14768: loss=2.8110, lr=0.000288, tokens/sec=1372071.59, grad_norm=0.3357, duration=0.38s
Step 14769: loss=2.8706, lr=0.000288, tokens/sec=1379431.36, grad_norm=0.3473, duration=0.38s
Step 14770: loss=2.8471, lr=0.000288, tokens/sec=1375126.95, grad_norm=0.3317, duration=0.38s
Step 14771: loss=2.8608, lr=0.000288, tokens/sec=1374367.20, grad_norm=0.3095, duration=0.38s
Step 14772: loss=2.8778, lr=0.000288, tokens/sec=1377205.06, grad_norm=0.3348, duration=0.38s
Step 14773: loss=2.8727, lr=0.000288, tokens/sec=1375655.14, grad_norm=0.3390, duration=0.38s
Step 14774: loss=2.9153, lr=0.000288, tokens/sec=1374697.98, grad_norm=0.3536, duration=0.38s
Step 14775: loss=2.8339, lr=0.000288, tokens/sec=1377697.73, grad_norm=0.3285, duration=0.38s
Step 14776: loss=2.8654, lr=0.000288, tokens/sec=1377063.62, grad_norm=0.3405, duration=0.38s
Step 14777: loss=2.8767, lr=0.000288, tokens/sec=1377494.06, grad_norm=0.3604, duration=0.38s
Step 14778: loss=2.9616, lr=0.000288, tokens/sec=1375865.15, grad_norm=0.3607, duration=0.38s
Step 14779: loss=2.8794, lr=0.000288, tokens/sec=1374399.84, grad_norm=0.3327, duration=0.38s
Step 14780: loss=2.8982, lr=0.000288, tokens/sec=1374216.90, grad_norm=0.3552, duration=0.38s
Step 14781: loss=2.8428, lr=0.000288, tokens/sec=1376303.46, grad_norm=0.3671, duration=0.38s
Step 14782: loss=2.8541, lr=0.000288, tokens/sec=1377886.78, grad_norm=0.3614, duration=0.38s
Step 14783: loss=2.8496, lr=0.000287, tokens/sec=1376468.86, grad_norm=0.3165, duration=0.38s
Step 14784: loss=2.8064, lr=0.000287, tokens/sec=1375501.11, grad_norm=0.3286, duration=0.38s
Step 14785: loss=2.8317, lr=0.000287, tokens/sec=1377667.52, grad_norm=0.3712, duration=0.38s
Step 14786: loss=2.7971, lr=0.000287, tokens/sec=1374977.34, grad_norm=0.3757, duration=0.38s
Step 14787: loss=2.8780, lr=0.000287, tokens/sec=1376469.72, grad_norm=0.3453, duration=0.38s
Step 14788: loss=2.9299, lr=0.000287, tokens/sec=1374819.17, grad_norm=0.3316, duration=0.38s
Step 14789: loss=2.8707, lr=0.000287, tokens/sec=1375038.38, grad_norm=0.3627, duration=0.38s
Step 14790: loss=2.8299, lr=0.000287, tokens/sec=1378064.66, grad_norm=0.3773, duration=0.38s
Step 14791: loss=2.8570, lr=0.000287, tokens/sec=1376898.07, grad_norm=0.3555, duration=0.38s
Step 14792: loss=2.9003, lr=0.000287, tokens/sec=1374985.93, grad_norm=0.3211, duration=0.38s
Step 14793: loss=2.9005, lr=0.000287, tokens/sec=1378455.12, grad_norm=0.3097, duration=0.38s
Step 14794: loss=2.8186, lr=0.000287, tokens/sec=1375214.66, grad_norm=0.3489, duration=0.38s
Step 14795: loss=2.8611, lr=0.000287, tokens/sec=1381486.92, grad_norm=0.3573, duration=0.38s
Step 14796: loss=2.8643, lr=0.000287, tokens/sec=1377619.19, grad_norm=0.3401, duration=0.38s
Step 14797: loss=2.8764, lr=0.000287, tokens/sec=1374870.74, grad_norm=0.3148, duration=0.38s
Step 14798: loss=2.9099, lr=0.000287, tokens/sec=1376188.90, grad_norm=0.3233, duration=0.38s
Step 14799: loss=2.8410, lr=0.000287, tokens/sec=1374583.69, grad_norm=0.3374, duration=0.38s
Step 14800/19073 (77.6%), Elapsed time: 5859.86s, Steps per hour: 9092.37, Estimated hours remaining: 0.47
Step 14800: loss=2.8294, lr=0.000287, tokens/sec=1379462.51, grad_norm=0.3359, duration=0.38s
Step 14801: loss=2.8379, lr=0.000287, tokens/sec=1378751.56, grad_norm=0.3434, duration=0.38s
Step 14802: loss=2.8002, lr=0.000287, tokens/sec=1376372.37, grad_norm=0.3046, duration=0.38s
Step 14803: loss=2.8325, lr=0.000287, tokens/sec=1376445.60, grad_norm=0.3057, duration=0.38s
Step 14804: loss=2.8309, lr=0.000287, tokens/sec=1374164.52, grad_norm=0.3394, duration=0.38s
Step 14805: loss=2.7899, lr=0.000287, tokens/sec=1375028.92, grad_norm=0.3343, duration=0.38s
Step 14806: loss=2.8042, lr=0.000287, tokens/sec=1380850.19, grad_norm=0.3095, duration=0.38s
Step 14807: loss=2.8763, lr=0.000287, tokens/sec=1374566.51, grad_norm=0.3018, duration=0.38s
Step 14808: loss=2.7900, lr=0.000287, tokens/sec=1375943.49, grad_norm=0.3123, duration=0.38s
Step 14809: loss=2.8064, lr=0.000287, tokens/sec=1375138.13, grad_norm=0.3247, duration=0.38s
Step 14810: loss=2.7659, lr=0.000287, tokens/sec=1376543.83, grad_norm=0.3081, duration=0.38s
Step 14811: loss=2.7765, lr=0.000287, tokens/sec=1379362.14, grad_norm=0.2948, duration=0.38s
Step 14812: loss=2.8289, lr=0.000287, tokens/sec=1375761.86, grad_norm=0.3036, duration=0.38s
Step 14813: loss=2.8013, lr=0.000287, tokens/sec=1375237.02, grad_norm=0.3080, duration=0.38s
Step 14814: loss=2.8329, lr=0.000287, tokens/sec=1379487.60, grad_norm=0.3067, duration=0.38s
Step 14815: loss=2.8747, lr=0.000287, tokens/sec=1375538.97, grad_norm=0.3197, duration=0.38s
Step 14816: loss=2.8668, lr=0.000287, tokens/sec=1377564.82, grad_norm=0.3144, duration=0.38s
Step 14817: loss=2.8584, lr=0.000287, tokens/sec=1378865.68, grad_norm=0.3060, duration=0.38s
Step 14818: loss=2.8403, lr=0.000287, tokens/sec=1373552.53, grad_norm=0.2964, duration=0.38s
Step 14819: loss=2.8922, lr=0.000287, tokens/sec=1372687.40, grad_norm=0.3172, duration=0.38s
Step 14820: loss=2.8319, lr=0.000287, tokens/sec=1374514.10, grad_norm=0.3121, duration=0.38s
Step 14821: loss=2.8685, lr=0.000287, tokens/sec=1375302.39, grad_norm=0.3027, duration=0.38s
Step 14822: loss=2.8668, lr=0.000287, tokens/sec=1375101.15, grad_norm=0.3012, duration=0.38s
Step 14823: loss=2.9455, lr=0.000287, tokens/sec=1377311.16, grad_norm=0.3223, duration=0.38s
Step 14824: loss=2.8885, lr=0.000287, tokens/sec=1376079.53, grad_norm=0.3041, duration=0.38s
Step 14825: loss=2.8791, lr=0.000287, tokens/sec=1377664.07, grad_norm=0.2963, duration=0.38s
Step 14826: loss=2.9365, lr=0.000287, tokens/sec=1380299.81, grad_norm=0.2956, duration=0.38s
Step 14827: loss=2.8739, lr=0.000287, tokens/sec=1372738.82, grad_norm=0.3085, duration=0.38s
Step 14828: loss=2.8785, lr=0.000287, tokens/sec=1371085.22, grad_norm=0.3109, duration=0.38s
Step 14829: loss=2.8670, lr=0.000287, tokens/sec=1374383.52, grad_norm=0.3016, duration=0.38s
Step 14830: loss=2.8857, lr=0.000287, tokens/sec=1376015.81, grad_norm=0.2959, duration=0.38s
Step 14831: loss=2.7896, lr=0.000286, tokens/sec=1376582.60, grad_norm=0.2980, duration=0.38s
Step 14832: loss=2.8580, lr=0.000286, tokens/sec=1374540.73, grad_norm=0.3143, duration=0.38s
Step 14833: loss=2.7985, lr=0.000286, tokens/sec=1376208.71, grad_norm=0.2929, duration=0.38s
Step 14834: loss=2.9062, lr=0.000286, tokens/sec=1376036.48, grad_norm=0.2995, duration=0.38s
Step 14835: loss=2.8831, lr=0.000286, tokens/sec=1374168.81, grad_norm=0.3322, duration=0.38s
Step 14836: loss=2.8353, lr=0.000286, tokens/sec=1379156.24, grad_norm=0.3259, duration=0.38s
Step 14837: loss=2.8966, lr=0.000286, tokens/sec=1374674.78, grad_norm=0.3169, duration=0.38s
Step 14838: loss=2.8135, lr=0.000286, tokens/sec=1376917.90, grad_norm=0.3142, duration=0.38s
Step 14839: loss=2.7647, lr=0.000286, tokens/sec=1376859.28, grad_norm=0.3664, duration=0.38s
Step 14840: loss=2.7855, lr=0.000286, tokens/sec=1374319.96, grad_norm=0.3082, duration=0.38s
Step 14841: loss=2.8732, lr=0.000286, tokens/sec=1376636.89, grad_norm=0.3084, duration=0.38s
Step 14842: loss=2.9470, lr=0.000286, tokens/sec=1376803.24, grad_norm=0.3264, duration=0.38s
Step 14843: loss=2.7885, lr=0.000286, tokens/sec=1376586.91, grad_norm=0.3093, duration=0.38s
Step 14844: loss=2.9242, lr=0.000286, tokens/sec=1379049.86, grad_norm=0.3031, duration=0.38s
Step 14845: loss=2.8802, lr=0.000286, tokens/sec=1375684.40, grad_norm=0.3123, duration=0.38s
Step 14846: loss=2.8415, lr=0.000286, tokens/sec=1374980.78, grad_norm=0.3051, duration=0.38s
Step 14847: loss=2.8649, lr=0.000286, tokens/sec=1376701.53, grad_norm=0.2965, duration=0.38s
Step 14848: loss=2.8760, lr=0.000286, tokens/sec=1372971.08, grad_norm=0.3044, duration=0.38s
Step 14849: loss=2.8751, lr=0.000286, tokens/sec=1370576.76, grad_norm=0.3067, duration=0.38s
Step 14850: loss=2.7824, lr=0.000286, tokens/sec=1375606.09, grad_norm=0.3110, duration=0.38s
Step 14851: loss=2.8459, lr=0.000286, tokens/sec=1374997.11, grad_norm=0.3228, duration=0.38s
Step 14852: loss=2.8216, lr=0.000286, tokens/sec=1373000.23, grad_norm=0.3296, duration=0.38s
Step 14853: loss=2.8919, lr=0.000286, tokens/sec=1374279.59, grad_norm=0.3044, duration=0.38s
Step 14854: loss=2.8350, lr=0.000286, tokens/sec=1375542.41, grad_norm=0.2988, duration=0.38s
Step 14855: loss=2.7482, lr=0.000286, tokens/sec=1371938.91, grad_norm=0.3165, duration=0.38s
Step 14856: loss=2.8185, lr=0.000286, tokens/sec=1377771.10, grad_norm=0.3055, duration=0.38s
Step 14857: loss=2.7586, lr=0.000286, tokens/sec=1372349.88, grad_norm=0.3024, duration=0.38s
Step 14858: loss=2.7715, lr=0.000286, tokens/sec=1373589.42, grad_norm=0.2920, duration=0.38s
Step 14859: loss=2.7978, lr=0.000286, tokens/sec=1375516.60, grad_norm=0.2961, duration=0.38s
Step 14860: loss=2.8138, lr=0.000286, tokens/sec=1378101.80, grad_norm=0.3102, duration=0.38s
Step 14861: loss=2.8041, lr=0.000286, tokens/sec=1375526.92, grad_norm=0.3049, duration=0.38s
Step 14862: loss=2.8295, lr=0.000286, tokens/sec=1375872.90, grad_norm=0.3032, duration=0.38s
Step 14863: loss=2.8049, lr=0.000286, tokens/sec=1375842.77, grad_norm=0.2962, duration=0.38s
Step 14864: loss=2.7841, lr=0.000286, tokens/sec=1375755.83, grad_norm=0.2930, duration=0.38s
Step 14865: loss=2.7938, lr=0.000286, tokens/sec=1377687.37, grad_norm=0.3016, duration=0.38s
Step 14866: loss=2.8017, lr=0.000286, tokens/sec=1379300.71, grad_norm=0.2996, duration=0.38s
Step 14867: loss=2.8470, lr=0.000286, tokens/sec=1375668.91, grad_norm=0.3220, duration=0.38s
Step 14868: loss=2.9070, lr=0.000286, tokens/sec=1377376.72, grad_norm=0.3185, duration=0.38s
Step 14869: loss=2.8725, lr=0.000286, tokens/sec=1377554.47, grad_norm=0.3101, duration=0.38s
Step 14870: loss=2.8436, lr=0.000286, tokens/sec=1376257.80, grad_norm=0.3204, duration=0.38s
Step 14871: loss=2.8616, lr=0.000286, tokens/sec=1375956.41, grad_norm=0.3224, duration=0.38s
Step 14872: loss=2.8694, lr=0.000286, tokens/sec=1370108.80, grad_norm=0.3180, duration=0.38s
Step 14873: loss=2.8625, lr=0.000286, tokens/sec=1373404.98, grad_norm=0.3202, duration=0.38s
Step 14874: loss=2.8922, lr=0.000286, tokens/sec=1374525.27, grad_norm=0.3433, duration=0.38s
Step 14875: loss=2.8714, lr=0.000286, tokens/sec=1373842.57, grad_norm=0.3330, duration=0.38s
Step 14876: loss=2.8565, lr=0.000286, tokens/sec=1376280.20, grad_norm=0.3189, duration=0.38s
Step 14877: loss=2.8465, lr=0.000286, tokens/sec=1374425.61, grad_norm=0.3148, duration=0.38s
Step 14878: loss=2.8071, lr=0.000286, tokens/sec=1373564.54, grad_norm=0.3153, duration=0.38s
Step 14879: loss=2.8516, lr=0.000285, tokens/sec=1375520.04, grad_norm=0.3459, duration=0.38s
Step 14880: loss=2.8629, lr=0.000285, tokens/sec=1372790.23, grad_norm=0.3426, duration=0.38s
Step 14881: loss=2.7700, lr=0.000285, tokens/sec=1375642.23, grad_norm=0.3180, duration=0.38s
Step 14882: loss=2.8596, lr=0.000285, tokens/sec=1380658.59, grad_norm=0.3002, duration=0.38s
Step 14883: loss=2.8875, lr=0.000285, tokens/sec=1373968.76, grad_norm=0.3356, duration=0.38s
Step 14884: loss=2.8446, lr=0.000285, tokens/sec=1377048.10, grad_norm=0.3508, duration=0.38s
Step 14885: loss=2.9181, lr=0.000285, tokens/sec=1374598.30, grad_norm=0.3660, duration=0.38s
Step 14886: loss=2.8277, lr=0.000285, tokens/sec=1375501.97, grad_norm=0.3071, duration=0.38s
Step 14887: loss=2.8964, lr=0.000285, tokens/sec=1373218.01, grad_norm=0.3229, duration=0.38s
Step 14888: loss=2.8914, lr=0.000285, tokens/sec=1375215.52, grad_norm=0.3498, duration=0.38s
Step 14889: loss=2.8664, lr=0.000285, tokens/sec=1375958.99, grad_norm=0.3641, duration=0.38s
Step 14890: loss=2.8628, lr=0.000285, tokens/sec=1373181.14, grad_norm=0.3316, duration=0.38s
Step 14891: loss=2.8472, lr=0.000285, tokens/sec=1378094.02, grad_norm=0.3265, duration=0.38s
Step 14892: loss=2.8217, lr=0.000285, tokens/sec=1374722.05, grad_norm=0.3199, duration=0.38s
Step 14893: loss=2.8650, lr=0.000285, tokens/sec=1376305.18, grad_norm=0.3307, duration=0.38s
Step 14894: loss=2.8639, lr=0.000285, tokens/sec=1372438.96, grad_norm=0.3321, duration=0.38s
Step 14895: loss=2.8788, lr=0.000285, tokens/sec=1372628.28, grad_norm=0.3215, duration=0.38s
Step 14896: loss=2.8548, lr=0.000285, tokens/sec=1372962.51, grad_norm=0.3197, duration=0.38s
Step 14897: loss=2.8098, lr=0.000285, tokens/sec=1375727.43, grad_norm=0.3305, duration=0.38s
Step 14898: loss=2.8598, lr=0.000285, tokens/sec=1370641.69, grad_norm=0.3225, duration=0.38s
Step 14899: loss=2.8359, lr=0.000285, tokens/sec=1372534.04, grad_norm=0.3263, duration=0.38s
Step 14900/19073 (78.1%), Elapsed time: 5898.07s, Steps per hour: 9094.50, Estimated hours remaining: 0.46
Step 14900: loss=2.8689, lr=0.000285, tokens/sec=1374237.51, grad_norm=0.3482, duration=0.38s
Step 14901: loss=2.7960, lr=0.000285, tokens/sec=1375603.50, grad_norm=0.3365, duration=0.38s
Step 14902: loss=2.8808, lr=0.000285, tokens/sec=1373591.13, grad_norm=0.3202, duration=0.38s
Step 14903: loss=2.8096, lr=0.000285, tokens/sec=1377793.55, grad_norm=0.3266, duration=0.38s
Step 14904: loss=2.8146, lr=0.000285, tokens/sec=1370400.81, grad_norm=0.3316, duration=0.38s
Step 14905: loss=2.7656, lr=0.000285, tokens/sec=1376381.85, grad_norm=0.3250, duration=0.38s
Step 14906: loss=2.8020, lr=0.000285, tokens/sec=1371011.70, grad_norm=0.3363, duration=0.38s
Step 14907: loss=2.8030, lr=0.000285, tokens/sec=1368573.10, grad_norm=0.3263, duration=0.38s
Step 14908: loss=2.7640, lr=0.000285, tokens/sec=1370742.50, grad_norm=0.3249, duration=0.38s
Step 14909: loss=2.7905, lr=0.000285, tokens/sec=1375460.67, grad_norm=0.3238, duration=0.38s
Step 14910: loss=2.7736, lr=0.000285, tokens/sec=1372851.08, grad_norm=0.3415, duration=0.38s
Step 14911: loss=2.8188, lr=0.000285, tokens/sec=1372905.94, grad_norm=0.3535, duration=0.38s
Step 14912: loss=2.7933, lr=0.000285, tokens/sec=1372490.35, grad_norm=0.3377, duration=0.38s
Step 14913: loss=2.8425, lr=0.000285, tokens/sec=1375433.14, grad_norm=0.3219, duration=0.38s
Step 14914: loss=2.8853, lr=0.000285, tokens/sec=1374975.62, grad_norm=0.3362, duration=0.38s
Step 14915: loss=2.8603, lr=0.000285, tokens/sec=1373435.86, grad_norm=0.3502, duration=0.38s
Step 14916: loss=2.8596, lr=0.000285, tokens/sec=1373827.98, grad_norm=0.3535, duration=0.38s
Step 14917: loss=2.8692, lr=0.000285, tokens/sec=1375348.84, grad_norm=0.3360, duration=0.38s
Step 14918: loss=2.8001, lr=0.000285, tokens/sec=1372563.17, grad_norm=0.3336, duration=0.38s
Step 14919: loss=2.8641, lr=0.000285, tokens/sec=1371589.78, grad_norm=0.3347, duration=0.38s
Step 14920: loss=2.8143, lr=0.000285, tokens/sec=1372900.80, grad_norm=0.3429, duration=0.38s
Step 14921: loss=2.8560, lr=0.000285, tokens/sec=1375303.25, grad_norm=0.3286, duration=0.38s
Step 14922: loss=2.8668, lr=0.000285, tokens/sec=1373824.55, grad_norm=0.3189, duration=0.38s
Step 14923: loss=2.8524, lr=0.000285, tokens/sec=1373992.79, grad_norm=0.3276, duration=0.38s
Step 14924: loss=2.8577, lr=0.000285, tokens/sec=1374619.78, grad_norm=0.3094, duration=0.38s
Step 14925: loss=2.8246, lr=0.000285, tokens/sec=1374508.95, grad_norm=0.3087, duration=0.38s
Step 14926: loss=2.8095, lr=0.000285, tokens/sec=1376430.09, grad_norm=0.3126, duration=0.38s
Step 14927: loss=2.8356, lr=0.000284, tokens/sec=1377280.10, grad_norm=0.3224, duration=0.38s
Step 14928: loss=2.8828, lr=0.000284, tokens/sec=1371728.38, grad_norm=0.3260, duration=0.38s
Step 14929: loss=2.8365, lr=0.000284, tokens/sec=1371778.01, grad_norm=0.3145, duration=0.38s
Step 14930: loss=2.8692, lr=0.000284, tokens/sec=1370677.57, grad_norm=0.3049, duration=0.38s
Step 14931: loss=2.8417, lr=0.000284, tokens/sec=1374554.48, grad_norm=0.2916, duration=0.38s
Step 14932: loss=2.8749, lr=0.000284, tokens/sec=1377000.67, grad_norm=0.3239, duration=0.38s
Step 14933: loss=2.7972, lr=0.000284, tokens/sec=1375471.86, grad_norm=0.3297, duration=0.38s
Step 14934: loss=2.8917, lr=0.000284, tokens/sec=1372361.87, grad_norm=0.3221, duration=0.38s
Step 14935: loss=2.7969, lr=0.000284, tokens/sec=1373838.28, grad_norm=0.3029, duration=0.38s
Step 14936: loss=2.8758, lr=0.000284, tokens/sec=1375114.05, grad_norm=0.3310, duration=0.38s
Step 14937: loss=2.8415, lr=0.000284, tokens/sec=1374466.85, grad_norm=0.3239, duration=0.38s
Step 14938: loss=2.8897, lr=0.000284, tokens/sec=1378182.12, grad_norm=0.3126, duration=0.38s
Step 14939: loss=2.8768, lr=0.000284, tokens/sec=1375336.80, grad_norm=0.3208, duration=0.38s
Step 14940: loss=2.8659, lr=0.000284, tokens/sec=1378744.64, grad_norm=0.3106, duration=0.38s
Step 14941: loss=2.8619, lr=0.000284, tokens/sec=1372568.31, grad_norm=0.3072, duration=0.38s
Step 14942: loss=2.8937, lr=0.000284, tokens/sec=1378171.75, grad_norm=0.3209, duration=0.38s
Step 14943: loss=2.8084, lr=0.000284, tokens/sec=1375606.95, grad_norm=0.3377, duration=0.38s
Step 14944: loss=2.8709, lr=0.000284, tokens/sec=1374320.82, grad_norm=0.3321, duration=0.38s
Step 14945: loss=2.8732, lr=0.000284, tokens/sec=1378110.43, grad_norm=0.3101, duration=0.38s
Step 14946: loss=2.8819, lr=0.000284, tokens/sec=1377272.34, grad_norm=0.3273, duration=0.38s
Step 14947: loss=2.8313, lr=0.000284, tokens/sec=1376812.73, grad_norm=0.3421, duration=0.38s
Step 14948: loss=2.8461, lr=0.000284, tokens/sec=1377230.93, grad_norm=0.3165, duration=0.38s
Step 14949: loss=2.7344, lr=0.000284, tokens/sec=1374256.40, grad_norm=0.3433, duration=0.38s
Step 14950: loss=2.8286, lr=0.000284, tokens/sec=1377098.11, grad_norm=0.3355, duration=0.38s
Step 14951: loss=2.7478, lr=0.000284, tokens/sec=1374812.29, grad_norm=0.3623, duration=0.38s
Step 14952: loss=2.7761, lr=0.000284, tokens/sec=1375200.90, grad_norm=0.3326, duration=0.38s
Step 14953: loss=2.7936, lr=0.000284, tokens/sec=1373493.33, grad_norm=0.3112, duration=0.38s
Step 14954: loss=2.8393, lr=0.000284, tokens/sec=1375799.73, grad_norm=0.3367, duration=0.38s
Step 14955: loss=2.8086, lr=0.000284, tokens/sec=1376005.48, grad_norm=0.3594, duration=0.38s
Step 14956: loss=2.7794, lr=0.000284, tokens/sec=1377214.55, grad_norm=0.3449, duration=0.38s
Step 14957: loss=2.7795, lr=0.000284, tokens/sec=1378678.09, grad_norm=0.3114, duration=0.38s
Step 14958: loss=2.8341, lr=0.000284, tokens/sec=1375278.31, grad_norm=0.3247, duration=0.38s
Step 14959: loss=2.8324, lr=0.000284, tokens/sec=1377577.77, grad_norm=0.3599, duration=0.38s
Step 14960: loss=2.8379, lr=0.000284, tokens/sec=1373602.29, grad_norm=0.3733, duration=0.38s
Step 14961: loss=2.8691, lr=0.000284, tokens/sec=1376365.48, grad_norm=0.3159, duration=0.38s
Step 14962: loss=2.8893, lr=0.000284, tokens/sec=1373524.21, grad_norm=0.3197, duration=0.38s
Step 14963: loss=2.8843, lr=0.000284, tokens/sec=1375796.29, grad_norm=0.3597, duration=0.38s
Step 14964: loss=2.8817, lr=0.000284, tokens/sec=1376923.07, grad_norm=0.3733, duration=0.38s
Step 14965: loss=2.8395, lr=0.000284, tokens/sec=1378123.39, grad_norm=0.3526, duration=0.38s
Step 14966: loss=2.8293, lr=0.000284, tokens/sec=1376540.38, grad_norm=0.3090, duration=0.38s
Step 14967: loss=2.8982, lr=0.000284, tokens/sec=1375033.22, grad_norm=0.3586, duration=0.38s
Step 14968: loss=2.9305, lr=0.000284, tokens/sec=1370780.95, grad_norm=0.3923, duration=0.38s
Step 14969: loss=2.9053, lr=0.000284, tokens/sec=1374194.57, grad_norm=0.3693, duration=0.38s
Step 14970: loss=2.8876, lr=0.000284, tokens/sec=1375599.20, grad_norm=0.3213, duration=0.38s
Step 14971: loss=2.8180, lr=0.000284, tokens/sec=1373719.85, grad_norm=0.3411, duration=0.38s
Step 14972: loss=2.8887, lr=0.000284, tokens/sec=1374084.66, grad_norm=0.3808, duration=0.38s
Step 14973: loss=2.8368, lr=0.000284, tokens/sec=1373598.86, grad_norm=0.3587, duration=0.38s
Step 14974: loss=2.7375, lr=0.000284, tokens/sec=1372923.94, grad_norm=0.3342, duration=0.38s
Step 14975: loss=2.8770, lr=0.000284, tokens/sec=1374235.79, grad_norm=0.3232, duration=0.38s
Step 14976: loss=2.7914, lr=0.000283, tokens/sec=1369488.48, grad_norm=0.3584, duration=0.38s
Step 14977: loss=2.9045, lr=0.000283, tokens/sec=1374100.97, grad_norm=0.3802, duration=0.38s
Step 14978: loss=2.9146, lr=0.000283, tokens/sec=1373159.70, grad_norm=0.3399, duration=0.38s
Step 14979: loss=2.8726, lr=0.000283, tokens/sec=1378329.84, grad_norm=0.3325, duration=0.38s
Step 14980: loss=2.8056, lr=0.000283, tokens/sec=1375136.41, grad_norm=0.3299, duration=0.38s
Step 14981: loss=2.8707, lr=0.000283, tokens/sec=1375557.90, grad_norm=0.3602, duration=0.38s
Step 14982: loss=2.9301, lr=0.000283, tokens/sec=1373496.76, grad_norm=0.3433, duration=0.38s
Step 14983: loss=2.8891, lr=0.000283, tokens/sec=1373886.35, grad_norm=0.3218, duration=0.38s
Step 14984: loss=2.8014, lr=0.000283, tokens/sec=1380337.06, grad_norm=0.3277, duration=0.38s
Step 14985: loss=2.8559, lr=0.000283, tokens/sec=1374685.95, grad_norm=0.3423, duration=0.38s
Step 14986: loss=2.8551, lr=0.000283, tokens/sec=1372954.80, grad_norm=0.3364, duration=0.38s
Step 14987: loss=2.8812, lr=0.000283, tokens/sec=1373837.42, grad_norm=0.3410, duration=0.38s
Step 14988: loss=2.9146, lr=0.000283, tokens/sec=1378899.40, grad_norm=0.3386, duration=0.38s
Step 14989: loss=2.8139, lr=0.000283, tokens/sec=1376304.32, grad_norm=0.3203, duration=0.38s
Step 14990: loss=2.8347, lr=0.000283, tokens/sec=1376269.86, grad_norm=0.3241, duration=0.38s
Step 14991: loss=2.8381, lr=0.000283, tokens/sec=1374666.19, grad_norm=0.3420, duration=0.38s
Step 14992: loss=2.7918, lr=0.000283, tokens/sec=1372120.39, grad_norm=0.3360, duration=0.38s
Step 14993: loss=2.8612, lr=0.000283, tokens/sec=1373759.32, grad_norm=0.3219, duration=0.38s
Step 14994: loss=2.8004, lr=0.000283, tokens/sec=1373365.52, grad_norm=0.3231, duration=0.38s
Step 14995: loss=2.7939, lr=0.000283, tokens/sec=1373383.53, grad_norm=0.3346, duration=0.38s
Step 14996: loss=2.8171, lr=0.000283, tokens/sec=1374779.63, grad_norm=0.3403, duration=0.38s
Step 14997: loss=2.8334, lr=0.000283, tokens/sec=1373186.28, grad_norm=0.3149, duration=0.38s
Step 14998: loss=2.8193, lr=0.000283, tokens/sec=1376008.06, grad_norm=0.2982, duration=0.38s
Step 14999: loss=2.7875, lr=0.000283, tokens/sec=1374600.02, grad_norm=0.3245, duration=0.38s
Step 15000/19073 (78.6%), Elapsed time: 5936.30s, Steps per hour: 9096.57, Estimated hours remaining: 0.45
Validation loss at step 15000: 3.8187568187713623
Checkpoint saved to model_checkpoints/model_15000.pt
Step 15000: loss=2.7476, lr=0.000283, tokens/sec=93373.38, grad_norm=0.3251, duration=5.61s
Step 15001: loss=2.7895, lr=0.000283, tokens/sec=1380395.12, grad_norm=0.3076, duration=0.38s
Step 15002: loss=2.8193, lr=0.000283, tokens/sec=1378156.21, grad_norm=0.3062, duration=0.38s
Step 15003: loss=2.8107, lr=0.000283, tokens/sec=1374460.83, grad_norm=0.3279, duration=0.38s
Step 15004: loss=2.8273, lr=0.000283, tokens/sec=1372805.66, grad_norm=0.3215, duration=0.38s
Step 15005: loss=2.8639, lr=0.000283, tokens/sec=1377268.89, grad_norm=0.3017, duration=0.38s
Step 15006: loss=2.8500, lr=0.000283, tokens/sec=1378532.89, grad_norm=0.3131, duration=0.38s
Step 15007: loss=2.8627, lr=0.000283, tokens/sec=1378067.25, grad_norm=0.3260, duration=0.38s
Step 15008: loss=2.8603, lr=0.000283, tokens/sec=1378521.65, grad_norm=0.3187, duration=0.38s
Step 15009: loss=2.8542, lr=0.000283, tokens/sec=1378955.60, grad_norm=0.3143, duration=0.38s
Step 15010: loss=2.8328, lr=0.000283, tokens/sec=1380139.54, grad_norm=0.3068, duration=0.38s
Step 15011: loss=2.8603, lr=0.000283, tokens/sec=1378965.98, grad_norm=0.3203, duration=0.38s
Step 15012: loss=2.8718, lr=0.000283, tokens/sec=1375221.54, grad_norm=0.3156, duration=0.38s
Step 15013: loss=2.9479, lr=0.000283, tokens/sec=1376163.93, grad_norm=0.3322, duration=0.38s
Step 15014: loss=2.8558, lr=0.000283, tokens/sec=1379450.39, grad_norm=0.3168, duration=0.38s
Step 15015: loss=2.8892, lr=0.000283, tokens/sec=1378917.56, grad_norm=0.3177, duration=0.38s
Step 15016: loss=2.9285, lr=0.000283, tokens/sec=1371606.03, grad_norm=0.3120, duration=0.38s
Step 15017: loss=2.8619, lr=0.000283, tokens/sec=1373507.91, grad_norm=0.3065, duration=0.38s
Step 15018: loss=2.8836, lr=0.000283, tokens/sec=1372827.94, grad_norm=0.3299, duration=0.38s
Step 15019: loss=2.8703, lr=0.000283, tokens/sec=1375231.00, grad_norm=0.3367, duration=0.38s
Step 15020: loss=2.8155, lr=0.000283, tokens/sec=1374158.50, grad_norm=0.3100, duration=0.38s
Step 15021: loss=2.8453, lr=0.000283, tokens/sec=1376314.65, grad_norm=0.3016, duration=0.38s
Step 15022: loss=2.8522, lr=0.000283, tokens/sec=1366104.11, grad_norm=0.3158, duration=0.38s
Step 15023: loss=2.8301, lr=0.000283, tokens/sec=1372437.24, grad_norm=0.3244, duration=0.38s
Step 15024: loss=2.8955, lr=0.000283, tokens/sec=1379795.75, grad_norm=0.3122, duration=0.38s
Step 15025: loss=2.8760, lr=0.000283, tokens/sec=1377986.94, grad_norm=0.3490, duration=0.38s
Step 15026: loss=2.8256, lr=0.000282, tokens/sec=1371392.19, grad_norm=0.3202, duration=0.38s
Step 15027: loss=2.8938, lr=0.000282, tokens/sec=1369989.30, grad_norm=0.3211, duration=0.38s
Step 15028: loss=2.7919, lr=0.000282, tokens/sec=1374572.52, grad_norm=0.3266, duration=0.38s
Step 15029: loss=2.7391, lr=0.000282, tokens/sec=1373525.93, grad_norm=0.4217, duration=0.38s
Step 15030: loss=2.8271, lr=0.000282, tokens/sec=1373344.08, grad_norm=0.3195, duration=0.38s
Step 15031: loss=2.8673, lr=0.000282, tokens/sec=1373769.62, grad_norm=0.3133, duration=0.38s
Step 15032: loss=2.9349, lr=0.000282, tokens/sec=1375561.34, grad_norm=0.3097, duration=0.38s
Step 15033: loss=2.8112, lr=0.000282, tokens/sec=1375200.04, grad_norm=0.3217, duration=0.38s
Step 15034: loss=2.8868, lr=0.000282, tokens/sec=1373296.05, grad_norm=0.3273, duration=0.38s
Step 15035: loss=2.8947, lr=0.000282, tokens/sec=1376970.49, grad_norm=0.3223, duration=0.38s
Step 15036: loss=2.8268, lr=0.000282, tokens/sec=1377190.40, grad_norm=0.2985, duration=0.38s
Step 15037: loss=2.8518, lr=0.000282, tokens/sec=1374610.33, grad_norm=0.3086, duration=0.38s
Step 15038: loss=2.8929, lr=0.000282, tokens/sec=1374527.85, grad_norm=0.3127, duration=0.38s
Step 15039: loss=2.8162, lr=0.000282, tokens/sec=1376091.59, grad_norm=0.3151, duration=0.38s
Step 15040: loss=2.8106, lr=0.000282, tokens/sec=1375832.44, grad_norm=0.3046, duration=0.38s
Step 15041: loss=2.8081, lr=0.000282, tokens/sec=1374528.71, grad_norm=0.3120, duration=0.38s
Step 15042: loss=2.8552, lr=0.000282, tokens/sec=1376536.07, grad_norm=0.3376, duration=0.38s
Step 15043: loss=2.8834, lr=0.000282, tokens/sec=1375126.95, grad_norm=0.3135, duration=0.38s
Step 15044: loss=2.7952, lr=0.000282, tokens/sec=1377180.91, grad_norm=0.3027, duration=0.38s
Step 15045: loss=2.7700, lr=0.000282, tokens/sec=1377875.56, grad_norm=0.3093, duration=0.38s
Step 15046: loss=2.8005, lr=0.000282, tokens/sec=1379390.69, grad_norm=0.3270, duration=0.38s
Step 15047: loss=2.7702, lr=0.000282, tokens/sec=1375403.90, grad_norm=0.3104, duration=0.38s
Step 15048: loss=2.7815, lr=0.000282, tokens/sec=1373844.29, grad_norm=0.3117, duration=0.38s
Step 15049: loss=2.7927, lr=0.000282, tokens/sec=1378297.01, grad_norm=0.3035, duration=0.38s
Step 15050: loss=2.7972, lr=0.000282, tokens/sec=1376192.35, grad_norm=0.3182, duration=0.38s
Step 15051: loss=2.8226, lr=0.000282, tokens/sec=1374556.20, grad_norm=0.3131, duration=0.38s
Step 15052: loss=2.7836, lr=0.000282, tokens/sec=1376918.76, grad_norm=0.3123, duration=0.38s
Step 15053: loss=2.8063, lr=0.000282, tokens/sec=1378367.85, grad_norm=0.3097, duration=0.38s
Step 15054: loss=2.7935, lr=0.000282, tokens/sec=1373573.12, grad_norm=0.2994, duration=0.38s
Step 15055: loss=2.7856, lr=0.000282, tokens/sec=1374962.72, grad_norm=0.2950, duration=0.38s
Step 15056: loss=2.7782, lr=0.000282, tokens/sec=1374751.27, grad_norm=0.3188, duration=0.38s
Step 15057: loss=2.8625, lr=0.000282, tokens/sec=1376149.29, grad_norm=0.3196, duration=0.38s
Step 15058: loss=2.9111, lr=0.000282, tokens/sec=1375917.66, grad_norm=0.3053, duration=0.38s
Step 15059: loss=2.8422, lr=0.000282, tokens/sec=1375866.87, grad_norm=0.3034, duration=0.38s
Step 15060: loss=2.8320, lr=0.000282, tokens/sec=1376624.83, grad_norm=0.3209, duration=0.38s
Step 15061: loss=2.8720, lr=0.000282, tokens/sec=1377819.44, grad_norm=0.3337, duration=0.38s
Step 15062: loss=2.8584, lr=0.000282, tokens/sec=1375521.76, grad_norm=0.3211, duration=0.38s
Step 15063: loss=2.9234, lr=0.000282, tokens/sec=1376309.49, grad_norm=0.3278, duration=0.38s
Step 15064: loss=2.8766, lr=0.000282, tokens/sec=1376925.66, grad_norm=0.3225, duration=0.38s
Step 15065: loss=2.8290, lr=0.000282, tokens/sec=1376865.31, grad_norm=0.3430, duration=0.38s
Step 15066: loss=2.8584, lr=0.000282, tokens/sec=1377190.40, grad_norm=0.3543, duration=0.38s
Step 15067: loss=2.8065, lr=0.000282, tokens/sec=1375457.23, grad_norm=0.3144, duration=0.38s
Step 15068: loss=2.8436, lr=0.000282, tokens/sec=1377564.82, grad_norm=0.3096, duration=0.38s
Step 15069: loss=2.8565, lr=0.000282, tokens/sec=1377004.12, grad_norm=0.3439, duration=0.38s
Step 15070: loss=2.8433, lr=0.000282, tokens/sec=1373522.50, grad_norm=0.3885, duration=0.38s
Step 15071: loss=2.7981, lr=0.000282, tokens/sec=1373383.53, grad_norm=0.3551, duration=0.38s
Step 15072: loss=2.8413, lr=0.000282, tokens/sec=1374210.03, grad_norm=0.3059, duration=0.38s
Step 15073: loss=2.8516, lr=0.000282, tokens/sec=1378826.77, grad_norm=0.3378, duration=0.38s
Step 15074: loss=2.8766, lr=0.000282, tokens/sec=1377147.27, grad_norm=0.3972, duration=0.38s
Step 15075: loss=2.8819, lr=0.000282, tokens/sec=1375704.19, grad_norm=0.3773, duration=0.38s
Step 15076: loss=2.8197, lr=0.000281, tokens/sec=1376729.98, grad_norm=0.3191, duration=0.38s
Step 15077: loss=2.9200, lr=0.000281, tokens/sec=1377186.08, grad_norm=0.3241, duration=0.38s
Step 15078: loss=2.8719, lr=0.000281, tokens/sec=1375467.56, grad_norm=0.3366, duration=0.38s
Step 15079: loss=2.8562, lr=0.000281, tokens/sec=1374509.80, grad_norm=0.3564, duration=0.38s
Step 15080: loss=2.8414, lr=0.000281, tokens/sec=1375185.42, grad_norm=0.3538, duration=0.38s
Step 15081: loss=2.8405, lr=0.000281, tokens/sec=1373604.00, grad_norm=0.3223, duration=0.38s
Step 15082: loss=2.8415, lr=0.000281, tokens/sec=1374043.45, grad_norm=0.3130, duration=0.38s
Step 15083: loss=2.8611, lr=0.000281, tokens/sec=1376362.03, grad_norm=0.3191, duration=0.38s
Step 15084: loss=2.8804, lr=0.000281, tokens/sec=1375217.24, grad_norm=0.3371, duration=0.38s
Step 15085: loss=2.8472, lr=0.000281, tokens/sec=1376475.76, grad_norm=0.3205, duration=0.38s
Step 15086: loss=2.8486, lr=0.000281, tokens/sec=1380965.52, grad_norm=0.3099, duration=0.38s
Step 15087: loss=2.8210, lr=0.000281, tokens/sec=1378424.01, grad_norm=0.3185, duration=0.38s
Step 15088: loss=2.8440, lr=0.000281, tokens/sec=1375990.84, grad_norm=0.3216, duration=0.38s
Step 15089: loss=2.8527, lr=0.000281, tokens/sec=1376731.70, grad_norm=0.3142, duration=0.38s
Step 15090: loss=2.8584, lr=0.000281, tokens/sec=1376071.78, grad_norm=0.3181, duration=0.38s
Step 15091: loss=2.8127, lr=0.000281, tokens/sec=1379001.43, grad_norm=0.3167, duration=0.38s
Step 15092: loss=2.8566, lr=0.000281, tokens/sec=1376041.64, grad_norm=0.3241, duration=0.38s
Step 15093: loss=2.8017, lr=0.000281, tokens/sec=1374911.14, grad_norm=0.3259, duration=0.38s
Step 15094: loss=2.8101, lr=0.000281, tokens/sec=1377977.44, grad_norm=0.3120, duration=0.38s
Step 15095: loss=2.7675, lr=0.000281, tokens/sec=1376937.73, grad_norm=0.3015, duration=0.38s
Step 15096: loss=2.7818, lr=0.000281, tokens/sec=1375170.80, grad_norm=0.3157, duration=0.38s
Step 15097: loss=2.8052, lr=0.000281, tokens/sec=1371215.17, grad_norm=0.3199, duration=0.38s
Step 15098: loss=2.7654, lr=0.000281, tokens/sec=1373037.95, grad_norm=0.3431, duration=0.38s
Step 15099: loss=2.7880, lr=0.000281, tokens/sec=1375712.80, grad_norm=0.3161, duration=0.38s
Step 15100/19073 (79.2%), Elapsed time: 5979.73s, Steps per hour: 9090.72, Estimated hours remaining: 0.44
Step 15100: loss=2.7558, lr=0.000281, tokens/sec=1376132.92, grad_norm=0.3025, duration=0.38s
Step 15101: loss=2.8067, lr=0.000281, tokens/sec=1375583.71, grad_norm=0.3295, duration=0.38s
Step 15102: loss=2.8188, lr=0.000281, tokens/sec=1376256.08, grad_norm=0.3479, duration=0.38s
Step 15103: loss=2.8399, lr=0.000281, tokens/sec=1374902.55, grad_norm=0.3329, duration=0.38s
Step 15104: loss=2.8564, lr=0.000281, tokens/sec=1376315.52, grad_norm=0.3237, duration=0.38s
Step 15105: loss=2.8499, lr=0.000281, tokens/sec=1376269.00, grad_norm=0.3259, duration=0.38s
Step 15106: loss=2.8656, lr=0.000281, tokens/sec=1375056.44, grad_norm=0.3532, duration=0.38s
Step 15107: loss=2.8576, lr=0.000281, tokens/sec=1373210.29, grad_norm=0.3333, duration=0.38s
Step 15108: loss=2.8127, lr=0.000281, tokens/sec=1374871.60, grad_norm=0.3313, duration=0.38s
Step 15109: loss=2.8427, lr=0.000281, tokens/sec=1371835.35, grad_norm=0.3258, duration=0.38s
Step 15110: loss=2.7894, lr=0.000281, tokens/sec=1376870.48, grad_norm=0.3315, duration=0.38s
Step 15111: loss=2.8704, lr=0.000281, tokens/sec=1379182.19, grad_norm=0.3321, duration=0.38s
Step 15112: loss=2.8614, lr=0.000281, tokens/sec=1376108.81, grad_norm=0.3155, duration=0.38s
Step 15113: loss=2.8549, lr=0.000281, tokens/sec=1375553.60, grad_norm=0.3182, duration=0.38s
Step 15114: loss=2.8543, lr=0.000281, tokens/sec=1373797.94, grad_norm=0.3140, duration=0.38s
Step 15115: loss=2.7934, lr=0.000281, tokens/sec=1376501.60, grad_norm=0.3109, duration=0.38s
Step 15116: loss=2.8224, lr=0.000281, tokens/sec=1375688.70, grad_norm=0.3196, duration=0.38s
Step 15117: loss=2.8727, lr=0.000281, tokens/sec=1377074.83, grad_norm=0.3013, duration=0.38s
Step 15118: loss=2.8824, lr=0.000281, tokens/sec=1373725.85, grad_norm=0.3042, duration=0.38s
Step 15119: loss=2.8151, lr=0.000281, tokens/sec=1375884.95, grad_norm=0.3202, duration=0.38s
Step 15120: loss=2.8527, lr=0.000281, tokens/sec=1378263.32, grad_norm=0.3198, duration=0.38s
Step 15121: loss=2.8575, lr=0.000281, tokens/sec=1378914.10, grad_norm=0.3061, duration=0.38s
Step 15122: loss=2.8543, lr=0.000281, tokens/sec=1377659.75, grad_norm=0.3033, duration=0.38s
Step 15123: loss=2.8163, lr=0.000281, tokens/sec=1374528.71, grad_norm=0.3322, duration=0.38s
Step 15124: loss=2.8425, lr=0.000281, tokens/sec=1373238.59, grad_norm=0.3352, duration=0.38s
Step 15125: loss=2.8157, lr=0.000281, tokens/sec=1377483.71, grad_norm=0.3142, duration=0.38s
Step 15126: loss=2.8647, lr=0.000280, tokens/sec=1376057.14, grad_norm=0.3207, duration=0.38s
Step 15127: loss=2.8655, lr=0.000280, tokens/sec=1376798.07, grad_norm=0.3110, duration=0.38s
Step 15128: loss=2.8923, lr=0.000280, tokens/sec=1379798.34, grad_norm=0.3188, duration=0.38s
Step 15129: loss=2.8529, lr=0.000280, tokens/sec=1376712.74, grad_norm=0.3150, duration=0.38s
Step 15130: loss=2.8579, lr=0.000280, tokens/sec=1375847.93, grad_norm=0.3112, duration=0.38s
Step 15131: loss=2.8844, lr=0.000280, tokens/sec=1375965.88, grad_norm=0.3106, duration=0.38s
Step 15132: loss=2.8238, lr=0.000280, tokens/sec=1374154.21, grad_norm=0.3048, duration=0.38s
Step 15133: loss=2.8484, lr=0.000280, tokens/sec=1375399.59, grad_norm=0.3331, duration=0.38s
Step 15134: loss=2.8701, lr=0.000280, tokens/sec=1376884.28, grad_norm=0.3356, duration=0.38s
Step 15135: loss=2.8869, lr=0.000280, tokens/sec=1371164.73, grad_norm=0.3290, duration=0.38s
Step 15136: loss=2.8339, lr=0.000280, tokens/sec=1374309.65, grad_norm=0.3231, duration=0.38s
Step 15137: loss=2.8449, lr=0.000280, tokens/sec=1373117.68, grad_norm=0.3387, duration=0.38s
Step 15138: loss=2.7853, lr=0.000280, tokens/sec=1376692.05, grad_norm=0.3839, duration=0.38s
Step 15139: loss=2.7931, lr=0.000280, tokens/sec=1375915.94, grad_norm=0.3255, duration=0.38s
Step 15140: loss=2.8247, lr=0.000280, tokens/sec=1372656.56, grad_norm=0.3216, duration=0.38s
Step 15141: loss=2.7251, lr=0.000280, tokens/sec=1373269.46, grad_norm=0.3322, duration=0.38s
Step 15142: loss=2.7896, lr=0.000280, tokens/sec=1373822.83, grad_norm=0.3449, duration=0.38s
Step 15143: loss=2.7722, lr=0.000280, tokens/sec=1376210.43, grad_norm=0.3370, duration=0.38s
Step 15144: loss=2.8385, lr=0.000280, tokens/sec=1379633.87, grad_norm=0.3125, duration=0.38s
Step 15145: loss=2.7989, lr=0.000280, tokens/sec=1380020.02, grad_norm=0.3388, duration=0.38s
Step 15146: loss=2.7876, lr=0.000280, tokens/sec=1374371.50, grad_norm=0.3632, duration=0.38s
Step 15147: loss=2.8001, lr=0.000280, tokens/sec=1378711.80, grad_norm=0.3248, duration=0.38s
Step 15148: loss=2.7971, lr=0.000280, tokens/sec=1379029.97, grad_norm=0.3099, duration=0.38s
Step 15149: loss=2.8233, lr=0.000280, tokens/sec=1373100.54, grad_norm=0.3378, duration=0.38s
Step 15150: loss=2.8456, lr=0.000280, tokens/sec=1373773.05, grad_norm=0.3802, duration=0.38s
Step 15151: loss=2.8803, lr=0.000280, tokens/sec=1375178.54, grad_norm=0.3450, duration=0.38s
Step 15152: loss=2.8997, lr=0.000280, tokens/sec=1375553.60, grad_norm=0.3035, duration=0.38s
Step 15153: loss=2.8483, lr=0.000280, tokens/sec=1375787.68, grad_norm=0.3084, duration=0.38s
Step 15154: loss=2.8887, lr=0.000280, tokens/sec=1374563.07, grad_norm=0.3840, duration=0.38s
Step 15155: loss=2.8053, lr=0.000280, tokens/sec=1376671.37, grad_norm=0.3559, duration=0.38s
Step 15156: loss=2.8523, lr=0.000280, tokens/sec=1377449.19, grad_norm=0.3186, duration=0.38s
Step 15157: loss=2.8666, lr=0.000280, tokens/sec=1375392.71, grad_norm=0.3218, duration=0.38s
Step 15158: loss=2.9563, lr=0.000280, tokens/sec=1377255.09, grad_norm=0.3631, duration=0.38s
Step 15159: loss=2.8969, lr=0.000280, tokens/sec=1371740.36, grad_norm=0.3643, duration=0.38s
Step 15160: loss=2.8655, lr=0.000280, tokens/sec=1353378.34, grad_norm=0.3389, duration=0.39s
Step 15161: loss=2.8530, lr=0.000280, tokens/sec=1377673.56, grad_norm=0.3256, duration=0.38s
Step 15162: loss=2.8724, lr=0.000280, tokens/sec=1372848.51, grad_norm=0.3391, duration=0.38s
Step 15163: loss=2.7663, lr=0.000280, tokens/sec=1371745.50, grad_norm=0.3542, duration=0.38s
Step 15164: loss=2.7885, lr=0.000280, tokens/sec=1377296.49, grad_norm=0.3486, duration=0.38s
Step 15165: loss=2.8750, lr=0.000280, tokens/sec=1373180.28, grad_norm=0.3229, duration=0.38s
Step 15166: loss=2.8167, lr=0.000280, tokens/sec=1377634.73, grad_norm=0.3306, duration=0.38s
Step 15167: loss=2.8917, lr=0.000280, tokens/sec=1377585.53, grad_norm=0.3385, duration=0.38s
Step 15168: loss=2.9169, lr=0.000280, tokens/sec=1375374.65, grad_norm=0.3511, duration=0.38s
Step 15169: loss=2.8474, lr=0.000280, tokens/sec=1376536.93, grad_norm=0.3408, duration=0.38s
Step 15170: loss=2.8159, lr=0.000280, tokens/sec=1374690.25, grad_norm=0.3334, duration=0.38s
Step 15171: loss=2.8994, lr=0.000280, tokens/sec=1373667.50, grad_norm=0.3590, duration=0.38s
Step 15172: loss=2.9162, lr=0.000280, tokens/sec=1376012.37, grad_norm=0.3361, duration=0.38s
Step 15173: loss=2.8702, lr=0.000280, tokens/sec=1375988.26, grad_norm=0.3220, duration=0.38s
Step 15174: loss=2.7958, lr=0.000280, tokens/sec=1373102.25, grad_norm=0.3314, duration=0.38s
Step 15175: loss=2.8452, lr=0.000280, tokens/sec=1373540.51, grad_norm=0.3293, duration=0.38s
Step 15176: loss=2.8555, lr=0.000280, tokens/sec=1375664.60, grad_norm=0.3236, duration=0.38s
Step 15177: loss=2.8835, lr=0.000279, tokens/sec=1376509.36, grad_norm=0.3173, duration=0.38s
Step 15178: loss=2.8888, lr=0.000279, tokens/sec=1377508.73, grad_norm=0.3341, duration=0.38s
Step 15179: loss=2.8185, lr=0.000279, tokens/sec=1376144.12, grad_norm=0.3262, duration=0.38s
Step 15180: loss=2.8344, lr=0.000279, tokens/sec=1374781.35, grad_norm=0.3328, duration=0.38s
Step 15181: loss=2.8276, lr=0.000279, tokens/sec=1375705.05, grad_norm=0.3324, duration=0.38s
Step 15182: loss=2.8193, lr=0.000279, tokens/sec=1375438.31, grad_norm=0.3131, duration=0.38s
Step 15183: loss=2.8309, lr=0.000279, tokens/sec=1378833.69, grad_norm=0.3261, duration=0.38s
Step 15184: loss=2.8041, lr=0.000279, tokens/sec=1377063.62, grad_norm=0.3087, duration=0.38s
Step 15185: loss=2.8066, lr=0.000279, tokens/sec=1374229.78, grad_norm=0.3324, duration=0.38s
Step 15186: loss=2.7715, lr=0.000279, tokens/sec=1375302.39, grad_norm=0.3359, duration=0.38s
Step 15187: loss=2.8658, lr=0.000279, tokens/sec=1377421.58, grad_norm=0.3240, duration=0.38s
Step 15188: loss=2.8047, lr=0.000279, tokens/sec=1374294.19, grad_norm=0.2952, duration=0.38s
Step 15189: loss=2.7680, lr=0.000279, tokens/sec=1376670.51, grad_norm=0.3024, duration=0.38s
Step 15190: loss=2.7610, lr=0.000279, tokens/sec=1375235.30, grad_norm=0.3234, duration=0.38s
Step 15191: loss=2.7785, lr=0.000279, tokens/sec=1378243.45, grad_norm=0.3352, duration=0.38s
Step 15192: loss=2.8290, lr=0.000279, tokens/sec=1377409.51, grad_norm=0.3099, duration=0.38s
Step 15193: loss=2.8060, lr=0.000279, tokens/sec=1375823.83, grad_norm=0.3138, duration=0.38s
Step 15194: loss=2.8172, lr=0.000279, tokens/sec=1376358.59, grad_norm=0.3245, duration=0.38s
Step 15195: loss=2.8475, lr=0.000279, tokens/sec=1375785.96, grad_norm=0.3167, duration=0.38s
Step 15196: loss=2.8548, lr=0.000279, tokens/sec=1376842.03, grad_norm=0.3205, duration=0.38s
Step 15197: loss=2.8857, lr=0.000279, tokens/sec=1378508.69, grad_norm=0.3234, duration=0.38s
Step 15198: loss=2.8221, lr=0.000279, tokens/sec=1371153.61, grad_norm=0.3181, duration=0.38s
Step 15199: loss=2.8566, lr=0.000279, tokens/sec=1374258.98, grad_norm=0.3157, duration=0.38s
Step 15200/19073 (79.7%), Elapsed time: 6017.93s, Steps per hour: 9092.83, Estimated hours remaining: 0.43
Step 15200: loss=2.8228, lr=0.000279, tokens/sec=1371800.26, grad_norm=0.3194, duration=0.38s
Step 15201: loss=2.8630, lr=0.000279, tokens/sec=1373003.66, grad_norm=0.3149, duration=0.38s
Step 15202: loss=2.8741, lr=0.000279, tokens/sec=1375669.77, grad_norm=0.3126, duration=0.38s
Step 15203: loss=2.9147, lr=0.000279, tokens/sec=1376108.81, grad_norm=0.3202, duration=0.38s
Step 15204: loss=2.8683, lr=0.000279, tokens/sec=1378527.70, grad_norm=0.3248, duration=0.38s
Step 15205: loss=2.8794, lr=0.000279, tokens/sec=1375298.95, grad_norm=0.3354, duration=0.38s
Step 15206: loss=2.9193, lr=0.000279, tokens/sec=1376135.51, grad_norm=0.3301, duration=0.38s
Step 15207: loss=2.8673, lr=0.000279, tokens/sec=1377555.33, grad_norm=0.3096, duration=0.38s
Step 15208: loss=2.8844, lr=0.000279, tokens/sec=1375162.20, grad_norm=0.3291, duration=0.38s
Step 15209: loss=2.7993, lr=0.000279, tokens/sec=1376476.62, grad_norm=0.3370, duration=0.38s
Step 15210: loss=2.8729, lr=0.000279, tokens/sec=1376305.18, grad_norm=0.3320, duration=0.38s
Step 15211: loss=2.8383, lr=0.000279, tokens/sec=1377130.02, grad_norm=0.2979, duration=0.38s
Step 15212: loss=2.8861, lr=0.000279, tokens/sec=1374655.01, grad_norm=0.3159, duration=0.38s
Step 15213: loss=2.8189, lr=0.000279, tokens/sec=1375124.37, grad_norm=0.3301, duration=0.38s
Step 15214: loss=2.8924, lr=0.000279, tokens/sec=1372978.80, grad_norm=0.3307, duration=0.38s
Step 15215: loss=2.8652, lr=0.000279, tokens/sec=1373614.30, grad_norm=0.3519, duration=0.38s
Step 15216: loss=2.8247, lr=0.000279, tokens/sec=1370513.55, grad_norm=0.3264, duration=0.38s
Step 15217: loss=2.8733, lr=0.000279, tokens/sec=1374732.36, grad_norm=0.3187, duration=0.38s
Step 15218: loss=2.7655, lr=0.000279, tokens/sec=1375170.80, grad_norm=0.3162, duration=0.38s
Step 15219: loss=2.7799, lr=0.000279, tokens/sec=1373746.45, grad_norm=0.3528, duration=0.38s
Step 15220: loss=2.8210, lr=0.000279, tokens/sec=1374573.38, grad_norm=0.3215, duration=0.38s
Step 15221: loss=2.8591, lr=0.000279, tokens/sec=1374185.98, grad_norm=0.3149, duration=0.38s
Step 15222: loss=2.9596, lr=0.000279, tokens/sec=1377395.70, grad_norm=0.3170, duration=0.38s
Step 15223: loss=2.7728, lr=0.000279, tokens/sec=1375453.79, grad_norm=0.3113, duration=0.38s
Step 15224: loss=2.9036, lr=0.000279, tokens/sec=1373700.97, grad_norm=0.3196, duration=0.38s
Step 15225: loss=2.8805, lr=0.000279, tokens/sec=1373893.22, grad_norm=0.3075, duration=0.38s
Step 15226: loss=2.8122, lr=0.000279, tokens/sec=1373062.81, grad_norm=0.3271, duration=0.38s
Step 15227: loss=2.8671, lr=0.000279, tokens/sec=1372454.37, grad_norm=0.3171, duration=0.38s
Step 15228: loss=2.8364, lr=0.000279, tokens/sec=1372670.27, grad_norm=0.3081, duration=0.38s
Step 15229: loss=2.8438, lr=0.000278, tokens/sec=1371883.28, grad_norm=0.3109, duration=0.38s
Step 15230: loss=2.7723, lr=0.000278, tokens/sec=1375833.30, grad_norm=0.3194, duration=0.38s
Step 15231: loss=2.8422, lr=0.000278, tokens/sec=1377697.73, grad_norm=0.3119, duration=0.38s
Step 15232: loss=2.8421, lr=0.000278, tokens/sec=1372781.66, grad_norm=0.3213, duration=0.38s
Step 15233: loss=2.8465, lr=0.000278, tokens/sec=1376378.40, grad_norm=0.3295, duration=0.38s
Step 15234: loss=2.8198, lr=0.000278, tokens/sec=1377297.35, grad_norm=0.3084, duration=0.38s
Step 15235: loss=2.7545, lr=0.000278, tokens/sec=1372206.87, grad_norm=0.2995, duration=0.38s
Step 15236: loss=2.8167, lr=0.000278, tokens/sec=1378662.53, grad_norm=0.3090, duration=0.38s
Step 15237: loss=2.7821, lr=0.000278, tokens/sec=1375900.45, grad_norm=0.3090, duration=0.38s
Step 15238: loss=2.7774, lr=0.000278, tokens/sec=1375325.62, grad_norm=0.3025, duration=0.38s
Step 15239: loss=2.7770, lr=0.000278, tokens/sec=1377092.08, grad_norm=0.3064, duration=0.38s
Step 15240: loss=2.8147, lr=0.000278, tokens/sec=1374834.64, grad_norm=0.3009, duration=0.38s
Step 15241: loss=2.7761, lr=0.000278, tokens/sec=1373090.25, grad_norm=0.3040, duration=0.38s
Step 15242: loss=2.7856, lr=0.000278, tokens/sec=1377740.03, grad_norm=0.3149, duration=0.38s
Step 15243: loss=2.8142, lr=0.000278, tokens/sec=1375255.95, grad_norm=0.3128, duration=0.38s
Step 15244: loss=2.7864, lr=0.000278, tokens/sec=1376383.57, grad_norm=0.3005, duration=0.38s
Step 15245: loss=2.7615, lr=0.000278, tokens/sec=1377475.08, grad_norm=0.2828, duration=0.38s
Step 15246: loss=2.7940, lr=0.000278, tokens/sec=1378585.60, grad_norm=0.2995, duration=0.38s
Step 15247: loss=2.8681, lr=0.000278, tokens/sec=1375691.28, grad_norm=0.3194, duration=0.38s
Step 15248: loss=2.8846, lr=0.000278, tokens/sec=1373315.77, grad_norm=0.3259, duration=0.38s
Step 15249: loss=2.8331, lr=0.000278, tokens/sec=1374922.32, grad_norm=0.3063, duration=0.38s
Validation loss at step 15250: 3.8357160091400146
Step 15250: loss=2.8421, lr=0.000278, tokens/sec=152532.46, grad_norm=0.3156, duration=3.44s
Step 15251: loss=2.8591, lr=0.000278, tokens/sec=1376132.06, grad_norm=0.3294, duration=0.38s
Step 15252: loss=2.9202, lr=0.000278, tokens/sec=1378105.25, grad_norm=0.3345, duration=0.38s
Step 15253: loss=2.9056, lr=0.000278, tokens/sec=1377887.65, grad_norm=0.3108, duration=0.38s
Step 15254: loss=2.8314, lr=0.000278, tokens/sec=1377673.56, grad_norm=0.3072, duration=0.38s
Step 15255: loss=2.8286, lr=0.000278, tokens/sec=1375018.61, grad_norm=0.3281, duration=0.38s
Step 15256: loss=2.8207, lr=0.000278, tokens/sec=1377667.52, grad_norm=0.3473, duration=0.38s
Step 15257: loss=2.8466, lr=0.000278, tokens/sec=1378116.48, grad_norm=0.3294, duration=0.38s
Step 15258: loss=2.8534, lr=0.000278, tokens/sec=1374259.84, grad_norm=0.3007, duration=0.38s
Step 15259: loss=2.8391, lr=0.000278, tokens/sec=1375311.85, grad_norm=0.3275, duration=0.38s
Step 15260: loss=2.8709, lr=0.000278, tokens/sec=1378242.58, grad_norm=0.3561, duration=0.38s
Step 15261: loss=2.7792, lr=0.000278, tokens/sec=1367972.89, grad_norm=0.3585, duration=0.38s
Step 15262: loss=2.8059, lr=0.000278, tokens/sec=1379465.10, grad_norm=0.3242, duration=0.38s
Step 15263: loss=2.8837, lr=0.000278, tokens/sec=1373872.62, grad_norm=0.3180, duration=0.38s
Step 15264: loss=2.8406, lr=0.000278, tokens/sec=1373454.73, grad_norm=0.3446, duration=0.38s
Step 15265: loss=2.8734, lr=0.000278, tokens/sec=1372378.14, grad_norm=0.3635, duration=0.38s
Step 15266: loss=2.8413, lr=0.000278, tokens/sec=1374751.27, grad_norm=0.3370, duration=0.38s
Step 15267: loss=2.9028, lr=0.000278, tokens/sec=1370562.24, grad_norm=0.3245, duration=0.38s
Step 15268: loss=2.8641, lr=0.000278, tokens/sec=1377040.34, grad_norm=0.3139, duration=0.38s
Step 15269: loss=2.8333, lr=0.000278, tokens/sec=1372309.63, grad_norm=0.3431, duration=0.38s
Step 15270: loss=2.8331, lr=0.000278, tokens/sec=1373717.27, grad_norm=0.3318, duration=0.38s
Step 15271: loss=2.8598, lr=0.000278, tokens/sec=1372900.80, grad_norm=0.3228, duration=0.38s
Step 15272: loss=2.8353, lr=0.000278, tokens/sec=1368942.85, grad_norm=0.3048, duration=0.38s
Step 15273: loss=2.8736, lr=0.000278, tokens/sec=1374902.55, grad_norm=0.3119, duration=0.38s
Step 15274: loss=2.8462, lr=0.000278, tokens/sec=1377834.12, grad_norm=0.3169, duration=0.38s
Step 15275: loss=2.8402, lr=0.000278, tokens/sec=1374736.66, grad_norm=0.3233, duration=0.38s
Step 15276: loss=2.8569, lr=0.000278, tokens/sec=1374742.67, grad_norm=0.3228, duration=0.38s
Step 15277: loss=2.8012, lr=0.000278, tokens/sec=1375711.08, grad_norm=0.3042, duration=0.38s
Step 15278: loss=2.8597, lr=0.000278, tokens/sec=1376980.84, grad_norm=0.3086, duration=0.38s
Step 15279: loss=2.8445, lr=0.000278, tokens/sec=1373811.68, grad_norm=0.3282, duration=0.38s
Step 15280: loss=2.8782, lr=0.000278, tokens/sec=1374043.45, grad_norm=0.3155, duration=0.38s
Step 15281: loss=2.7896, lr=0.000277, tokens/sec=1373278.90, grad_norm=0.2956, duration=0.38s
Step 15282: loss=2.8467, lr=0.000277, tokens/sec=1377612.29, grad_norm=0.3082, duration=0.38s
Step 15283: loss=2.7960, lr=0.000277, tokens/sec=1378068.12, grad_norm=0.3369, duration=0.38s
Step 15284: loss=2.8116, lr=0.000277, tokens/sec=1377335.31, grad_norm=0.3131, duration=0.38s
Step 15285: loss=2.7423, lr=0.000277, tokens/sec=1378456.84, grad_norm=0.2941, duration=0.38s
Step 15286: loss=2.7861, lr=0.000277, tokens/sec=1373609.15, grad_norm=0.3054, duration=0.38s
Step 15287: loss=2.8087, lr=0.000277, tokens/sec=1375316.15, grad_norm=0.3169, duration=0.38s
Step 15288: loss=2.7620, lr=0.000277, tokens/sec=1375138.99, grad_norm=0.3299, duration=0.38s
Step 15289: loss=2.7728, lr=0.000277, tokens/sec=1375786.82, grad_norm=0.3154, duration=0.38s
Step 15290: loss=2.7460, lr=0.000277, tokens/sec=1372422.68, grad_norm=0.3175, duration=0.38s
Step 15291: loss=2.8295, lr=0.000277, tokens/sec=1377667.52, grad_norm=0.3177, duration=0.38s
Step 15292: loss=2.8129, lr=0.000277, tokens/sec=1377665.80, grad_norm=0.3193, duration=0.38s
Step 15293: loss=2.8102, lr=0.000277, tokens/sec=1377484.57, grad_norm=0.3305, duration=0.38s
Step 15294: loss=2.8448, lr=0.000277, tokens/sec=1374968.74, grad_norm=0.3303, duration=0.38s
Step 15295: loss=2.8575, lr=0.000277, tokens/sec=1378522.52, grad_norm=0.3144, duration=0.38s
Step 15296: loss=2.8537, lr=0.000277, tokens/sec=1377246.46, grad_norm=0.3301, duration=0.38s
Step 15297: loss=2.8719, lr=0.000277, tokens/sec=1373544.80, grad_norm=0.3372, duration=0.38s
Step 15298: loss=2.7916, lr=0.000277, tokens/sec=1374478.88, grad_norm=0.3329, duration=0.38s
Step 15299: loss=2.8177, lr=0.000277, tokens/sec=1375173.38, grad_norm=0.3087, duration=0.38s
Step 15300/19073 (80.2%), Elapsed time: 6059.19s, Steps per hour: 9090.32, Estimated hours remaining: 0.42
Step 15300: loss=2.8030, lr=0.000277, tokens/sec=1376346.53, grad_norm=0.3117, duration=0.38s
Step 15301: loss=2.8669, lr=0.000277, tokens/sec=1378993.65, grad_norm=0.3167, duration=0.38s
Step 15302: loss=2.8617, lr=0.000277, tokens/sec=1374836.36, grad_norm=0.3121, duration=0.38s
Step 15303: loss=2.8512, lr=0.000277, tokens/sec=1376037.34, grad_norm=0.3148, duration=0.38s
Step 15304: loss=2.8217, lr=0.000277, tokens/sec=1377099.84, grad_norm=0.3087, duration=0.38s
Step 15305: loss=2.8049, lr=0.000277, tokens/sec=1374961.00, grad_norm=0.3049, duration=0.38s
Step 15306: loss=2.8605, lr=0.000277, tokens/sec=1372938.51, grad_norm=0.3154, duration=0.38s
Step 15307: loss=2.8717, lr=0.000277, tokens/sec=1374365.48, grad_norm=0.3135, duration=0.38s
Step 15308: loss=2.8602, lr=0.000277, tokens/sec=1377475.08, grad_norm=0.2939, duration=0.38s
Step 15309: loss=2.7956, lr=0.000277, tokens/sec=1374967.02, grad_norm=0.3032, duration=0.38s
Step 15310: loss=2.8665, lr=0.000277, tokens/sec=1376777.38, grad_norm=0.3160, duration=0.38s
Step 15311: loss=2.8363, lr=0.000277, tokens/sec=1373790.22, grad_norm=0.3136, duration=0.38s
Step 15312: loss=2.8741, lr=0.000277, tokens/sec=1375674.93, grad_norm=0.2987, duration=0.38s
Step 15313: loss=2.7715, lr=0.000277, tokens/sec=1372327.61, grad_norm=0.2917, duration=0.38s
Step 15314: loss=2.8625, lr=0.000277, tokens/sec=1378200.26, grad_norm=0.3182, duration=0.38s
Step 15315: loss=2.8055, lr=0.000277, tokens/sec=1378158.80, grad_norm=0.3037, duration=0.38s
Step 15316: loss=2.8911, lr=0.000277, tokens/sec=1375366.90, grad_norm=0.3103, duration=0.38s
Step 15317: loss=2.8674, lr=0.000277, tokens/sec=1375470.14, grad_norm=0.3018, duration=0.38s
Step 15318: loss=2.8672, lr=0.000277, tokens/sec=1379906.57, grad_norm=0.3035, duration=0.38s
Step 15319: loss=2.8450, lr=0.000277, tokens/sec=1375483.04, grad_norm=0.3082, duration=0.38s
Step 15320: loss=2.8826, lr=0.000277, tokens/sec=1374647.28, grad_norm=0.3112, duration=0.38s
Step 15321: loss=2.8162, lr=0.000277, tokens/sec=1375267.99, grad_norm=0.3093, duration=0.38s
Step 15322: loss=2.8640, lr=0.000277, tokens/sec=1376243.16, grad_norm=0.3128, duration=0.38s
Step 15323: loss=2.8478, lr=0.000277, tokens/sec=1372631.71, grad_norm=0.3108, duration=0.38s
Step 15324: loss=2.8842, lr=0.000277, tokens/sec=1377830.67, grad_norm=0.3140, duration=0.38s
Step 15325: loss=2.8401, lr=0.000277, tokens/sec=1374691.97, grad_norm=0.3260, duration=0.38s
Step 15326: loss=2.8477, lr=0.000277, tokens/sec=1377081.73, grad_norm=0.3036, duration=0.38s
Step 15327: loss=2.7796, lr=0.000277, tokens/sec=1371229.71, grad_norm=0.3520, duration=0.38s
Step 15328: loss=2.8446, lr=0.000277, tokens/sec=1375465.84, grad_norm=0.3338, duration=0.38s
Step 15329: loss=2.7894, lr=0.000277, tokens/sec=1376644.65, grad_norm=0.3394, duration=0.38s
Step 15330: loss=2.8016, lr=0.000277, tokens/sec=1373705.26, grad_norm=0.3173, duration=0.38s
Step 15331: loss=2.7408, lr=0.000277, tokens/sec=1375280.89, grad_norm=0.3225, duration=0.38s
Step 15332: loss=2.7678, lr=0.000277, tokens/sec=1375238.74, grad_norm=0.3639, duration=0.38s
Step 15333: loss=2.7714, lr=0.000276, tokens/sec=1376256.08, grad_norm=0.3539, duration=0.38s
Step 15334: loss=2.8255, lr=0.000276, tokens/sec=1379031.70, grad_norm=0.3260, duration=0.38s
Step 15335: loss=2.8061, lr=0.000276, tokens/sec=1374678.22, grad_norm=0.3363, duration=0.38s
Step 15336: loss=2.8121, lr=0.000276, tokens/sec=1373670.93, grad_norm=0.3733, duration=0.38s
Step 15337: loss=2.7637, lr=0.000276, tokens/sec=1377267.16, grad_norm=0.3512, duration=0.38s
Step 15338: loss=2.7894, lr=0.000276, tokens/sec=1376336.19, grad_norm=0.3313, duration=0.38s
Step 15339: loss=2.8313, lr=0.000276, tokens/sec=1373276.32, grad_norm=0.3281, duration=0.38s
Step 15340: loss=2.8562, lr=0.000276, tokens/sec=1376323.27, grad_norm=0.3578, duration=0.38s
Step 15341: loss=2.8883, lr=0.000276, tokens/sec=1374476.30, grad_norm=0.3742, duration=0.38s
Step 15342: loss=2.8654, lr=0.000276, tokens/sec=1375116.63, grad_norm=0.3534, duration=0.38s
Step 15343: loss=2.8527, lr=0.000276, tokens/sec=1374830.34, grad_norm=0.3211, duration=0.38s
Step 15344: loss=2.8529, lr=0.000276, tokens/sec=1374356.89, grad_norm=0.3422, duration=0.38s
Step 15345: loss=2.8269, lr=0.000276, tokens/sec=1374589.71, grad_norm=0.3688, duration=0.38s
Step 15346: loss=2.8247, lr=0.000276, tokens/sec=1373738.73, grad_norm=0.3439, duration=0.38s
Step 15347: loss=2.8940, lr=0.000276, tokens/sec=1375784.24, grad_norm=0.3282, duration=0.38s
Step 15348: loss=2.9468, lr=0.000276, tokens/sec=1376415.45, grad_norm=0.3532, duration=0.38s
Step 15349: loss=2.8738, lr=0.000276, tokens/sec=1376429.23, grad_norm=0.3727, duration=0.38s
Step 15350: loss=2.9001, lr=0.000276, tokens/sec=1373286.61, grad_norm=0.3712, duration=0.38s
Step 15351: loss=2.8397, lr=0.000276, tokens/sec=1376185.46, grad_norm=0.3269, duration=0.38s
Step 15352: loss=2.8003, lr=0.000276, tokens/sec=1374578.54, grad_norm=0.3464, duration=0.38s
Step 15353: loss=2.8161, lr=0.000276, tokens/sec=1375671.49, grad_norm=0.3730, duration=0.38s
Step 15354: loss=2.7852, lr=0.000276, tokens/sec=1374643.84, grad_norm=0.3611, duration=0.38s
Step 15355: loss=2.9021, lr=0.000276, tokens/sec=1377132.61, grad_norm=0.3275, duration=0.38s
Step 15356: loss=2.8043, lr=0.000276, tokens/sec=1376117.42, grad_norm=0.3303, duration=0.38s
Step 15357: loss=2.8918, lr=0.000276, tokens/sec=1374961.00, grad_norm=0.3752, duration=0.38s
Step 15358: loss=2.8915, lr=0.000276, tokens/sec=1373423.85, grad_norm=0.3827, duration=0.38s
Step 15359: loss=2.8561, lr=0.000276, tokens/sec=1376315.52, grad_norm=0.3424, duration=0.38s
Step 15360: loss=2.8470, lr=0.000276, tokens/sec=1375554.46, grad_norm=0.3277, duration=0.38s
Step 15361: loss=2.8845, lr=0.000276, tokens/sec=1371523.91, grad_norm=0.3579, duration=0.38s
Step 15362: loss=2.8999, lr=0.000276, tokens/sec=1374233.22, grad_norm=0.3509, duration=0.38s
Step 15363: loss=2.8659, lr=0.000276, tokens/sec=1374979.06, grad_norm=0.3389, duration=0.38s
Step 15364: loss=2.7861, lr=0.000276, tokens/sec=1376045.09, grad_norm=0.3284, duration=0.38s
Step 15365: loss=2.8474, lr=0.000276, tokens/sec=1374414.45, grad_norm=0.3359, duration=0.38s
Step 15366: loss=2.8621, lr=0.000276, tokens/sec=1372829.66, grad_norm=0.3370, duration=0.38s
Step 15367: loss=2.8586, lr=0.000276, tokens/sec=1374826.04, grad_norm=0.3400, duration=0.38s
Step 15368: loss=2.8938, lr=0.000276, tokens/sec=1375601.78, grad_norm=0.3497, duration=0.38s
Step 15369: loss=2.8186, lr=0.000276, tokens/sec=1376578.29, grad_norm=0.3300, duration=0.38s
Step 15370: loss=2.8258, lr=0.000276, tokens/sec=1378906.31, grad_norm=0.3173, duration=0.38s
Step 15371: loss=2.8572, lr=0.000276, tokens/sec=1370370.07, grad_norm=0.3299, duration=0.38s
Step 15372: loss=2.7886, lr=0.000276, tokens/sec=1372114.40, grad_norm=0.3490, duration=0.38s
Step 15373: loss=2.8371, lr=0.000276, tokens/sec=1374199.72, grad_norm=0.3476, duration=0.38s
Step 15374: loss=2.8180, lr=0.000276, tokens/sec=1374983.36, grad_norm=0.3193, duration=0.38s
Step 15375: loss=2.7638, lr=0.000276, tokens/sec=1373188.00, grad_norm=0.3131, duration=0.38s
Step 15376: loss=2.8024, lr=0.000276, tokens/sec=1377142.96, grad_norm=0.3359, duration=0.38s
Step 15377: loss=2.8501, lr=0.000276, tokens/sec=1374203.16, grad_norm=0.3523, duration=0.38s
Step 15378: loss=2.7884, lr=0.000276, tokens/sec=1379010.95, grad_norm=0.3308, duration=0.38s
Step 15379: loss=2.7843, lr=0.000276, tokens/sec=1375498.53, grad_norm=0.3091, duration=0.38s
Step 15380: loss=2.7478, lr=0.000276, tokens/sec=1375907.33, grad_norm=0.3237, duration=0.38s
Step 15381: loss=2.7873, lr=0.000276, tokens/sec=1378602.89, grad_norm=0.3320, duration=0.38s
Step 15382: loss=2.8227, lr=0.000276, tokens/sec=1377285.28, grad_norm=0.3250, duration=0.38s
Step 15383: loss=2.7948, lr=0.000276, tokens/sec=1377509.59, grad_norm=0.3357, duration=0.38s
Step 15384: loss=2.8010, lr=0.000276, tokens/sec=1375711.08, grad_norm=0.3427, duration=0.38s
Step 15385: loss=2.8527, lr=0.000276, tokens/sec=1378713.53, grad_norm=0.3467, duration=0.38s
Step 15386: loss=2.8746, lr=0.000276, tokens/sec=1376898.07, grad_norm=0.3308, duration=0.38s
Step 15387: loss=2.8499, lr=0.000275, tokens/sec=1376488.68, grad_norm=0.3273, duration=0.38s
Step 15388: loss=2.8260, lr=0.000275, tokens/sec=1377049.82, grad_norm=0.3402, duration=0.38s
Step 15389: loss=2.8459, lr=0.000275, tokens/sec=1376758.42, grad_norm=0.3332, duration=0.38s
Step 15390: loss=2.8305, lr=0.000275, tokens/sec=1374650.72, grad_norm=0.3360, duration=0.38s
Step 15391: loss=2.8650, lr=0.000275, tokens/sec=1377607.97, grad_norm=0.3354, duration=0.38s
Step 15392: loss=2.8412, lr=0.000275, tokens/sec=1377258.54, grad_norm=0.3284, duration=0.38s
Step 15393: loss=2.9263, lr=0.000275, tokens/sec=1376972.22, grad_norm=0.3395, duration=0.38s
Step 15394: loss=2.8594, lr=0.000275, tokens/sec=1376710.15, grad_norm=0.3288, duration=0.38s
Step 15395: loss=2.8732, lr=0.000275, tokens/sec=1376343.94, grad_norm=0.3549, duration=0.38s
Step 15396: loss=2.9252, lr=0.000275, tokens/sec=1375460.67, grad_norm=0.3522, duration=0.38s
Step 15397: loss=2.8691, lr=0.000275, tokens/sec=1375271.43, grad_norm=0.3209, duration=0.38s
Step 15398: loss=2.8185, lr=0.000275, tokens/sec=1374071.78, grad_norm=0.3341, duration=0.38s
Step 15399: loss=2.8555, lr=0.000275, tokens/sec=1378541.53, grad_norm=0.3543, duration=0.38s
Step 15400/19073 (80.7%), Elapsed time: 6097.39s, Steps per hour: 9092.42, Estimated hours remaining: 0.40
Step 15400: loss=2.8702, lr=0.000275, tokens/sec=1375660.30, grad_norm=0.3527, duration=0.38s
Step 15401: loss=2.8737, lr=0.000275, tokens/sec=1373613.44, grad_norm=0.3310, duration=0.38s
Step 15402: loss=2.8757, lr=0.000275, tokens/sec=1378911.50, grad_norm=0.3257, duration=0.38s
Step 15403: loss=2.8178, lr=0.000275, tokens/sec=1377425.90, grad_norm=0.3458, duration=0.38s
Step 15404: loss=2.8813, lr=0.000275, tokens/sec=1376070.06, grad_norm=0.3430, duration=0.38s
Step 15405: loss=2.8666, lr=0.000275, tokens/sec=1378190.76, grad_norm=0.3712, duration=0.38s
Step 15406: loss=2.8038, lr=0.000275, tokens/sec=1377433.66, grad_norm=0.3508, duration=0.38s
Step 15407: loss=2.8445, lr=0.000275, tokens/sec=1375787.68, grad_norm=0.3372, duration=0.38s
Step 15408: loss=2.8079, lr=0.000275, tokens/sec=1373139.98, grad_norm=0.3423, duration=0.38s
Step 15409: loss=2.7763, lr=0.000275, tokens/sec=1375820.39, grad_norm=0.4259, duration=0.38s
Step 15410: loss=2.8144, lr=0.000275, tokens/sec=1374411.01, grad_norm=0.3603, duration=0.38s
Step 15411: loss=2.8837, lr=0.000275, tokens/sec=1378187.30, grad_norm=0.3492, duration=0.38s
Step 15412: loss=2.9271, lr=0.000275, tokens/sec=1375014.31, grad_norm=0.3572, duration=0.38s
Step 15413: loss=2.7886, lr=0.000275, tokens/sec=1374891.37, grad_norm=0.3180, duration=0.38s
Step 15414: loss=2.8900, lr=0.000275, tokens/sec=1377072.24, grad_norm=0.3146, duration=0.38s
Step 15415: loss=2.8682, lr=0.000275, tokens/sec=1373535.37, grad_norm=0.3251, duration=0.38s
Step 15416: loss=2.8297, lr=0.000275, tokens/sec=1377309.43, grad_norm=0.3345, duration=0.38s
Step 15417: loss=2.8118, lr=0.000275, tokens/sec=1376673.09, grad_norm=0.3223, duration=0.38s
Step 15418: loss=2.8642, lr=0.000275, tokens/sec=1377940.31, grad_norm=0.3030, duration=0.38s
Step 15419: loss=2.8097, lr=0.000275, tokens/sec=1376123.45, grad_norm=0.3170, duration=0.38s
Step 15420: loss=2.8078, lr=0.000275, tokens/sec=1374547.61, grad_norm=0.3339, duration=0.38s
Step 15421: loss=2.8370, lr=0.000275, tokens/sec=1378592.52, grad_norm=0.3195, duration=0.38s
Step 15422: loss=2.8092, lr=0.000275, tokens/sec=1375878.92, grad_norm=0.3296, duration=0.38s
Step 15423: loss=2.8690, lr=0.000275, tokens/sec=1374915.44, grad_norm=0.3215, duration=0.38s
Step 15424: loss=2.8051, lr=0.000275, tokens/sec=1374365.48, grad_norm=0.3074, duration=0.38s
Step 15425: loss=2.7712, lr=0.000275, tokens/sec=1378695.37, grad_norm=0.3138, duration=0.38s
Step 15426: loss=2.8278, lr=0.000275, tokens/sec=1374026.28, grad_norm=0.3132, duration=0.38s
Step 15427: loss=2.7794, lr=0.000275, tokens/sec=1374958.42, grad_norm=0.2969, duration=0.38s
Step 15428: loss=2.7599, lr=0.000275, tokens/sec=1375957.27, grad_norm=0.3119, duration=0.38s
Step 15429: loss=2.8009, lr=0.000275, tokens/sec=1373747.31, grad_norm=0.3056, duration=0.38s
Step 15430: loss=2.7699, lr=0.000275, tokens/sec=1376105.37, grad_norm=0.3058, duration=0.38s
Step 15431: loss=2.7787, lr=0.000275, tokens/sec=1378525.97, grad_norm=0.3038, duration=0.38s
Step 15432: loss=2.7941, lr=0.000275, tokens/sec=1378612.40, grad_norm=0.2946, duration=0.38s
Step 15433: loss=2.8096, lr=0.000275, tokens/sec=1375280.89, grad_norm=0.3272, duration=0.38s
Step 15434: loss=2.7641, lr=0.000275, tokens/sec=1376823.07, grad_norm=0.3226, duration=0.38s
Step 15435: loss=2.7776, lr=0.000275, tokens/sec=1374899.97, grad_norm=0.2807, duration=0.38s
Step 15436: loss=2.8005, lr=0.000275, tokens/sec=1374875.90, grad_norm=0.2812, duration=0.38s
Step 15437: loss=2.8409, lr=0.000275, tokens/sec=1372971.08, grad_norm=0.3142, duration=0.38s
Step 15438: loss=2.8730, lr=0.000275, tokens/sec=1377510.46, grad_norm=0.3192, duration=0.38s
Step 15439: loss=2.8424, lr=0.000275, tokens/sec=1373046.52, grad_norm=0.3027, duration=0.38s
Step 15440: loss=2.8304, lr=0.000275, tokens/sec=1375188.00, grad_norm=0.3041, duration=0.38s
Step 15441: loss=2.9206, lr=0.000274, tokens/sec=1374301.92, grad_norm=0.3329, duration=0.38s
Step 15442: loss=2.9034, lr=0.000274, tokens/sec=1372045.91, grad_norm=0.3261, duration=0.38s
Step 15443: loss=2.8635, lr=0.000274, tokens/sec=1374294.19, grad_norm=0.3182, duration=0.38s
Step 15444: loss=2.8352, lr=0.000274, tokens/sec=1375721.41, grad_norm=0.3035, duration=0.38s
Step 15445: loss=2.7922, lr=0.000274, tokens/sec=1373787.64, grad_norm=0.3117, duration=0.38s
Step 15446: loss=2.8622, lr=0.000274, tokens/sec=1373580.84, grad_norm=0.3369, duration=0.38s
Step 15447: loss=2.8554, lr=0.000274, tokens/sec=1373442.72, grad_norm=0.3342, duration=0.38s
Step 15448: loss=2.8356, lr=0.000274, tokens/sec=1377349.98, grad_norm=0.3155, duration=0.38s
Step 15449: loss=2.8682, lr=0.000274, tokens/sec=1376408.55, grad_norm=0.3167, duration=0.38s
Step 15450: loss=2.8520, lr=0.000274, tokens/sec=1378297.87, grad_norm=0.3411, duration=0.38s
Step 15451: loss=2.7468, lr=0.000274, tokens/sec=1372505.77, grad_norm=0.3425, duration=0.38s
Step 15452: loss=2.8373, lr=0.000274, tokens/sec=1373577.41, grad_norm=0.3302, duration=0.38s
Step 15453: loss=2.8453, lr=0.000274, tokens/sec=1376936.87, grad_norm=0.3090, duration=0.38s
Step 15454: loss=2.8319, lr=0.000274, tokens/sec=1375662.02, grad_norm=0.3175, duration=0.38s
Step 15455: loss=2.8927, lr=0.000274, tokens/sec=1372501.49, grad_norm=0.3691, duration=0.38s
Step 15456: loss=2.8233, lr=0.000274, tokens/sec=1376168.23, grad_norm=0.3335, duration=0.38s
Step 15457: loss=2.8931, lr=0.000274, tokens/sec=1375530.37, grad_norm=0.3154, duration=0.38s
Step 15458: loss=2.8431, lr=0.000274, tokens/sec=1377543.25, grad_norm=0.3191, duration=0.38s
Step 15459: loss=2.8268, lr=0.000274, tokens/sec=1377834.98, grad_norm=0.3312, duration=0.38s
Step 15460: loss=2.8549, lr=0.000274, tokens/sec=1374086.38, grad_norm=0.3341, duration=0.38s
Step 15461: loss=2.8553, lr=0.000274, tokens/sec=1377324.96, grad_norm=0.3206, duration=0.38s
Step 15462: loss=2.8517, lr=0.000274, tokens/sec=1377207.65, grad_norm=0.3104, duration=0.38s
Step 15463: loss=2.8434, lr=0.000274, tokens/sec=1375698.17, grad_norm=0.3198, duration=0.38s
Step 15464: loss=2.8408, lr=0.000274, tokens/sec=1373489.90, grad_norm=0.3111, duration=0.38s
Step 15465: loss=2.8474, lr=0.000274, tokens/sec=1376505.05, grad_norm=0.3246, duration=0.38s
Step 15466: loss=2.8419, lr=0.000274, tokens/sec=1376783.42, grad_norm=0.3170, duration=0.38s
Step 15467: loss=2.8195, lr=0.000274, tokens/sec=1375730.87, grad_norm=0.2942, duration=0.38s
Step 15468: loss=2.8500, lr=0.000274, tokens/sec=1374716.03, grad_norm=0.2996, duration=0.38s
Step 15469: loss=2.8641, lr=0.000274, tokens/sec=1370534.05, grad_norm=0.3085, duration=0.38s
Step 15470: loss=2.8565, lr=0.000274, tokens/sec=1374221.19, grad_norm=0.3115, duration=0.38s
Step 15471: loss=2.7793, lr=0.000274, tokens/sec=1376412.00, grad_norm=0.2958, duration=0.38s
Step 15472: loss=2.8418, lr=0.000274, tokens/sec=1371096.33, grad_norm=0.2981, duration=0.38s
Step 15473: loss=2.7963, lr=0.000274, tokens/sec=1374224.63, grad_norm=0.3049, duration=0.38s
Step 15474: loss=2.7887, lr=0.000274, tokens/sec=1375525.20, grad_norm=0.3052, duration=0.38s
Step 15475: loss=2.7523, lr=0.000274, tokens/sec=1374577.68, grad_norm=0.3124, duration=0.38s
Step 15476: loss=2.7881, lr=0.000274, tokens/sec=1373438.43, grad_norm=0.3052, duration=0.38s
Step 15477: loss=2.8059, lr=0.000274, tokens/sec=1375899.58, grad_norm=0.2939, duration=0.38s
Step 15478: loss=2.7472, lr=0.000274, tokens/sec=1374189.42, grad_norm=0.3060, duration=0.38s
Step 15479: loss=2.7620, lr=0.000274, tokens/sec=1375032.36, grad_norm=0.3056, duration=0.38s
Step 15480: loss=2.7712, lr=0.000274, tokens/sec=1376496.43, grad_norm=0.3168, duration=0.38s
Step 15481: loss=2.8254, lr=0.000274, tokens/sec=1376372.37, grad_norm=0.3098, duration=0.38s
Step 15482: loss=2.7809, lr=0.000274, tokens/sec=1376392.19, grad_norm=0.3037, duration=0.38s
Step 15483: loss=2.7956, lr=0.000274, tokens/sec=1375129.53, grad_norm=0.3072, duration=0.38s
Step 15484: loss=2.8501, lr=0.000274, tokens/sec=1372367.01, grad_norm=0.3180, duration=0.38s
Step 15485: loss=2.8476, lr=0.000274, tokens/sec=1376183.73, grad_norm=0.3050, duration=0.38s
Step 15486: loss=2.8692, lr=0.000274, tokens/sec=1375728.29, grad_norm=0.3090, duration=0.38s
Step 15487: loss=2.8538, lr=0.000274, tokens/sec=1374219.48, grad_norm=0.3228, duration=0.38s
Step 15488: loss=2.7680, lr=0.000274, tokens/sec=1373271.18, grad_norm=0.3270, duration=0.38s
Step 15489: loss=2.8314, lr=0.000274, tokens/sec=1374999.69, grad_norm=0.3205, duration=0.38s
Step 15490: loss=2.7984, lr=0.000274, tokens/sec=1372845.94, grad_norm=0.3040, duration=0.38s
Step 15491: loss=2.8693, lr=0.000274, tokens/sec=1373761.04, grad_norm=0.3084, duration=0.38s
Step 15492: loss=2.8595, lr=0.000274, tokens/sec=1374309.65, grad_norm=0.3117, duration=0.38s
Step 15493: loss=2.8186, lr=0.000274, tokens/sec=1372724.25, grad_norm=0.3213, duration=0.38s
Step 15494: loss=2.8359, lr=0.000274, tokens/sec=1374026.28, grad_norm=0.3111, duration=0.38s
Step 15495: loss=2.8481, lr=0.000274, tokens/sec=1379574.15, grad_norm=0.3064, duration=0.38s
Step 15496: loss=2.8617, lr=0.000273, tokens/sec=1376795.48, grad_norm=0.3155, duration=0.38s
Step 15497: loss=2.8522, lr=0.000273, tokens/sec=1377645.08, grad_norm=0.3178, duration=0.38s
Step 15498: loss=2.8429, lr=0.000273, tokens/sec=1376833.41, grad_norm=0.3064, duration=0.38s
Step 15499: loss=2.8093, lr=0.000273, tokens/sec=1373801.38, grad_norm=0.3030, duration=0.38s
Step 15500/19073 (81.3%), Elapsed time: 6135.60s, Steps per hour: 9094.46, Estimated hours remaining: 0.39
Validation loss at step 15500: 3.82185697555542
Step 15500: loss=2.8438, lr=0.000273, tokens/sec=154571.38, grad_norm=0.3017, duration=3.39s
Step 15501: loss=2.8564, lr=0.000273, tokens/sec=1379851.16, grad_norm=0.3120, duration=0.38s
Step 15502: loss=2.8289, lr=0.000273, tokens/sec=1377223.17, grad_norm=0.3006, duration=0.38s
Step 15503: loss=2.7918, lr=0.000273, tokens/sec=1381004.55, grad_norm=0.3047, duration=0.38s
Step 15504: loss=2.8552, lr=0.000273, tokens/sec=1376075.23, grad_norm=0.3082, duration=0.38s
Step 15505: loss=2.8314, lr=0.000273, tokens/sec=1373406.69, grad_norm=0.3058, duration=0.38s
Step 15506: loss=2.8929, lr=0.000273, tokens/sec=1378199.39, grad_norm=0.3121, duration=0.38s
Step 15507: loss=2.8443, lr=0.000273, tokens/sec=1376959.28, grad_norm=0.3007, duration=0.38s
Step 15508: loss=2.8642, lr=0.000273, tokens/sec=1374707.44, grad_norm=0.3248, duration=0.38s
Step 15509: loss=2.8701, lr=0.000273, tokens/sec=1375705.05, grad_norm=0.3146, duration=0.38s
Step 15510: loss=2.8120, lr=0.000273, tokens/sec=1373891.50, grad_norm=0.2948, duration=0.38s
Step 15511: loss=2.8550, lr=0.000273, tokens/sec=1374454.82, grad_norm=0.2993, duration=0.38s
Step 15512: loss=2.8638, lr=0.000273, tokens/sec=1376509.36, grad_norm=0.3070, duration=0.38s
Step 15513: loss=2.8625, lr=0.000273, tokens/sec=1371382.78, grad_norm=0.3261, duration=0.38s
Step 15514: loss=2.8361, lr=0.000273, tokens/sec=1370319.69, grad_norm=0.3195, duration=0.38s
Step 15515: loss=2.8553, lr=0.000273, tokens/sec=1372956.51, grad_norm=0.3084, duration=0.38s
Step 15516: loss=2.7838, lr=0.000273, tokens/sec=1374873.32, grad_norm=0.3199, duration=0.38s
Step 15517: loss=2.8423, lr=0.000273, tokens/sec=1375926.27, grad_norm=0.3124, duration=0.38s
Step 15518: loss=2.8388, lr=0.000273, tokens/sec=1376898.93, grad_norm=0.3333, duration=0.38s
Step 15519: loss=2.7670, lr=0.000273, tokens/sec=1376659.30, grad_norm=0.3188, duration=0.38s
Step 15520: loss=2.8160, lr=0.000273, tokens/sec=1375517.46, grad_norm=0.3163, duration=0.38s
Step 15521: loss=2.7158, lr=0.000273, tokens/sec=1377293.90, grad_norm=0.3081, duration=0.38s
Step 15522: loss=2.7688, lr=0.000273, tokens/sec=1372488.64, grad_norm=0.3368, duration=0.38s
Step 15523: loss=2.7591, lr=0.000273, tokens/sec=1372128.10, grad_norm=0.3480, duration=0.38s
Step 15524: loss=2.8336, lr=0.000273, tokens/sec=1377513.05, grad_norm=0.3305, duration=0.38s
Step 15525: loss=2.8280, lr=0.000273, tokens/sec=1371909.81, grad_norm=0.3367, duration=0.38s
Step 15526: loss=2.7725, lr=0.000273, tokens/sec=1370106.24, grad_norm=0.3418, duration=0.38s
Step 15527: loss=2.7572, lr=0.000273, tokens/sec=1371734.37, grad_norm=0.3440, duration=0.38s
Step 15528: loss=2.8001, lr=0.000273, tokens/sec=1375495.09, grad_norm=0.3339, duration=0.38s
Step 15529: loss=2.8440, lr=0.000273, tokens/sec=1376675.68, grad_norm=0.3400, duration=0.38s
Step 15530: loss=2.8654, lr=0.000273, tokens/sec=1376798.93, grad_norm=0.3421, duration=0.38s
Step 15531: loss=2.8564, lr=0.000273, tokens/sec=1375077.07, grad_norm=0.3480, duration=0.38s
Step 15532: loss=2.8717, lr=0.000273, tokens/sec=1379292.06, grad_norm=0.3528, duration=0.38s
Step 15533: loss=2.8200, lr=0.000273, tokens/sec=1375530.37, grad_norm=0.3389, duration=0.38s
Step 15534: loss=2.8738, lr=0.000273, tokens/sec=1374697.12, grad_norm=0.3328, duration=0.38s
Step 15535: loss=2.7977, lr=0.000273, tokens/sec=1377212.82, grad_norm=0.3498, duration=0.38s
Step 15536: loss=2.8502, lr=0.000273, tokens/sec=1374561.35, grad_norm=0.3460, duration=0.38s
Step 15537: loss=2.8854, lr=0.000273, tokens/sec=1375241.32, grad_norm=0.3421, duration=0.38s
Step 15538: loss=2.9239, lr=0.000273, tokens/sec=1376293.98, grad_norm=0.3373, duration=0.38s
Step 15539: loss=2.9059, lr=0.000273, tokens/sec=1379873.67, grad_norm=0.3417, duration=0.38s
Step 15540: loss=2.8851, lr=0.000273, tokens/sec=1375373.79, grad_norm=0.3697, duration=0.38s
Step 15541: loss=2.7706, lr=0.000273, tokens/sec=1372750.81, grad_norm=0.3344, duration=0.38s
Step 15542: loss=2.8509, lr=0.000273, tokens/sec=1378502.64, grad_norm=0.3250, duration=0.38s
Step 15543: loss=2.8090, lr=0.000273, tokens/sec=1379382.04, grad_norm=0.3579, duration=0.38s
Step 15544: loss=2.8108, lr=0.000273, tokens/sec=1378392.04, grad_norm=0.3770, duration=0.38s
Step 15545: loss=2.8888, lr=0.000273, tokens/sec=1375934.02, grad_norm=0.3240, duration=0.38s
Step 15546: loss=2.8052, lr=0.000273, tokens/sec=1382517.01, grad_norm=0.3208, duration=0.38s
Step 15547: loss=2.8660, lr=0.000273, tokens/sec=1373226.58, grad_norm=0.3542, duration=0.38s
Step 15548: loss=2.9027, lr=0.000273, tokens/sec=1376400.80, grad_norm=0.3747, duration=0.38s
Step 15549: loss=2.8860, lr=0.000273, tokens/sec=1378296.14, grad_norm=0.3406, duration=0.38s
Step 15550: loss=2.8317, lr=0.000273, tokens/sec=1376070.92, grad_norm=0.3143, duration=0.38s
Step 15551: loss=2.8665, lr=0.000272, tokens/sec=1377014.47, grad_norm=0.3440, duration=0.38s
Step 15552: loss=2.8960, lr=0.000272, tokens/sec=1377434.53, grad_norm=0.3525, duration=0.38s
Step 15553: loss=2.8590, lr=0.000272, tokens/sec=1377942.04, grad_norm=0.3323, duration=0.38s
Step 15554: loss=2.7898, lr=0.000272, tokens/sec=1377065.34, grad_norm=0.3180, duration=0.38s
Step 15555: loss=2.8522, lr=0.000272, tokens/sec=1375421.96, grad_norm=0.3193, duration=0.38s
Step 15556: loss=2.8335, lr=0.000272, tokens/sec=1375306.69, grad_norm=0.3147, duration=0.38s
Step 15557: loss=2.8617, lr=0.000272, tokens/sec=1374348.30, grad_norm=0.3306, duration=0.38s
Step 15558: loss=2.8921, lr=0.000272, tokens/sec=1374639.55, grad_norm=0.3443, duration=0.38s
Step 15559: loss=2.8134, lr=0.000272, tokens/sec=1376239.72, grad_norm=0.3289, duration=0.38s
Step 15560: loss=2.8541, lr=0.000272, tokens/sec=1378949.55, grad_norm=0.3087, duration=0.38s
Step 15561: loss=2.8278, lr=0.000272, tokens/sec=1377243.87, grad_norm=0.3038, duration=0.38s
Step 15562: loss=2.7950, lr=0.000272, tokens/sec=1375199.18, grad_norm=0.3301, duration=0.38s
Step 15563: loss=2.8469, lr=0.000272, tokens/sec=1377041.20, grad_norm=0.3404, duration=0.38s
Step 15564: loss=2.7751, lr=0.000272, tokens/sec=1375617.27, grad_norm=0.3242, duration=0.38s
Step 15565: loss=2.7895, lr=0.000272, tokens/sec=1373501.91, grad_norm=0.2993, duration=0.38s
Step 15566: loss=2.7853, lr=0.000272, tokens/sec=1373720.70, grad_norm=0.3144, duration=0.38s
Step 15567: loss=2.8312, lr=0.000272, tokens/sec=1376102.78, grad_norm=0.3379, duration=0.38s
Step 15568: loss=2.8010, lr=0.000272, tokens/sec=1377908.37, grad_norm=0.3309, duration=0.38s
Step 15569: loss=2.7730, lr=0.000272, tokens/sec=1375075.35, grad_norm=0.3110, duration=0.38s
Step 15570: loss=2.7609, lr=0.000272, tokens/sec=1372578.59, grad_norm=0.3213, duration=0.38s
Step 15571: loss=2.7841, lr=0.000272, tokens/sec=1375446.05, grad_norm=0.3386, duration=0.38s
Step 15572: loss=2.8121, lr=0.000272, tokens/sec=1375039.24, grad_norm=0.3194, duration=0.38s
Step 15573: loss=2.7797, lr=0.000272, tokens/sec=1378487.95, grad_norm=0.3182, duration=0.38s
Step 15574: loss=2.8057, lr=0.000272, tokens/sec=1375104.59, grad_norm=0.3247, duration=0.38s
Step 15575: loss=2.8750, lr=0.000272, tokens/sec=1373110.83, grad_norm=0.3486, duration=0.38s
Step 15576: loss=2.8425, lr=0.000272, tokens/sec=1377270.61, grad_norm=0.3427, duration=0.38s
Step 15577: loss=2.8503, lr=0.000272, tokens/sec=1374084.66, grad_norm=0.3306, duration=0.38s
Step 15578: loss=2.8159, lr=0.000272, tokens/sec=1376225.07, grad_norm=0.3196, duration=0.38s
Step 15579: loss=2.8517, lr=0.000272, tokens/sec=1377430.21, grad_norm=0.3154, duration=0.38s
Step 15580: loss=2.8306, lr=0.000272, tokens/sec=1375678.37, grad_norm=0.3239, duration=0.38s
Step 15581: loss=2.8330, lr=0.000272, tokens/sec=1374747.83, grad_norm=0.3286, duration=0.38s
Step 15582: loss=2.8538, lr=0.000272, tokens/sec=1375705.91, grad_norm=0.3336, duration=0.38s
Step 15583: loss=2.9161, lr=0.000272, tokens/sec=1376088.14, grad_norm=0.3422, duration=0.38s
Step 15584: loss=2.8499, lr=0.000272, tokens/sec=1374795.96, grad_norm=0.3265, duration=0.38s
Step 15585: loss=2.8790, lr=0.000272, tokens/sec=1377143.82, grad_norm=0.3357, duration=0.38s
Step 15586: loss=2.9278, lr=0.000272, tokens/sec=1375551.88, grad_norm=0.3581, duration=0.38s
Step 15587: loss=2.8005, lr=0.000272, tokens/sec=1378954.74, grad_norm=0.3391, duration=0.38s
Step 15588: loss=2.8734, lr=0.000272, tokens/sec=1376492.99, grad_norm=0.3318, duration=0.38s
Step 15589: loss=2.8517, lr=0.000272, tokens/sec=1378539.80, grad_norm=0.3514, duration=0.38s
Step 15590: loss=2.9007, lr=0.000272, tokens/sec=1375538.11, grad_norm=0.3714, duration=0.38s
Step 15591: loss=2.8632, lr=0.000272, tokens/sec=1374350.02, grad_norm=0.3410, duration=0.38s
Step 15592: loss=2.8690, lr=0.000272, tokens/sec=1378212.35, grad_norm=0.3241, duration=0.38s
Step 15593: loss=2.8074, lr=0.000272, tokens/sec=1376934.28, grad_norm=0.3260, duration=0.38s
Step 15594: loss=2.8800, lr=0.000272, tokens/sec=1378486.22, grad_norm=0.3456, duration=0.38s
Step 15595: loss=2.8453, lr=0.000272, tokens/sec=1381235.28, grad_norm=0.3592, duration=0.38s
Step 15596: loss=2.7749, lr=0.000272, tokens/sec=1374810.57, grad_norm=0.3490, duration=0.38s
Step 15597: loss=2.8902, lr=0.000272, tokens/sec=1379347.43, grad_norm=0.3240, duration=0.38s
Step 15598: loss=2.8059, lr=0.000272, tokens/sec=1376813.59, grad_norm=0.3493, duration=0.38s
Step 15599: loss=2.7639, lr=0.000272, tokens/sec=1373461.59, grad_norm=0.3611, duration=0.38s
Step 15600/19073 (81.8%), Elapsed time: 6176.80s, Steps per hour: 9092.09, Estimated hours remaining: 0.38
Step 15600: loss=2.8366, lr=0.000272, tokens/sec=1373848.58, grad_norm=0.3394, duration=0.38s
Step 15601: loss=2.8469, lr=0.000272, tokens/sec=1377161.93, grad_norm=0.3294, duration=0.38s
Step 15602: loss=2.9395, lr=0.000272, tokens/sec=1379883.19, grad_norm=0.3362, duration=0.38s
Step 15603: loss=2.7788, lr=0.000272, tokens/sec=1377941.18, grad_norm=0.3320, duration=0.38s
Step 15604: loss=2.8785, lr=0.000272, tokens/sec=1370861.28, grad_norm=0.3199, duration=0.38s
Step 15605: loss=2.8835, lr=0.000272, tokens/sec=1377845.34, grad_norm=0.3064, duration=0.38s
Step 15606: loss=2.7723, lr=0.000272, tokens/sec=1376380.99, grad_norm=0.3176, duration=0.38s
Step 15607: loss=2.8377, lr=0.000272, tokens/sec=1373994.51, grad_norm=0.3211, duration=0.38s
Step 15608: loss=2.8277, lr=0.000271, tokens/sec=1372412.40, grad_norm=0.3076, duration=0.38s
Step 15609: loss=2.8420, lr=0.000271, tokens/sec=1380140.41, grad_norm=0.2952, duration=0.38s
Step 15610: loss=2.7970, lr=0.000271, tokens/sec=1375656.86, grad_norm=0.3190, duration=0.38s
Step 15611: loss=2.7999, lr=0.000271, tokens/sec=1375252.51, grad_norm=0.3348, duration=0.38s
Step 15612: loss=2.8325, lr=0.000271, tokens/sec=1376711.88, grad_norm=0.3481, duration=0.38s
Step 15613: loss=2.8543, lr=0.000271, tokens/sec=1378876.05, grad_norm=0.3021, duration=0.38s
Step 15614: loss=2.8192, lr=0.000271, tokens/sec=1376314.65, grad_norm=0.2818, duration=0.38s
Step 15615: loss=2.7815, lr=0.000271, tokens/sec=1377105.01, grad_norm=0.3038, duration=0.38s
Step 15616: loss=2.8210, lr=0.000271, tokens/sec=1372521.19, grad_norm=0.3267, duration=0.38s
Step 15617: loss=2.7601, lr=0.000271, tokens/sec=1374173.96, grad_norm=0.2948, duration=0.38s
Step 15618: loss=2.7801, lr=0.000271, tokens/sec=1372590.58, grad_norm=0.2924, duration=0.38s
Step 15619: loss=2.7529, lr=0.000271, tokens/sec=1371228.85, grad_norm=0.2975, duration=0.38s
Step 15620: loss=2.7721, lr=0.000271, tokens/sec=1372862.23, grad_norm=0.3073, duration=0.38s
Step 15621: loss=2.7880, lr=0.000271, tokens/sec=1374661.89, grad_norm=0.2997, duration=0.38s
Step 15622: loss=2.7865, lr=0.000271, tokens/sec=1373268.60, grad_norm=0.2888, duration=0.38s
Step 15623: loss=2.7851, lr=0.000271, tokens/sec=1372576.88, grad_norm=0.3072, duration=0.38s
Step 15624: loss=2.7779, lr=0.000271, tokens/sec=1373729.29, grad_norm=0.3102, duration=0.38s
Step 15625: loss=2.7860, lr=0.000271, tokens/sec=1375829.00, grad_norm=0.2988, duration=0.38s
Step 15626: loss=2.7720, lr=0.000271, tokens/sec=1374318.24, grad_norm=0.2781, duration=0.38s
Step 15627: loss=2.8289, lr=0.000271, tokens/sec=1373986.78, grad_norm=0.3064, duration=0.38s
Step 15628: loss=2.8839, lr=0.000271, tokens/sec=1376225.94, grad_norm=0.3305, duration=0.38s
Step 15629: loss=2.8339, lr=0.000271, tokens/sec=1371942.33, grad_norm=0.3082, duration=0.38s
Step 15630: loss=2.8918, lr=0.000271, tokens/sec=1370178.80, grad_norm=0.3053, duration=0.38s
Step 15631: loss=2.9037, lr=0.000271, tokens/sec=1372293.36, grad_norm=0.3139, duration=0.38s
Step 15632: loss=2.8597, lr=0.000271, tokens/sec=1371626.56, grad_norm=0.3227, duration=0.38s
Step 15633: loss=2.8665, lr=0.000271, tokens/sec=1379122.51, grad_norm=0.3310, duration=0.38s
Step 15634: loss=2.8002, lr=0.000271, tokens/sec=1375483.04, grad_norm=0.3221, duration=0.38s
Step 15635: loss=2.8299, lr=0.000271, tokens/sec=1372253.11, grad_norm=0.3101, duration=0.38s
Step 15636: loss=2.8694, lr=0.000271, tokens/sec=1377793.55, grad_norm=0.3118, duration=0.38s
Step 15637: loss=2.8373, lr=0.000271, tokens/sec=1377987.81, grad_norm=0.3273, duration=0.38s
Step 15638: loss=2.8655, lr=0.000271, tokens/sec=1373832.27, grad_norm=0.3263, duration=0.38s
Step 15639: loss=2.8492, lr=0.000271, tokens/sec=1375898.72, grad_norm=0.3247, duration=0.38s
Step 15640: loss=2.8158, lr=0.000271, tokens/sec=1373521.64, grad_norm=0.3226, duration=0.38s
Step 15641: loss=2.7759, lr=0.000271, tokens/sec=1373844.29, grad_norm=0.3297, duration=0.38s
Step 15642: loss=2.8018, lr=0.000271, tokens/sec=1375807.48, grad_norm=0.3285, duration=0.38s
Step 15643: loss=2.8389, lr=0.000271, tokens/sec=1375499.39, grad_norm=0.3150, duration=0.38s
Step 15644: loss=2.8538, lr=0.000271, tokens/sec=1376999.81, grad_norm=0.3111, duration=0.38s
Step 15645: loss=2.8747, lr=0.000271, tokens/sec=1374542.45, grad_norm=0.3429, duration=0.38s
Step 15646: loss=2.8167, lr=0.000271, tokens/sec=1374115.57, grad_norm=0.3366, duration=0.38s
Step 15647: loss=2.8703, lr=0.000271, tokens/sec=1375267.99, grad_norm=0.3331, duration=0.38s
Step 15648: loss=2.8342, lr=0.000271, tokens/sec=1377579.49, grad_norm=0.2933, duration=0.38s
Step 15649: loss=2.8464, lr=0.000271, tokens/sec=1377180.05, grad_norm=0.3128, duration=0.38s
Step 15650: loss=2.8468, lr=0.000271, tokens/sec=1375644.81, grad_norm=0.3202, duration=0.38s
Step 15651: loss=2.8725, lr=0.000271, tokens/sec=1376936.01, grad_norm=0.3216, duration=0.38s
Step 15652: loss=2.8200, lr=0.000271, tokens/sec=1374210.89, grad_norm=0.2984, duration=0.38s
Step 15653: loss=2.8382, lr=0.000271, tokens/sec=1376158.76, grad_norm=0.2972, duration=0.38s
Step 15654: loss=2.8483, lr=0.000271, tokens/sec=1377873.83, grad_norm=0.3038, duration=0.38s
Step 15655: loss=2.8306, lr=0.000271, tokens/sec=1376581.74, grad_norm=0.3076, duration=0.38s
Step 15656: loss=2.8596, lr=0.000271, tokens/sec=1375330.78, grad_norm=0.3008, duration=0.38s
Step 15657: loss=2.8089, lr=0.000271, tokens/sec=1375095.13, grad_norm=0.2832, duration=0.38s
Step 15658: loss=2.8697, lr=0.000271, tokens/sec=1373323.49, grad_norm=0.2950, duration=0.38s
Step 15659: loss=2.8398, lr=0.000271, tokens/sec=1375686.98, grad_norm=0.3153, duration=0.38s
Step 15660: loss=2.8481, lr=0.000271, tokens/sec=1377145.55, grad_norm=0.3114, duration=0.38s
Step 15661: loss=2.7755, lr=0.000271, tokens/sec=1375046.12, grad_norm=0.3008, duration=0.38s
Step 15662: loss=2.8429, lr=0.000271, tokens/sec=1373822.83, grad_norm=0.3006, duration=0.38s
Step 15663: loss=2.7735, lr=0.000271, tokens/sec=1374842.37, grad_norm=0.3097, duration=0.38s
Step 15664: loss=2.7905, lr=0.000271, tokens/sec=1376546.41, grad_norm=0.3208, duration=0.38s
Step 15665: loss=2.7514, lr=0.000270, tokens/sec=1374799.40, grad_norm=0.3004, duration=0.38s
Step 15666: loss=2.7845, lr=0.000270, tokens/sec=1373335.50, grad_norm=0.2991, duration=0.38s
Step 15667: loss=2.7898, lr=0.000270, tokens/sec=1373718.99, grad_norm=0.3025, duration=0.38s
Step 15668: loss=2.7361, lr=0.000270, tokens/sec=1375430.56, grad_norm=0.3013, duration=0.38s
Step 15669: loss=2.7843, lr=0.000270, tokens/sec=1375113.19, grad_norm=0.2911, duration=0.38s
Step 15670: loss=2.7633, lr=0.000270, tokens/sec=1378542.39, grad_norm=0.3043, duration=0.38s
Step 15671: loss=2.7931, lr=0.000270, tokens/sec=1376417.17, grad_norm=0.3034, duration=0.38s
Step 15672: loss=2.7713, lr=0.000270, tokens/sec=1380414.18, grad_norm=0.2990, duration=0.38s
Step 15673: loss=2.8021, lr=0.000270, tokens/sec=1377184.36, grad_norm=0.3006, duration=0.38s
Step 15674: loss=2.8411, lr=0.000270, tokens/sec=1373278.04, grad_norm=0.3095, duration=0.38s
Step 15675: loss=2.8609, lr=0.000270, tokens/sec=1377812.54, grad_norm=0.3035, duration=0.38s
Step 15676: loss=2.8482, lr=0.000270, tokens/sec=1379297.25, grad_norm=0.2917, duration=0.38s
Step 15677: loss=2.8299, lr=0.000270, tokens/sec=1374379.23, grad_norm=0.3053, duration=0.38s
Step 15678: loss=2.7796, lr=0.000270, tokens/sec=1376467.14, grad_norm=0.3203, duration=0.38s
Step 15679: loss=2.8279, lr=0.000270, tokens/sec=1376200.96, grad_norm=0.3119, duration=0.38s
Step 15680: loss=2.7995, lr=0.000270, tokens/sec=1379790.55, grad_norm=0.2950, duration=0.38s
Step 15681: loss=2.8628, lr=0.000270, tokens/sec=1376429.23, grad_norm=0.2950, duration=0.38s
Step 15682: loss=2.8266, lr=0.000270, tokens/sec=1374626.66, grad_norm=0.2993, duration=0.38s
Step 15683: loss=2.8310, lr=0.000270, tokens/sec=1379097.43, grad_norm=0.3099, duration=0.38s
Step 15684: loss=2.8760, lr=0.000270, tokens/sec=1377222.31, grad_norm=0.3117, duration=0.38s
Step 15685: loss=2.8454, lr=0.000270, tokens/sec=1378112.16, grad_norm=0.2882, duration=0.38s
Step 15686: loss=2.8419, lr=0.000270, tokens/sec=1380078.91, grad_norm=0.2998, duration=0.38s
Step 15687: loss=2.8345, lr=0.000270, tokens/sec=1372074.16, grad_norm=0.2993, duration=0.38s
Step 15688: loss=2.8557, lr=0.000270, tokens/sec=1377702.91, grad_norm=0.3029, duration=0.38s
Step 15689: loss=2.7891, lr=0.000270, tokens/sec=1373633.18, grad_norm=0.3046, duration=0.38s
Step 15690: loss=2.8614, lr=0.000270, tokens/sec=1374683.37, grad_norm=0.3018, duration=0.38s
Step 15691: loss=2.8093, lr=0.000270, tokens/sec=1378538.94, grad_norm=0.2947, duration=0.38s
Step 15692: loss=2.8500, lr=0.000270, tokens/sec=1376744.63, grad_norm=0.2920, duration=0.38s
Step 15693: loss=2.7859, lr=0.000270, tokens/sec=1379006.62, grad_norm=0.3036, duration=0.38s
Step 15694: loss=2.8787, lr=0.000270, tokens/sec=1376967.91, grad_norm=0.3100, duration=0.38s
Step 15695: loss=2.8343, lr=0.000270, tokens/sec=1372652.27, grad_norm=0.3095, duration=0.38s
Step 15696: loss=2.8687, lr=0.000270, tokens/sec=1377437.98, grad_norm=0.2957, duration=0.38s
Step 15697: loss=2.8363, lr=0.000270, tokens/sec=1371084.36, grad_norm=0.2890, duration=0.38s
Step 15698: loss=2.8860, lr=0.000270, tokens/sec=1377562.23, grad_norm=0.3080, duration=0.38s
Step 15699: loss=2.8002, lr=0.000270, tokens/sec=1379412.32, grad_norm=0.3075, duration=0.38s
Step 15700/19073 (82.3%), Elapsed time: 6215.00s, Steps per hour: 9094.13, Estimated hours remaining: 0.37
Step 15700: loss=2.8536, lr=0.000270, tokens/sec=1373194.00, grad_norm=0.3151, duration=0.38s
Step 15701: loss=2.8539, lr=0.000270, tokens/sec=1377278.38, grad_norm=0.2933, duration=0.38s
Step 15702: loss=2.8750, lr=0.000270, tokens/sec=1377689.10, grad_norm=0.2886, duration=0.38s
Step 15703: loss=2.8117, lr=0.000270, tokens/sec=1374585.41, grad_norm=0.3260, duration=0.38s
Step 15704: loss=2.8521, lr=0.000270, tokens/sec=1374441.94, grad_norm=0.3264, duration=0.38s
Step 15705: loss=2.7907, lr=0.000270, tokens/sec=1380188.92, grad_norm=0.3455, duration=0.38s
Step 15706: loss=2.8471, lr=0.000270, tokens/sec=1377931.68, grad_norm=0.2885, duration=0.38s
Step 15707: loss=2.8362, lr=0.000270, tokens/sec=1380240.03, grad_norm=0.2984, duration=0.38s
Step 15708: loss=2.8160, lr=0.000270, tokens/sec=1376671.37, grad_norm=0.3154, duration=0.38s
Step 15709: loss=2.7806, lr=0.000270, tokens/sec=1375526.92, grad_norm=0.3148, duration=0.38s
Step 15710: loss=2.7936, lr=0.000270, tokens/sec=1380509.51, grad_norm=0.3026, duration=0.38s
Step 15711: loss=2.7200, lr=0.000270, tokens/sec=1376098.48, grad_norm=0.3040, duration=0.38s
Step 15712: loss=2.7553, lr=0.000270, tokens/sec=1376541.24, grad_norm=0.3350, duration=0.38s
Step 15713: loss=2.7671, lr=0.000270, tokens/sec=1373504.48, grad_norm=0.3372, duration=0.38s
Step 15714: loss=2.8583, lr=0.000270, tokens/sec=1376645.51, grad_norm=0.3188, duration=0.38s
Step 15715: loss=2.7876, lr=0.000270, tokens/sec=1376163.93, grad_norm=0.3137, duration=0.38s
Step 15716: loss=2.7640, lr=0.000270, tokens/sec=1379567.22, grad_norm=0.3402, duration=0.38s
Step 15717: loss=2.7666, lr=0.000270, tokens/sec=1379811.33, grad_norm=0.3465, duration=0.38s
Step 15718: loss=2.8075, lr=0.000270, tokens/sec=1374012.54, grad_norm=0.3403, duration=0.38s
Step 15719: loss=2.8530, lr=0.000270, tokens/sec=1377959.31, grad_norm=0.3110, duration=0.38s
Step 15720: loss=2.8328, lr=0.000270, tokens/sec=1377419.86, grad_norm=0.3350, duration=0.38s
Step 15721: loss=2.8620, lr=0.000270, tokens/sec=1374910.28, grad_norm=0.3588, duration=0.38s
Step 15722: loss=2.8390, lr=0.000270, tokens/sec=1377567.41, grad_norm=0.3622, duration=0.38s
Step 15723: loss=2.8403, lr=0.000269, tokens/sec=1377313.74, grad_norm=0.3149, duration=0.38s
Step 15724: loss=2.8415, lr=0.000269, tokens/sec=1377090.35, grad_norm=0.3074, duration=0.38s
Step 15725: loss=2.8223, lr=0.000269, tokens/sec=1377969.67, grad_norm=0.3511, duration=0.38s
Step 15726: loss=2.8430, lr=0.000269, tokens/sec=1377890.24, grad_norm=0.3556, duration=0.38s
Step 15727: loss=2.8616, lr=0.000269, tokens/sec=1372102.41, grad_norm=0.3345, duration=0.38s
Step 15728: loss=2.9572, lr=0.000269, tokens/sec=1378768.85, grad_norm=0.3223, duration=0.38s
Step 15729: loss=2.8906, lr=0.000269, tokens/sec=1374283.89, grad_norm=0.3318, duration=0.38s
Step 15730: loss=2.8157, lr=0.000269, tokens/sec=1374554.48, grad_norm=0.3785, duration=0.38s
Step 15731: loss=2.8183, lr=0.000269, tokens/sec=1377210.23, grad_norm=0.3430, duration=0.38s
Step 15732: loss=2.8462, lr=0.000269, tokens/sec=1378829.37, grad_norm=0.3159, duration=0.38s
Step 15733: loss=2.8358, lr=0.000269, tokens/sec=1375658.58, grad_norm=0.3353, duration=0.38s
Step 15734: loss=2.7959, lr=0.000269, tokens/sec=1377984.35, grad_norm=0.3777, duration=0.38s
Step 15735: loss=2.8896, lr=0.000269, tokens/sec=1377851.39, grad_norm=0.3372, duration=0.38s
Step 15736: loss=2.7811, lr=0.000269, tokens/sec=1377791.82, grad_norm=0.3130, duration=0.38s
Step 15737: loss=2.8780, lr=0.000269, tokens/sec=1377442.29, grad_norm=0.3387, duration=0.38s
Step 15738: loss=2.9316, lr=0.000269, tokens/sec=1375443.47, grad_norm=0.3756, duration=0.38s
Step 15739: loss=2.8740, lr=0.000269, tokens/sec=1373218.01, grad_norm=0.3502, duration=0.38s
Step 15740: loss=2.8136, lr=0.000269, tokens/sec=1376009.79, grad_norm=0.3136, duration=0.38s
Step 15741: loss=2.8621, lr=0.000269, tokens/sec=1376248.33, grad_norm=0.3286, duration=0.38s
Step 15742: loss=2.8874, lr=0.000269, tokens/sec=1376920.49, grad_norm=0.3252, duration=0.38s
Step 15743: loss=2.8627, lr=0.000269, tokens/sec=1376138.09, grad_norm=0.3449, duration=0.38s
Step 15744: loss=2.7941, lr=0.000269, tokens/sec=1375398.73, grad_norm=0.3292, duration=0.38s
Step 15745: loss=2.8262, lr=0.000269, tokens/sec=1376707.57, grad_norm=0.3153, duration=0.38s
Step 15746: loss=2.8396, lr=0.000269, tokens/sec=1375981.37, grad_norm=0.3115, duration=0.38s
Step 15747: loss=2.8596, lr=0.000269, tokens/sec=1377551.88, grad_norm=0.3040, duration=0.38s
Step 15748: loss=2.8826, lr=0.000269, tokens/sec=1374275.30, grad_norm=0.3266, duration=0.38s
Step 15749: loss=2.8388, lr=0.000269, tokens/sec=1377096.39, grad_norm=0.3422, duration=0.38s
Validation loss at step 15750: 3.829209327697754
Step 15750: loss=2.8240, lr=0.000269, tokens/sec=156736.64, grad_norm=0.3221, duration=3.35s
Step 15751: loss=2.8306, lr=0.000269, tokens/sec=1379700.52, grad_norm=0.2925, duration=0.38s
Step 15752: loss=2.8066, lr=0.000269, tokens/sec=1379731.68, grad_norm=0.3056, duration=0.38s
Step 15753: loss=2.8066, lr=0.000269, tokens/sec=1378614.13, grad_norm=0.3376, duration=0.38s
Step 15754: loss=2.8064, lr=0.000269, tokens/sec=1378583.88, grad_norm=0.3374, duration=0.38s
Step 15755: loss=2.7771, lr=0.000269, tokens/sec=1376254.36, grad_norm=0.3106, duration=0.38s
Step 15756: loss=2.7703, lr=0.000269, tokens/sec=1375169.94, grad_norm=0.3084, duration=0.38s
Step 15757: loss=2.8459, lr=0.000269, tokens/sec=1380037.34, grad_norm=0.3272, duration=0.38s
Step 15758: loss=2.7869, lr=0.000269, tokens/sec=1375964.15, grad_norm=0.3217, duration=0.38s
Step 15759: loss=2.7839, lr=0.000269, tokens/sec=1377585.53, grad_norm=0.3145, duration=0.38s
Step 15760: loss=2.7543, lr=0.000269, tokens/sec=1377601.93, grad_norm=0.3283, duration=0.38s
Step 15761: loss=2.7733, lr=0.000269, tokens/sec=1373985.93, grad_norm=0.3255, duration=0.38s
Step 15762: loss=2.7975, lr=0.000269, tokens/sec=1376900.66, grad_norm=0.3188, duration=0.38s
Step 15763: loss=2.7831, lr=0.000269, tokens/sec=1376932.56, grad_norm=0.3159, duration=0.38s
Step 15764: loss=2.8256, lr=0.000269, tokens/sec=1372169.19, grad_norm=0.3216, duration=0.38s
Step 15765: loss=2.8394, lr=0.000269, tokens/sec=1374923.18, grad_norm=0.3347, duration=0.38s
Step 15766: loss=2.8417, lr=0.000269, tokens/sec=1376929.11, grad_norm=0.3330, duration=0.38s
Step 15767: loss=2.8400, lr=0.000269, tokens/sec=1369390.40, grad_norm=0.3298, duration=0.38s
Step 15768: loss=2.8228, lr=0.000269, tokens/sec=1375817.81, grad_norm=0.3380, duration=0.38s
Step 15769: loss=2.8539, lr=0.000269, tokens/sec=1375327.34, grad_norm=0.3263, duration=0.38s
Step 15770: loss=2.8007, lr=0.000269, tokens/sec=1377559.64, grad_norm=0.3227, duration=0.38s
Step 15771: loss=2.8444, lr=0.000269, tokens/sec=1377384.49, grad_norm=0.3297, duration=0.38s
Step 15772: loss=2.8467, lr=0.000269, tokens/sec=1375734.32, grad_norm=0.3390, duration=0.38s
Step 15773: loss=2.9092, lr=0.000269, tokens/sec=1373570.54, grad_norm=0.3433, duration=0.38s
Step 15774: loss=2.8557, lr=0.000269, tokens/sec=1376420.62, grad_norm=0.3393, duration=0.38s
Step 15775: loss=2.8783, lr=0.000269, tokens/sec=1374620.64, grad_norm=0.3373, duration=0.38s
Step 15776: loss=2.8560, lr=0.000269, tokens/sec=1375857.40, grad_norm=0.3494, duration=0.38s
Step 15777: loss=2.8544, lr=0.000269, tokens/sec=1371777.16, grad_norm=0.3395, duration=0.38s
Step 15778: loss=2.8675, lr=0.000269, tokens/sec=1378672.90, grad_norm=0.3208, duration=0.38s
Step 15779: loss=2.8833, lr=0.000269, tokens/sec=1380020.88, grad_norm=0.3546, duration=0.38s
Step 15780: loss=2.8910, lr=0.000269, tokens/sec=1373369.81, grad_norm=0.3546, duration=0.38s
Step 15781: loss=2.8581, lr=0.000268, tokens/sec=1376292.26, grad_norm=0.3227, duration=0.38s
Step 15782: loss=2.8591, lr=0.000268, tokens/sec=1374156.79, grad_norm=0.3167, duration=0.38s
Step 15783: loss=2.8042, lr=0.000268, tokens/sec=1372600.01, grad_norm=0.3262, duration=0.38s
Step 15784: loss=2.8606, lr=0.000268, tokens/sec=1370855.30, grad_norm=0.3346, duration=0.38s
Step 15785: loss=2.8168, lr=0.000268, tokens/sec=1376043.37, grad_norm=0.3538, duration=0.38s
Step 15786: loss=2.8226, lr=0.000268, tokens/sec=1375577.69, grad_norm=0.3340, duration=0.38s
Step 15787: loss=2.8845, lr=0.000268, tokens/sec=1376590.36, grad_norm=0.3268, duration=0.38s
Step 15788: loss=2.7926, lr=0.000268, tokens/sec=1376330.16, grad_norm=0.3441, duration=0.38s
Step 15789: loss=2.7877, lr=0.000268, tokens/sec=1375661.16, grad_norm=0.3462, duration=0.38s
Step 15790: loss=2.8005, lr=0.000268, tokens/sec=1379981.05, grad_norm=0.3319, duration=0.38s
Step 15791: loss=2.8610, lr=0.000268, tokens/sec=1374021.13, grad_norm=0.3211, duration=0.38s
Step 15792: loss=2.9265, lr=0.000268, tokens/sec=1374259.84, grad_norm=0.3370, duration=0.38s
Step 15793: loss=2.7638, lr=0.000268, tokens/sec=1375352.28, grad_norm=0.3331, duration=0.38s
Step 15794: loss=2.8929, lr=0.000268, tokens/sec=1376380.99, grad_norm=0.3229, duration=0.38s
Step 15795: loss=2.8286, lr=0.000268, tokens/sec=1371368.24, grad_norm=0.3092, duration=0.38s
Step 15796: loss=2.8004, lr=0.000268, tokens/sec=1372609.43, grad_norm=0.3026, duration=0.38s
Step 15797: loss=2.8034, lr=0.000268, tokens/sec=1375745.50, grad_norm=0.3173, duration=0.38s
Step 15798: loss=2.8581, lr=0.000268, tokens/sec=1373799.66, grad_norm=0.3124, duration=0.38s
Step 15799: loss=2.8364, lr=0.000268, tokens/sec=1372448.38, grad_norm=0.2992, duration=0.38s
Step 15800/19073 (82.8%), Elapsed time: 6256.14s, Steps per hour: 9091.86, Estimated hours remaining: 0.36
Step 15800: loss=2.7610, lr=0.000268, tokens/sec=1376078.67, grad_norm=0.3032, duration=0.38s
Step 15801: loss=2.8227, lr=0.000268, tokens/sec=1372011.67, grad_norm=0.3186, duration=0.38s
Step 15802: loss=2.8136, lr=0.000268, tokens/sec=1375318.73, grad_norm=0.3256, duration=0.38s
Step 15803: loss=2.8696, lr=0.000268, tokens/sec=1373198.29, grad_norm=0.3093, duration=0.38s
Step 15804: loss=2.8294, lr=0.000268, tokens/sec=1370819.41, grad_norm=0.2915, duration=0.38s
Step 15805: loss=2.7772, lr=0.000268, tokens/sec=1374201.44, grad_norm=0.2955, duration=0.38s
Step 15806: loss=2.8047, lr=0.000268, tokens/sec=1371044.19, grad_norm=0.3059, duration=0.38s
Step 15807: loss=2.7812, lr=0.000268, tokens/sec=1374433.34, grad_norm=0.3054, duration=0.38s
Step 15808: loss=2.7372, lr=0.000268, tokens/sec=1375078.79, grad_norm=0.3008, duration=0.38s
Step 15809: loss=2.7550, lr=0.000268, tokens/sec=1374319.96, grad_norm=0.2959, duration=0.38s
Step 15810: loss=2.7825, lr=0.000268, tokens/sec=1374647.28, grad_norm=0.2911, duration=0.38s
Step 15811: loss=2.7799, lr=0.000268, tokens/sec=1377601.07, grad_norm=0.2966, duration=0.38s
Step 15812: loss=2.7640, lr=0.000268, tokens/sec=1376314.65, grad_norm=0.2917, duration=0.38s
Step 15813: loss=2.8002, lr=0.000268, tokens/sec=1377404.33, grad_norm=0.3079, duration=0.38s
Step 15814: loss=2.7839, lr=0.000268, tokens/sec=1377585.53, grad_norm=0.3112, duration=0.38s
Step 15815: loss=2.7543, lr=0.000268, tokens/sec=1372356.73, grad_norm=0.2877, duration=0.38s
Step 15816: loss=2.7611, lr=0.000268, tokens/sec=1374877.62, grad_norm=0.2795, duration=0.38s
Step 15817: loss=2.8386, lr=0.000268, tokens/sec=1377782.32, grad_norm=0.2954, duration=0.38s
Step 15818: loss=2.8733, lr=0.000268, tokens/sec=1378281.46, grad_norm=0.3133, duration=0.38s
Step 15819: loss=2.8964, lr=0.000268, tokens/sec=1372695.11, grad_norm=0.3113, duration=0.38s
Step 15820: loss=2.8757, lr=0.000268, tokens/sec=1375792.84, grad_norm=0.2984, duration=0.38s
Step 15821: loss=2.8606, lr=0.000268, tokens/sec=1378893.35, grad_norm=0.2926, duration=0.38s
Step 15822: loss=2.8613, lr=0.000268, tokens/sec=1376967.91, grad_norm=0.3059, duration=0.38s
Step 15823: loss=2.8287, lr=0.000268, tokens/sec=1376327.58, grad_norm=0.3056, duration=0.38s
Step 15824: loss=2.8350, lr=0.000268, tokens/sec=1375887.53, grad_norm=0.3047, duration=0.38s
Step 15825: loss=2.8406, lr=0.000268, tokens/sec=1373605.72, grad_norm=0.2977, duration=0.38s
Step 15826: loss=2.8515, lr=0.000268, tokens/sec=1373507.06, grad_norm=0.3170, duration=0.38s
Step 15827: loss=2.8638, lr=0.000268, tokens/sec=1377792.68, grad_norm=0.3019, duration=0.38s
Step 15828: loss=2.8453, lr=0.000268, tokens/sec=1368344.02, grad_norm=0.3053, duration=0.38s
Step 15829: loss=2.8133, lr=0.000268, tokens/sec=1372816.80, grad_norm=0.3085, duration=0.38s
Step 15830: loss=2.8498, lr=0.000268, tokens/sec=1375635.34, grad_norm=0.3089, duration=0.38s
Step 15831: loss=2.7402, lr=0.000268, tokens/sec=1373844.29, grad_norm=0.3072, duration=0.38s
Step 15832: loss=2.7936, lr=0.000268, tokens/sec=1376033.03, grad_norm=0.3061, duration=0.38s
Step 15833: loss=2.8596, lr=0.000268, tokens/sec=1373075.67, grad_norm=0.3163, duration=0.38s
Step 15834: loss=2.8348, lr=0.000268, tokens/sec=1373224.87, grad_norm=0.2978, duration=0.38s
Step 15835: loss=2.8677, lr=0.000268, tokens/sec=1372851.08, grad_norm=0.3244, duration=0.38s
Step 15836: loss=2.7953, lr=0.000268, tokens/sec=1373821.12, grad_norm=0.3262, duration=0.38s
Step 15837: loss=2.8651, lr=0.000268, tokens/sec=1372139.23, grad_norm=0.3119, duration=0.38s
Step 15838: loss=2.8554, lr=0.000268, tokens/sec=1373799.66, grad_norm=0.2892, duration=0.38s
Step 15839: loss=2.8391, lr=0.000268, tokens/sec=1376466.28, grad_norm=0.3050, duration=0.38s
Step 15840: loss=2.8617, lr=0.000268, tokens/sec=1379228.04, grad_norm=0.3110, duration=0.38s
Step 15841: loss=2.8403, lr=0.000267, tokens/sec=1372530.61, grad_norm=0.2948, duration=0.38s
Step 15842: loss=2.8145, lr=0.000267, tokens/sec=1378696.24, grad_norm=0.2955, duration=0.38s
Step 15843: loss=2.8458, lr=0.000267, tokens/sec=1373564.54, grad_norm=0.3034, duration=0.38s
Step 15844: loss=2.8335, lr=0.000267, tokens/sec=1374392.11, grad_norm=0.2930, duration=0.38s
Step 15845: loss=2.8493, lr=0.000267, tokens/sec=1378822.45, grad_norm=0.3046, duration=0.38s
Step 15846: loss=2.8475, lr=0.000267, tokens/sec=1374852.69, grad_norm=0.3049, duration=0.38s
Step 15847: loss=2.8310, lr=0.000267, tokens/sec=1374315.67, grad_norm=0.2955, duration=0.38s
Step 15848: loss=2.8470, lr=0.000267, tokens/sec=1377548.43, grad_norm=0.3042, duration=0.38s
Step 15849: loss=2.8314, lr=0.000267, tokens/sec=1372392.70, grad_norm=0.3133, duration=0.38s
Step 15850: loss=2.8446, lr=0.000267, tokens/sec=1378665.98, grad_norm=0.3078, duration=0.38s
Step 15851: loss=2.7753, lr=0.000267, tokens/sec=1381670.94, grad_norm=0.3103, duration=0.38s
Step 15852: loss=2.8236, lr=0.000267, tokens/sec=1374935.21, grad_norm=0.3025, duration=0.38s
Step 15853: loss=2.7785, lr=0.000267, tokens/sec=1375835.88, grad_norm=0.3039, duration=0.38s
Step 15854: loss=2.7954, lr=0.000267, tokens/sec=1371301.54, grad_norm=0.3138, duration=0.38s
Step 15855: loss=2.7509, lr=0.000267, tokens/sec=1376648.96, grad_norm=0.3042, duration=0.38s
Step 15856: loss=2.7700, lr=0.000267, tokens/sec=1374944.67, grad_norm=0.3069, duration=0.38s
Step 15857: loss=2.7806, lr=0.000267, tokens/sec=1374039.15, grad_norm=0.2958, duration=0.38s
Step 15858: loss=2.7624, lr=0.000267, tokens/sec=1375115.77, grad_norm=0.2981, duration=0.38s
Step 15859: loss=2.7807, lr=0.000267, tokens/sec=1378288.37, grad_norm=0.2933, duration=0.38s
Step 15860: loss=2.7345, lr=0.000267, tokens/sec=1377000.67, grad_norm=0.3073, duration=0.38s
Step 15861: loss=2.7815, lr=0.000267, tokens/sec=1372850.23, grad_norm=0.3012, duration=0.38s
Step 15862: loss=2.7752, lr=0.000267, tokens/sec=1374617.21, grad_norm=0.3059, duration=0.38s
Step 15863: loss=2.7902, lr=0.000267, tokens/sec=1373079.10, grad_norm=0.3039, duration=0.38s
Step 15864: loss=2.8532, lr=0.000267, tokens/sec=1376454.22, grad_norm=0.3054, duration=0.38s
Step 15865: loss=2.8370, lr=0.000267, tokens/sec=1373870.04, grad_norm=0.2911, duration=0.38s
Step 15866: loss=2.8277, lr=0.000267, tokens/sec=1374541.59, grad_norm=0.3016, duration=0.38s
Step 15867: loss=2.8428, lr=0.000267, tokens/sec=1373883.77, grad_norm=0.3090, duration=0.38s
Step 15868: loss=2.7759, lr=0.000267, tokens/sec=1377111.05, grad_norm=0.3143, duration=0.38s
Step 15869: loss=2.8297, lr=0.000267, tokens/sec=1373990.22, grad_norm=0.3117, duration=0.38s
Step 15870: loss=2.7969, lr=0.000267, tokens/sec=1374139.61, grad_norm=0.3020, duration=0.38s
Step 15871: loss=2.8325, lr=0.000267, tokens/sec=1377393.98, grad_norm=0.3015, duration=0.38s
Step 15872: loss=2.8403, lr=0.000267, tokens/sec=1374593.15, grad_norm=0.3041, duration=0.38s
Step 15873: loss=2.8726, lr=0.000267, tokens/sec=1376224.21, grad_norm=0.3028, duration=0.38s
Step 15874: loss=2.8769, lr=0.000267, tokens/sec=1376592.08, grad_norm=0.3123, duration=0.38s
Step 15875: loss=2.8257, lr=0.000267, tokens/sec=1375037.52, grad_norm=0.3012, duration=0.38s
Step 15876: loss=2.8231, lr=0.000267, tokens/sec=1375463.26, grad_norm=0.3086, duration=0.38s
Step 15877: loss=2.8499, lr=0.000267, tokens/sec=1373220.58, grad_norm=0.3053, duration=0.38s
Step 15878: loss=2.8373, lr=0.000267, tokens/sec=1375375.51, grad_norm=0.3013, duration=0.38s
Step 15879: loss=2.8077, lr=0.000267, tokens/sec=1374093.25, grad_norm=0.2959, duration=0.38s
Step 15880: loss=2.8178, lr=0.000267, tokens/sec=1380264.28, grad_norm=0.2951, duration=0.38s
Step 15881: loss=2.8314, lr=0.000267, tokens/sec=1372470.65, grad_norm=0.3054, duration=0.38s
Step 15882: loss=2.8419, lr=0.000267, tokens/sec=1379151.05, grad_norm=0.2975, duration=0.38s
Step 15883: loss=2.8087, lr=0.000267, tokens/sec=1376550.72, grad_norm=0.2964, duration=0.38s
Step 15884: loss=2.8830, lr=0.000267, tokens/sec=1379109.54, grad_norm=0.3078, duration=0.38s
Step 15885: loss=2.8098, lr=0.000267, tokens/sec=1375816.94, grad_norm=0.2990, duration=0.38s
Step 15886: loss=2.8619, lr=0.000267, tokens/sec=1377616.60, grad_norm=0.2984, duration=0.38s
Step 15887: loss=2.8611, lr=0.000267, tokens/sec=1375795.43, grad_norm=0.2908, duration=0.38s
Step 15888: loss=2.8152, lr=0.000267, tokens/sec=1377397.43, grad_norm=0.2925, duration=0.38s
Step 15889: loss=2.8418, lr=0.000267, tokens/sec=1376977.39, grad_norm=0.3039, duration=0.38s
Step 15890: loss=2.8527, lr=0.000267, tokens/sec=1377280.10, grad_norm=0.3079, duration=0.38s
Step 15891: loss=2.8684, lr=0.000267, tokens/sec=1374463.41, grad_norm=0.2940, duration=0.38s
Step 15892: loss=2.8285, lr=0.000267, tokens/sec=1372620.57, grad_norm=0.2887, duration=0.38s
Step 15893: loss=2.8275, lr=0.000267, tokens/sec=1369463.74, grad_norm=0.2919, duration=0.38s
Step 15894: loss=2.7870, lr=0.000267, tokens/sec=1373014.80, grad_norm=0.3493, duration=0.38s
Step 15895: loss=2.8530, lr=0.000267, tokens/sec=1372062.17, grad_norm=0.3138, duration=0.38s
Step 15896: loss=2.8418, lr=0.000267, tokens/sec=1371061.28, grad_norm=0.3057, duration=0.38s
Step 15897: loss=2.8154, lr=0.000267, tokens/sec=1374413.59, grad_norm=0.2924, duration=0.38s
Step 15898: loss=2.8290, lr=0.000267, tokens/sec=1375617.27, grad_norm=0.3063, duration=0.38s
Step 15899: loss=2.7582, lr=0.000267, tokens/sec=1372241.12, grad_norm=0.3108, duration=0.38s
Step 15900/19073 (83.4%), Elapsed time: 6294.36s, Steps per hour: 9093.86, Estimated hours remaining: 0.35
Step 15900: loss=2.7934, lr=0.000267, tokens/sec=1378272.82, grad_norm=0.3103, duration=0.38s
Step 15901: loss=2.7076, lr=0.000267, tokens/sec=1375884.95, grad_norm=0.2921, duration=0.38s
Step 15902: loss=2.7633, lr=0.000266, tokens/sec=1377744.34, grad_norm=0.3097, duration=0.38s
Step 15903: loss=2.7918, lr=0.000266, tokens/sec=1377200.75, grad_norm=0.3416, duration=0.38s
Step 15904: loss=2.8184, lr=0.000266, tokens/sec=1377541.52, grad_norm=0.3188, duration=0.38s
Step 15905: loss=2.7793, lr=0.000266, tokens/sec=1377141.24, grad_norm=0.2965, duration=0.38s
Step 15906: loss=2.7727, lr=0.000266, tokens/sec=1376699.81, grad_norm=0.3072, duration=0.38s
Step 15907: loss=2.7746, lr=0.000266, tokens/sec=1376576.57, grad_norm=0.3267, duration=0.38s
Step 15908: loss=2.8192, lr=0.000266, tokens/sec=1378424.01, grad_norm=0.3441, duration=0.38s
Step 15909: loss=2.8203, lr=0.000266, tokens/sec=1378099.21, grad_norm=0.3234, duration=0.38s
Step 15910: loss=2.8400, lr=0.000266, tokens/sec=1373242.88, grad_norm=0.3146, duration=0.38s
Step 15911: loss=2.8294, lr=0.000266, tokens/sec=1378076.75, grad_norm=0.3221, duration=0.38s
Step 15912: loss=2.8571, lr=0.000266, tokens/sec=1377102.43, grad_norm=0.3538, duration=0.38s
Step 15913: loss=2.8110, lr=0.000266, tokens/sec=1374015.97, grad_norm=0.3247, duration=0.38s
Step 15914: loss=2.8654, lr=0.000266, tokens/sec=1374574.24, grad_norm=0.2993, duration=0.38s
Step 15915: loss=2.8097, lr=0.000266, tokens/sec=1374219.48, grad_norm=0.3176, duration=0.38s
Step 15916: loss=2.8153, lr=0.000266, tokens/sec=1375675.79, grad_norm=0.3469, duration=0.38s
Step 15917: loss=2.8973, lr=0.000266, tokens/sec=1372620.57, grad_norm=0.3410, duration=0.38s
Step 15918: loss=2.9425, lr=0.000266, tokens/sec=1372198.31, grad_norm=0.3022, duration=0.38s
Step 15919: loss=2.8242, lr=0.000266, tokens/sec=1374783.93, grad_norm=0.3320, duration=0.38s
Step 15920: loss=2.8614, lr=0.000266, tokens/sec=1378422.28, grad_norm=0.3578, duration=0.38s
Step 15921: loss=2.8129, lr=0.000266, tokens/sec=1377595.03, grad_norm=0.3397, duration=0.38s
Step 15922: loss=2.8709, lr=0.000266, tokens/sec=1371704.42, grad_norm=0.3293, duration=0.38s
Step 15923: loss=2.8244, lr=0.000266, tokens/sec=1376804.10, grad_norm=0.3203, duration=0.38s
Step 15924: loss=2.7973, lr=0.000266, tokens/sec=1371605.18, grad_norm=0.3695, duration=0.38s
Step 15925: loss=2.8649, lr=0.000266, tokens/sec=1374247.82, grad_norm=0.3457, duration=0.38s
Step 15926: loss=2.7908, lr=0.000266, tokens/sec=1378698.83, grad_norm=0.3227, duration=0.38s
Step 15927: loss=2.9063, lr=0.000266, tokens/sec=1375371.21, grad_norm=0.3216, duration=0.38s
Step 15928: loss=2.9189, lr=0.000266, tokens/sec=1374801.12, grad_norm=0.3572, duration=0.38s
Step 15929: loss=2.8545, lr=0.000266, tokens/sec=1377674.43, grad_norm=0.3581, duration=0.38s
Step 15930: loss=2.8088, lr=0.000266, tokens/sec=1370880.08, grad_norm=0.3347, duration=0.38s
Step 15931: loss=2.8536, lr=0.000266, tokens/sec=1373973.05, grad_norm=0.3316, duration=0.38s
Step 15932: loss=2.8909, lr=0.000266, tokens/sec=1374119.86, grad_norm=0.3195, duration=0.38s
Step 15933: loss=2.8652, lr=0.000266, tokens/sec=1376969.63, grad_norm=0.3240, duration=0.38s
Step 15934: loss=2.7696, lr=0.000266, tokens/sec=1376984.29, grad_norm=0.3289, duration=0.38s
Step 15935: loss=2.8327, lr=0.000266, tokens/sec=1373259.17, grad_norm=0.3259, duration=0.38s
Step 15936: loss=2.8388, lr=0.000266, tokens/sec=1373779.06, grad_norm=0.3189, duration=0.38s
Step 15937: loss=2.8511, lr=0.000266, tokens/sec=1376021.84, grad_norm=0.3076, duration=0.38s
Step 15938: loss=2.9112, lr=0.000266, tokens/sec=1377807.36, grad_norm=0.3125, duration=0.38s
Step 15939: loss=2.8089, lr=0.000266, tokens/sec=1374391.25, grad_norm=0.3283, duration=0.38s
Step 15940: loss=2.8273, lr=0.000266, tokens/sec=1375839.33, grad_norm=0.3292, duration=0.38s
Step 15941: loss=2.8429, lr=0.000266, tokens/sec=1376580.88, grad_norm=0.3194, duration=0.38s
Step 15942: loss=2.7632, lr=0.000266, tokens/sec=1374100.97, grad_norm=0.3083, duration=0.38s
Step 15943: loss=2.8368, lr=0.000266, tokens/sec=1375531.23, grad_norm=0.3210, duration=0.38s
Step 15944: loss=2.7897, lr=0.000266, tokens/sec=1373788.50, grad_norm=0.3156, duration=0.38s
Step 15945: loss=2.7615, lr=0.000266, tokens/sec=1376145.84, grad_norm=0.3259, duration=0.38s
Step 15946: loss=2.7834, lr=0.000266, tokens/sec=1376703.26, grad_norm=0.3284, duration=0.38s
Step 15947: loss=2.8332, lr=0.000266, tokens/sec=1377258.54, grad_norm=0.3284, duration=0.38s
Step 15948: loss=2.7986, lr=0.000266, tokens/sec=1377790.09, grad_norm=0.3126, duration=0.38s
Step 15949: loss=2.7790, lr=0.000266, tokens/sec=1374722.91, grad_norm=0.3139, duration=0.38s
Step 15950: loss=2.7444, lr=0.000266, tokens/sec=1377048.96, grad_norm=0.3177, duration=0.38s
Step 15951: loss=2.7596, lr=0.000266, tokens/sec=1377454.37, grad_norm=0.3226, duration=0.38s
Step 15952: loss=2.8006, lr=0.000266, tokens/sec=1369613.86, grad_norm=0.3191, duration=0.38s
Step 15953: loss=2.8064, lr=0.000266, tokens/sec=1373533.65, grad_norm=0.3161, duration=0.38s
Step 15954: loss=2.7919, lr=0.000266, tokens/sec=1377611.42, grad_norm=0.3209, duration=0.38s
Step 15955: loss=2.8399, lr=0.000266, tokens/sec=1373398.11, grad_norm=0.3289, duration=0.38s
Step 15956: loss=2.8342, lr=0.000266, tokens/sec=1375589.74, grad_norm=0.3319, duration=0.38s
Step 15957: loss=2.8451, lr=0.000266, tokens/sec=1376412.86, grad_norm=0.3257, duration=0.38s
Step 15958: loss=2.8237, lr=0.000266, tokens/sec=1377270.61, grad_norm=0.3290, duration=0.38s
Step 15959: loss=2.8227, lr=0.000266, tokens/sec=1375151.02, grad_norm=0.3410, duration=0.38s
Step 15960: loss=2.8080, lr=0.000266, tokens/sec=1375707.63, grad_norm=0.3300, duration=0.38s
Step 15961: loss=2.8374, lr=0.000266, tokens/sec=1378341.07, grad_norm=0.3061, duration=0.38s
Step 15962: loss=2.8364, lr=0.000266, tokens/sec=1374515.82, grad_norm=0.3011, duration=0.38s
Step 15963: loss=2.9158, lr=0.000266, tokens/sec=1373828.84, grad_norm=0.3353, duration=0.38s
Step 15964: loss=2.8571, lr=0.000265, tokens/sec=1377089.49, grad_norm=0.3456, duration=0.38s
Step 15965: loss=2.8077, lr=0.000265, tokens/sec=1376404.25, grad_norm=0.3333, duration=0.38s
Step 15966: loss=2.9118, lr=0.000265, tokens/sec=1372696.83, grad_norm=0.3419, duration=0.38s
Step 15967: loss=2.8502, lr=0.000265, tokens/sec=1377660.62, grad_norm=0.3051, duration=0.38s
Step 15968: loss=2.9009, lr=0.000265, tokens/sec=1375360.02, grad_norm=0.3312, duration=0.38s
Step 15969: loss=2.8730, lr=0.000265, tokens/sec=1374850.11, grad_norm=0.3454, duration=0.38s
Step 15970: loss=2.8857, lr=0.000265, tokens/sec=1377130.89, grad_norm=0.3561, duration=0.38s
Step 15971: loss=2.8468, lr=0.000265, tokens/sec=1375192.30, grad_norm=0.3192, duration=0.38s
Step 15972: loss=2.8568, lr=0.000265, tokens/sec=1377841.89, grad_norm=0.3124, duration=0.38s
Step 15973: loss=2.7832, lr=0.000265, tokens/sec=1376751.52, grad_norm=0.3137, duration=0.38s
Step 15974: loss=2.8322, lr=0.000265, tokens/sec=1374990.23, grad_norm=0.3362, duration=0.38s
Step 15975: loss=2.8636, lr=0.000265, tokens/sec=1378901.13, grad_norm=0.3749, duration=0.38s
Step 15976: loss=2.8173, lr=0.000265, tokens/sec=1376654.13, grad_norm=0.3333, duration=0.38s
Step 15977: loss=2.8763, lr=0.000265, tokens/sec=1374228.06, grad_norm=0.3147, duration=0.38s
Step 15978: loss=2.8197, lr=0.000265, tokens/sec=1372670.27, grad_norm=0.3411, duration=0.38s
Step 15979: loss=2.7514, lr=0.000265, tokens/sec=1375434.00, grad_norm=0.3764, duration=0.38s
Step 15980: loss=2.8157, lr=0.000265, tokens/sec=1377550.15, grad_norm=0.3422, duration=0.38s
Step 15981: loss=2.8476, lr=0.000265, tokens/sec=1379164.89, grad_norm=0.3160, duration=0.38s
Step 15982: loss=2.9121, lr=0.000265, tokens/sec=1371356.27, grad_norm=0.3190, duration=0.38s
Step 15983: loss=2.7809, lr=0.000265, tokens/sec=1373638.33, grad_norm=0.3310, duration=0.38s
Step 15984: loss=2.8386, lr=0.000265, tokens/sec=1376292.26, grad_norm=0.3257, duration=0.38s
Step 15985: loss=2.8573, lr=0.000265, tokens/sec=1374108.70, grad_norm=0.3142, duration=0.38s
Step 15986: loss=2.7634, lr=0.000265, tokens/sec=1373558.53, grad_norm=0.3187, duration=0.38s
Step 15987: loss=2.8360, lr=0.000265, tokens/sec=1373072.24, grad_norm=0.3068, duration=0.38s
Step 15988: loss=2.8521, lr=0.000265, tokens/sec=1375121.79, grad_norm=0.3122, duration=0.38s
Step 15989: loss=2.7970, lr=0.000265, tokens/sec=1374661.89, grad_norm=0.2989, duration=0.38s
Step 15990: loss=2.7857, lr=0.000265, tokens/sec=1375592.32, grad_norm=0.2958, duration=0.38s
Step 15991: loss=2.8072, lr=0.000265, tokens/sec=1379794.88, grad_norm=0.3051, duration=0.38s
Step 15992: loss=2.8331, lr=0.000265, tokens/sec=1374875.90, grad_norm=0.3567, duration=0.38s
Step 15993: loss=2.8803, lr=0.000265, tokens/sec=1374813.15, grad_norm=0.3222, duration=0.38s
Step 15994: loss=2.8274, lr=0.000265, tokens/sec=1375577.69, grad_norm=0.3007, duration=0.38s
Step 15995: loss=2.7598, lr=0.000265, tokens/sec=1376715.32, grad_norm=0.2862, duration=0.38s
Step 15996: loss=2.8255, lr=0.000265, tokens/sec=1377716.72, grad_norm=0.2976, duration=0.38s
Step 15997: loss=2.7366, lr=0.000265, tokens/sec=1373050.81, grad_norm=0.3020, duration=0.38s
Step 15998: loss=2.7396, lr=0.000265, tokens/sec=1373513.06, grad_norm=0.3136, duration=0.38s
Step 15999: loss=2.7648, lr=0.000265, tokens/sec=1379947.27, grad_norm=0.2954, duration=0.38s
Step 16000/19073 (83.9%), Elapsed time: 6332.56s, Steps per hour: 9095.85, Estimated hours remaining: 0.34
Validation loss at step 16000: 3.8368937969207764
Step 16000: loss=2.7747, lr=0.000265, tokens/sec=155849.49, grad_norm=0.2967, duration=3.36s
Step 16001: loss=2.7582, lr=0.000265, tokens/sec=1375047.84, grad_norm=0.2867, duration=0.38s
Step 16002: loss=2.7804, lr=0.000265, tokens/sec=1375782.51, grad_norm=0.2946, duration=0.38s
Step 16003: loss=2.8057, lr=0.000265, tokens/sec=1377838.44, grad_norm=0.2945, duration=0.38s
Step 16004: loss=2.7581, lr=0.000265, tokens/sec=1374489.19, grad_norm=0.3008, duration=0.38s
Step 16005: loss=2.7440, lr=0.000265, tokens/sec=1374891.37, grad_norm=0.3015, duration=0.38s
Step 16006: loss=2.7717, lr=0.000265, tokens/sec=1375569.95, grad_norm=0.2780, duration=0.38s
Step 16007: loss=2.8306, lr=0.000265, tokens/sec=1372006.53, grad_norm=0.2934, duration=0.38s
Step 16008: loss=2.9346, lr=0.000265, tokens/sec=1373133.98, grad_norm=0.3131, duration=0.38s
Step 16009: loss=2.8801, lr=0.000265, tokens/sec=1375201.76, grad_norm=0.3036, duration=0.38s
Step 16010: loss=2.8345, lr=0.000265, tokens/sec=1373510.49, grad_norm=0.3054, duration=0.38s
Step 16011: loss=2.8633, lr=0.000265, tokens/sec=1375007.43, grad_norm=0.3173, duration=0.38s
Step 16012: loss=2.8252, lr=0.000265, tokens/sec=1377012.74, grad_norm=0.3091, duration=0.38s
Step 16013: loss=2.8686, lr=0.000265, tokens/sec=1378930.53, grad_norm=0.3050, duration=0.38s
Step 16014: loss=2.8476, lr=0.000265, tokens/sec=1373975.62, grad_norm=0.2968, duration=0.38s
Step 16015: loss=2.8216, lr=0.000265, tokens/sec=1374119.01, grad_norm=0.3152, duration=0.38s
Step 16016: loss=2.8810, lr=0.000265, tokens/sec=1375243.90, grad_norm=0.3134, duration=0.38s
Step 16017: loss=2.8465, lr=0.000265, tokens/sec=1377202.47, grad_norm=0.2992, duration=0.38s
Step 16018: loss=2.8105, lr=0.000265, tokens/sec=1374991.95, grad_norm=0.3094, duration=0.38s
Step 16019: loss=2.8462, lr=0.000265, tokens/sec=1376933.42, grad_norm=0.3080, duration=0.38s
Step 16020: loss=2.8128, lr=0.000265, tokens/sec=1372989.94, grad_norm=0.3097, duration=0.38s
Step 16021: loss=2.7326, lr=0.000265, tokens/sec=1373543.09, grad_norm=0.3097, duration=0.38s
Step 16022: loss=2.8156, lr=0.000265, tokens/sec=1370981.79, grad_norm=0.3176, duration=0.38s
Step 16023: loss=2.8430, lr=0.000265, tokens/sec=1375241.32, grad_norm=0.3161, duration=0.38s
Step 16024: loss=2.8302, lr=0.000265, tokens/sec=1375673.21, grad_norm=0.3028, duration=0.38s
Step 16025: loss=2.8457, lr=0.000265, tokens/sec=1375707.63, grad_norm=0.3355, duration=0.38s
Step 16026: loss=2.7899, lr=0.000265, tokens/sec=1375716.24, grad_norm=0.3276, duration=0.38s
Step 16027: loss=2.8858, lr=0.000264, tokens/sec=1373852.02, grad_norm=0.3129, duration=0.38s
Step 16028: loss=2.8479, lr=0.000264, tokens/sec=1374709.15, grad_norm=0.2994, duration=0.38s
Step 16029: loss=2.8577, lr=0.000264, tokens/sec=1373333.78, grad_norm=0.3209, duration=0.38s
Step 16030: loss=2.8322, lr=0.000264, tokens/sec=1377362.05, grad_norm=0.3246, duration=0.38s
Step 16031: loss=2.8359, lr=0.000264, tokens/sec=1375079.65, grad_norm=0.3131, duration=0.38s
Step 16032: loss=2.8239, lr=0.000264, tokens/sec=1377189.53, grad_norm=0.2908, duration=0.38s
Step 16033: loss=2.8292, lr=0.000264, tokens/sec=1372319.05, grad_norm=0.2874, duration=0.38s
Step 16034: loss=2.8518, lr=0.000264, tokens/sec=1375325.62, grad_norm=0.3085, duration=0.38s
Step 16035: loss=2.8399, lr=0.000264, tokens/sec=1372903.37, grad_norm=0.3307, duration=0.38s
Step 16036: loss=2.8711, lr=0.000264, tokens/sec=1375179.40, grad_norm=0.3052, duration=0.38s
Step 16037: loss=2.8057, lr=0.000264, tokens/sec=1375303.25, grad_norm=0.2879, duration=0.38s
Step 16038: loss=2.8409, lr=0.000264, tokens/sec=1376967.91, grad_norm=0.2987, duration=0.38s
Step 16039: loss=2.8279, lr=0.000264, tokens/sec=1374095.82, grad_norm=0.3109, duration=0.38s
Step 16040: loss=2.8448, lr=0.000264, tokens/sec=1372152.07, grad_norm=0.3118, duration=0.38s
Step 16041: loss=2.7572, lr=0.000264, tokens/sec=1375107.17, grad_norm=0.3093, duration=0.38s
Step 16042: loss=2.8273, lr=0.000264, tokens/sec=1375254.23, grad_norm=0.3007, duration=0.38s
Step 16043: loss=2.7826, lr=0.000264, tokens/sec=1374335.42, grad_norm=0.2956, duration=0.38s
Step 16044: loss=2.7926, lr=0.000264, tokens/sec=1373955.88, grad_norm=0.3018, duration=0.38s
Step 16045: loss=2.7341, lr=0.000264, tokens/sec=1373616.88, grad_norm=0.3027, duration=0.38s
Step 16046: loss=2.7595, lr=0.000264, tokens/sec=1373761.04, grad_norm=0.3091, duration=0.38s
Step 16047: loss=2.8040, lr=0.000264, tokens/sec=1373595.42, grad_norm=0.2955, duration=0.38s
Step 16048: loss=2.7564, lr=0.000264, tokens/sec=1374199.72, grad_norm=0.2964, duration=0.38s
Step 16049: loss=2.7517, lr=0.000264, tokens/sec=1375584.57, grad_norm=0.2914, duration=0.38s
Step 16050: loss=2.7249, lr=0.000264, tokens/sec=1376925.66, grad_norm=0.3014, duration=0.38s
Step 16051: loss=2.7893, lr=0.000264, tokens/sec=1375912.50, grad_norm=0.2953, duration=0.38s
Step 16052: loss=2.7658, lr=0.000264, tokens/sec=1375115.77, grad_norm=0.2975, duration=0.38s
Step 16053: loss=2.8057, lr=0.000264, tokens/sec=1373323.49, grad_norm=0.2993, duration=0.38s
Step 16054: loss=2.8323, lr=0.000264, tokens/sec=1376068.34, grad_norm=0.2994, duration=0.38s
Step 16055: loss=2.8170, lr=0.000264, tokens/sec=1375547.57, grad_norm=0.3068, duration=0.38s
Step 16056: loss=2.8406, lr=0.000264, tokens/sec=1375836.74, grad_norm=0.3083, duration=0.38s
Step 16057: loss=2.8406, lr=0.000264, tokens/sec=1378266.77, grad_norm=0.2922, duration=0.38s
Step 16058: loss=2.7776, lr=0.000264, tokens/sec=1373197.43, grad_norm=0.3051, duration=0.38s
Step 16059: loss=2.8267, lr=0.000264, tokens/sec=1371363.11, grad_norm=0.3292, duration=0.38s
Step 16060: loss=2.7654, lr=0.000264, tokens/sec=1374481.45, grad_norm=0.3178, duration=0.38s
Step 16061: loss=2.8455, lr=0.000264, tokens/sec=1376657.58, grad_norm=0.3057, duration=0.38s
Step 16062: loss=2.8796, lr=0.000264, tokens/sec=1374152.49, grad_norm=0.3026, duration=0.38s
Step 16063: loss=2.8738, lr=0.000264, tokens/sec=1374423.04, grad_norm=0.3115, duration=0.38s
Step 16064: loss=2.8569, lr=0.000264, tokens/sec=1373628.89, grad_norm=0.3279, duration=0.38s
Step 16065: loss=2.8095, lr=0.000264, tokens/sec=1371441.79, grad_norm=0.3195, duration=0.38s
Step 16066: loss=2.8379, lr=0.000264, tokens/sec=1372219.71, grad_norm=0.3149, duration=0.38s
Step 16067: loss=2.8297, lr=0.000264, tokens/sec=1375553.60, grad_norm=0.3079, duration=0.38s
Step 16068: loss=2.8547, lr=0.000264, tokens/sec=1372576.02, grad_norm=0.3086, duration=0.38s
Step 16069: loss=2.7643, lr=0.000264, tokens/sec=1372167.48, grad_norm=0.3110, duration=0.38s
Step 16070: loss=2.8401, lr=0.000264, tokens/sec=1373145.12, grad_norm=0.3185, duration=0.38s
Step 16071: loss=2.8245, lr=0.000264, tokens/sec=1372887.94, grad_norm=0.3122, duration=0.38s
Step 16072: loss=2.8680, lr=0.000264, tokens/sec=1376455.08, grad_norm=0.3008, duration=0.38s
Step 16073: loss=2.8162, lr=0.000264, tokens/sec=1375474.44, grad_norm=0.3141, duration=0.38s
Step 16074: loss=2.8598, lr=0.000264, tokens/sec=1374913.72, grad_norm=0.3181, duration=0.38s
Step 16075: loss=2.8065, lr=0.000264, tokens/sec=1376759.28, grad_norm=0.3096, duration=0.38s
Step 16076: loss=2.8877, lr=0.000264, tokens/sec=1372271.09, grad_norm=0.3085, duration=0.38s
Step 16077: loss=2.7919, lr=0.000264, tokens/sec=1376020.12, grad_norm=0.3035, duration=0.38s
Step 16078: loss=2.8568, lr=0.000264, tokens/sec=1375379.81, grad_norm=0.3062, duration=0.38s
Step 16079: loss=2.8411, lr=0.000264, tokens/sec=1372930.80, grad_norm=0.3183, duration=0.38s
Step 16080: loss=2.8656, lr=0.000264, tokens/sec=1379138.94, grad_norm=0.3228, duration=0.38s
Step 16081: loss=2.8227, lr=0.000264, tokens/sec=1377042.92, grad_norm=0.3255, duration=0.38s
Step 16082: loss=2.8445, lr=0.000264, tokens/sec=1370788.64, grad_norm=0.3020, duration=0.38s
Step 16083: loss=2.7642, lr=0.000264, tokens/sec=1375384.11, grad_norm=0.3182, duration=0.38s
Step 16084: loss=2.8502, lr=0.000264, tokens/sec=1373432.42, grad_norm=0.3234, duration=0.38s
Step 16085: loss=2.8480, lr=0.000264, tokens/sec=1373676.08, grad_norm=0.3465, duration=0.38s
Step 16086: loss=2.8228, lr=0.000264, tokens/sec=1373949.87, grad_norm=0.3261, duration=0.38s
Step 16087: loss=2.8301, lr=0.000264, tokens/sec=1372635.14, grad_norm=0.2950, duration=0.38s
Step 16088: loss=2.8081, lr=0.000264, tokens/sec=1375481.32, grad_norm=0.3049, duration=0.38s
Step 16089: loss=2.7594, lr=0.000264, tokens/sec=1374465.99, grad_norm=0.3207, duration=0.38s
Step 16090: loss=2.7820, lr=0.000264, tokens/sec=1375167.36, grad_norm=0.3347, duration=0.38s
Step 16091: loss=2.7149, lr=0.000263, tokens/sec=1372467.22, grad_norm=0.3070, duration=0.38s
Step 16092: loss=2.7899, lr=0.000263, tokens/sec=1373605.72, grad_norm=0.3063, duration=0.38s
Step 16093: loss=2.7527, lr=0.000263, tokens/sec=1376457.66, grad_norm=0.3335, duration=0.38s
Step 16094: loss=2.8133, lr=0.000263, tokens/sec=1377002.40, grad_norm=0.3285, duration=0.38s
Step 16095: loss=2.7911, lr=0.000263, tokens/sec=1373873.47, grad_norm=0.3274, duration=0.38s
Step 16096: loss=2.7821, lr=0.000263, tokens/sec=1373825.41, grad_norm=0.2982, duration=0.38s
Step 16097: loss=2.7858, lr=0.000263, tokens/sec=1375397.87, grad_norm=0.3105, duration=0.38s
Step 16098: loss=2.7859, lr=0.000263, tokens/sec=1375676.65, grad_norm=0.3412, duration=0.38s
Step 16099: loss=2.8276, lr=0.000263, tokens/sec=1375963.29, grad_norm=0.3535, duration=0.38s
Step 16100/19073 (84.4%), Elapsed time: 6373.75s, Steps per hour: 9093.54, Estimated hours remaining: 0.33
Step 16100: loss=2.8055, lr=0.000263, tokens/sec=1374363.77, grad_norm=0.3239, duration=0.38s
Step 16101: loss=2.8490, lr=0.000263, tokens/sec=1373612.59, grad_norm=0.3032, duration=0.38s
Step 16102: loss=2.8281, lr=0.000263, tokens/sec=1375303.25, grad_norm=0.3298, duration=0.38s
Step 16103: loss=2.8356, lr=0.000263, tokens/sec=1376804.97, grad_norm=0.3469, duration=0.38s
Step 16104: loss=2.8592, lr=0.000263, tokens/sec=1380453.18, grad_norm=0.3293, duration=0.38s
Step 16105: loss=2.7875, lr=0.000263, tokens/sec=1372859.65, grad_norm=0.3057, duration=0.38s
Step 16106: loss=2.8517, lr=0.000263, tokens/sec=1374220.33, grad_norm=0.3338, duration=0.38s
Step 16107: loss=2.8813, lr=0.000263, tokens/sec=1375665.46, grad_norm=0.3417, duration=0.38s
Step 16108: loss=2.8749, lr=0.000263, tokens/sec=1372740.53, grad_norm=0.3369, duration=0.38s
Step 16109: loss=2.8714, lr=0.000263, tokens/sec=1371858.46, grad_norm=0.3124, duration=0.38s
Step 16110: loss=2.8589, lr=0.000263, tokens/sec=1373430.71, grad_norm=0.3307, duration=0.38s
Step 16111: loss=2.8392, lr=0.000263, tokens/sec=1370417.89, grad_norm=0.3603, duration=0.38s
Step 16112: loss=2.8582, lr=0.000263, tokens/sec=1374263.27, grad_norm=0.3385, duration=0.38s
Step 16113: loss=2.8234, lr=0.000263, tokens/sec=1372334.46, grad_norm=0.3282, duration=0.38s
Step 16114: loss=2.7746, lr=0.000263, tokens/sec=1374307.08, grad_norm=0.3455, duration=0.38s
Step 16115: loss=2.8761, lr=0.000263, tokens/sec=1374964.44, grad_norm=0.3359, duration=0.38s
Step 16116: loss=2.8222, lr=0.000263, tokens/sec=1371647.95, grad_norm=0.3392, duration=0.38s
Step 16117: loss=2.8934, lr=0.000263, tokens/sec=1377821.17, grad_norm=0.3358, duration=0.38s
Step 16118: loss=2.9005, lr=0.000263, tokens/sec=1372903.37, grad_norm=0.3479, duration=0.38s
Step 16119: loss=2.8511, lr=0.000263, tokens/sec=1376711.01, grad_norm=0.3437, duration=0.38s
Step 16120: loss=2.8016, lr=0.000263, tokens/sec=1372731.10, grad_norm=0.3227, duration=0.38s
Step 16121: loss=2.8543, lr=0.000263, tokens/sec=1374435.06, grad_norm=0.3387, duration=0.38s
Step 16122: loss=2.8938, lr=0.000263, tokens/sec=1375059.02, grad_norm=0.3278, duration=0.38s
Step 16123: loss=2.8402, lr=0.000263, tokens/sec=1370302.61, grad_norm=0.3189, duration=0.38s
Step 16124: loss=2.7752, lr=0.000263, tokens/sec=1370186.49, grad_norm=0.3168, duration=0.38s
Step 16125: loss=2.8302, lr=0.000263, tokens/sec=1373029.38, grad_norm=0.3047, duration=0.38s
Step 16126: loss=2.8306, lr=0.000263, tokens/sec=1375626.74, grad_norm=0.3114, duration=0.38s
Step 16127: loss=2.8827, lr=0.000263, tokens/sec=1376547.27, grad_norm=0.3254, duration=0.38s
Step 16128: loss=2.8827, lr=0.000263, tokens/sec=1372848.51, grad_norm=0.3135, duration=0.38s
Step 16129: loss=2.8134, lr=0.000263, tokens/sec=1373461.59, grad_norm=0.3074, duration=0.38s
Step 16130: loss=2.8401, lr=0.000263, tokens/sec=1374957.56, grad_norm=0.3113, duration=0.38s
Step 16131: loss=2.8017, lr=0.000263, tokens/sec=1374875.04, grad_norm=0.3132, duration=0.38s
Step 16132: loss=2.7953, lr=0.000263, tokens/sec=1371990.27, grad_norm=0.3178, duration=0.38s
Step 16133: loss=2.8224, lr=0.000263, tokens/sec=1378386.86, grad_norm=0.3312, duration=0.38s
Step 16134: loss=2.7738, lr=0.000263, tokens/sec=1374704.86, grad_norm=0.3097, duration=0.38s
Step 16135: loss=2.7739, lr=0.000263, tokens/sec=1371962.02, grad_norm=0.2980, duration=0.38s
Step 16136: loss=2.7711, lr=0.000263, tokens/sec=1380238.29, grad_norm=0.3144, duration=0.38s
Step 16137: loss=2.8432, lr=0.000263, tokens/sec=1376606.73, grad_norm=0.3272, duration=0.38s
Step 16138: loss=2.7947, lr=0.000263, tokens/sec=1376286.23, grad_norm=0.3196, duration=0.38s
Step 16139: loss=2.7672, lr=0.000263, tokens/sec=1374663.61, grad_norm=0.3103, duration=0.38s
Step 16140: loss=2.7288, lr=0.000263, tokens/sec=1373587.70, grad_norm=0.3000, duration=0.38s
Step 16141: loss=2.7599, lr=0.000263, tokens/sec=1376231.10, grad_norm=0.3147, duration=0.38s
Step 16142: loss=2.8238, lr=0.000263, tokens/sec=1374799.40, grad_norm=0.3142, duration=0.38s
Step 16143: loss=2.7727, lr=0.000263, tokens/sec=1375872.04, grad_norm=0.3186, duration=0.38s
Step 16144: loss=2.7941, lr=0.000263, tokens/sec=1375545.85, grad_norm=0.3264, duration=0.38s
Step 16145: loss=2.8299, lr=0.000263, tokens/sec=1377035.16, grad_norm=0.3404, duration=0.38s
Step 16146: loss=2.8390, lr=0.000263, tokens/sec=1373815.97, grad_norm=0.3331, duration=0.38s
Step 16147: loss=2.8483, lr=0.000263, tokens/sec=1376693.78, grad_norm=0.3168, duration=0.38s
Step 16148: loss=2.7921, lr=0.000263, tokens/sec=1374105.27, grad_norm=0.3143, duration=0.38s
Step 16149: loss=2.8339, lr=0.000263, tokens/sec=1375059.88, grad_norm=0.3485, duration=0.38s
Step 16150: loss=2.8031, lr=0.000263, tokens/sec=1374345.73, grad_norm=0.3438, duration=0.38s
Step 16151: loss=2.8302, lr=0.000263, tokens/sec=1376425.78, grad_norm=0.3147, duration=0.38s
Step 16152: loss=2.8430, lr=0.000263, tokens/sec=1373445.29, grad_norm=0.2979, duration=0.38s
Step 16153: loss=2.9156, lr=0.000263, tokens/sec=1373732.72, grad_norm=0.3267, duration=0.38s
Step 16154: loss=2.7873, lr=0.000263, tokens/sec=1369936.39, grad_norm=0.3357, duration=0.38s
Step 16155: loss=2.8669, lr=0.000263, tokens/sec=1373369.81, grad_norm=0.3565, duration=0.38s
Step 16156: loss=2.9084, lr=0.000263, tokens/sec=1378496.59, grad_norm=0.3436, duration=0.38s
Step 16157: loss=2.8831, lr=0.000262, tokens/sec=1376014.95, grad_norm=0.3176, duration=0.38s
Step 16158: loss=2.8889, lr=0.000262, tokens/sec=1376034.76, grad_norm=0.3183, duration=0.38s
Step 16159: loss=2.8700, lr=0.000262, tokens/sec=1376042.51, grad_norm=0.3364, duration=0.38s
Step 16160: loss=2.8761, lr=0.000262, tokens/sec=1378312.56, grad_norm=0.3578, duration=0.38s
Step 16161: loss=2.8462, lr=0.000262, tokens/sec=1375792.84, grad_norm=0.3320, duration=0.38s
Step 16162: loss=2.8388, lr=0.000262, tokens/sec=1375362.60, grad_norm=0.3229, duration=0.38s
Step 16163: loss=2.7549, lr=0.000262, tokens/sec=1374582.84, grad_norm=0.2988, duration=0.38s
Step 16164: loss=2.8750, lr=0.000262, tokens/sec=1375904.75, grad_norm=0.3334, duration=0.38s
Step 16165: loss=2.8554, lr=0.000262, tokens/sec=1376745.49, grad_norm=0.3797, duration=0.38s
Step 16166: loss=2.8063, lr=0.000262, tokens/sec=1378953.01, grad_norm=0.3369, duration=0.38s
Step 16167: loss=2.8992, lr=0.000262, tokens/sec=1375969.32, grad_norm=0.3172, duration=0.38s
Step 16168: loss=2.7816, lr=0.000262, tokens/sec=1375127.81, grad_norm=0.3253, duration=0.38s
Step 16169: loss=2.7698, lr=0.000262, tokens/sec=1374346.59, grad_norm=0.3902, duration=0.38s
Step 16170: loss=2.8044, lr=0.000262, tokens/sec=1374498.64, grad_norm=0.3371, duration=0.38s
Step 16171: loss=2.8358, lr=0.000262, tokens/sec=1374867.30, grad_norm=0.3222, duration=0.38s
Step 16172: loss=2.9269, lr=0.000262, tokens/sec=1371304.96, grad_norm=0.3240, duration=0.38s
Step 16173: loss=2.7239, lr=0.000262, tokens/sec=1373882.06, grad_norm=0.3227, duration=0.38s
Step 16174: loss=2.8639, lr=0.000262, tokens/sec=1372447.52, grad_norm=0.3230, duration=0.38s
Step 16175: loss=2.8177, lr=0.000262, tokens/sec=1374769.31, grad_norm=0.3195, duration=0.38s
Step 16176: loss=2.7958, lr=0.000262, tokens/sec=1375903.89, grad_norm=0.3146, duration=0.38s
Step 16177: loss=2.8284, lr=0.000262, tokens/sec=1373440.14, grad_norm=0.3136, duration=0.38s
Step 16178: loss=2.8174, lr=0.000262, tokens/sec=1371716.40, grad_norm=0.3132, duration=0.38s
Step 16179: loss=2.8219, lr=0.000262, tokens/sec=1371026.24, grad_norm=0.2942, duration=0.38s
Step 16180: loss=2.7692, lr=0.000262, tokens/sec=1372708.82, grad_norm=0.2977, duration=0.38s
Step 16181: loss=2.8254, lr=0.000262, tokens/sec=1376041.64, grad_norm=0.3110, duration=0.38s
Step 16182: loss=2.8465, lr=0.000262, tokens/sec=1374862.14, grad_norm=0.3222, duration=0.38s
Step 16183: loss=2.8768, lr=0.000262, tokens/sec=1375046.98, grad_norm=0.3181, duration=0.38s
Step 16184: loss=2.8102, lr=0.000262, tokens/sec=1375815.22, grad_norm=0.3110, duration=0.38s
Step 16185: loss=2.7811, lr=0.000262, tokens/sec=1375531.23, grad_norm=0.2921, duration=0.38s
Step 16186: loss=2.7794, lr=0.000262, tokens/sec=1377366.37, grad_norm=0.2918, duration=0.38s
Step 16187: loss=2.7384, lr=0.000262, tokens/sec=1371266.48, grad_norm=0.2904, duration=0.38s
Step 16188: loss=2.7469, lr=0.000262, tokens/sec=1375219.82, grad_norm=0.3045, duration=0.38s
Step 16189: loss=2.7582, lr=0.000262, tokens/sec=1373442.72, grad_norm=0.3056, duration=0.38s
Step 16190: loss=2.7513, lr=0.000262, tokens/sec=1371773.73, grad_norm=0.2917, duration=0.38s
Step 16191: loss=2.7753, lr=0.000262, tokens/sec=1370346.16, grad_norm=0.2907, duration=0.38s
Step 16192: loss=2.7871, lr=0.000262, tokens/sec=1374851.83, grad_norm=0.2778, duration=0.38s
Step 16193: loss=2.7808, lr=0.000262, tokens/sec=1373562.82, grad_norm=0.2879, duration=0.38s
Step 16194: loss=2.7459, lr=0.000262, tokens/sec=1372478.36, grad_norm=0.2936, duration=0.38s
Step 16195: loss=2.7588, lr=0.000262, tokens/sec=1373102.25, grad_norm=0.2870, duration=0.38s
Step 16196: loss=2.7644, lr=0.000262, tokens/sec=1376610.18, grad_norm=0.2754, duration=0.38s
Step 16197: loss=2.8913, lr=0.000262, tokens/sec=1375174.24, grad_norm=0.3001, duration=0.38s
Step 16198: loss=2.9185, lr=0.000262, tokens/sec=1372420.11, grad_norm=0.3123, duration=0.38s
Step 16199: loss=2.8376, lr=0.000262, tokens/sec=1375491.65, grad_norm=0.2878, duration=0.38s
Step 16200/19073 (84.9%), Elapsed time: 6411.99s, Steps per hour: 9095.46, Estimated hours remaining: 0.32
Step 16200: loss=2.8367, lr=0.000262, tokens/sec=1375318.73, grad_norm=0.2936, duration=0.38s
Step 16201: loss=2.8264, lr=0.000262, tokens/sec=1372869.94, grad_norm=0.3111, duration=0.38s
Step 16202: loss=2.8638, lr=0.000262, tokens/sec=1377488.88, grad_norm=0.3140, duration=0.38s
Step 16203: loss=2.8779, lr=0.000262, tokens/sec=1374025.42, grad_norm=0.2979, duration=0.38s
Step 16204: loss=2.8275, lr=0.000262, tokens/sec=1374540.73, grad_norm=0.2983, duration=0.38s
Step 16205: loss=2.8502, lr=0.000262, tokens/sec=1373565.40, grad_norm=0.2951, duration=0.38s
Step 16206: loss=2.8618, lr=0.000262, tokens/sec=1374838.94, grad_norm=0.3013, duration=0.38s
Step 16207: loss=2.8123, lr=0.000262, tokens/sec=1374209.17, grad_norm=0.3074, duration=0.38s
Step 16208: loss=2.8424, lr=0.000262, tokens/sec=1378550.17, grad_norm=0.3008, duration=0.38s
Step 16209: loss=2.8077, lr=0.000262, tokens/sec=1371924.36, grad_norm=0.2914, duration=0.38s
Step 16210: loss=2.8047, lr=0.000262, tokens/sec=1376022.70, grad_norm=0.3093, duration=0.38s
Step 16211: loss=2.7548, lr=0.000262, tokens/sec=1376863.59, grad_norm=0.3147, duration=0.38s
Step 16212: loss=2.7980, lr=0.000262, tokens/sec=1377506.14, grad_norm=0.3081, duration=0.38s
Step 16213: loss=2.8351, lr=0.000262, tokens/sec=1375170.80, grad_norm=0.3124, duration=0.38s
Step 16214: loss=2.8062, lr=0.000262, tokens/sec=1374958.42, grad_norm=0.3062, duration=0.38s
Step 16215: loss=2.8416, lr=0.000262, tokens/sec=1377205.06, grad_norm=0.3318, duration=0.38s
Step 16216: loss=2.8093, lr=0.000262, tokens/sec=1372984.80, grad_norm=0.3140, duration=0.38s
Step 16217: loss=2.8803, lr=0.000262, tokens/sec=1378359.21, grad_norm=0.3149, duration=0.38s
Step 16218: loss=2.8660, lr=0.000262, tokens/sec=1373679.51, grad_norm=0.2941, duration=0.38s
Step 16219: loss=2.8279, lr=0.000262, tokens/sec=1379280.81, grad_norm=0.3068, duration=0.38s
Step 16220: loss=2.8273, lr=0.000262, tokens/sec=1376739.46, grad_norm=0.3188, duration=0.38s
Step 16221: loss=2.8432, lr=0.000262, tokens/sec=1378051.71, grad_norm=0.3127, duration=0.38s
Step 16222: loss=2.8065, lr=0.000262, tokens/sec=1374989.37, grad_norm=0.3032, duration=0.38s
Step 16223: loss=2.8485, lr=0.000262, tokens/sec=1379649.45, grad_norm=0.2869, duration=0.38s
Step 16224: loss=2.8430, lr=0.000261, tokens/sec=1378643.51, grad_norm=0.2924, duration=0.38s
Step 16225: loss=2.8610, lr=0.000261, tokens/sec=1375192.30, grad_norm=0.3070, duration=0.38s
Step 16226: loss=2.8464, lr=0.000261, tokens/sec=1377267.16, grad_norm=0.3106, duration=0.38s
Step 16227: loss=2.7968, lr=0.000261, tokens/sec=1375554.46, grad_norm=0.2960, duration=0.38s
Step 16228: loss=2.8343, lr=0.000261, tokens/sec=1377275.79, grad_norm=0.2938, duration=0.38s
Step 16229: loss=2.8266, lr=0.000261, tokens/sec=1378475.85, grad_norm=0.2934, duration=0.38s
Step 16230: loss=2.8237, lr=0.000261, tokens/sec=1375494.23, grad_norm=0.3073, duration=0.38s
Step 16231: loss=2.7609, lr=0.000261, tokens/sec=1378144.98, grad_norm=0.3114, duration=0.38s
Step 16232: loss=2.8303, lr=0.000261, tokens/sec=1373543.95, grad_norm=0.3110, duration=0.38s
Step 16233: loss=2.7801, lr=0.000261, tokens/sec=1375435.73, grad_norm=0.2928, duration=0.38s
Step 16234: loss=2.7787, lr=0.000261, tokens/sec=1377042.92, grad_norm=0.2925, duration=0.38s
Step 16235: loss=2.7244, lr=0.000261, tokens/sec=1380217.50, grad_norm=0.3048, duration=0.38s
Step 16236: loss=2.7848, lr=0.000261, tokens/sec=1378117.34, grad_norm=0.3199, duration=0.38s
Step 16237: loss=2.8010, lr=0.000261, tokens/sec=1374541.59, grad_norm=0.3107, duration=0.38s
Step 16238: loss=2.7295, lr=0.000261, tokens/sec=1377704.64, grad_norm=0.2959, duration=0.38s
Step 16239: loss=2.7399, lr=0.000261, tokens/sec=1379703.98, grad_norm=0.2909, duration=0.38s
Step 16240: loss=2.7311, lr=0.000261, tokens/sec=1373350.08, grad_norm=0.3146, duration=0.38s
Step 16241: loss=2.7790, lr=0.000261, tokens/sec=1378324.65, grad_norm=0.3098, duration=0.38s
Step 16242: loss=2.7803, lr=0.000261, tokens/sec=1377705.50, grad_norm=0.3025, duration=0.38s
Step 16243: loss=2.7860, lr=0.000261, tokens/sec=1376283.65, grad_norm=0.2994, duration=0.38s
Step 16244: loss=2.8097, lr=0.000261, tokens/sec=1375161.34, grad_norm=0.2904, duration=0.38s
Step 16245: loss=2.8294, lr=0.000261, tokens/sec=1374584.55, grad_norm=0.3108, duration=0.38s
Step 16246: loss=2.8392, lr=0.000261, tokens/sec=1374568.23, grad_norm=0.3189, duration=0.38s
Step 16247: loss=2.8439, lr=0.000261, tokens/sec=1379020.46, grad_norm=0.3041, duration=0.38s
Step 16248: loss=2.7782, lr=0.000261, tokens/sec=1373031.95, grad_norm=0.3146, duration=0.38s
Step 16249: loss=2.7953, lr=0.000261, tokens/sec=1375587.16, grad_norm=0.3062, duration=0.38s
Validation loss at step 16250: 3.843003034591675
Step 16250: loss=2.7801, lr=0.000261, tokens/sec=153336.66, grad_norm=0.3315, duration=3.42s
Step 16251: loss=2.8893, lr=0.000261, tokens/sec=1376262.11, grad_norm=0.3392, duration=0.38s
Step 16252: loss=2.8825, lr=0.000261, tokens/sec=1377264.57, grad_norm=0.3206, duration=0.38s
Step 16253: loss=2.8533, lr=0.000261, tokens/sec=1379368.19, grad_norm=0.3037, duration=0.38s
Step 16254: loss=2.8408, lr=0.000261, tokens/sec=1376280.20, grad_norm=0.3194, duration=0.38s
Step 16255: loss=2.8252, lr=0.000261, tokens/sec=1373104.82, grad_norm=0.3346, duration=0.38s
Step 16256: loss=2.8205, lr=0.000261, tokens/sec=1377715.86, grad_norm=0.3322, duration=0.38s
Step 16257: loss=2.8494, lr=0.000261, tokens/sec=1376430.09, grad_norm=0.3182, duration=0.38s
Step 16258: loss=2.8117, lr=0.000261, tokens/sec=1372872.51, grad_norm=0.3101, duration=0.38s
Step 16259: loss=2.7854, lr=0.000261, tokens/sec=1376705.84, grad_norm=0.3128, duration=0.38s
Step 16260: loss=2.8330, lr=0.000261, tokens/sec=1379738.61, grad_norm=0.3184, duration=0.38s
Step 16261: loss=2.8483, lr=0.000261, tokens/sec=1378697.97, grad_norm=0.3317, duration=0.38s
Step 16262: loss=2.8732, lr=0.000261, tokens/sec=1371585.50, grad_norm=0.3231, duration=0.38s
Step 16263: loss=2.7898, lr=0.000261, tokens/sec=1374766.74, grad_norm=0.3080, duration=0.38s
Step 16264: loss=2.8544, lr=0.000261, tokens/sec=1375722.27, grad_norm=0.3168, duration=0.38s
Step 16265: loss=2.8297, lr=0.000261, tokens/sec=1374284.75, grad_norm=0.3221, duration=0.38s
Step 16266: loss=2.8186, lr=0.000261, tokens/sec=1376622.24, grad_norm=0.3145, duration=0.38s
Step 16267: loss=2.8341, lr=0.000261, tokens/sec=1370037.10, grad_norm=0.3069, duration=0.38s
Step 16268: loss=2.8569, lr=0.000261, tokens/sec=1373132.26, grad_norm=0.3158, duration=0.38s
Step 16269: loss=2.8553, lr=0.000261, tokens/sec=1373231.73, grad_norm=0.3137, duration=0.38s
Step 16270: loss=2.8218, lr=0.000261, tokens/sec=1372575.16, grad_norm=0.3215, duration=0.38s
Step 16271: loss=2.8417, lr=0.000261, tokens/sec=1377060.17, grad_norm=0.3189, duration=0.38s
Step 16272: loss=2.7821, lr=0.000261, tokens/sec=1375413.36, grad_norm=0.3331, duration=0.38s
Step 16273: loss=2.8302, lr=0.000261, tokens/sec=1376615.35, grad_norm=0.3152, duration=0.38s
Step 16274: loss=2.8469, lr=0.000261, tokens/sec=1375235.30, grad_norm=0.3178, duration=0.38s
Step 16275: loss=2.8268, lr=0.000261, tokens/sec=1378659.93, grad_norm=0.3269, duration=0.38s
Step 16276: loss=2.8350, lr=0.000261, tokens/sec=1375020.33, grad_norm=0.3376, duration=0.38s
Step 16277: loss=2.8094, lr=0.000261, tokens/sec=1374964.44, grad_norm=0.3208, duration=0.38s
Step 16278: loss=2.8102, lr=0.000261, tokens/sec=1374429.91, grad_norm=0.3000, duration=0.38s
Step 16279: loss=2.7484, lr=0.000261, tokens/sec=1376385.29, grad_norm=0.3105, duration=0.38s
Step 16280: loss=2.7908, lr=0.000261, tokens/sec=1377456.96, grad_norm=0.3224, duration=0.38s
Step 16281: loss=2.7400, lr=0.000261, tokens/sec=1372834.80, grad_norm=0.3260, duration=0.38s
Step 16282: loss=2.7519, lr=0.000261, tokens/sec=1376040.78, grad_norm=0.3224, duration=0.38s
Step 16283: loss=2.7453, lr=0.000261, tokens/sec=1378229.63, grad_norm=0.3143, duration=0.38s
Step 16284: loss=2.8213, lr=0.000261, tokens/sec=1371051.88, grad_norm=0.3193, duration=0.38s
Step 16285: loss=2.7994, lr=0.000261, tokens/sec=1376185.46, grad_norm=0.3315, duration=0.38s
Step 16286: loss=2.7954, lr=0.000261, tokens/sec=1373813.39, grad_norm=0.3261, duration=0.38s
Step 16287: loss=2.7532, lr=0.000261, tokens/sec=1377841.03, grad_norm=0.3114, duration=0.38s
Step 16288: loss=2.7926, lr=0.000261, tokens/sec=1376093.31, grad_norm=0.3231, duration=0.38s
Step 16289: loss=2.7924, lr=0.000261, tokens/sec=1377014.47, grad_norm=0.3207, duration=0.38s
Step 16290: loss=2.8263, lr=0.000261, tokens/sec=1376051.12, grad_norm=0.3294, duration=0.38s
Step 16291: loss=2.8222, lr=0.000261, tokens/sec=1375295.51, grad_norm=0.3101, duration=0.38s
Step 16292: loss=2.8527, lr=0.000260, tokens/sec=1377628.68, grad_norm=0.3237, duration=0.38s
Step 16293: loss=2.8272, lr=0.000260, tokens/sec=1375565.64, grad_norm=0.3238, duration=0.38s
Step 16294: loss=2.8349, lr=0.000260, tokens/sec=1377052.41, grad_norm=0.3143, duration=0.38s
Step 16295: loss=2.8206, lr=0.000260, tokens/sec=1376978.25, grad_norm=0.3136, duration=0.38s
Step 16296: loss=2.8351, lr=0.000260, tokens/sec=1372624.00, grad_norm=0.3137, duration=0.38s
Step 16297: loss=2.8142, lr=0.000260, tokens/sec=1374386.10, grad_norm=0.3357, duration=0.38s
Step 16298: loss=2.9236, lr=0.000260, tokens/sec=1375876.34, grad_norm=0.3213, duration=0.38s
Step 16299: loss=2.8658, lr=0.000260, tokens/sec=1374702.28, grad_norm=0.3093, duration=0.38s
Step 16300/19073 (85.5%), Elapsed time: 6453.21s, Steps per hour: 9093.15, Estimated hours remaining: 0.30
Step 16300: loss=2.8849, lr=0.000260, tokens/sec=1376125.17, grad_norm=0.3251, duration=0.38s
Step 16301: loss=2.8279, lr=0.000260, tokens/sec=1378138.93, grad_norm=0.3290, duration=0.38s
Step 16302: loss=2.8570, lr=0.000260, tokens/sec=1377967.08, grad_norm=0.3376, duration=0.38s
Step 16303: loss=2.8015, lr=0.000260, tokens/sec=1371483.70, grad_norm=0.3307, duration=0.38s
Step 16304: loss=2.7820, lr=0.000260, tokens/sec=1377115.36, grad_norm=0.3355, duration=0.38s
Step 16305: loss=2.9071, lr=0.000260, tokens/sec=1376782.55, grad_norm=0.3268, duration=0.38s
Step 16306: loss=2.8109, lr=0.000260, tokens/sec=1375859.99, grad_norm=0.3261, duration=0.38s
Step 16307: loss=2.8752, lr=0.000260, tokens/sec=1379868.47, grad_norm=0.3226, duration=0.38s
Step 16308: loss=2.8971, lr=0.000260, tokens/sec=1375451.21, grad_norm=0.3494, duration=0.38s
Step 16309: loss=2.8429, lr=0.000260, tokens/sec=1376496.43, grad_norm=0.3455, duration=0.38s
Step 16310: loss=2.8036, lr=0.000260, tokens/sec=1376598.11, grad_norm=0.3239, duration=0.38s
Step 16311: loss=2.8591, lr=0.000260, tokens/sec=1375859.12, grad_norm=0.3180, duration=0.38s
Step 16312: loss=2.8695, lr=0.000260, tokens/sec=1376195.79, grad_norm=0.3158, duration=0.38s
Step 16313: loss=2.8468, lr=0.000260, tokens/sec=1373994.51, grad_norm=0.3311, duration=0.38s
Step 16314: loss=2.7742, lr=0.000260, tokens/sec=1372855.37, grad_norm=0.3233, duration=0.38s
Step 16315: loss=2.8243, lr=0.000260, tokens/sec=1377101.56, grad_norm=0.3048, duration=0.38s
Step 16316: loss=2.8588, lr=0.000260, tokens/sec=1377383.62, grad_norm=0.3036, duration=0.38s
Step 16317: loss=2.8531, lr=0.000260, tokens/sec=1373835.71, grad_norm=0.3043, duration=0.38s
Step 16318: loss=2.8879, lr=0.000260, tokens/sec=1375319.59, grad_norm=0.3210, duration=0.38s
Step 16319: loss=2.8258, lr=0.000260, tokens/sec=1376566.23, grad_norm=0.3219, duration=0.38s
Step 16320: loss=2.7976, lr=0.000260, tokens/sec=1379283.40, grad_norm=0.3042, duration=0.38s
Step 16321: loss=2.8326, lr=0.000260, tokens/sec=1377477.67, grad_norm=0.3104, duration=0.38s
Step 16322: loss=2.7815, lr=0.000260, tokens/sec=1375280.03, grad_norm=0.3222, duration=0.38s
Step 16323: loss=2.8065, lr=0.000260, tokens/sec=1376623.97, grad_norm=0.3236, duration=0.38s
Step 16324: loss=2.7861, lr=0.000260, tokens/sec=1378485.36, grad_norm=0.3111, duration=0.38s
Step 16325: loss=2.7615, lr=0.000260, tokens/sec=1376945.49, grad_norm=0.3070, duration=0.38s
Step 16326: loss=2.7812, lr=0.000260, tokens/sec=1377590.71, grad_norm=0.3082, duration=0.38s
Step 16327: loss=2.8382, lr=0.000260, tokens/sec=1375427.98, grad_norm=0.3140, duration=0.38s
Step 16328: loss=2.7844, lr=0.000260, tokens/sec=1375101.15, grad_norm=0.3310, duration=0.38s
Step 16329: loss=2.7544, lr=0.000260, tokens/sec=1375041.82, grad_norm=0.3191, duration=0.38s
Step 16330: loss=2.7330, lr=0.000260, tokens/sec=1377084.32, grad_norm=0.2997, duration=0.38s
Step 16331: loss=2.7850, lr=0.000260, tokens/sec=1376761.01, grad_norm=0.3024, duration=0.38s
Step 16332: loss=2.7909, lr=0.000260, tokens/sec=1373720.70, grad_norm=0.3078, duration=0.38s
Step 16333: loss=2.7740, lr=0.000260, tokens/sec=1378617.58, grad_norm=0.3160, duration=0.38s
Step 16334: loss=2.7829, lr=0.000260, tokens/sec=1374179.11, grad_norm=0.3204, duration=0.38s
Step 16335: loss=2.8359, lr=0.000260, tokens/sec=1376774.80, grad_norm=0.3299, duration=0.38s
Step 16336: loss=2.8379, lr=0.000260, tokens/sec=1372572.59, grad_norm=0.3202, duration=0.38s
Step 16337: loss=2.8146, lr=0.000260, tokens/sec=1373732.72, grad_norm=0.3022, duration=0.38s
Step 16338: loss=2.8020, lr=0.000260, tokens/sec=1375810.92, grad_norm=0.3044, duration=0.38s
Step 16339: loss=2.8267, lr=0.000260, tokens/sec=1375256.81, grad_norm=0.3264, duration=0.38s
Step 16340: loss=2.7978, lr=0.000260, tokens/sec=1374424.75, grad_norm=0.3432, duration=0.38s
Step 16341: loss=2.8353, lr=0.000260, tokens/sec=1377090.35, grad_norm=0.3273, duration=0.38s
Step 16342: loss=2.8452, lr=0.000260, tokens/sec=1378618.45, grad_norm=0.2957, duration=0.38s
Step 16343: loss=2.8457, lr=0.000260, tokens/sec=1379239.28, grad_norm=0.2998, duration=0.38s
Step 16344: loss=2.8423, lr=0.000260, tokens/sec=1374617.21, grad_norm=0.3260, duration=0.38s
Step 16345: loss=2.8629, lr=0.000260, tokens/sec=1375947.80, grad_norm=0.3313, duration=0.38s
Step 16346: loss=2.9391, lr=0.000260, tokens/sec=1377090.35, grad_norm=0.3409, duration=0.38s
Step 16347: loss=2.8723, lr=0.000260, tokens/sec=1376719.63, grad_norm=0.3105, duration=0.38s
Step 16348: loss=2.8875, lr=0.000260, tokens/sec=1373196.57, grad_norm=0.3237, duration=0.38s
Step 16349: loss=2.8587, lr=0.000260, tokens/sec=1376121.73, grad_norm=0.3277, duration=0.38s
Step 16350: loss=2.8745, lr=0.000260, tokens/sec=1377842.75, grad_norm=0.3312, duration=0.38s
Step 16351: loss=2.8274, lr=0.000260, tokens/sec=1377321.51, grad_norm=0.3173, duration=0.38s
Step 16352: loss=2.8088, lr=0.000260, tokens/sec=1375517.46, grad_norm=0.3219, duration=0.38s
Step 16353: loss=2.8005, lr=0.000260, tokens/sec=1375801.45, grad_norm=0.3035, duration=0.38s
Step 16354: loss=2.8694, lr=0.000260, tokens/sec=1373314.06, grad_norm=0.3040, duration=0.38s
Step 16355: loss=2.8451, lr=0.000260, tokens/sec=1375704.19, grad_norm=0.3352, duration=0.38s
Step 16356: loss=2.8314, lr=0.000260, tokens/sec=1377263.71, grad_norm=0.3404, duration=0.38s
Step 16357: loss=2.8631, lr=0.000260, tokens/sec=1377481.12, grad_norm=0.3255, duration=0.38s
Step 16358: loss=2.7996, lr=0.000260, tokens/sec=1376126.03, grad_norm=0.3140, duration=0.38s
Step 16359: loss=2.7549, lr=0.000260, tokens/sec=1374546.75, grad_norm=0.3273, duration=0.38s
Step 16360: loss=2.7905, lr=0.000260, tokens/sec=1373985.93, grad_norm=0.3288, duration=0.38s
Step 16361: loss=2.8504, lr=0.000260, tokens/sec=1374054.61, grad_norm=0.3155, duration=0.38s
Step 16362: loss=2.8709, lr=0.000259, tokens/sec=1373011.37, grad_norm=0.3204, duration=0.38s
Step 16363: loss=2.7505, lr=0.000259, tokens/sec=1373860.60, grad_norm=0.3315, duration=0.38s
Step 16364: loss=2.8256, lr=0.000259, tokens/sec=1375781.65, grad_norm=0.3224, duration=0.38s
Step 16365: loss=2.8532, lr=0.000259, tokens/sec=1376449.91, grad_norm=0.2950, duration=0.38s
Step 16366: loss=2.7895, lr=0.000259, tokens/sec=1375823.83, grad_norm=0.2964, duration=0.38s
Step 16367: loss=2.7917, lr=0.000259, tokens/sec=1377382.76, grad_norm=0.3115, duration=0.38s
Step 16368: loss=2.8416, lr=0.000259, tokens/sec=1375647.39, grad_norm=0.3173, duration=0.38s
Step 16369: loss=2.8060, lr=0.000259, tokens/sec=1376289.67, grad_norm=0.3022, duration=0.38s
Step 16370: loss=2.7852, lr=0.000259, tokens/sec=1377494.06, grad_norm=0.2898, duration=0.38s
Step 16371: loss=2.8349, lr=0.000259, tokens/sec=1374119.86, grad_norm=0.2904, duration=0.38s
Step 16372: loss=2.8369, lr=0.000259, tokens/sec=1376041.64, grad_norm=0.3124, duration=0.38s
Step 16373: loss=2.8596, lr=0.000259, tokens/sec=1379450.39, grad_norm=0.3250, duration=0.38s
Step 16374: loss=2.8339, lr=0.000259, tokens/sec=1378825.91, grad_norm=0.3125, duration=0.38s
Step 16375: loss=2.7365, lr=0.000259, tokens/sec=1376380.12, grad_norm=0.2865, duration=0.38s
Step 16376: loss=2.7851, lr=0.000259, tokens/sec=1378125.12, grad_norm=0.2879, duration=0.38s
Step 16377: loss=2.7479, lr=0.000259, tokens/sec=1378438.70, grad_norm=0.2948, duration=0.38s
Step 16378: loss=2.7378, lr=0.000259, tokens/sec=1376249.19, grad_norm=0.2977, duration=0.38s
Step 16379: loss=2.7338, lr=0.000259, tokens/sec=1377249.05, grad_norm=0.2957, duration=0.38s
Step 16380: loss=2.7662, lr=0.000259, tokens/sec=1377273.20, grad_norm=0.2990, duration=0.38s
Step 16381: loss=2.7814, lr=0.000259, tokens/sec=1377561.37, grad_norm=0.3081, duration=0.38s
Step 16382: loss=2.7575, lr=0.000259, tokens/sec=1377130.02, grad_norm=0.2839, duration=0.38s
Step 16383: loss=2.7666, lr=0.000259, tokens/sec=1374615.49, grad_norm=0.2777, duration=0.38s
Step 16384: loss=2.7566, lr=0.000259, tokens/sec=1378117.34, grad_norm=0.2858, duration=0.38s
Step 16385: loss=2.7497, lr=0.000259, tokens/sec=1375498.53, grad_norm=0.2950, duration=0.38s
Step 16386: loss=2.8262, lr=0.000259, tokens/sec=1376964.46, grad_norm=0.2863, duration=0.38s
Step 16387: loss=2.8779, lr=0.000259, tokens/sec=1376245.75, grad_norm=0.3065, duration=0.38s
Step 16388: loss=2.8764, lr=0.000259, tokens/sec=1376101.06, grad_norm=0.3009, duration=0.38s
Step 16389: loss=2.8393, lr=0.000259, tokens/sec=1373643.47, grad_norm=0.2973, duration=0.38s
Step 16390: loss=2.8011, lr=0.000259, tokens/sec=1374301.06, grad_norm=0.3003, duration=0.38s
Step 16391: loss=2.8682, lr=0.000259, tokens/sec=1376196.65, grad_norm=0.3150, duration=0.38s
Step 16392: loss=2.8735, lr=0.000259, tokens/sec=1375778.21, grad_norm=0.3140, duration=0.38s
Step 16393: loss=2.8603, lr=0.000259, tokens/sec=1377557.05, grad_norm=0.3090, duration=0.38s
Step 16394: loss=2.8592, lr=0.000259, tokens/sec=1374522.69, grad_norm=0.3025, duration=0.38s
Step 16395: loss=2.8334, lr=0.000259, tokens/sec=1372866.51, grad_norm=0.3114, duration=0.38s
Step 16396: loss=2.8284, lr=0.000259, tokens/sec=1373158.84, grad_norm=0.3059, duration=0.38s
Step 16397: loss=2.8446, lr=0.000259, tokens/sec=1376200.96, grad_norm=0.3021, duration=0.38s
Step 16398: loss=2.8097, lr=0.000259, tokens/sec=1375479.60, grad_norm=0.3010, duration=0.38s
Step 16399: loss=2.8022, lr=0.000259, tokens/sec=1373064.53, grad_norm=0.3003, duration=0.38s
Step 16400/19073 (86.0%), Elapsed time: 6491.40s, Steps per hour: 9095.11, Estimated hours remaining: 0.29
Step 16400: loss=2.8275, lr=0.000259, tokens/sec=1373852.87, grad_norm=0.3097, duration=0.38s
Step 16401: loss=2.7388, lr=0.000259, tokens/sec=1371796.84, grad_norm=0.3094, duration=0.38s
Step 16402: loss=2.7895, lr=0.000259, tokens/sec=1373274.61, grad_norm=0.3023, duration=0.38s
Step 16403: loss=2.8132, lr=0.000259, tokens/sec=1373979.06, grad_norm=0.3152, duration=0.38s
Step 16404: loss=2.8021, lr=0.000259, tokens/sec=1375667.19, grad_norm=0.3083, duration=0.38s
Step 16405: loss=2.8612, lr=0.000259, tokens/sec=1373857.17, grad_norm=0.3146, duration=0.38s
Step 16406: loss=2.8034, lr=0.000259, tokens/sec=1373019.09, grad_norm=0.3126, duration=0.38s
Step 16407: loss=2.9002, lr=0.000259, tokens/sec=1378953.01, grad_norm=0.3167, duration=0.38s
Step 16408: loss=2.8343, lr=0.000259, tokens/sec=1377527.72, grad_norm=0.3018, duration=0.38s
Step 16409: loss=2.8218, lr=0.000259, tokens/sec=1373043.09, grad_norm=0.2985, duration=0.38s
Step 16410: loss=2.8345, lr=0.000259, tokens/sec=1375600.92, grad_norm=0.3081, duration=0.38s
Step 16411: loss=2.8276, lr=0.000259, tokens/sec=1374857.85, grad_norm=0.3136, duration=0.38s
Step 16412: loss=2.8235, lr=0.000259, tokens/sec=1373046.52, grad_norm=0.2924, duration=0.38s
Step 16413: loss=2.8386, lr=0.000259, tokens/sec=1373936.99, grad_norm=0.2906, duration=0.38s
Step 16414: loss=2.8654, lr=0.000259, tokens/sec=1375064.18, grad_norm=0.2889, duration=0.38s
Step 16415: loss=2.8396, lr=0.000259, tokens/sec=1378975.49, grad_norm=0.3223, duration=0.38s
Step 16416: loss=2.8392, lr=0.000259, tokens/sec=1373942.14, grad_norm=0.3073, duration=0.38s
Step 16417: loss=2.7937, lr=0.000259, tokens/sec=1377100.70, grad_norm=0.2863, duration=0.38s
Step 16418: loss=2.8365, lr=0.000259, tokens/sec=1373941.29, grad_norm=0.2805, duration=0.38s
Step 16419: loss=2.8057, lr=0.000259, tokens/sec=1375208.64, grad_norm=0.2860, duration=0.38s
Step 16420: loss=2.8285, lr=0.000259, tokens/sec=1374844.09, grad_norm=0.3045, duration=0.38s
Step 16421: loss=2.7648, lr=0.000259, tokens/sec=1375122.65, grad_norm=0.3105, duration=0.38s
Step 16422: loss=2.8283, lr=0.000259, tokens/sec=1376175.98, grad_norm=0.2992, duration=0.38s
Step 16423: loss=2.7654, lr=0.000259, tokens/sec=1374918.02, grad_norm=0.2881, duration=0.38s
Step 16424: loss=2.7658, lr=0.000259, tokens/sec=1377260.26, grad_norm=0.2920, duration=0.38s
Step 16425: loss=2.7493, lr=0.000259, tokens/sec=1377910.96, grad_norm=0.3044, duration=0.38s
Step 16426: loss=2.7814, lr=0.000259, tokens/sec=1378471.53, grad_norm=0.3218, duration=0.38s
Step 16427: loss=2.7723, lr=0.000259, tokens/sec=1375750.67, grad_norm=0.2994, duration=0.38s
Step 16428: loss=2.7186, lr=0.000259, tokens/sec=1374972.18, grad_norm=0.2916, duration=0.38s
Step 16429: loss=2.7473, lr=0.000259, tokens/sec=1376183.73, grad_norm=0.2927, duration=0.38s
Step 16430: loss=2.7198, lr=0.000259, tokens/sec=1377919.59, grad_norm=0.2946, duration=0.38s
Step 16431: loss=2.7947, lr=0.000259, tokens/sec=1377348.25, grad_norm=0.3013, duration=0.38s
Step 16432: loss=2.7604, lr=0.000259, tokens/sec=1372311.34, grad_norm=0.3206, duration=0.38s
Step 16433: loss=2.7660, lr=0.000259, tokens/sec=1377978.31, grad_norm=0.2990, duration=0.38s
Step 16434: loss=2.8239, lr=0.000258, tokens/sec=1379695.33, grad_norm=0.2927, duration=0.38s
Step 16435: loss=2.8278, lr=0.000258, tokens/sec=1375901.31, grad_norm=0.2924, duration=0.38s
Step 16436: loss=2.8394, lr=0.000258, tokens/sec=1375774.77, grad_norm=0.3053, duration=0.38s
Step 16437: loss=2.8417, lr=0.000258, tokens/sec=1375934.88, grad_norm=0.2984, duration=0.38s
Step 16438: loss=2.7450, lr=0.000258, tokens/sec=1374515.82, grad_norm=0.3019, duration=0.38s
Step 16439: loss=2.8095, lr=0.000258, tokens/sec=1375066.76, grad_norm=0.2952, duration=0.38s
Step 16440: loss=2.8191, lr=0.000258, tokens/sec=1377118.81, grad_norm=0.3099, duration=0.38s
Step 16441: loss=2.8885, lr=0.000258, tokens/sec=1377745.21, grad_norm=0.3240, duration=0.38s
Step 16442: loss=2.8630, lr=0.000258, tokens/sec=1375791.98, grad_norm=0.3193, duration=0.38s
Step 16443: loss=2.8365, lr=0.000258, tokens/sec=1375684.40, grad_norm=0.3014, duration=0.38s
Step 16444: loss=2.8535, lr=0.000258, tokens/sec=1376479.20, grad_norm=0.3063, duration=0.38s
Step 16445: loss=2.8056, lr=0.000258, tokens/sec=1376636.89, grad_norm=0.3052, duration=0.38s
Step 16446: loss=2.8409, lr=0.000258, tokens/sec=1372274.52, grad_norm=0.3202, duration=0.38s
Step 16447: loss=2.8077, lr=0.000258, tokens/sec=1377071.38, grad_norm=0.3112, duration=0.38s
Step 16448: loss=2.8331, lr=0.000258, tokens/sec=1378282.32, grad_norm=0.3125, duration=0.38s
Step 16449: loss=2.7781, lr=0.000258, tokens/sec=1377715.86, grad_norm=0.2995, duration=0.38s
Step 16450: loss=2.8577, lr=0.000258, tokens/sec=1376312.07, grad_norm=0.3000, duration=0.38s
Step 16451: loss=2.8536, lr=0.000258, tokens/sec=1379440.87, grad_norm=0.3231, duration=0.38s
Step 16452: loss=2.8479, lr=0.000258, tokens/sec=1381512.09, grad_norm=0.3105, duration=0.38s
Step 16453: loss=2.7858, lr=0.000258, tokens/sec=1377601.93, grad_norm=0.3151, duration=0.38s
Step 16454: loss=2.8785, lr=0.000258, tokens/sec=1375639.65, grad_norm=0.3147, duration=0.38s
Step 16455: loss=2.7608, lr=0.000258, tokens/sec=1377644.22, grad_norm=0.3124, duration=0.38s
Step 16456: loss=2.8594, lr=0.000258, tokens/sec=1374247.82, grad_norm=0.3120, duration=0.38s
Step 16457: loss=2.8330, lr=0.000258, tokens/sec=1376523.15, grad_norm=0.3012, duration=0.38s
Step 16458: loss=2.8710, lr=0.000258, tokens/sec=1377087.77, grad_norm=0.3101, duration=0.38s
Step 16459: loss=2.8077, lr=0.000258, tokens/sec=1376476.62, grad_norm=0.3094, duration=0.38s
Step 16460: loss=2.8404, lr=0.000258, tokens/sec=1375996.01, grad_norm=0.3166, duration=0.38s
Step 16461: loss=2.7735, lr=0.000258, tokens/sec=1373893.22, grad_norm=0.3339, duration=0.38s
Step 16462: loss=2.8466, lr=0.000258, tokens/sec=1379312.82, grad_norm=0.3181, duration=0.38s
Step 16463: loss=2.8241, lr=0.000258, tokens/sec=1377553.60, grad_norm=0.3208, duration=0.38s
Step 16464: loss=2.8252, lr=0.000258, tokens/sec=1374708.30, grad_norm=0.3140, duration=0.38s
Step 16465: loss=2.8402, lr=0.000258, tokens/sec=1377658.03, grad_norm=0.3310, duration=0.38s
Step 16466: loss=2.8125, lr=0.000258, tokens/sec=1379577.61, grad_norm=0.3181, duration=0.38s
Step 16467: loss=2.8098, lr=0.000258, tokens/sec=1374090.67, grad_norm=0.3115, duration=0.38s
Step 16468: loss=2.7999, lr=0.000258, tokens/sec=1377269.75, grad_norm=0.3066, duration=0.38s
Step 16469: loss=2.7573, lr=0.000258, tokens/sec=1375828.14, grad_norm=0.3024, duration=0.38s
Step 16470: loss=2.8153, lr=0.000258, tokens/sec=1377557.92, grad_norm=0.3145, duration=0.38s
Step 16471: loss=2.7005, lr=0.000258, tokens/sec=1373308.91, grad_norm=0.3152, duration=0.38s
Step 16472: loss=2.7432, lr=0.000258, tokens/sec=1371592.34, grad_norm=0.3170, duration=0.38s
Step 16473: loss=2.7549, lr=0.000258, tokens/sec=1374282.17, grad_norm=0.3050, duration=0.38s
Step 16474: loss=2.8297, lr=0.000258, tokens/sec=1378543.26, grad_norm=0.3057, duration=0.38s
Step 16475: loss=2.8122, lr=0.000258, tokens/sec=1371755.76, grad_norm=0.3190, duration=0.38s
Step 16476: loss=2.7623, lr=0.000258, tokens/sec=1373387.82, grad_norm=0.3246, duration=0.38s
Step 16477: loss=2.7593, lr=0.000258, tokens/sec=1375744.64, grad_norm=0.3132, duration=0.38s
Step 16478: loss=2.7585, lr=0.000258, tokens/sec=1373840.86, grad_norm=0.3058, duration=0.38s
Step 16479: loss=2.8146, lr=0.000258, tokens/sec=1373746.45, grad_norm=0.3239, duration=0.38s
Step 16480: loss=2.7987, lr=0.000258, tokens/sec=1373048.24, grad_norm=0.3316, duration=0.38s
Step 16481: loss=2.8476, lr=0.000258, tokens/sec=1376644.65, grad_norm=0.3238, duration=0.38s
Step 16482: loss=2.8434, lr=0.000258, tokens/sec=1370074.66, grad_norm=0.3198, duration=0.38s
Step 16483: loss=2.8063, lr=0.000258, tokens/sec=1377664.07, grad_norm=0.3350, duration=0.38s
Step 16484: loss=2.8695, lr=0.000258, tokens/sec=1375551.02, grad_norm=0.3322, duration=0.38s
Step 16485: loss=2.8085, lr=0.000258, tokens/sec=1374059.76, grad_norm=0.3299, duration=0.38s
Step 16486: loss=2.7688, lr=0.000258, tokens/sec=1376263.83, grad_norm=0.3238, duration=0.38s
Step 16487: loss=2.8623, lr=0.000258, tokens/sec=1375454.65, grad_norm=0.3204, duration=0.38s
Step 16488: loss=2.9204, lr=0.000258, tokens/sec=1373694.10, grad_norm=0.3310, duration=0.38s
Step 16489: loss=2.8941, lr=0.000258, tokens/sec=1374347.45, grad_norm=0.3354, duration=0.38s
Step 16490: loss=2.8724, lr=0.000258, tokens/sec=1373848.58, grad_norm=0.3447, duration=0.38s
Step 16491: loss=2.8286, lr=0.000258, tokens/sec=1371880.71, grad_norm=0.3468, duration=0.38s
Step 16492: loss=2.8353, lr=0.000258, tokens/sec=1370877.52, grad_norm=0.3391, duration=0.38s
Step 16493: loss=2.8121, lr=0.000258, tokens/sec=1373192.28, grad_norm=0.3267, duration=0.38s
Step 16494: loss=2.8147, lr=0.000258, tokens/sec=1375360.88, grad_norm=0.3596, duration=0.38s
Step 16495: loss=2.8954, lr=0.000258, tokens/sec=1375540.69, grad_norm=0.3577, duration=0.38s
Step 16496: loss=2.7919, lr=0.000258, tokens/sec=1372436.39, grad_norm=0.3471, duration=0.38s
Step 16497: loss=2.8731, lr=0.000258, tokens/sec=1376279.34, grad_norm=0.3243, duration=0.38s
Step 16498: loss=2.8859, lr=0.000258, tokens/sec=1375501.97, grad_norm=0.3431, duration=0.38s
Step 16499: loss=2.8451, lr=0.000258, tokens/sec=1376912.73, grad_norm=0.3702, duration=0.38s
Step 16500/19073 (86.5%), Elapsed time: 6529.60s, Steps per hour: 9097.03, Estimated hours remaining: 0.28
Validation loss at step 16500: 3.8381056785583496
Step 16500: loss=2.8087, lr=0.000258, tokens/sec=156546.94, grad_norm=0.3468, duration=3.35s
Step 16501: loss=2.8337, lr=0.000258, tokens/sec=1379624.35, grad_norm=0.3428, duration=0.38s
Step 16502: loss=2.8742, lr=0.000258, tokens/sec=1375862.57, grad_norm=0.3073, duration=0.38s
Step 16503: loss=2.8466, lr=0.000258, tokens/sec=1373389.54, grad_norm=0.3355, duration=0.38s
Step 16504: loss=2.7670, lr=0.000258, tokens/sec=1377961.04, grad_norm=0.3510, duration=0.38s
Step 16505: loss=2.8553, lr=0.000258, tokens/sec=1375893.56, grad_norm=0.3262, duration=0.38s
Step 16506: loss=2.8314, lr=0.000258, tokens/sec=1370347.86, grad_norm=0.2996, duration=0.38s
Step 16507: loss=2.8580, lr=0.000257, tokens/sec=1376193.21, grad_norm=0.3099, duration=0.38s
Step 16508: loss=2.9006, lr=0.000257, tokens/sec=1375231.86, grad_norm=0.3356, duration=0.38s
Step 16509: loss=2.7857, lr=0.000257, tokens/sec=1371582.93, grad_norm=0.3270, duration=0.38s
Step 16510: loss=2.8287, lr=0.000257, tokens/sec=1376334.47, grad_norm=0.3140, duration=0.38s
Step 16511: loss=2.8185, lr=0.000257, tokens/sec=1375108.89, grad_norm=0.3106, duration=0.38s
Step 16512: loss=2.7657, lr=0.000257, tokens/sec=1376391.32, grad_norm=0.3098, duration=0.38s
Step 16513: loss=2.8188, lr=0.000257, tokens/sec=1373811.68, grad_norm=0.3243, duration=0.38s
Step 16514: loss=2.7765, lr=0.000257, tokens/sec=1376427.51, grad_norm=0.3340, duration=0.38s
Step 16515: loss=2.7727, lr=0.000257, tokens/sec=1374038.30, grad_norm=0.3239, duration=0.38s
Step 16516: loss=2.7760, lr=0.000257, tokens/sec=1376804.97, grad_norm=0.3049, duration=0.38s
Step 16517: loss=2.8264, lr=0.000257, tokens/sec=1370116.48, grad_norm=0.3047, duration=0.38s
Step 16518: loss=2.7705, lr=0.000257, tokens/sec=1371380.21, grad_norm=0.3270, duration=0.38s
Step 16519: loss=2.7585, lr=0.000257, tokens/sec=1373434.14, grad_norm=0.3306, duration=0.38s
Step 16520: loss=2.7578, lr=0.000257, tokens/sec=1375532.09, grad_norm=0.3138, duration=0.38s
Step 16521: loss=2.7520, lr=0.000257, tokens/sec=1372857.08, grad_norm=0.3175, duration=0.38s
Step 16522: loss=2.7924, lr=0.000257, tokens/sec=1369973.94, grad_norm=0.3065, duration=0.38s
Step 16523: loss=2.7652, lr=0.000257, tokens/sec=1375830.72, grad_norm=0.3031, duration=0.38s
Step 16524: loss=2.7927, lr=0.000257, tokens/sec=1370100.26, grad_norm=0.3074, duration=0.38s
Step 16525: loss=2.8385, lr=0.000257, tokens/sec=1372902.51, grad_norm=0.3256, duration=0.38s
Step 16526: loss=2.8084, lr=0.000257, tokens/sec=1372277.94, grad_norm=0.3282, duration=0.38s
Step 16527: loss=2.8260, lr=0.000257, tokens/sec=1378169.16, grad_norm=0.3146, duration=0.38s
Step 16528: loss=2.7946, lr=0.000257, tokens/sec=1374070.06, grad_norm=0.2974, duration=0.38s
Step 16529: loss=2.8195, lr=0.000257, tokens/sec=1374544.17, grad_norm=0.3075, duration=0.38s
Step 16530: loss=2.8007, lr=0.000257, tokens/sec=1372115.25, grad_norm=0.3256, duration=0.38s
Step 16531: loss=2.8388, lr=0.000257, tokens/sec=1372782.52, grad_norm=0.3253, duration=0.38s
Step 16532: loss=2.7781, lr=0.000257, tokens/sec=1371268.19, grad_norm=0.3014, duration=0.38s
Step 16533: loss=2.9042, lr=0.000257, tokens/sec=1373884.63, grad_norm=0.3068, duration=0.38s
Step 16534: loss=2.8393, lr=0.000257, tokens/sec=1371603.47, grad_norm=0.3150, duration=0.38s
Step 16535: loss=2.8951, lr=0.000257, tokens/sec=1373067.96, grad_norm=0.3271, duration=0.38s
Step 16536: loss=2.9299, lr=0.000257, tokens/sec=1373826.27, grad_norm=0.3313, duration=0.38s
Step 16537: loss=2.8695, lr=0.000257, tokens/sec=1374029.71, grad_norm=0.3115, duration=0.38s
Step 16538: loss=2.8763, lr=0.000257, tokens/sec=1373256.60, grad_norm=0.3142, duration=0.38s
Step 16539: loss=2.8597, lr=0.000257, tokens/sec=1376776.52, grad_norm=0.3240, duration=0.38s
Step 16540: loss=2.8572, lr=0.000257, tokens/sec=1375046.12, grad_norm=0.3266, duration=0.38s
Step 16541: loss=2.8003, lr=0.000257, tokens/sec=1374823.46, grad_norm=0.3222, duration=0.38s
Step 16542: loss=2.8530, lr=0.000257, tokens/sec=1374133.60, grad_norm=0.3236, duration=0.38s
Step 16543: loss=2.7968, lr=0.000257, tokens/sec=1372911.08, grad_norm=0.3133, duration=0.38s
Step 16544: loss=2.8596, lr=0.000257, tokens/sec=1374046.02, grad_norm=0.3032, duration=0.38s
Step 16545: loss=2.8682, lr=0.000257, tokens/sec=1371516.21, grad_norm=0.3519, duration=0.38s
Step 16546: loss=2.7928, lr=0.000257, tokens/sec=1376143.26, grad_norm=0.3254, duration=0.38s
Step 16547: loss=2.8798, lr=0.000257, tokens/sec=1375982.23, grad_norm=0.3189, duration=0.38s
Step 16548: loss=2.7852, lr=0.000257, tokens/sec=1375891.84, grad_norm=0.3127, duration=0.38s
Step 16549: loss=2.7455, lr=0.000257, tokens/sec=1376396.49, grad_norm=0.4256, duration=0.38s
Step 16550: loss=2.8070, lr=0.000257, tokens/sec=1379985.38, grad_norm=0.3227, duration=0.38s
Step 16551: loss=2.7932, lr=0.000257, tokens/sec=1375255.95, grad_norm=0.2996, duration=0.38s
Step 16552: loss=2.8977, lr=0.000257, tokens/sec=1374142.19, grad_norm=0.3069, duration=0.38s
Step 16553: loss=2.7131, lr=0.000257, tokens/sec=1371900.39, grad_norm=0.3188, duration=0.38s
Step 16554: loss=2.8590, lr=0.000257, tokens/sec=1376197.51, grad_norm=0.3171, duration=0.38s
Step 16555: loss=2.8474, lr=0.000257, tokens/sec=1376405.97, grad_norm=0.3007, duration=0.38s
Step 16556: loss=2.7548, lr=0.000257, tokens/sec=1373121.11, grad_norm=0.2988, duration=0.38s
Step 16557: loss=2.8191, lr=0.000257, tokens/sec=1375951.24, grad_norm=0.2978, duration=0.38s
Step 16558: loss=2.8266, lr=0.000257, tokens/sec=1375307.55, grad_norm=0.3022, duration=0.38s
Step 16559: loss=2.8239, lr=0.000257, tokens/sec=1376795.48, grad_norm=0.3065, duration=0.38s
Step 16560: loss=2.7971, lr=0.000257, tokens/sec=1371019.40, grad_norm=0.2967, duration=0.38s
Step 16561: loss=2.8289, lr=0.000257, tokens/sec=1372857.08, grad_norm=0.2876, duration=0.38s
Step 16562: loss=2.8201, lr=0.000257, tokens/sec=1374157.65, grad_norm=0.3136, duration=0.38s
Step 16563: loss=2.8795, lr=0.000257, tokens/sec=1375134.69, grad_norm=0.3136, duration=0.38s
Step 16564: loss=2.7876, lr=0.000257, tokens/sec=1375273.15, grad_norm=0.3113, duration=0.38s
Step 16565: loss=2.7395, lr=0.000257, tokens/sec=1371938.05, grad_norm=0.2969, duration=0.38s
Step 16566: loss=2.7923, lr=0.000257, tokens/sec=1375639.65, grad_norm=0.2846, duration=0.38s
Step 16567: loss=2.7412, lr=0.000257, tokens/sec=1377343.94, grad_norm=0.2868, duration=0.38s
Step 16568: loss=2.7165, lr=0.000257, tokens/sec=1371356.27, grad_norm=0.3093, duration=0.38s
Step 16569: loss=2.7494, lr=0.000257, tokens/sec=1373174.28, grad_norm=0.2956, duration=0.38s
Step 16570: loss=2.7734, lr=0.000257, tokens/sec=1375446.91, grad_norm=0.3035, duration=0.38s
Step 16571: loss=2.7519, lr=0.000257, tokens/sec=1373364.66, grad_norm=0.3018, duration=0.38s
Step 16572: loss=2.7468, lr=0.000257, tokens/sec=1376456.80, grad_norm=0.2864, duration=0.38s
Step 16573: loss=2.7805, lr=0.000257, tokens/sec=1372618.00, grad_norm=0.2836, duration=0.38s
Step 16574: loss=2.7482, lr=0.000257, tokens/sec=1373772.20, grad_norm=0.2842, duration=0.38s
Step 16575: loss=2.8114, lr=0.000257, tokens/sec=1374124.16, grad_norm=0.3051, duration=0.38s
Step 16576: loss=2.8104, lr=0.000257, tokens/sec=1372683.12, grad_norm=0.2905, duration=0.38s
Step 16577: loss=2.8342, lr=0.000257, tokens/sec=1376599.84, grad_norm=0.3038, duration=0.38s
Step 16578: loss=2.8782, lr=0.000257, tokens/sec=1376630.86, grad_norm=0.3078, duration=0.38s
Step 16579: loss=2.8021, lr=0.000257, tokens/sec=1374204.88, grad_norm=0.3032, duration=0.38s
Step 16580: loss=2.8392, lr=0.000257, tokens/sec=1376308.63, grad_norm=0.3022, duration=0.38s
Step 16581: loss=2.8760, lr=0.000257, tokens/sec=1374448.81, grad_norm=0.3046, duration=0.38s
Step 16582: loss=2.8545, lr=0.000257, tokens/sec=1375795.43, grad_norm=0.3134, duration=0.38s
Step 16583: loss=2.8890, lr=0.000256, tokens/sec=1379428.76, grad_norm=0.3188, duration=0.38s
Step 16584: loss=2.8402, lr=0.000256, tokens/sec=1376179.43, grad_norm=0.3042, duration=0.38s
Step 16585: loss=2.7993, lr=0.000256, tokens/sec=1375518.32, grad_norm=0.3039, duration=0.38s
Step 16586: loss=2.8609, lr=0.000256, tokens/sec=1374670.48, grad_norm=0.3182, duration=0.38s
Step 16587: loss=2.8103, lr=0.000256, tokens/sec=1376466.28, grad_norm=0.3095, duration=0.38s
Step 16588: loss=2.8026, lr=0.000256, tokens/sec=1379599.25, grad_norm=0.3158, duration=0.38s
Step 16589: loss=2.8249, lr=0.000256, tokens/sec=1373363.80, grad_norm=0.3032, duration=0.38s
Step 16590: loss=2.8090, lr=0.000256, tokens/sec=1372772.24, grad_norm=0.3120, duration=0.38s
Step 16591: loss=2.7303, lr=0.000256, tokens/sec=1376797.21, grad_norm=0.3124, duration=0.38s
Step 16592: loss=2.7685, lr=0.000256, tokens/sec=1374965.30, grad_norm=0.3225, duration=0.38s
Step 16593: loss=2.8081, lr=0.000256, tokens/sec=1374907.70, grad_norm=0.3033, duration=0.38s
Step 16594: loss=2.8225, lr=0.000256, tokens/sec=1373828.84, grad_norm=0.3118, duration=0.38s
Step 16595: loss=2.8549, lr=0.000256, tokens/sec=1373037.09, grad_norm=0.3154, duration=0.38s
Step 16596: loss=2.8219, lr=0.000256, tokens/sec=1377765.06, grad_norm=0.3245, duration=0.38s
Step 16597: loss=2.8686, lr=0.000256, tokens/sec=1373864.03, grad_norm=0.3252, duration=0.38s
Step 16598: loss=2.8322, lr=0.000256, tokens/sec=1376623.11, grad_norm=0.3154, duration=0.38s
Step 16599: loss=2.8291, lr=0.000256, tokens/sec=1379108.67, grad_norm=0.3055, duration=0.38s
Step 16600/19073 (87.0%), Elapsed time: 6570.81s, Steps per hour: 9094.77, Estimated hours remaining: 0.27
Step 16600: loss=2.8220, lr=0.000256, tokens/sec=1373014.80, grad_norm=0.3214, duration=0.38s
Step 16601: loss=2.8479, lr=0.000256, tokens/sec=1377689.96, grad_norm=0.3241, duration=0.38s
Step 16602: loss=2.8166, lr=0.000256, tokens/sec=1375847.07, grad_norm=0.3085, duration=0.38s
Step 16603: loss=2.8612, lr=0.000256, tokens/sec=1374502.07, grad_norm=0.2979, duration=0.38s
Step 16604: loss=2.8415, lr=0.000256, tokens/sec=1373501.91, grad_norm=0.2869, duration=0.38s
Step 16605: loss=2.8314, lr=0.000256, tokens/sec=1378068.12, grad_norm=0.3001, duration=0.38s
Step 16606: loss=2.8360, lr=0.000256, tokens/sec=1373755.89, grad_norm=0.3228, duration=0.38s
Step 16607: loss=2.7973, lr=0.000256, tokens/sec=1374333.70, grad_norm=0.3064, duration=0.38s
Step 16608: loss=2.8183, lr=0.000256, tokens/sec=1373609.15, grad_norm=0.2916, duration=0.38s
Step 16609: loss=2.8131, lr=0.000256, tokens/sec=1377198.16, grad_norm=0.2872, duration=0.38s
Step 16610: loss=2.8322, lr=0.000256, tokens/sec=1375685.26, grad_norm=0.3042, duration=0.38s
Step 16611: loss=2.7624, lr=0.000256, tokens/sec=1376648.10, grad_norm=0.3175, duration=0.38s
Step 16612: loss=2.8130, lr=0.000256, tokens/sec=1372712.25, grad_norm=0.3069, duration=0.38s
Step 16613: loss=2.7534, lr=0.000256, tokens/sec=1375254.23, grad_norm=0.2886, duration=0.38s
Step 16614: loss=2.7939, lr=0.000256, tokens/sec=1374677.36, grad_norm=0.2958, duration=0.38s
Step 16615: loss=2.7442, lr=0.000256, tokens/sec=1377040.34, grad_norm=0.2953, duration=0.38s
Step 16616: loss=2.7514, lr=0.000256, tokens/sec=1371213.46, grad_norm=0.3126, duration=0.38s
Step 16617: loss=2.7616, lr=0.000256, tokens/sec=1376405.97, grad_norm=0.3122, duration=0.38s
Step 16618: loss=2.7261, lr=0.000256, tokens/sec=1377956.72, grad_norm=0.2924, duration=0.38s
Step 16619: loss=2.7375, lr=0.000256, tokens/sec=1375571.67, grad_norm=0.2905, duration=0.38s
Step 16620: loss=2.7346, lr=0.000256, tokens/sec=1376557.61, grad_norm=0.3006, duration=0.38s
Step 16621: loss=2.7741, lr=0.000256, tokens/sec=1377608.83, grad_norm=0.3025, duration=0.38s
Step 16622: loss=2.7398, lr=0.000256, tokens/sec=1376111.39, grad_norm=0.3101, duration=0.38s
Step 16623: loss=2.7779, lr=0.000256, tokens/sec=1376687.74, grad_norm=0.3023, duration=0.38s
Step 16624: loss=2.8214, lr=0.000256, tokens/sec=1373114.25, grad_norm=0.3009, duration=0.38s
Step 16625: loss=2.8299, lr=0.000256, tokens/sec=1376983.43, grad_norm=0.2917, duration=0.38s
Step 16626: loss=2.8387, lr=0.000256, tokens/sec=1377142.96, grad_norm=0.2957, duration=0.38s
Step 16627: loss=2.8096, lr=0.000256, tokens/sec=1375882.37, grad_norm=0.3064, duration=0.38s
Step 16628: loss=2.7603, lr=0.000256, tokens/sec=1373493.33, grad_norm=0.3080, duration=0.38s
Step 16629: loss=2.8509, lr=0.000256, tokens/sec=1375605.23, grad_norm=0.2999, duration=0.38s
Step 16630: loss=2.8199, lr=0.000256, tokens/sec=1375355.72, grad_norm=0.3124, duration=0.38s
Step 16631: loss=2.8681, lr=0.000256, tokens/sec=1372259.10, grad_norm=0.3153, duration=0.38s
Step 16632: loss=2.8451, lr=0.000256, tokens/sec=1369226.69, grad_norm=0.3143, duration=0.38s
Step 16633: loss=2.8487, lr=0.000256, tokens/sec=1375118.35, grad_norm=0.3189, duration=0.38s
Step 16634: loss=2.8339, lr=0.000256, tokens/sec=1373900.94, grad_norm=0.3066, duration=0.38s
Step 16635: loss=2.8235, lr=0.000256, tokens/sec=1372962.51, grad_norm=0.3070, duration=0.38s
Step 16636: loss=2.7972, lr=0.000256, tokens/sec=1373744.73, grad_norm=0.3171, duration=0.38s
Step 16637: loss=2.8280, lr=0.000256, tokens/sec=1371580.37, grad_norm=0.3084, duration=0.38s
Step 16638: loss=2.8257, lr=0.000256, tokens/sec=1371505.09, grad_norm=0.3168, duration=0.38s
Step 16639: loss=2.8038, lr=0.000256, tokens/sec=1376912.73, grad_norm=0.3053, duration=0.38s
Step 16640: loss=2.8616, lr=0.000256, tokens/sec=1372507.48, grad_norm=0.3077, duration=0.38s
Step 16641: loss=2.8278, lr=0.000256, tokens/sec=1372152.93, grad_norm=0.3096, duration=0.38s
Step 16642: loss=2.8433, lr=0.000256, tokens/sec=1371879.85, grad_norm=0.3012, duration=0.38s
Step 16643: loss=2.8102, lr=0.000256, tokens/sec=1372349.02, grad_norm=0.3051, duration=0.38s
Step 16644: loss=2.8108, lr=0.000256, tokens/sec=1375438.31, grad_norm=0.3109, duration=0.38s
Step 16645: loss=2.8035, lr=0.000256, tokens/sec=1373348.36, grad_norm=0.3227, duration=0.38s
Step 16646: loss=2.8608, lr=0.000256, tokens/sec=1376674.81, grad_norm=0.3062, duration=0.38s
Step 16647: loss=2.8483, lr=0.000256, tokens/sec=1377542.38, grad_norm=0.3142, duration=0.38s
Step 16648: loss=2.8232, lr=0.000256, tokens/sec=1367949.91, grad_norm=0.3018, duration=0.38s
Step 16649: loss=2.8261, lr=0.000256, tokens/sec=1370756.17, grad_norm=0.3006, duration=0.38s
Step 16650: loss=2.7717, lr=0.000256, tokens/sec=1373349.22, grad_norm=0.3370, duration=0.38s
Step 16651: loss=2.8404, lr=0.000256, tokens/sec=1375145.86, grad_norm=0.3200, duration=0.38s
Step 16652: loss=2.8439, lr=0.000256, tokens/sec=1372742.24, grad_norm=0.3184, duration=0.38s
Step 16653: loss=2.8048, lr=0.000256, tokens/sec=1376893.76, grad_norm=0.3179, duration=0.38s
Step 16654: loss=2.8407, lr=0.000256, tokens/sec=1376598.11, grad_norm=0.3162, duration=0.38s
Step 16655: loss=2.8199, lr=0.000256, tokens/sec=1374015.12, grad_norm=0.3184, duration=0.38s
Step 16656: loss=2.8156, lr=0.000256, tokens/sec=1374576.82, grad_norm=0.3256, duration=0.38s
Step 16657: loss=2.7984, lr=0.000256, tokens/sec=1375808.34, grad_norm=0.3148, duration=0.38s
Step 16658: loss=2.8061, lr=0.000256, tokens/sec=1374710.87, grad_norm=0.3155, duration=0.38s
Step 16659: loss=2.7789, lr=0.000256, tokens/sec=1374045.16, grad_norm=0.3262, duration=0.38s
Step 16660: loss=2.7788, lr=0.000255, tokens/sec=1374335.42, grad_norm=0.3180, duration=0.38s
Step 16661: loss=2.6955, lr=0.000255, tokens/sec=1373045.67, grad_norm=0.3182, duration=0.38s
Step 16662: loss=2.7559, lr=0.000255, tokens/sec=1373181.99, grad_norm=0.3239, duration=0.38s
Step 16663: loss=2.7667, lr=0.000255, tokens/sec=1376337.05, grad_norm=0.3148, duration=0.38s
Step 16664: loss=2.8427, lr=0.000255, tokens/sec=1374155.93, grad_norm=0.3005, duration=0.38s
Step 16665: loss=2.7792, lr=0.000255, tokens/sec=1372069.02, grad_norm=0.3116, duration=0.38s
Step 16666: loss=2.7670, lr=0.000255, tokens/sec=1370194.17, grad_norm=0.3128, duration=0.38s
Step 16667: loss=2.7270, lr=0.000255, tokens/sec=1373917.25, grad_norm=0.3112, duration=0.38s
Step 16668: loss=2.7811, lr=0.000255, tokens/sec=1375482.18, grad_norm=0.3092, duration=0.38s
Step 16669: loss=2.7899, lr=0.000255, tokens/sec=1372892.22, grad_norm=0.3215, duration=0.38s
Step 16670: loss=2.8245, lr=0.000255, tokens/sec=1374533.00, grad_norm=0.3449, duration=0.38s
Step 16671: loss=2.8370, lr=0.000255, tokens/sec=1373906.09, grad_norm=0.3415, duration=0.38s
Step 16672: loss=2.8230, lr=0.000255, tokens/sec=1373469.31, grad_norm=0.3226, duration=0.38s
Step 16673: loss=2.8410, lr=0.000255, tokens/sec=1375648.25, grad_norm=0.3194, duration=0.38s
Step 16674: loss=2.8566, lr=0.000255, tokens/sec=1373921.54, grad_norm=0.3495, duration=0.38s
Step 16675: loss=2.7417, lr=0.000255, tokens/sec=1372664.27, grad_norm=0.3500, duration=0.38s
Step 16676: loss=2.8179, lr=0.000255, tokens/sec=1373895.79, grad_norm=0.3275, duration=0.38s
Step 16677: loss=2.8591, lr=0.000255, tokens/sec=1375058.16, grad_norm=0.3341, duration=0.38s
Step 16678: loss=2.9478, lr=0.000255, tokens/sec=1374092.39, grad_norm=0.3477, duration=0.38s
Step 16679: loss=2.8830, lr=0.000255, tokens/sec=1373767.90, grad_norm=0.3366, duration=0.38s
Step 16680: loss=2.8728, lr=0.000255, tokens/sec=1370606.66, grad_norm=0.3307, duration=0.38s
Step 16681: loss=2.8052, lr=0.000255, tokens/sec=1372306.20, grad_norm=0.3426, duration=0.38s
Step 16682: loss=2.8466, lr=0.000255, tokens/sec=1372617.14, grad_norm=0.3376, duration=0.38s
Step 16683: loss=2.8427, lr=0.000255, tokens/sec=1373887.21, grad_norm=0.3368, duration=0.38s
Step 16684: loss=2.8012, lr=0.000255, tokens/sec=1372624.00, grad_norm=0.3548, duration=0.38s
Step 16685: loss=2.8762, lr=0.000255, tokens/sec=1370159.17, grad_norm=0.3570, duration=0.38s
Step 16686: loss=2.7888, lr=0.000255, tokens/sec=1374511.52, grad_norm=0.3646, duration=0.38s
Step 16687: loss=2.8644, lr=0.000255, tokens/sec=1372208.58, grad_norm=0.3365, duration=0.38s
Step 16688: loss=2.8905, lr=0.000255, tokens/sec=1375360.02, grad_norm=0.3331, duration=0.38s
Step 16689: loss=2.8516, lr=0.000255, tokens/sec=1372349.88, grad_norm=0.3713, duration=0.38s
Step 16690: loss=2.7848, lr=0.000255, tokens/sec=1377344.80, grad_norm=0.3725, duration=0.38s
Step 16691: loss=2.8405, lr=0.000255, tokens/sec=1374772.75, grad_norm=0.3621, duration=0.38s
Step 16692: loss=2.8742, lr=0.000255, tokens/sec=1373400.69, grad_norm=0.3102, duration=0.38s
Step 16693: loss=2.8375, lr=0.000255, tokens/sec=1370740.79, grad_norm=0.3130, duration=0.38s
Step 16694: loss=2.7986, lr=0.000255, tokens/sec=1374402.42, grad_norm=0.3426, duration=0.38s
Step 16695: loss=2.8246, lr=0.000255, tokens/sec=1375347.12, grad_norm=0.3440, duration=0.38s
Step 16696: loss=2.8356, lr=0.000255, tokens/sec=1373225.73, grad_norm=0.3097, duration=0.38s
Step 16697: loss=2.8706, lr=0.000255, tokens/sec=1371194.65, grad_norm=0.3058, duration=0.38s
Step 16698: loss=2.8585, lr=0.000255, tokens/sec=1373987.64, grad_norm=0.3169, duration=0.38s
Step 16699: loss=2.8159, lr=0.000255, tokens/sec=1372801.38, grad_norm=0.3280, duration=0.38s
Step 16700/19073 (87.6%), Elapsed time: 6609.05s, Steps per hour: 9096.61, Estimated hours remaining: 0.26
Step 16700: loss=2.8151, lr=0.000255, tokens/sec=1378316.01, grad_norm=0.3218, duration=0.38s
Step 16701: loss=2.8043, lr=0.000255, tokens/sec=1374338.00, grad_norm=0.3269, duration=0.38s
Step 16702: loss=2.7798, lr=0.000255, tokens/sec=1376223.35, grad_norm=0.3125, duration=0.38s
Step 16703: loss=2.8052, lr=0.000255, tokens/sec=1376944.63, grad_norm=0.3110, duration=0.38s
Step 16704: loss=2.7891, lr=0.000255, tokens/sec=1373788.50, grad_norm=0.3225, duration=0.38s
Step 16705: loss=2.7701, lr=0.000255, tokens/sec=1373572.26, grad_norm=0.3309, duration=0.38s
Step 16706: loss=2.7657, lr=0.000255, tokens/sec=1370190.76, grad_norm=0.3154, duration=0.38s
Step 16707: loss=2.8139, lr=0.000255, tokens/sec=1372075.87, grad_norm=0.3131, duration=0.38s
Step 16708: loss=2.7758, lr=0.000255, tokens/sec=1377507.01, grad_norm=0.3193, duration=0.38s
Step 16709: loss=2.7793, lr=0.000255, tokens/sec=1374471.14, grad_norm=0.3119, duration=0.38s
Step 16710: loss=2.7231, lr=0.000255, tokens/sec=1374661.89, grad_norm=0.3156, duration=0.38s
Step 16711: loss=2.7544, lr=0.000255, tokens/sec=1370829.66, grad_norm=0.3278, duration=0.38s
Step 16712: loss=2.7821, lr=0.000255, tokens/sec=1370293.21, grad_norm=0.3085, duration=0.38s
Step 16713: loss=2.7706, lr=0.000255, tokens/sec=1372101.56, grad_norm=0.3026, duration=0.38s
Step 16714: loss=2.7931, lr=0.000255, tokens/sec=1375288.63, grad_norm=0.3109, duration=0.38s
Step 16715: loss=2.8089, lr=0.000255, tokens/sec=1373109.11, grad_norm=0.3136, duration=0.38s
Step 16716: loss=2.8195, lr=0.000255, tokens/sec=1370136.97, grad_norm=0.3214, duration=0.38s
Step 16717: loss=2.8203, lr=0.000255, tokens/sec=1373588.56, grad_norm=0.3308, duration=0.38s
Step 16718: loss=2.7890, lr=0.000255, tokens/sec=1375878.06, grad_norm=0.3108, duration=0.38s
Step 16719: loss=2.8243, lr=0.000255, tokens/sec=1376904.97, grad_norm=0.3049, duration=0.38s
Step 16720: loss=2.8045, lr=0.000255, tokens/sec=1378413.64, grad_norm=0.3244, duration=0.38s
Step 16721: loss=2.7693, lr=0.000255, tokens/sec=1374118.15, grad_norm=0.3313, duration=0.38s
Step 16722: loss=2.8339, lr=0.000255, tokens/sec=1373862.32, grad_norm=0.3215, duration=0.38s
Step 16723: loss=2.9012, lr=0.000255, tokens/sec=1375083.95, grad_norm=0.3023, duration=0.38s
Step 16724: loss=2.8735, lr=0.000255, tokens/sec=1375971.90, grad_norm=0.3213, duration=0.38s
Step 16725: loss=2.8868, lr=0.000255, tokens/sec=1373379.24, grad_norm=0.3441, duration=0.38s
Step 16726: loss=2.9278, lr=0.000255, tokens/sec=1377290.45, grad_norm=0.3495, duration=0.38s
Step 16727: loss=2.8608, lr=0.000255, tokens/sec=1372217.14, grad_norm=0.3188, duration=0.38s
Step 16728: loss=2.8755, lr=0.000255, tokens/sec=1372685.69, grad_norm=0.3199, duration=0.38s
Step 16729: loss=2.8391, lr=0.000255, tokens/sec=1377978.31, grad_norm=0.3145, duration=0.38s
Step 16730: loss=2.8276, lr=0.000255, tokens/sec=1373764.47, grad_norm=0.3182, duration=0.38s
Step 16731: loss=2.8434, lr=0.000255, tokens/sec=1375327.34, grad_norm=0.3213, duration=0.38s
Step 16732: loss=2.8515, lr=0.000255, tokens/sec=1376163.93, grad_norm=0.3359, duration=0.38s
Step 16733: loss=2.7869, lr=0.000255, tokens/sec=1376220.77, grad_norm=0.3186, duration=0.38s
Step 16734: loss=2.8858, lr=0.000255, tokens/sec=1373958.46, grad_norm=0.3115, duration=0.38s
Step 16735: loss=2.8353, lr=0.000255, tokens/sec=1373394.68, grad_norm=0.3428, duration=0.38s
Step 16736: loss=2.8119, lr=0.000255, tokens/sec=1376827.38, grad_norm=0.3333, duration=0.38s
Step 16737: loss=2.8669, lr=0.000255, tokens/sec=1376271.59, grad_norm=0.3277, duration=0.38s
Step 16738: loss=2.7737, lr=0.000255, tokens/sec=1376261.25, grad_norm=0.3279, duration=0.38s
Step 16739: loss=2.7614, lr=0.000255, tokens/sec=1377291.32, grad_norm=0.3938, duration=0.38s
Step 16740: loss=2.7512, lr=0.000255, tokens/sec=1374681.65, grad_norm=0.3306, duration=0.38s
Step 16741: loss=2.8232, lr=0.000254, tokens/sec=1374450.53, grad_norm=0.3232, duration=0.38s
Step 16742: loss=2.8621, lr=0.000254, tokens/sec=1375946.93, grad_norm=0.3119, duration=0.38s
Step 16743: loss=2.7469, lr=0.000254, tokens/sec=1378255.54, grad_norm=0.3126, duration=0.38s
Step 16744: loss=2.8543, lr=0.000254, tokens/sec=1373785.07, grad_norm=0.3181, duration=0.38s
Step 16745: loss=2.8117, lr=0.000254, tokens/sec=1379826.05, grad_norm=0.3436, duration=0.38s
Step 16746: loss=2.7760, lr=0.000254, tokens/sec=1372030.50, grad_norm=0.2947, duration=0.38s
Step 16747: loss=2.8035, lr=0.000254, tokens/sec=1371612.88, grad_norm=0.2893, duration=0.38s
Step 16748: loss=2.8437, lr=0.000254, tokens/sec=1374453.96, grad_norm=0.3151, duration=0.38s
Step 16749: loss=2.8364, lr=0.000254, tokens/sec=1373491.61, grad_norm=0.3286, duration=0.38s
Validation loss at step 16750: 3.8284006118774414
Step 16750: loss=2.7943, lr=0.000254, tokens/sec=155832.49, grad_norm=0.3197, duration=3.36s
Step 16751: loss=2.8126, lr=0.000254, tokens/sec=1379370.79, grad_norm=0.3008, duration=0.38s
Step 16752: loss=2.8446, lr=0.000254, tokens/sec=1375564.78, grad_norm=0.3137, duration=0.38s
Step 16753: loss=2.8389, lr=0.000254, tokens/sec=1373728.43, grad_norm=0.3185, duration=0.38s
Step 16754: loss=2.7914, lr=0.000254, tokens/sec=1372954.80, grad_norm=0.3241, duration=0.38s
Step 16755: loss=2.7493, lr=0.000254, tokens/sec=1379275.62, grad_norm=0.3145, duration=0.38s
Step 16756: loss=2.7862, lr=0.000254, tokens/sec=1373401.54, grad_norm=0.3024, duration=0.38s
Step 16757: loss=2.7199, lr=0.000254, tokens/sec=1375921.11, grad_norm=0.3022, duration=0.38s
Step 16758: loss=2.7339, lr=0.000254, tokens/sec=1379055.92, grad_norm=0.3095, duration=0.38s
Step 16759: loss=2.7572, lr=0.000254, tokens/sec=1372981.37, grad_norm=0.2997, duration=0.38s
Step 16760: loss=2.7458, lr=0.000254, tokens/sec=1377427.62, grad_norm=0.2993, duration=0.38s
Step 16761: loss=2.7433, lr=0.000254, tokens/sec=1372650.56, grad_norm=0.3244, duration=0.38s
Step 16762: loss=2.7600, lr=0.000254, tokens/sec=1375674.93, grad_norm=0.3162, duration=0.38s
Step 16763: loss=2.7704, lr=0.000254, tokens/sec=1372978.80, grad_norm=0.3047, duration=0.38s
Step 16764: loss=2.8102, lr=0.000254, tokens/sec=1377030.85, grad_norm=0.2938, duration=0.38s
Step 16765: loss=2.7964, lr=0.000254, tokens/sec=1375684.40, grad_norm=0.2939, duration=0.38s
Step 16766: loss=2.7694, lr=0.000254, tokens/sec=1375009.15, grad_norm=0.3112, duration=0.38s
Step 16767: loss=2.8382, lr=0.000254, tokens/sec=1372843.37, grad_norm=0.3204, duration=0.38s
Step 16768: loss=2.8430, lr=0.000254, tokens/sec=1372836.51, grad_norm=0.3128, duration=0.38s
Step 16769: loss=2.8432, lr=0.000254, tokens/sec=1375841.05, grad_norm=0.3005, duration=0.38s
Step 16770: loss=2.8491, lr=0.000254, tokens/sec=1367380.00, grad_norm=0.3128, duration=0.38s
Step 16771: loss=2.8597, lr=0.000254, tokens/sec=1375619.85, grad_norm=0.3216, duration=0.38s
Step 16772: loss=2.8846, lr=0.000254, tokens/sec=1375639.65, grad_norm=0.3135, duration=0.38s
Step 16773: loss=2.8706, lr=0.000254, tokens/sec=1375300.67, grad_norm=0.3117, duration=0.38s
Step 16774: loss=2.8076, lr=0.000254, tokens/sec=1373755.89, grad_norm=0.3059, duration=0.38s
Step 16775: loss=2.8338, lr=0.000254, tokens/sec=1368723.02, grad_norm=0.3257, duration=0.38s
Step 16776: loss=2.8248, lr=0.000254, tokens/sec=1372138.37, grad_norm=0.3185, duration=0.38s
Step 16777: loss=2.8016, lr=0.000254, tokens/sec=1370732.25, grad_norm=0.3033, duration=0.38s
Step 16778: loss=2.8222, lr=0.000254, tokens/sec=1372756.81, grad_norm=0.3151, duration=0.38s
Step 16779: loss=2.8091, lr=0.000254, tokens/sec=1371851.61, grad_norm=0.3022, duration=0.38s
Step 16780: loss=2.8025, lr=0.000254, tokens/sec=1373706.12, grad_norm=0.3000, duration=0.38s
Step 16781: loss=2.7082, lr=0.000254, tokens/sec=1372831.37, grad_norm=0.3259, duration=0.38s
Step 16782: loss=2.7663, lr=0.000254, tokens/sec=1375850.52, grad_norm=0.3258, duration=0.38s
Step 16783: loss=2.8301, lr=0.000254, tokens/sec=1375668.05, grad_norm=0.3107, duration=0.38s
Step 16784: loss=2.8192, lr=0.000254, tokens/sec=1377252.50, grad_norm=0.2938, duration=0.38s
Step 16785: loss=2.8738, lr=0.000254, tokens/sec=1382816.94, grad_norm=0.3106, duration=0.38s
Step 16786: loss=2.7916, lr=0.000254, tokens/sec=1374872.46, grad_norm=0.3259, duration=0.38s
Step 16787: loss=2.8641, lr=0.000254, tokens/sec=1371905.53, grad_norm=0.3405, duration=0.38s
Step 16788: loss=2.8415, lr=0.000254, tokens/sec=1380551.11, grad_norm=0.3233, duration=0.38s
Step 16789: loss=2.8168, lr=0.000254, tokens/sec=1374295.91, grad_norm=0.2969, duration=0.38s
Step 16790: loss=2.8397, lr=0.000254, tokens/sec=1377095.53, grad_norm=0.3006, duration=0.38s
Step 16791: loss=2.8402, lr=0.000254, tokens/sec=1374997.97, grad_norm=0.3214, duration=0.38s
Step 16792: loss=2.8397, lr=0.000254, tokens/sec=1378400.68, grad_norm=0.3189, duration=0.38s
Step 16793: loss=2.8385, lr=0.000254, tokens/sec=1377537.21, grad_norm=0.3151, duration=0.38s
Step 16794: loss=2.8333, lr=0.000254, tokens/sec=1376895.48, grad_norm=0.2948, duration=0.38s
Step 16795: loss=2.8261, lr=0.000254, tokens/sec=1376380.99, grad_norm=0.2999, duration=0.38s
Step 16796: loss=2.8369, lr=0.000254, tokens/sec=1376087.28, grad_norm=0.3089, duration=0.38s
Step 16797: loss=2.7783, lr=0.000254, tokens/sec=1372540.04, grad_norm=0.3149, duration=0.38s
Step 16798: loss=2.8228, lr=0.000254, tokens/sec=1370971.53, grad_norm=0.3124, duration=0.38s
Step 16799: loss=2.8169, lr=0.000254, tokens/sec=1374972.18, grad_norm=0.2999, duration=0.38s
Step 16800/19073 (88.1%), Elapsed time: 6650.27s, Steps per hour: 9094.37, Estimated hours remaining: 0.25
Step 16800: loss=2.8300, lr=0.000254, tokens/sec=1376999.81, grad_norm=0.3010, duration=0.38s
Step 16801: loss=2.7491, lr=0.000254, tokens/sec=1375812.64, grad_norm=0.3083, duration=0.38s
Step 16802: loss=2.8029, lr=0.000254, tokens/sec=1374093.25, grad_norm=0.3201, duration=0.38s
Step 16803: loss=2.7816, lr=0.000254, tokens/sec=1374631.81, grad_norm=0.3130, duration=0.38s
Step 16804: loss=2.7878, lr=0.000254, tokens/sec=1373096.25, grad_norm=0.2942, duration=0.38s
Step 16805: loss=2.7170, lr=0.000254, tokens/sec=1374003.95, grad_norm=0.2978, duration=0.38s
Step 16806: loss=2.7398, lr=0.000254, tokens/sec=1375470.14, grad_norm=0.3099, duration=0.38s
Step 16807: loss=2.7712, lr=0.000254, tokens/sec=1376269.86, grad_norm=0.3109, duration=0.38s
Step 16808: loss=2.7156, lr=0.000254, tokens/sec=1375733.45, grad_norm=0.3055, duration=0.38s
Step 16809: loss=2.7548, lr=0.000254, tokens/sec=1375807.48, grad_norm=0.2973, duration=0.38s
Step 16810: loss=2.7198, lr=0.000254, tokens/sec=1374052.89, grad_norm=0.3022, duration=0.38s
Step 16811: loss=2.7519, lr=0.000254, tokens/sec=1375911.64, grad_norm=0.3035, duration=0.38s
Step 16812: loss=2.7521, lr=0.000254, tokens/sec=1376642.93, grad_norm=0.2961, duration=0.38s
Step 16813: loss=2.7759, lr=0.000254, tokens/sec=1377651.99, grad_norm=0.3094, duration=0.38s
Step 16814: loss=2.8259, lr=0.000254, tokens/sec=1376829.97, grad_norm=0.3119, duration=0.38s
Step 16815: loss=2.8292, lr=0.000254, tokens/sec=1377043.79, grad_norm=0.2956, duration=0.38s
Step 16816: loss=2.8068, lr=0.000254, tokens/sec=1376301.73, grad_norm=0.2940, duration=0.38s
Step 16817: loss=2.8266, lr=0.000254, tokens/sec=1376845.48, grad_norm=0.3038, duration=0.38s
Step 16818: loss=2.7999, lr=0.000254, tokens/sec=1375894.42, grad_norm=0.3144, duration=0.38s
Step 16819: loss=2.8517, lr=0.000254, tokens/sec=1374754.70, grad_norm=0.2962, duration=0.38s
Step 16820: loss=2.7995, lr=0.000254, tokens/sec=1377134.34, grad_norm=0.2956, duration=0.38s
Step 16821: loss=2.8512, lr=0.000254, tokens/sec=1375630.18, grad_norm=0.2990, duration=0.38s
Step 16822: loss=2.8598, lr=0.000254, tokens/sec=1380911.75, grad_norm=0.3219, duration=0.38s
Step 16823: loss=2.8329, lr=0.000254, tokens/sec=1373391.25, grad_norm=0.3263, duration=0.38s
Step 16824: loss=2.8562, lr=0.000253, tokens/sec=1376906.69, grad_norm=0.2945, duration=0.38s
Step 16825: loss=2.7815, lr=0.000253, tokens/sec=1377946.36, grad_norm=0.2843, duration=0.38s
Step 16826: loss=2.8175, lr=0.000253, tokens/sec=1374243.52, grad_norm=0.3109, duration=0.38s
Step 16827: loss=2.8201, lr=0.000253, tokens/sec=1371743.78, grad_norm=0.3176, duration=0.38s
Step 16828: loss=2.8513, lr=0.000253, tokens/sec=1373580.84, grad_norm=0.3006, duration=0.38s
Step 16829: loss=2.8079, lr=0.000253, tokens/sec=1372643.70, grad_norm=0.2984, duration=0.38s
Step 16830: loss=2.8384, lr=0.000253, tokens/sec=1375245.63, grad_norm=0.2891, duration=0.38s
Step 16831: loss=2.8215, lr=0.000253, tokens/sec=1376019.26, grad_norm=0.3019, duration=0.38s
Step 16832: loss=2.8682, lr=0.000253, tokens/sec=1373411.84, grad_norm=0.3059, duration=0.38s
Step 16833: loss=2.7405, lr=0.000253, tokens/sec=1375773.05, grad_norm=0.3059, duration=0.38s
Step 16834: loss=2.8495, lr=0.000253, tokens/sec=1375773.05, grad_norm=0.3071, duration=0.38s
Step 16835: loss=2.8034, lr=0.000253, tokens/sec=1373743.88, grad_norm=0.3123, duration=0.38s
Step 16836: loss=2.8738, lr=0.000253, tokens/sec=1375153.60, grad_norm=0.3147, duration=0.38s
Step 16837: loss=2.7995, lr=0.000253, tokens/sec=1376425.78, grad_norm=0.3158, duration=0.38s
Step 16838: loss=2.8421, lr=0.000253, tokens/sec=1373767.05, grad_norm=0.3096, duration=0.38s
Step 16839: loss=2.7579, lr=0.000253, tokens/sec=1376664.47, grad_norm=0.3320, duration=0.38s
Step 16840: loss=2.8371, lr=0.000253, tokens/sec=1376027.01, grad_norm=0.2966, duration=0.38s
Step 16841: loss=2.8341, lr=0.000253, tokens/sec=1373771.34, grad_norm=0.3218, duration=0.38s
Step 16842: loss=2.8224, lr=0.000253, tokens/sec=1374955.84, grad_norm=0.3324, duration=0.38s
Step 16843: loss=2.8193, lr=0.000253, tokens/sec=1377368.09, grad_norm=0.3204, duration=0.38s
Step 16844: loss=2.8198, lr=0.000253, tokens/sec=1374351.74, grad_norm=0.3035, duration=0.38s
Step 16845: loss=2.8232, lr=0.000253, tokens/sec=1376337.91, grad_norm=0.3186, duration=0.38s
Step 16846: loss=2.8022, lr=0.000253, tokens/sec=1376153.59, grad_norm=0.3477, duration=0.38s
Step 16847: loss=2.8061, lr=0.000253, tokens/sec=1375156.18, grad_norm=0.3477, duration=0.38s
Step 16848: loss=2.8312, lr=0.000253, tokens/sec=1377236.97, grad_norm=0.3154, duration=0.38s
Step 16849: loss=2.7428, lr=0.000253, tokens/sec=1374956.70, grad_norm=0.3058, duration=0.38s
Step 16850: loss=2.7696, lr=0.000253, tokens/sec=1374234.07, grad_norm=0.3071, duration=0.38s
Step 16851: loss=2.7053, lr=0.000253, tokens/sec=1375082.23, grad_norm=0.3280, duration=0.38s
Step 16852: loss=2.7635, lr=0.000253, tokens/sec=1377056.72, grad_norm=0.3185, duration=0.38s
Step 16853: loss=2.7773, lr=0.000253, tokens/sec=1376840.31, grad_norm=0.3095, duration=0.38s
Step 16854: loss=2.8104, lr=0.000253, tokens/sec=1378683.27, grad_norm=0.3038, duration=0.38s
Step 16855: loss=2.7854, lr=0.000253, tokens/sec=1375377.23, grad_norm=0.2919, duration=0.38s
Step 16856: loss=2.7361, lr=0.000253, tokens/sec=1375483.04, grad_norm=0.3068, duration=0.38s
Step 16857: loss=2.7477, lr=0.000253, tokens/sec=1377149.86, grad_norm=0.3184, duration=0.38s
Step 16858: loss=2.7517, lr=0.000253, tokens/sec=1373156.27, grad_norm=0.3084, duration=0.38s
Step 16859: loss=2.8110, lr=0.000253, tokens/sec=1374266.71, grad_norm=0.3203, duration=0.38s
Step 16860: loss=2.8171, lr=0.000253, tokens/sec=1375809.20, grad_norm=0.3187, duration=0.38s
Step 16861: loss=2.8153, lr=0.000253, tokens/sec=1373017.38, grad_norm=0.3306, duration=0.38s
Step 16862: loss=2.8569, lr=0.000253, tokens/sec=1376368.06, grad_norm=0.3357, duration=0.38s
Step 16863: loss=2.8268, lr=0.000253, tokens/sec=1378703.15, grad_norm=0.3310, duration=0.38s
Step 16864: loss=2.7911, lr=0.000253, tokens/sec=1373936.14, grad_norm=0.3539, duration=0.38s
Step 16865: loss=2.7897, lr=0.000253, tokens/sec=1377673.56, grad_norm=0.3417, duration=0.38s
Step 16866: loss=2.8122, lr=0.000253, tokens/sec=1379038.62, grad_norm=0.3406, duration=0.38s
Step 16867: loss=2.8853, lr=0.000253, tokens/sec=1373840.00, grad_norm=0.3504, duration=0.38s
Step 16868: loss=2.9369, lr=0.000253, tokens/sec=1375969.32, grad_norm=0.3518, duration=0.38s
Step 16869: loss=2.8827, lr=0.000253, tokens/sec=1378595.98, grad_norm=0.3300, duration=0.38s
Step 16870: loss=2.8521, lr=0.000253, tokens/sec=1376710.15, grad_norm=0.3589, duration=0.38s
Step 16871: loss=2.8185, lr=0.000253, tokens/sec=1372624.85, grad_norm=0.3559, duration=0.38s
Step 16872: loss=2.8768, lr=0.000253, tokens/sec=1375693.86, grad_norm=0.3493, duration=0.38s
Step 16873: loss=2.8274, lr=0.000253, tokens/sec=1375937.46, grad_norm=0.3158, duration=0.38s
Step 16874: loss=2.7853, lr=0.000253, tokens/sec=1374462.55, grad_norm=0.3382, duration=0.38s
Step 16875: loss=2.8736, lr=0.000253, tokens/sec=1377150.72, grad_norm=0.3684, duration=0.38s
Step 16876: loss=2.7819, lr=0.000253, tokens/sec=1372911.08, grad_norm=0.3835, duration=0.38s
Step 16877: loss=2.8691, lr=0.000253, tokens/sec=1374921.46, grad_norm=0.3609, duration=0.38s
Step 16878: loss=2.8938, lr=0.000253, tokens/sec=1379353.48, grad_norm=0.3277, duration=0.38s
Step 16879: loss=2.8246, lr=0.000253, tokens/sec=1377189.53, grad_norm=0.3428, duration=0.38s
Step 16880: loss=2.7909, lr=0.000253, tokens/sec=1380095.37, grad_norm=0.3670, duration=0.38s
Step 16881: loss=2.8414, lr=0.000253, tokens/sec=1373253.17, grad_norm=0.3844, duration=0.38s
Step 16882: loss=2.8672, lr=0.000253, tokens/sec=1377108.46, grad_norm=0.3319, duration=0.38s
Step 16883: loss=2.8655, lr=0.000253, tokens/sec=1373596.28, grad_norm=0.3032, duration=0.38s
Step 16884: loss=2.7671, lr=0.000253, tokens/sec=1378270.23, grad_norm=0.3232, duration=0.38s
Step 16885: loss=2.8287, lr=0.000253, tokens/sec=1376654.13, grad_norm=0.3405, duration=0.38s
Step 16886: loss=2.8493, lr=0.000253, tokens/sec=1378561.41, grad_norm=0.3295, duration=0.38s
Step 16887: loss=2.8308, lr=0.000253, tokens/sec=1377111.05, grad_norm=0.3233, duration=0.38s
Step 16888: loss=2.8911, lr=0.000253, tokens/sec=1375317.87, grad_norm=0.3235, duration=0.38s
Step 16889: loss=2.8012, lr=0.000253, tokens/sec=1374097.54, grad_norm=0.3129, duration=0.38s
Step 16890: loss=2.7971, lr=0.000253, tokens/sec=1376227.66, grad_norm=0.3020, duration=0.38s
Step 16891: loss=2.8174, lr=0.000253, tokens/sec=1376671.37, grad_norm=0.3277, duration=0.38s
Step 16892: loss=2.7676, lr=0.000253, tokens/sec=1376874.79, grad_norm=0.3145, duration=0.38s
Step 16893: loss=2.8179, lr=0.000253, tokens/sec=1376293.98, grad_norm=0.3138, duration=0.38s
Step 16894: loss=2.7822, lr=0.000253, tokens/sec=1378054.30, grad_norm=0.3167, duration=0.38s
Step 16895: loss=2.7582, lr=0.000253, tokens/sec=1373465.88, grad_norm=0.3016, duration=0.38s
Step 16896: loss=2.7537, lr=0.000253, tokens/sec=1376706.70, grad_norm=0.2981, duration=0.38s
Step 16897: loss=2.8193, lr=0.000253, tokens/sec=1374432.49, grad_norm=0.3058, duration=0.38s
Step 16898: loss=2.7959, lr=0.000253, tokens/sec=1375089.97, grad_norm=0.3072, duration=0.38s
Step 16899: loss=2.7471, lr=0.000253, tokens/sec=1379150.19, grad_norm=0.3003, duration=0.38s
Step 16900/19073 (88.6%), Elapsed time: 6688.46s, Steps per hour: 9096.27, Estimated hours remaining: 0.24
Step 16900: loss=2.7251, lr=0.000253, tokens/sec=1373357.80, grad_norm=0.2974, duration=0.38s
Step 16901: loss=2.7435, lr=0.000253, tokens/sec=1372105.84, grad_norm=0.2989, duration=0.38s
Step 16902: loss=2.7869, lr=0.000253, tokens/sec=1375017.75, grad_norm=0.2946, duration=0.38s
Step 16903: loss=2.7720, lr=0.000253, tokens/sec=1375772.19, grad_norm=0.3046, duration=0.38s
Step 16904: loss=2.7610, lr=0.000253, tokens/sec=1375625.02, grad_norm=0.3010, duration=0.38s
Step 16905: loss=2.8198, lr=0.000253, tokens/sec=1376880.83, grad_norm=0.3079, duration=0.38s
Step 16906: loss=2.8141, lr=0.000253, tokens/sec=1373737.01, grad_norm=0.3045, duration=0.38s
Step 16907: loss=2.8121, lr=0.000253, tokens/sec=1375229.28, grad_norm=0.3138, duration=0.38s
Step 16908: loss=2.7941, lr=0.000253, tokens/sec=1374657.59, grad_norm=0.3214, duration=0.38s
Step 16909: loss=2.8286, lr=0.000252, tokens/sec=1373159.70, grad_norm=0.3047, duration=0.38s
Step 16910: loss=2.7321, lr=0.000252, tokens/sec=1377323.23, grad_norm=0.2971, duration=0.38s
Step 16911: loss=2.8263, lr=0.000252, tokens/sec=1376453.35, grad_norm=0.3178, duration=0.38s
Step 16912: loss=2.8300, lr=0.000252, tokens/sec=1376649.82, grad_norm=0.3281, duration=0.38s
Step 16913: loss=2.9339, lr=0.000252, tokens/sec=1372296.78, grad_norm=0.3132, duration=0.38s
Step 16914: loss=2.8631, lr=0.000252, tokens/sec=1374454.82, grad_norm=0.2963, duration=0.38s
Step 16915: loss=2.8809, lr=0.000252, tokens/sec=1378920.15, grad_norm=0.3084, duration=0.38s
Step 16916: loss=2.9177, lr=0.000252, tokens/sec=1376396.49, grad_norm=0.3452, duration=0.38s
Step 16917: loss=2.8606, lr=0.000252, tokens/sec=1377198.16, grad_norm=0.3364, duration=0.38s
Step 16918: loss=2.8561, lr=0.000252, tokens/sec=1375866.01, grad_norm=0.3079, duration=0.38s
Step 16919: loss=2.8117, lr=0.000252, tokens/sec=1378087.11, grad_norm=0.2996, duration=0.38s
Step 16920: loss=2.8714, lr=0.000252, tokens/sec=1377042.06, grad_norm=0.3080, duration=0.38s
Step 16921: loss=2.8403, lr=0.000252, tokens/sec=1376191.49, grad_norm=0.3201, duration=0.38s
Step 16922: loss=2.8414, lr=0.000252, tokens/sec=1376596.39, grad_norm=0.3185, duration=0.38s
Step 16923: loss=2.8103, lr=0.000252, tokens/sec=1375518.32, grad_norm=0.3288, duration=0.38s
Step 16924: loss=2.8489, lr=0.000252, tokens/sec=1378966.84, grad_norm=0.3110, duration=0.38s
Step 16925: loss=2.8530, lr=0.000252, tokens/sec=1380438.44, grad_norm=0.3442, duration=0.38s
Step 16926: loss=2.7982, lr=0.000252, tokens/sec=1373191.43, grad_norm=0.3128, duration=0.38s
Step 16927: loss=2.8555, lr=0.000252, tokens/sec=1382198.09, grad_norm=0.3191, duration=0.38s
Step 16928: loss=2.7883, lr=0.000252, tokens/sec=1374635.25, grad_norm=0.3207, duration=0.38s
Step 16929: loss=2.7020, lr=0.000252, tokens/sec=1375818.67, grad_norm=0.3302, duration=0.38s
Step 16930: loss=2.7776, lr=0.000252, tokens/sec=1378102.66, grad_norm=0.3122, duration=0.38s
Step 16931: loss=2.7851, lr=0.000252, tokens/sec=1378299.60, grad_norm=0.3134, duration=0.38s
Step 16932: loss=2.8955, lr=0.000252, tokens/sec=1373555.96, grad_norm=0.3299, duration=0.38s
Step 16933: loss=2.7394, lr=0.000252, tokens/sec=1378554.49, grad_norm=0.3091, duration=0.38s
Step 16934: loss=2.8158, lr=0.000252, tokens/sec=1375965.01, grad_norm=0.3191, duration=0.38s
Step 16935: loss=2.8356, lr=0.000252, tokens/sec=1371180.97, grad_norm=0.3221, duration=0.38s
Step 16936: loss=2.7642, lr=0.000252, tokens/sec=1374253.83, grad_norm=0.3157, duration=0.38s
Step 16937: loss=2.8193, lr=0.000252, tokens/sec=1375878.06, grad_norm=0.2882, duration=0.38s
Step 16938: loss=2.8543, lr=0.000252, tokens/sec=1376847.21, grad_norm=0.2839, duration=0.38s
Step 16939: loss=2.8322, lr=0.000252, tokens/sec=1377108.46, grad_norm=0.3111, duration=0.38s
Step 16940: loss=2.7759, lr=0.000252, tokens/sec=1372683.12, grad_norm=0.3279, duration=0.38s
Step 16941: loss=2.8373, lr=0.000252, tokens/sec=1373940.43, grad_norm=0.3129, duration=0.38s
Step 16942: loss=2.7951, lr=0.000252, tokens/sec=1374685.95, grad_norm=0.3096, duration=0.38s
Step 16943: loss=2.8373, lr=0.000252, tokens/sec=1374219.48, grad_norm=0.3091, duration=0.38s
Step 16944: loss=2.7995, lr=0.000252, tokens/sec=1375237.02, grad_norm=0.3107, duration=0.38s
Step 16945: loss=2.7434, lr=0.000252, tokens/sec=1376663.61, grad_norm=0.3171, duration=0.38s
Step 16946: loss=2.7645, lr=0.000252, tokens/sec=1376827.38, grad_norm=0.3169, duration=0.38s
Step 16947: loss=2.7345, lr=0.000252, tokens/sec=1375498.53, grad_norm=0.3132, duration=0.38s
Step 16948: loss=2.7405, lr=0.000252, tokens/sec=1371987.70, grad_norm=0.3237, duration=0.38s
Step 16949: loss=2.7313, lr=0.000252, tokens/sec=1373704.40, grad_norm=0.3049, duration=0.38s
Step 16950: loss=2.7350, lr=0.000252, tokens/sec=1372122.10, grad_norm=0.3028, duration=0.38s
Step 16951: loss=2.7560, lr=0.000252, tokens/sec=1372584.59, grad_norm=0.3137, duration=0.38s
Step 16952: loss=2.7510, lr=0.000252, tokens/sec=1375529.50, grad_norm=0.3100, duration=0.38s
Step 16953: loss=2.8329, lr=0.000252, tokens/sec=1371954.32, grad_norm=0.3055, duration=0.38s
Step 16954: loss=2.7964, lr=0.000252, tokens/sec=1375622.44, grad_norm=0.2998, duration=0.38s
Step 16955: loss=2.7539, lr=0.000252, tokens/sec=1372786.81, grad_norm=0.2933, duration=0.38s
Step 16956: loss=2.7724, lr=0.000252, tokens/sec=1374611.19, grad_norm=0.2968, duration=0.38s
Step 16957: loss=2.8010, lr=0.000252, tokens/sec=1374045.16, grad_norm=0.3031, duration=0.38s
Step 16958: loss=2.8835, lr=0.000252, tokens/sec=1376227.66, grad_norm=0.3136, duration=0.38s
Step 16959: loss=2.8549, lr=0.000252, tokens/sec=1375478.74, grad_norm=0.3023, duration=0.38s
Step 16960: loss=2.8330, lr=0.000252, tokens/sec=1373867.47, grad_norm=0.3150, duration=0.38s
Step 16961: loss=2.8875, lr=0.000252, tokens/sec=1372486.07, grad_norm=0.3121, duration=0.38s
Step 16962: loss=2.8644, lr=0.000252, tokens/sec=1374667.90, grad_norm=0.3197, duration=0.38s
Step 16963: loss=2.8365, lr=0.000252, tokens/sec=1377603.66, grad_norm=0.3155, duration=0.38s
Step 16964: loss=2.8379, lr=0.000252, tokens/sec=1376139.81, grad_norm=0.3139, duration=0.38s
Step 16965: loss=2.7956, lr=0.000252, tokens/sec=1372860.51, grad_norm=0.3131, duration=0.38s
Step 16966: loss=2.8178, lr=0.000252, tokens/sec=1375817.81, grad_norm=0.3009, duration=0.38s
Step 16967: loss=2.8257, lr=0.000252, tokens/sec=1376998.95, grad_norm=0.3120, duration=0.38s
Step 16968: loss=2.8057, lr=0.000252, tokens/sec=1377987.81, grad_norm=0.3106, duration=0.38s
Step 16969: loss=2.8022, lr=0.000252, tokens/sec=1376920.49, grad_norm=0.3008, duration=0.38s
Step 16970: loss=2.7819, lr=0.000252, tokens/sec=1375816.94, grad_norm=0.3022, duration=0.38s
Step 16971: loss=2.7054, lr=0.000252, tokens/sec=1375525.20, grad_norm=0.3047, duration=0.38s
Step 16972: loss=2.7853, lr=0.000252, tokens/sec=1380391.65, grad_norm=0.3170, duration=0.38s
Step 16973: loss=2.8245, lr=0.000252, tokens/sec=1373453.01, grad_norm=0.3170, duration=0.38s
Step 16974: loss=2.8352, lr=0.000252, tokens/sec=1377257.67, grad_norm=0.2899, duration=0.38s
Step 16975: loss=2.8432, lr=0.000252, tokens/sec=1378356.62, grad_norm=0.2913, duration=0.38s
Step 16976: loss=2.7853, lr=0.000252, tokens/sec=1377450.92, grad_norm=0.3068, duration=0.38s
Step 16977: loss=2.8727, lr=0.000252, tokens/sec=1373058.53, grad_norm=0.3327, duration=0.38s
Step 16978: loss=2.8260, lr=0.000252, tokens/sec=1372307.06, grad_norm=0.3203, duration=0.38s
Step 16979: loss=2.8339, lr=0.000252, tokens/sec=1376331.88, grad_norm=0.2861, duration=0.38s
Step 16980: loss=2.8285, lr=0.000252, tokens/sec=1378035.30, grad_norm=0.2970, duration=0.38s
Step 16981: loss=2.8591, lr=0.000252, tokens/sec=1375092.55, grad_norm=0.3167, duration=0.38s
Step 16982: loss=2.8137, lr=0.000252, tokens/sec=1376055.42, grad_norm=0.3092, duration=0.38s
Step 16983: loss=2.8303, lr=0.000252, tokens/sec=1380343.99, grad_norm=0.3059, duration=0.38s
Step 16984: loss=2.8305, lr=0.000252, tokens/sec=1375412.50, grad_norm=0.2910, duration=0.38s
Step 16985: loss=2.8292, lr=0.000252, tokens/sec=1374362.05, grad_norm=0.2958, duration=0.38s
Step 16986: loss=2.8188, lr=0.000252, tokens/sec=1373136.55, grad_norm=0.2914, duration=0.38s
Step 16987: loss=2.7819, lr=0.000252, tokens/sec=1372189.74, grad_norm=0.3026, duration=0.38s
Step 16988: loss=2.8257, lr=0.000252, tokens/sec=1373956.74, grad_norm=0.3006, duration=0.38s
Step 16989: loss=2.8159, lr=0.000252, tokens/sec=1372695.11, grad_norm=0.3088, duration=0.38s
Step 16990: loss=2.8154, lr=0.000252, tokens/sec=1373124.54, grad_norm=0.2930, duration=0.38s
Step 16991: loss=2.7391, lr=0.000252, tokens/sec=1374143.91, grad_norm=0.2986, duration=0.38s
Step 16992: loss=2.8311, lr=0.000252, tokens/sec=1372905.08, grad_norm=0.3048, duration=0.38s
Step 16993: loss=2.7749, lr=0.000252, tokens/sec=1374635.25, grad_norm=0.3114, duration=0.38s
Step 16994: loss=2.7612, lr=0.000252, tokens/sec=1372098.99, grad_norm=0.2999, duration=0.38s
Step 16995: loss=2.7064, lr=0.000252, tokens/sec=1376143.26, grad_norm=0.2994, duration=0.38s
Step 16996: loss=2.7480, lr=0.000252, tokens/sec=1372704.54, grad_norm=0.3039, duration=0.38s
Step 16997: loss=2.7593, lr=0.000252, tokens/sec=1376436.12, grad_norm=0.3030, duration=0.38s
Step 16998: loss=2.7328, lr=0.000252, tokens/sec=1378024.94, grad_norm=0.3156, duration=0.38s
Step 16999: loss=2.7337, lr=0.000251, tokens/sec=1374470.28, grad_norm=0.2951, duration=0.38s
Step 17000/19073 (89.1%), Elapsed time: 6726.67s, Steps per hour: 9098.11, Estimated hours remaining: 0.23
Validation loss at step 17000: 3.843285083770752
Step 17000: loss=2.6964, lr=0.000251, tokens/sec=155903.73, grad_norm=0.2952, duration=3.36s
Step 17001: loss=2.7682, lr=0.000251, tokens/sec=1378372.17, grad_norm=0.3014, duration=0.38s
Step 17002: loss=2.7510, lr=0.000251, tokens/sec=1378341.93, grad_norm=0.3006, duration=0.38s
Step 17003: loss=2.7761, lr=0.000251, tokens/sec=1378376.49, grad_norm=0.3081, duration=0.38s
Step 17004: loss=2.8242, lr=0.000251, tokens/sec=1371157.89, grad_norm=0.3085, duration=0.38s
Step 17005: loss=2.7981, lr=0.000251, tokens/sec=1377638.18, grad_norm=0.3003, duration=0.38s
Step 17006: loss=2.8231, lr=0.000251, tokens/sec=1378761.93, grad_norm=0.2916, duration=0.38s
Step 17007: loss=2.8663, lr=0.000251, tokens/sec=1374046.88, grad_norm=0.2966, duration=0.38s
Step 17008: loss=2.8033, lr=0.000251, tokens/sec=1378063.80, grad_norm=0.3035, duration=0.38s
Step 17009: loss=2.8309, lr=0.000251, tokens/sec=1376363.76, grad_norm=0.3053, duration=0.38s
Step 17010: loss=2.7859, lr=0.000251, tokens/sec=1373622.88, grad_norm=0.2972, duration=0.38s
Step 17011: loss=2.8657, lr=0.000251, tokens/sec=1371346.86, grad_norm=0.2949, duration=0.38s
Step 17012: loss=2.8415, lr=0.000251, tokens/sec=1372502.34, grad_norm=0.3060, duration=0.38s
Step 17013: loss=2.8519, lr=0.000251, tokens/sec=1371372.52, grad_norm=0.3207, duration=0.38s
Step 17014: loss=2.8140, lr=0.000251, tokens/sec=1368435.98, grad_norm=0.2980, duration=0.38s
Step 17015: loss=2.8035, lr=0.000251, tokens/sec=1370509.28, grad_norm=0.2944, duration=0.38s
Step 17016: loss=2.8126, lr=0.000251, tokens/sec=1371721.54, grad_norm=0.3038, duration=0.38s
Step 17017: loss=2.8467, lr=0.000251, tokens/sec=1372076.73, grad_norm=0.2977, duration=0.38s
Step 17018: loss=2.8554, lr=0.000251, tokens/sec=1371747.21, grad_norm=0.3070, duration=0.38s
Step 17019: loss=2.7850, lr=0.000251, tokens/sec=1375802.31, grad_norm=0.2980, duration=0.38s
Step 17020: loss=2.8316, lr=0.000251, tokens/sec=1370886.06, grad_norm=0.2868, duration=0.38s
Step 17021: loss=2.8476, lr=0.000251, tokens/sec=1372320.76, grad_norm=0.2927, duration=0.38s
Step 17022: loss=2.7982, lr=0.000251, tokens/sec=1372387.56, grad_norm=0.2970, duration=0.38s
Step 17023: loss=2.7824, lr=0.000251, tokens/sec=1371648.81, grad_norm=0.3105, duration=0.38s
Step 17024: loss=2.8496, lr=0.000251, tokens/sec=1374486.61, grad_norm=0.3171, duration=0.38s
Step 17025: loss=2.8168, lr=0.000251, tokens/sec=1376582.60, grad_norm=0.3183, duration=0.38s
Step 17026: loss=2.8261, lr=0.000251, tokens/sec=1376954.97, grad_norm=0.3103, duration=0.38s
Step 17027: loss=2.8168, lr=0.000251, tokens/sec=1372354.16, grad_norm=0.2930, duration=0.38s
Step 17028: loss=2.7761, lr=0.000251, tokens/sec=1372690.83, grad_norm=0.3334, duration=0.38s
Step 17029: loss=2.8246, lr=0.000251, tokens/sec=1371000.59, grad_norm=0.2996, duration=0.38s
Step 17030: loss=2.8328, lr=0.000251, tokens/sec=1372922.22, grad_norm=0.2998, duration=0.38s
Step 17031: loss=2.8153, lr=0.000251, tokens/sec=1371807.96, grad_norm=0.3098, duration=0.38s
Step 17032: loss=2.8343, lr=0.000251, tokens/sec=1375087.39, grad_norm=0.3310, duration=0.38s
Step 17033: loss=2.7972, lr=0.000251, tokens/sec=1372361.87, grad_norm=0.3094, duration=0.38s
Step 17034: loss=2.8196, lr=0.000251, tokens/sec=1375230.14, grad_norm=0.3002, duration=0.38s
Step 17035: loss=2.8095, lr=0.000251, tokens/sec=1380155.13, grad_norm=0.3052, duration=0.38s
Step 17036: loss=2.8113, lr=0.000251, tokens/sec=1374116.43, grad_norm=0.3380, duration=0.38s
Step 17037: loss=2.8317, lr=0.000251, tokens/sec=1374767.60, grad_norm=0.3401, duration=0.38s
Step 17038: loss=2.7928, lr=0.000251, tokens/sec=1375519.18, grad_norm=0.3025, duration=0.38s
Step 17039: loss=2.7338, lr=0.000251, tokens/sec=1372456.94, grad_norm=0.2897, duration=0.38s
Step 17040: loss=2.7803, lr=0.000251, tokens/sec=1375257.67, grad_norm=0.3047, duration=0.38s
Step 17041: loss=2.7139, lr=0.000251, tokens/sec=1375121.79, grad_norm=0.3214, duration=0.38s
Step 17042: loss=2.7748, lr=0.000251, tokens/sec=1377202.47, grad_norm=0.3219, duration=0.38s
Step 17043: loss=2.7465, lr=0.000251, tokens/sec=1376261.25, grad_norm=0.3156, duration=0.38s
Step 17044: loss=2.8171, lr=0.000251, tokens/sec=1379494.53, grad_norm=0.3052, duration=0.38s
Step 17045: loss=2.7516, lr=0.000251, tokens/sec=1376865.31, grad_norm=0.2972, duration=0.38s
Step 17046: loss=2.7578, lr=0.000251, tokens/sec=1374895.67, grad_norm=0.3003, duration=0.38s
Step 17047: loss=2.7179, lr=0.000251, tokens/sec=1374443.65, grad_norm=0.3149, duration=0.38s
Step 17048: loss=2.7764, lr=0.000251, tokens/sec=1376000.32, grad_norm=0.3156, duration=0.38s
Step 17049: loss=2.8064, lr=0.000251, tokens/sec=1375874.62, grad_norm=0.3146, duration=0.38s
Step 17050: loss=2.7926, lr=0.000251, tokens/sec=1373251.45, grad_norm=0.3162, duration=0.38s
Step 17051: loss=2.8489, lr=0.000251, tokens/sec=1376260.39, grad_norm=0.3336, duration=0.38s
Step 17052: loss=2.8425, lr=0.000251, tokens/sec=1378742.92, grad_norm=0.3213, duration=0.38s
Step 17053: loss=2.7583, lr=0.000251, tokens/sec=1377491.47, grad_norm=0.3264, duration=0.38s
Step 17054: loss=2.8362, lr=0.000251, tokens/sec=1375543.27, grad_norm=0.3296, duration=0.38s
Step 17055: loss=2.7842, lr=0.000251, tokens/sec=1377270.61, grad_norm=0.3357, duration=0.38s
Step 17056: loss=2.8376, lr=0.000251, tokens/sec=1373803.95, grad_norm=0.3505, duration=0.38s
Step 17057: loss=2.8727, lr=0.000251, tokens/sec=1375234.44, grad_norm=0.3410, duration=0.38s
Step 17058: loss=2.9328, lr=0.000251, tokens/sec=1374079.51, grad_norm=0.3363, duration=0.38s
Step 17059: loss=2.8596, lr=0.000251, tokens/sec=1375233.58, grad_norm=0.3217, duration=0.38s
Step 17060: loss=2.8624, lr=0.000251, tokens/sec=1376158.76, grad_norm=0.3476, duration=0.38s
Step 17061: loss=2.8478, lr=0.000251, tokens/sec=1376195.79, grad_norm=0.3540, duration=0.38s
Step 17062: loss=2.8656, lr=0.000251, tokens/sec=1374440.22, grad_norm=0.3470, duration=0.38s
Step 17063: loss=2.8100, lr=0.000251, tokens/sec=1377140.37, grad_norm=0.3091, duration=0.38s
Step 17064: loss=2.7812, lr=0.000251, tokens/sec=1370444.37, grad_norm=0.3290, duration=0.38s
Step 17065: loss=2.8634, lr=0.000251, tokens/sec=1375818.67, grad_norm=0.3389, duration=0.38s
Step 17066: loss=2.7842, lr=0.000251, tokens/sec=1375907.33, grad_norm=0.3500, duration=0.38s
Step 17067: loss=2.8731, lr=0.000251, tokens/sec=1374239.23, grad_norm=0.3456, duration=0.38s
Step 17068: loss=2.8683, lr=0.000251, tokens/sec=1373901.80, grad_norm=0.3313, duration=0.38s
Step 17069: loss=2.8307, lr=0.000251, tokens/sec=1374009.11, grad_norm=0.3219, duration=0.38s
Step 17070: loss=2.7890, lr=0.000251, tokens/sec=1374818.31, grad_norm=0.3427, duration=0.38s
Step 17071: loss=2.8317, lr=0.000251, tokens/sec=1369363.97, grad_norm=0.3901, duration=0.38s
Step 17072: loss=2.8967, lr=0.000251, tokens/sec=1373505.34, grad_norm=0.3361, duration=0.38s
Step 17073: loss=2.8361, lr=0.000251, tokens/sec=1376567.09, grad_norm=0.2927, duration=0.38s
Step 17074: loss=2.7704, lr=0.000251, tokens/sec=1371786.57, grad_norm=0.3044, duration=0.38s
Step 17075: loss=2.8423, lr=0.000251, tokens/sec=1372175.19, grad_norm=0.3301, duration=0.38s
Step 17076: loss=2.8069, lr=0.000251, tokens/sec=1374789.94, grad_norm=0.3243, duration=0.38s
Step 17077: loss=2.8586, lr=0.000251, tokens/sec=1372977.94, grad_norm=0.3168, duration=0.38s
Step 17078: loss=2.8751, lr=0.000251, tokens/sec=1373780.78, grad_norm=0.3096, duration=0.38s
Step 17079: loss=2.7867, lr=0.000251, tokens/sec=1371127.96, grad_norm=0.3043, duration=0.38s
Step 17080: loss=2.8141, lr=0.000251, tokens/sec=1370730.54, grad_norm=0.3035, duration=0.38s
Step 17081: loss=2.8048, lr=0.000251, tokens/sec=1375341.10, grad_norm=0.3071, duration=0.38s
Step 17082: loss=2.7793, lr=0.000251, tokens/sec=1372932.51, grad_norm=0.3025, duration=0.38s
Step 17083: loss=2.8137, lr=0.000251, tokens/sec=1376568.82, grad_norm=0.3082, duration=0.38s
Step 17084: loss=2.7708, lr=0.000251, tokens/sec=1374587.99, grad_norm=0.3044, duration=0.38s
Step 17085: loss=2.7446, lr=0.000251, tokens/sec=1370618.62, grad_norm=0.3036, duration=0.38s
Step 17086: loss=2.7548, lr=0.000251, tokens/sec=1375908.19, grad_norm=0.2989, duration=0.38s
Step 17087: loss=2.8396, lr=0.000251, tokens/sec=1372186.32, grad_norm=0.3025, duration=0.38s
Step 17088: loss=2.7616, lr=0.000251, tokens/sec=1374673.06, grad_norm=0.2939, duration=0.38s
Step 17089: loss=2.7493, lr=0.000251, tokens/sec=1374243.52, grad_norm=0.2947, duration=0.38s
Step 17090: loss=2.7167, lr=0.000251, tokens/sec=1376848.93, grad_norm=0.2953, duration=0.38s
Step 17091: loss=2.7499, lr=0.000251, tokens/sec=1371098.04, grad_norm=0.3061, duration=0.38s
Step 17092: loss=2.7918, lr=0.000250, tokens/sec=1372965.08, grad_norm=0.3001, duration=0.38s
Step 17093: loss=2.7425, lr=0.000250, tokens/sec=1373808.24, grad_norm=0.2929, duration=0.38s
Step 17094: loss=2.7718, lr=0.000250, tokens/sec=1375403.90, grad_norm=0.2980, duration=0.38s
Step 17095: loss=2.8111, lr=0.000250, tokens/sec=1374425.61, grad_norm=0.2955, duration=0.38s
Step 17096: loss=2.8048, lr=0.000250, tokens/sec=1371934.63, grad_norm=0.2955, duration=0.38s
Step 17097: loss=2.8199, lr=0.000250, tokens/sec=1369799.00, grad_norm=0.3152, duration=0.38s
Step 17098: loss=2.7966, lr=0.000250, tokens/sec=1377143.82, grad_norm=0.3161, duration=0.38s
Step 17099: loss=2.7562, lr=0.000250, tokens/sec=1370960.42, grad_norm=0.2984, duration=0.38s
Step 17100/19073 (89.7%), Elapsed time: 6767.90s, Steps per hour: 9095.88, Estimated hours remaining: 0.22
Step 17100: loss=2.7912, lr=0.000250, tokens/sec=1369206.23, grad_norm=0.2949, duration=0.38s
Step 17101: loss=2.8231, lr=0.000250, tokens/sec=1375134.69, grad_norm=0.3094, duration=0.38s
Step 17102: loss=2.8614, lr=0.000250, tokens/sec=1374224.63, grad_norm=0.3190, duration=0.38s
Step 17103: loss=2.9236, lr=0.000250, tokens/sec=1373169.99, grad_norm=0.3128, duration=0.38s
Step 17104: loss=2.8600, lr=0.000250, tokens/sec=1372438.96, grad_norm=0.3033, duration=0.38s
Step 17105: loss=2.8732, lr=0.000250, tokens/sec=1375277.45, grad_norm=0.3079, duration=0.38s
Step 17106: loss=2.9196, lr=0.000250, tokens/sec=1374798.54, grad_norm=0.3332, duration=0.38s
Step 17107: loss=2.8410, lr=0.000250, tokens/sec=1374925.76, grad_norm=0.3348, duration=0.38s
Step 17108: loss=2.8295, lr=0.000250, tokens/sec=1378680.68, grad_norm=0.3216, duration=0.38s
Step 17109: loss=2.8557, lr=0.000250, tokens/sec=1375601.78, grad_norm=0.3077, duration=0.38s
Step 17110: loss=2.8685, lr=0.000250, tokens/sec=1373652.06, grad_norm=0.3109, duration=0.38s
Step 17111: loss=2.8307, lr=0.000250, tokens/sec=1371748.92, grad_norm=0.3124, duration=0.38s
Step 17112: loss=2.8666, lr=0.000250, tokens/sec=1373361.23, grad_norm=0.3326, duration=0.38s
Step 17113: loss=2.7757, lr=0.000250, tokens/sec=1369768.28, grad_norm=0.3213, duration=0.38s
Step 17114: loss=2.8689, lr=0.000250, tokens/sec=1372786.81, grad_norm=0.3042, duration=0.38s
Step 17115: loss=2.8393, lr=0.000250, tokens/sec=1376355.14, grad_norm=0.3335, duration=0.38s
Step 17116: loss=2.7859, lr=0.000250, tokens/sec=1373792.79, grad_norm=0.3197, duration=0.38s
Step 17117: loss=2.8701, lr=0.000250, tokens/sec=1376306.04, grad_norm=0.3202, duration=0.38s
Step 17118: loss=2.7318, lr=0.000250, tokens/sec=1373795.37, grad_norm=0.3235, duration=0.38s
Step 17119: loss=2.7312, lr=0.000250, tokens/sec=1377794.41, grad_norm=0.3535, duration=0.38s
Step 17120: loss=2.7385, lr=0.000250, tokens/sec=1375562.20, grad_norm=0.3200, duration=0.38s
Step 17121: loss=2.8188, lr=0.000250, tokens/sec=1374691.11, grad_norm=0.3232, duration=0.38s
Step 17122: loss=2.8888, lr=0.000250, tokens/sec=1371146.77, grad_norm=0.3337, duration=0.38s
Step 17123: loss=2.7041, lr=0.000250, tokens/sec=1374830.34, grad_norm=0.3320, duration=0.38s
Step 17124: loss=2.8416, lr=0.000250, tokens/sec=1374943.81, grad_norm=0.3066, duration=0.38s
Step 17125: loss=2.8188, lr=0.000250, tokens/sec=1366462.35, grad_norm=0.3254, duration=0.38s
Step 17126: loss=2.7810, lr=0.000250, tokens/sec=1374355.18, grad_norm=0.3247, duration=0.38s
Step 17127: loss=2.8326, lr=0.000250, tokens/sec=1376546.41, grad_norm=0.3169, duration=0.38s
Step 17128: loss=2.8508, lr=0.000250, tokens/sec=1372307.92, grad_norm=0.2999, duration=0.38s
Step 17129: loss=2.8159, lr=0.000250, tokens/sec=1376182.01, grad_norm=0.3087, duration=0.38s
Step 17130: loss=2.8023, lr=0.000250, tokens/sec=1368092.04, grad_norm=0.3246, duration=0.38s
Step 17131: loss=2.7925, lr=0.000250, tokens/sec=1374143.91, grad_norm=0.3233, duration=0.38s
Step 17132: loss=2.8023, lr=0.000250, tokens/sec=1374384.38, grad_norm=0.3486, duration=0.38s
Step 17133: loss=2.8479, lr=0.000250, tokens/sec=1372003.11, grad_norm=0.3327, duration=0.38s
Step 17134: loss=2.7944, lr=0.000250, tokens/sec=1372091.28, grad_norm=0.3354, duration=0.38s
Step 17135: loss=2.7228, lr=0.000250, tokens/sec=1378287.50, grad_norm=0.3286, duration=0.38s
Step 17136: loss=2.7805, lr=0.000250, tokens/sec=1378664.26, grad_norm=0.3172, duration=0.38s
Step 17137: loss=2.7429, lr=0.000250, tokens/sec=1373688.95, grad_norm=0.3128, duration=0.38s
Step 17138: loss=2.7164, lr=0.000250, tokens/sec=1378507.83, grad_norm=0.3282, duration=0.38s
Step 17139: loss=2.7198, lr=0.000250, tokens/sec=1374253.83, grad_norm=0.3358, duration=0.38s
Step 17140: loss=2.7498, lr=0.000250, tokens/sec=1376003.76, grad_norm=0.3217, duration=0.38s
Step 17141: loss=2.7448, lr=0.000250, tokens/sec=1372869.94, grad_norm=0.3141, duration=0.38s
Step 17142: loss=2.8112, lr=0.000250, tokens/sec=1372276.23, grad_norm=0.3069, duration=0.38s
Step 17143: loss=2.8204, lr=0.000250, tokens/sec=1375924.55, grad_norm=0.3098, duration=0.38s
Step 17144: loss=2.7555, lr=0.000250, tokens/sec=1376704.12, grad_norm=0.2981, duration=0.38s
Step 17145: loss=2.7571, lr=0.000250, tokens/sec=1372349.88, grad_norm=0.3003, duration=0.38s
Step 17146: loss=2.7370, lr=0.000250, tokens/sec=1374937.79, grad_norm=0.3044, duration=0.38s
Step 17147: loss=2.8451, lr=0.000250, tokens/sec=1379381.17, grad_norm=0.3171, duration=0.38s
Step 17148: loss=2.8942, lr=0.000250, tokens/sec=1375446.91, grad_norm=0.3139, duration=0.38s
Step 17149: loss=2.8379, lr=0.000250, tokens/sec=1376038.20, grad_norm=0.3182, duration=0.38s
Step 17150: loss=2.8622, lr=0.000250, tokens/sec=1373424.70, grad_norm=0.3201, duration=0.38s
Step 17151: loss=2.8703, lr=0.000250, tokens/sec=1376281.06, grad_norm=0.3066, duration=0.38s
Step 17152: loss=2.8313, lr=0.000250, tokens/sec=1374798.54, grad_norm=0.3107, duration=0.38s
Step 17153: loss=2.8698, lr=0.000250, tokens/sec=1378292.69, grad_norm=0.3238, duration=0.38s
Step 17154: loss=2.8026, lr=0.000250, tokens/sec=1375175.96, grad_norm=0.3016, duration=0.38s
Step 17155: loss=2.7889, lr=0.000250, tokens/sec=1376573.99, grad_norm=0.3163, duration=0.38s
Step 17156: loss=2.8402, lr=0.000250, tokens/sec=1376753.25, grad_norm=0.2987, duration=0.38s
Step 17157: loss=2.8104, lr=0.000250, tokens/sec=1381150.27, grad_norm=0.3002, duration=0.38s
Step 17158: loss=2.8004, lr=0.000250, tokens/sec=1374005.67, grad_norm=0.3003, duration=0.38s
Step 17159: loss=2.7812, lr=0.000250, tokens/sec=1371381.92, grad_norm=0.3060, duration=0.38s
Step 17160: loss=2.7804, lr=0.000250, tokens/sec=1373614.30, grad_norm=0.3037, duration=0.38s
Step 17161: loss=2.7283, lr=0.000250, tokens/sec=1377287.00, grad_norm=0.3184, duration=0.38s
Step 17162: loss=2.7802, lr=0.000250, tokens/sec=1378076.75, grad_norm=0.3115, duration=0.38s
Step 17163: loss=2.8422, lr=0.000250, tokens/sec=1373500.19, grad_norm=0.3078, duration=0.38s
Step 17164: loss=2.8042, lr=0.000250, tokens/sec=1377790.96, grad_norm=0.2969, duration=0.38s
Step 17165: loss=2.8394, lr=0.000250, tokens/sec=1377743.48, grad_norm=0.2996, duration=0.38s
Step 17166: loss=2.7970, lr=0.000250, tokens/sec=1378670.31, grad_norm=0.2964, duration=0.38s
Step 17167: loss=2.8584, lr=0.000250, tokens/sec=1373858.88, grad_norm=0.3167, duration=0.38s
Step 17168: loss=2.8457, lr=0.000250, tokens/sec=1373826.27, grad_norm=0.3112, duration=0.38s
Step 17169: loss=2.8249, lr=0.000250, tokens/sec=1375307.55, grad_norm=0.3001, duration=0.38s
Step 17170: loss=2.8529, lr=0.000250, tokens/sec=1379372.52, grad_norm=0.2895, duration=0.38s
Step 17171: loss=2.8355, lr=0.000250, tokens/sec=1379181.33, grad_norm=0.3060, duration=0.38s
Step 17172: loss=2.8081, lr=0.000250, tokens/sec=1379598.38, grad_norm=0.3121, duration=0.38s
Step 17173: loss=2.8259, lr=0.000250, tokens/sec=1374759.00, grad_norm=0.3037, duration=0.38s
Step 17174: loss=2.8330, lr=0.000250, tokens/sec=1379192.57, grad_norm=0.2932, duration=0.38s
Step 17175: loss=2.8073, lr=0.000250, tokens/sec=1372971.08, grad_norm=0.2936, duration=0.38s
Step 17176: loss=2.8232, lr=0.000250, tokens/sec=1377728.80, grad_norm=0.3166, duration=0.38s
Step 17177: loss=2.7850, lr=0.000250, tokens/sec=1376650.68, grad_norm=0.2945, duration=0.38s
Step 17178: loss=2.8238, lr=0.000250, tokens/sec=1372803.95, grad_norm=0.2919, duration=0.38s
Step 17179: loss=2.8016, lr=0.000250, tokens/sec=1370138.68, grad_norm=0.3004, duration=0.38s
Step 17180: loss=2.8058, lr=0.000250, tokens/sec=1378183.85, grad_norm=0.2922, duration=0.38s
Step 17181: loss=2.7665, lr=0.000250, tokens/sec=1375183.70, grad_norm=0.3025, duration=0.38s
Step 17182: loss=2.8238, lr=0.000250, tokens/sec=1376860.14, grad_norm=0.2999, duration=0.38s
Step 17183: loss=2.7487, lr=0.000250, tokens/sec=1375663.74, grad_norm=0.2983, duration=0.38s
Step 17184: loss=2.7489, lr=0.000250, tokens/sec=1373594.57, grad_norm=0.3054, duration=0.38s
Step 17185: loss=2.7135, lr=0.000250, tokens/sec=1374462.55, grad_norm=0.3003, duration=0.38s
Step 17186: loss=2.7395, lr=0.000250, tokens/sec=1375340.24, grad_norm=0.2927, duration=0.38s
Step 17187: loss=2.7744, lr=0.000250, tokens/sec=1377631.27, grad_norm=0.2984, duration=0.38s
Step 17188: loss=2.7143, lr=0.000250, tokens/sec=1377343.07, grad_norm=0.3153, duration=0.38s
Step 17189: loss=2.7153, lr=0.000249, tokens/sec=1376530.90, grad_norm=0.3098, duration=0.38s
Step 17190: loss=2.7102, lr=0.000249, tokens/sec=1376486.10, grad_norm=0.2964, duration=0.38s
Step 17191: loss=2.7644, lr=0.000249, tokens/sec=1377036.03, grad_norm=0.2996, duration=0.38s
Step 17192: loss=2.7552, lr=0.000249, tokens/sec=1373971.33, grad_norm=0.3092, duration=0.38s
Step 17193: loss=2.7780, lr=0.000249, tokens/sec=1374111.28, grad_norm=0.3122, duration=0.38s
Step 17194: loss=2.7901, lr=0.000249, tokens/sec=1376649.82, grad_norm=0.3070, duration=0.38s
Step 17195: loss=2.8150, lr=0.000249, tokens/sec=1375795.43, grad_norm=0.3078, duration=0.38s
Step 17196: loss=2.8644, lr=0.000249, tokens/sec=1377591.57, grad_norm=0.3067, duration=0.38s
Step 17197: loss=2.8678, lr=0.000249, tokens/sec=1374658.45, grad_norm=0.3079, duration=0.38s
Step 17198: loss=2.7849, lr=0.000249, tokens/sec=1374901.69, grad_norm=0.3280, duration=0.38s
Step 17199: loss=2.8170, lr=0.000249, tokens/sec=1373859.74, grad_norm=0.3136, duration=0.38s
Step 17200/19073 (90.2%), Elapsed time: 6806.11s, Steps per hour: 9097.71, Estimated hours remaining: 0.21
Step 17200: loss=2.7988, lr=0.000249, tokens/sec=1375745.50, grad_norm=0.3086, duration=0.38s
Step 17201: loss=2.8481, lr=0.000249, tokens/sec=1374032.29, grad_norm=0.3143, duration=0.38s
Step 17202: loss=2.8626, lr=0.000249, tokens/sec=1373528.50, grad_norm=0.3078, duration=0.38s
Step 17203: loss=2.8102, lr=0.000249, tokens/sec=1377664.93, grad_norm=0.3141, duration=0.38s
Step 17204: loss=2.8330, lr=0.000249, tokens/sec=1372717.39, grad_norm=0.3288, duration=0.38s
Step 17205: loss=2.7965, lr=0.000249, tokens/sec=1376792.90, grad_norm=0.3119, duration=0.38s
Step 17206: loss=2.8363, lr=0.000249, tokens/sec=1373263.46, grad_norm=0.3037, duration=0.38s
Step 17207: loss=2.8518, lr=0.000249, tokens/sec=1374716.03, grad_norm=0.2991, duration=0.38s
Step 17208: loss=2.8322, lr=0.000249, tokens/sec=1371345.15, grad_norm=0.3158, duration=0.38s
Step 17209: loss=2.7807, lr=0.000249, tokens/sec=1374086.38, grad_norm=0.3217, duration=0.38s
Step 17210: loss=2.8579, lr=0.000249, tokens/sec=1376937.73, grad_norm=0.3010, duration=0.38s
Step 17211: loss=2.7802, lr=0.000249, tokens/sec=1377245.60, grad_norm=0.2946, duration=0.38s
Step 17212: loss=2.8395, lr=0.000249, tokens/sec=1373167.42, grad_norm=0.2945, duration=0.38s
Step 17213: loss=2.7827, lr=0.000249, tokens/sec=1377645.08, grad_norm=0.3134, duration=0.38s
Step 17214: loss=2.8649, lr=0.000249, tokens/sec=1375216.38, grad_norm=0.3180, duration=0.38s
Step 17215: loss=2.7704, lr=0.000249, tokens/sec=1377237.83, grad_norm=0.3280, duration=0.38s
Step 17216: loss=2.8453, lr=0.000249, tokens/sec=1373719.85, grad_norm=0.3177, duration=0.38s
Step 17217: loss=2.7506, lr=0.000249, tokens/sec=1379692.73, grad_norm=0.3326, duration=0.38s
Step 17218: loss=2.8407, lr=0.000249, tokens/sec=1377593.30, grad_norm=0.3101, duration=0.38s
Step 17219: loss=2.8210, lr=0.000249, tokens/sec=1377571.73, grad_norm=0.3074, duration=0.38s
Step 17220: loss=2.8112, lr=0.000249, tokens/sec=1375277.45, grad_norm=0.3079, duration=0.38s
Step 17221: loss=2.8288, lr=0.000249, tokens/sec=1376077.81, grad_norm=0.3187, duration=0.38s
Step 17222: loss=2.8142, lr=0.000249, tokens/sec=1373186.28, grad_norm=0.3093, duration=0.38s
Step 17223: loss=2.7974, lr=0.000249, tokens/sec=1378486.22, grad_norm=0.3173, duration=0.38s
Step 17224: loss=2.8102, lr=0.000249, tokens/sec=1376490.40, grad_norm=0.3074, duration=0.38s
Step 17225: loss=2.8187, lr=0.000249, tokens/sec=1377479.39, grad_norm=0.3094, duration=0.38s
Step 17226: loss=2.8388, lr=0.000249, tokens/sec=1375288.63, grad_norm=0.3204, duration=0.38s
Step 17227: loss=2.7927, lr=0.000249, tokens/sec=1374338.00, grad_norm=0.3324, duration=0.38s
Step 17228: loss=2.7875, lr=0.000249, tokens/sec=1374338.86, grad_norm=0.3262, duration=0.38s
Step 17229: loss=2.7459, lr=0.000249, tokens/sec=1376639.48, grad_norm=0.2995, duration=0.38s
Step 17230: loss=2.7906, lr=0.000249, tokens/sec=1377240.42, grad_norm=0.3025, duration=0.38s
Step 17231: loss=2.7254, lr=0.000249, tokens/sec=1376859.28, grad_norm=0.3094, duration=0.38s
Step 17232: loss=2.7428, lr=0.000249, tokens/sec=1377105.88, grad_norm=0.3104, duration=0.38s
Step 17233: loss=2.7525, lr=0.000249, tokens/sec=1378456.84, grad_norm=0.3154, duration=0.38s
Step 17234: loss=2.7849, lr=0.000249, tokens/sec=1376555.03, grad_norm=0.3121, duration=0.38s
Step 17235: loss=2.7711, lr=0.000249, tokens/sec=1372241.98, grad_norm=0.3007, duration=0.38s
Step 17236: loss=2.7272, lr=0.000249, tokens/sec=1378556.22, grad_norm=0.3015, duration=0.38s
Step 17237: loss=2.7434, lr=0.000249, tokens/sec=1378767.98, grad_norm=0.3129, duration=0.38s
Step 17238: loss=2.7683, lr=0.000249, tokens/sec=1375562.20, grad_norm=0.3120, duration=0.38s
Step 17239: loss=2.7821, lr=0.000249, tokens/sec=1378449.07, grad_norm=0.3191, duration=0.38s
Step 17240: loss=2.8288, lr=0.000249, tokens/sec=1377793.55, grad_norm=0.3370, duration=0.38s
Step 17241: loss=2.8372, lr=0.000249, tokens/sec=1376512.81, grad_norm=0.3167, duration=0.38s
Step 17242: loss=2.7738, lr=0.000249, tokens/sec=1373389.54, grad_norm=0.3374, duration=0.38s
Step 17243: loss=2.8072, lr=0.000249, tokens/sec=1377111.05, grad_norm=0.3382, duration=0.38s
Step 17244: loss=2.8347, lr=0.000249, tokens/sec=1375551.02, grad_norm=0.3404, duration=0.38s
Step 17245: loss=2.8127, lr=0.000249, tokens/sec=1373865.75, grad_norm=0.3319, duration=0.38s
Step 17246: loss=2.8249, lr=0.000249, tokens/sec=1373718.99, grad_norm=0.3311, duration=0.38s
Step 17247: loss=2.8724, lr=0.000249, tokens/sec=1375028.06, grad_norm=0.3632, duration=0.38s
Step 17248: loss=2.9107, lr=0.000249, tokens/sec=1377286.14, grad_norm=0.3619, duration=0.38s
Step 17249: loss=2.8715, lr=0.000249, tokens/sec=1376316.38, grad_norm=0.3286, duration=0.38s
Validation loss at step 17250: 3.840698003768921
Step 17250: loss=2.8928, lr=0.000249, tokens/sec=153402.30, grad_norm=0.3224, duration=3.42s
Step 17251: loss=2.8358, lr=0.000249, tokens/sec=1378640.92, grad_norm=0.3333, duration=0.38s
Step 17252: loss=2.8478, lr=0.000249, tokens/sec=1375888.39, grad_norm=0.3584, duration=0.38s
Step 17253: loss=2.8070, lr=0.000249, tokens/sec=1375891.84, grad_norm=0.3290, duration=0.38s
Step 17254: loss=2.7727, lr=0.000249, tokens/sec=1373651.20, grad_norm=0.3228, duration=0.38s
Step 17255: loss=2.8675, lr=0.000249, tokens/sec=1377255.09, grad_norm=0.3284, duration=0.38s
Step 17256: loss=2.7884, lr=0.000249, tokens/sec=1375109.75, grad_norm=0.3470, duration=0.38s
Step 17257: loss=2.8463, lr=0.000249, tokens/sec=1371224.58, grad_norm=0.3497, duration=0.38s
Step 17258: loss=2.8764, lr=0.000249, tokens/sec=1376609.32, grad_norm=0.3280, duration=0.38s
Step 17259: loss=2.8308, lr=0.000249, tokens/sec=1377312.88, grad_norm=0.3106, duration=0.38s
Step 17260: loss=2.7815, lr=0.000249, tokens/sec=1374570.81, grad_norm=0.3331, duration=0.38s
Step 17261: loss=2.8604, lr=0.000249, tokens/sec=1375665.46, grad_norm=0.3555, duration=0.38s
Step 17262: loss=2.8666, lr=0.000249, tokens/sec=1375204.34, grad_norm=0.3275, duration=0.38s
Step 17263: loss=2.8426, lr=0.000249, tokens/sec=1374064.91, grad_norm=0.3119, duration=0.38s
Step 17264: loss=2.7837, lr=0.000249, tokens/sec=1374921.46, grad_norm=0.2993, duration=0.38s
Step 17265: loss=2.7991, lr=0.000249, tokens/sec=1374642.98, grad_norm=0.3020, duration=0.38s
Step 17266: loss=2.8361, lr=0.000249, tokens/sec=1373863.17, grad_norm=0.3118, duration=0.38s
Step 17267: loss=2.8442, lr=0.000249, tokens/sec=1375217.24, grad_norm=0.3211, duration=0.38s
Step 17268: loss=2.8599, lr=0.000249, tokens/sec=1372978.80, grad_norm=0.3148, duration=0.38s
Step 17269: loss=2.7977, lr=0.000249, tokens/sec=1372859.65, grad_norm=0.3032, duration=0.38s
Step 17270: loss=2.7997, lr=0.000249, tokens/sec=1369657.37, grad_norm=0.3009, duration=0.38s
Step 17271: loss=2.8165, lr=0.000249, tokens/sec=1375718.82, grad_norm=0.3036, duration=0.38s
Step 17272: loss=2.7742, lr=0.000249, tokens/sec=1375662.02, grad_norm=0.3028, duration=0.38s
Step 17273: loss=2.8027, lr=0.000249, tokens/sec=1377708.09, grad_norm=0.3052, duration=0.38s
Step 17274: loss=2.7586, lr=0.000249, tokens/sec=1374252.11, grad_norm=0.3013, duration=0.38s
Step 17275: loss=2.7466, lr=0.000249, tokens/sec=1377628.68, grad_norm=0.3041, duration=0.38s
Step 17276: loss=2.7791, lr=0.000249, tokens/sec=1378421.42, grad_norm=0.3069, duration=0.38s
Step 17277: loss=2.8060, lr=0.000249, tokens/sec=1375149.30, grad_norm=0.3075, duration=0.38s
Step 17278: loss=2.7637, lr=0.000249, tokens/sec=1374630.95, grad_norm=0.2922, duration=0.38s
Step 17279: loss=2.7390, lr=0.000249, tokens/sec=1378100.93, grad_norm=0.2971, duration=0.38s
Step 17280: loss=2.7200, lr=0.000249, tokens/sec=1375502.83, grad_norm=0.2975, duration=0.38s
Step 17281: loss=2.7507, lr=0.000249, tokens/sec=1375044.40, grad_norm=0.3025, duration=0.38s
Step 17282: loss=2.7608, lr=0.000249, tokens/sec=1372778.24, grad_norm=0.2999, duration=0.38s
Step 17283: loss=2.7526, lr=0.000249, tokens/sec=1378766.26, grad_norm=0.2954, duration=0.38s
Step 17284: loss=2.7664, lr=0.000249, tokens/sec=1374916.30, grad_norm=0.2955, duration=0.38s
Step 17285: loss=2.8030, lr=0.000249, tokens/sec=1372873.37, grad_norm=0.2987, duration=0.38s
Step 17286: loss=2.8106, lr=0.000249, tokens/sec=1369867.26, grad_norm=0.3030, duration=0.38s
Step 17287: loss=2.8225, lr=0.000249, tokens/sec=1375168.22, grad_norm=0.3114, duration=0.38s
Step 17288: loss=2.7272, lr=0.000249, tokens/sec=1373956.74, grad_norm=0.3090, duration=0.38s
Step 17289: loss=2.8139, lr=0.000249, tokens/sec=1377428.49, grad_norm=0.3128, duration=0.38s
Step 17290: loss=2.7891, lr=0.000249, tokens/sec=1376324.13, grad_norm=0.3128, duration=0.38s
Step 17291: loss=2.8549, lr=0.000249, tokens/sec=1376099.34, grad_norm=0.3132, duration=0.38s
Step 17292: loss=2.8534, lr=0.000248, tokens/sec=1374477.16, grad_norm=0.3193, duration=0.38s
Step 17293: loss=2.9206, lr=0.000248, tokens/sec=1377789.23, grad_norm=0.3153, duration=0.38s
Step 17294: loss=2.8495, lr=0.000248, tokens/sec=1374657.59, grad_norm=0.2977, duration=0.38s
Step 17295: loss=2.8709, lr=0.000248, tokens/sec=1376036.48, grad_norm=0.3066, duration=0.38s
Step 17296: loss=2.9003, lr=0.000248, tokens/sec=1371991.12, grad_norm=0.3341, duration=0.38s
Step 17297: loss=2.8156, lr=0.000248, tokens/sec=1373985.93, grad_norm=0.3203, duration=0.38s
Step 17298: loss=2.8724, lr=0.000248, tokens/sec=1373624.60, grad_norm=0.3162, duration=0.38s
Step 17299: loss=2.8508, lr=0.000248, tokens/sec=1372206.01, grad_norm=0.3111, duration=0.38s
Step 17300/19073 (90.7%), Elapsed time: 6847.35s, Steps per hour: 9095.49, Estimated hours remaining: 0.19
Step 17300: loss=2.8598, lr=0.000248, tokens/sec=1377105.01, grad_norm=0.3101, duration=0.38s
Step 17301: loss=2.8563, lr=0.000248, tokens/sec=1375705.05, grad_norm=0.3135, duration=0.38s
Step 17302: loss=2.8320, lr=0.000248, tokens/sec=1375742.92, grad_norm=0.3297, duration=0.38s
Step 17303: loss=2.7944, lr=0.000248, tokens/sec=1375396.15, grad_norm=0.3136, duration=0.38s
Step 17304: loss=2.8548, lr=0.000248, tokens/sec=1373937.85, grad_norm=0.3194, duration=0.38s
Step 17305: loss=2.8275, lr=0.000248, tokens/sec=1376190.62, grad_norm=0.3603, duration=0.38s
Step 17306: loss=2.8044, lr=0.000248, tokens/sec=1374857.85, grad_norm=0.3151, duration=0.38s
Step 17307: loss=2.8150, lr=0.000248, tokens/sec=1378138.07, grad_norm=0.3101, duration=0.38s
Step 17308: loss=2.7606, lr=0.000248, tokens/sec=1375760.14, grad_norm=0.3088, duration=0.38s
Step 17309: loss=2.6961, lr=0.000248, tokens/sec=1375584.57, grad_norm=0.3707, duration=0.38s
Step 17310: loss=2.7771, lr=0.000248, tokens/sec=1375635.34, grad_norm=0.3223, duration=0.38s
Step 17311: loss=2.8130, lr=0.000248, tokens/sec=1374043.45, grad_norm=0.3121, duration=0.38s
Step 17312: loss=2.8531, lr=0.000248, tokens/sec=1374256.40, grad_norm=0.3112, duration=0.38s
Step 17313: loss=2.7297, lr=0.000248, tokens/sec=1375048.70, grad_norm=0.3188, duration=0.38s
Step 17314: loss=2.8248, lr=0.000248, tokens/sec=1373078.24, grad_norm=0.3194, duration=0.38s
Step 17315: loss=2.8359, lr=0.000248, tokens/sec=1372558.88, grad_norm=0.3211, duration=0.38s
Step 17316: loss=2.7908, lr=0.000248, tokens/sec=1376096.75, grad_norm=0.3065, duration=0.38s
Step 17317: loss=2.8277, lr=0.000248, tokens/sec=1369526.86, grad_norm=0.3120, duration=0.38s
Step 17318: loss=2.8363, lr=0.000248, tokens/sec=1370390.56, grad_norm=0.3000, duration=0.38s
Step 17319: loss=2.8404, lr=0.000248, tokens/sec=1375051.28, grad_norm=0.3065, duration=0.38s
Step 17320: loss=2.7548, lr=0.000248, tokens/sec=1373147.69, grad_norm=0.3111, duration=0.38s
Step 17321: loss=2.7951, lr=0.000248, tokens/sec=1374269.29, grad_norm=0.3152, duration=0.38s
Step 17322: loss=2.8139, lr=0.000248, tokens/sec=1374197.15, grad_norm=0.3201, duration=0.38s
Step 17323: loss=2.8436, lr=0.000248, tokens/sec=1375575.11, grad_norm=0.3162, duration=0.38s
Step 17324: loss=2.7706, lr=0.000248, tokens/sec=1372197.45, grad_norm=0.3268, duration=0.38s
Step 17325: loss=2.7387, lr=0.000248, tokens/sec=1375195.74, grad_norm=0.3249, duration=0.38s
Step 17326: loss=2.7878, lr=0.000248, tokens/sec=1374789.08, grad_norm=0.3085, duration=0.38s
Step 17327: loss=2.7148, lr=0.000248, tokens/sec=1371791.70, grad_norm=0.2933, duration=0.38s
Step 17328: loss=2.7012, lr=0.000248, tokens/sec=1374389.53, grad_norm=0.3146, duration=0.38s
Step 17329: loss=2.7349, lr=0.000248, tokens/sec=1374948.97, grad_norm=0.3256, duration=0.38s
Step 17330: loss=2.7404, lr=0.000248, tokens/sec=1373804.81, grad_norm=0.3216, duration=0.38s
Step 17331: loss=2.8078, lr=0.000248, tokens/sec=1374285.60, grad_norm=0.3166, duration=0.38s
Step 17332: loss=2.7983, lr=0.000248, tokens/sec=1374381.80, grad_norm=0.3218, duration=0.38s
Step 17333: loss=2.7771, lr=0.000248, tokens/sec=1371552.99, grad_norm=0.3180, duration=0.38s
Step 17334: loss=2.7578, lr=0.000248, tokens/sec=1374253.83, grad_norm=0.3062, duration=0.38s
Step 17335: loss=2.7226, lr=0.000248, tokens/sec=1373384.39, grad_norm=0.2971, duration=0.38s
Step 17336: loss=2.7762, lr=0.000248, tokens/sec=1379169.22, grad_norm=0.3086, duration=0.38s
Step 17337: loss=2.8528, lr=0.000248, tokens/sec=1376082.12, grad_norm=0.3302, duration=0.38s
Step 17338: loss=2.8791, lr=0.000248, tokens/sec=1372736.25, grad_norm=0.3356, duration=0.38s
Step 17339: loss=2.8679, lr=0.000248, tokens/sec=1374746.11, grad_norm=0.3238, duration=0.38s
Step 17340: loss=2.8435, lr=0.000248, tokens/sec=1374114.71, grad_norm=0.3174, duration=0.38s
Step 17341: loss=2.8360, lr=0.000248, tokens/sec=1374884.49, grad_norm=0.3177, duration=0.38s
Step 17342: loss=2.8646, lr=0.000248, tokens/sec=1374419.60, grad_norm=0.3245, duration=0.38s
Step 17343: loss=2.8334, lr=0.000248, tokens/sec=1371335.74, grad_norm=0.3079, duration=0.38s
Step 17344: loss=2.7969, lr=0.000248, tokens/sec=1375076.21, grad_norm=0.3040, duration=0.38s
Step 17345: loss=2.8118, lr=0.000248, tokens/sec=1374894.81, grad_norm=0.3217, duration=0.38s
Step 17346: loss=2.8250, lr=0.000248, tokens/sec=1370418.75, grad_norm=0.3191, duration=0.38s
Step 17347: loss=2.8045, lr=0.000248, tokens/sec=1373674.37, grad_norm=0.3160, duration=0.38s
Step 17348: loss=2.7777, lr=0.000248, tokens/sec=1374176.54, grad_norm=0.3012, duration=0.38s
Step 17349: loss=2.7790, lr=0.000248, tokens/sec=1374885.35, grad_norm=0.2950, duration=0.38s
Step 17350: loss=2.7998, lr=0.000248, tokens/sec=1371359.69, grad_norm=0.3078, duration=0.38s
Step 17351: loss=2.7196, lr=0.000248, tokens/sec=1370780.95, grad_norm=0.3178, duration=0.38s
Step 17352: loss=2.7965, lr=0.000248, tokens/sec=1373137.41, grad_norm=0.3208, duration=0.38s
Step 17353: loss=2.8141, lr=0.000248, tokens/sec=1372559.74, grad_norm=0.3151, duration=0.38s
Step 17354: loss=2.8028, lr=0.000248, tokens/sec=1372391.85, grad_norm=0.2993, duration=0.38s
Step 17355: loss=2.8479, lr=0.000248, tokens/sec=1370880.94, grad_norm=0.3093, duration=0.38s
Step 17356: loss=2.7815, lr=0.000248, tokens/sec=1378770.58, grad_norm=0.3022, duration=0.38s
Step 17357: loss=2.8764, lr=0.000248, tokens/sec=1375962.43, grad_norm=0.3102, duration=0.38s
Step 17358: loss=2.8337, lr=0.000248, tokens/sec=1375731.73, grad_norm=0.3040, duration=0.38s
Step 17359: loss=2.8487, lr=0.000248, tokens/sec=1373898.37, grad_norm=0.3012, duration=0.38s
Step 17360: loss=2.8287, lr=0.000248, tokens/sec=1373266.89, grad_norm=0.2961, duration=0.38s
Step 17361: loss=2.8315, lr=0.000248, tokens/sec=1374100.97, grad_norm=0.3053, duration=0.38s
Step 17362: loss=2.8048, lr=0.000248, tokens/sec=1375532.09, grad_norm=0.3005, duration=0.38s
Step 17363: loss=2.8290, lr=0.000248, tokens/sec=1379301.57, grad_norm=0.2992, duration=0.38s
Step 17364: loss=2.8142, lr=0.000248, tokens/sec=1374106.13, grad_norm=0.3009, duration=0.38s
Step 17365: loss=2.8144, lr=0.000248, tokens/sec=1377787.50, grad_norm=0.2967, duration=0.38s
Step 17366: loss=2.8284, lr=0.000248, tokens/sec=1376663.61, grad_norm=0.2939, duration=0.38s
Step 17367: loss=2.7837, lr=0.000248, tokens/sec=1376669.64, grad_norm=0.2964, duration=0.38s
Step 17368: loss=2.8124, lr=0.000248, tokens/sec=1372842.51, grad_norm=0.3009, duration=0.38s
Step 17369: loss=2.7923, lr=0.000248, tokens/sec=1373973.05, grad_norm=0.3035, duration=0.38s
Step 17370: loss=2.8313, lr=0.000248, tokens/sec=1374331.13, grad_norm=0.2999, duration=0.38s
Step 17371: loss=2.7611, lr=0.000248, tokens/sec=1374091.53, grad_norm=0.3057, duration=0.38s
Step 17372: loss=2.7990, lr=0.000248, tokens/sec=1376562.78, grad_norm=0.2974, duration=0.38s
Step 17373: loss=2.7382, lr=0.000248, tokens/sec=1370909.99, grad_norm=0.2966, duration=0.38s
Step 17374: loss=2.7581, lr=0.000248, tokens/sec=1373781.64, grad_norm=0.3026, duration=0.38s
Step 17375: loss=2.7045, lr=0.000248, tokens/sec=1372904.22, grad_norm=0.3040, duration=0.38s
Step 17376: loss=2.7547, lr=0.000248, tokens/sec=1371749.77, grad_norm=0.3088, duration=0.38s
Step 17377: loss=2.7567, lr=0.000248, tokens/sec=1370614.35, grad_norm=0.2941, duration=0.38s
Step 17378: loss=2.6912, lr=0.000248, tokens/sec=1370853.59, grad_norm=0.3029, duration=0.38s
Step 17379: loss=2.7265, lr=0.000248, tokens/sec=1373436.71, grad_norm=0.3075, duration=0.38s
Step 17380: loss=2.7094, lr=0.000248, tokens/sec=1371177.55, grad_norm=0.2971, duration=0.38s
Step 17381: loss=2.7677, lr=0.000248, tokens/sec=1373779.06, grad_norm=0.2976, duration=0.38s
Step 17382: loss=2.7523, lr=0.000248, tokens/sec=1369871.53, grad_norm=0.2980, duration=0.38s
Step 17383: loss=2.7445, lr=0.000248, tokens/sec=1372191.46, grad_norm=0.3058, duration=0.38s
Step 17384: loss=2.8086, lr=0.000248, tokens/sec=1375773.91, grad_norm=0.3099, duration=0.38s
Step 17385: loss=2.8545, lr=0.000248, tokens/sec=1376807.55, grad_norm=0.3098, duration=0.38s
Step 17386: loss=2.8643, lr=0.000248, tokens/sec=1373799.66, grad_norm=0.3105, duration=0.38s
Step 17387: loss=2.8492, lr=0.000248, tokens/sec=1375552.74, grad_norm=0.2982, duration=0.38s
Step 17388: loss=2.7690, lr=0.000248, tokens/sec=1375417.66, grad_norm=0.3083, duration=0.38s
Step 17389: loss=2.8310, lr=0.000248, tokens/sec=1374895.67, grad_norm=0.3200, duration=0.38s
Step 17390: loss=2.7827, lr=0.000248, tokens/sec=1373781.64, grad_norm=0.3252, duration=0.38s
Step 17391: loss=2.8667, lr=0.000248, tokens/sec=1373264.32, grad_norm=0.3128, duration=0.38s
Step 17392: loss=2.8182, lr=0.000248, tokens/sec=1378024.07, grad_norm=0.3098, duration=0.38s
Step 17393: loss=2.8306, lr=0.000248, tokens/sec=1374960.14, grad_norm=0.3061, duration=0.38s
Step 17394: loss=2.8273, lr=0.000248, tokens/sec=1375022.90, grad_norm=0.3215, duration=0.38s
Step 17395: loss=2.8228, lr=0.000248, tokens/sec=1377041.20, grad_norm=0.3248, duration=0.38s
Step 17396: loss=2.8419, lr=0.000248, tokens/sec=1375291.21, grad_norm=0.3094, duration=0.38s
Step 17397: loss=2.8272, lr=0.000248, tokens/sec=1376039.06, grad_norm=0.3024, duration=0.38s
Step 17398: loss=2.8261, lr=0.000248, tokens/sec=1375586.29, grad_norm=0.3156, duration=0.38s
Step 17399: loss=2.8056, lr=0.000248, tokens/sec=1376568.82, grad_norm=0.3206, duration=0.38s
Step 17400/19073 (91.2%), Elapsed time: 6885.60s, Steps per hour: 9097.25, Estimated hours remaining: 0.18
Step 17400: loss=2.7889, lr=0.000248, tokens/sec=1374175.68, grad_norm=0.3067, duration=0.38s
Step 17401: loss=2.8191, lr=0.000247, tokens/sec=1374820.03, grad_norm=0.3068, duration=0.38s
Step 17402: loss=2.8387, lr=0.000247, tokens/sec=1375319.59, grad_norm=0.3056, duration=0.38s
Step 17403: loss=2.7964, lr=0.000247, tokens/sec=1375633.62, grad_norm=0.3167, duration=0.38s
Step 17404: loss=2.8182, lr=0.000247, tokens/sec=1377066.21, grad_norm=0.3253, duration=0.38s
Step 17405: loss=2.7876, lr=0.000247, tokens/sec=1372854.51, grad_norm=0.3184, duration=0.38s
Step 17406: loss=2.7781, lr=0.000247, tokens/sec=1376099.34, grad_norm=0.3415, duration=0.38s
Step 17407: loss=2.8191, lr=0.000247, tokens/sec=1379659.84, grad_norm=0.3179, duration=0.38s
Step 17408: loss=2.8385, lr=0.000247, tokens/sec=1374700.56, grad_norm=0.3174, duration=0.38s
Step 17409: loss=2.7983, lr=0.000247, tokens/sec=1375711.08, grad_norm=0.2926, duration=0.38s
Step 17410: loss=2.8257, lr=0.000247, tokens/sec=1370892.05, grad_norm=0.3087, duration=0.38s
Step 17411: loss=2.8087, lr=0.000247, tokens/sec=1373407.55, grad_norm=0.3281, duration=0.38s
Step 17412: loss=2.8161, lr=0.000247, tokens/sec=1377069.66, grad_norm=0.3221, duration=0.38s
Step 17413: loss=2.7874, lr=0.000247, tokens/sec=1372740.53, grad_norm=0.3150, duration=0.38s
Step 17414: loss=2.8173, lr=0.000247, tokens/sec=1365109.35, grad_norm=0.2957, duration=0.38s
Step 17415: loss=2.8432, lr=0.000247, tokens/sec=1374164.52, grad_norm=0.3184, duration=0.38s
Step 17416: loss=2.7988, lr=0.000247, tokens/sec=1373361.23, grad_norm=0.3182, duration=0.38s
Step 17417: loss=2.7858, lr=0.000247, tokens/sec=1376628.28, grad_norm=0.3155, duration=0.38s
Step 17418: loss=2.7970, lr=0.000247, tokens/sec=1373573.98, grad_norm=0.3104, duration=0.38s
Step 17419: loss=2.7548, lr=0.000247, tokens/sec=1374697.98, grad_norm=0.3019, duration=0.38s
Step 17420: loss=2.8005, lr=0.000247, tokens/sec=1372859.65, grad_norm=0.2964, duration=0.38s
Step 17421: loss=2.6932, lr=0.000247, tokens/sec=1376481.79, grad_norm=0.3002, duration=0.38s
Step 17422: loss=2.7498, lr=0.000247, tokens/sec=1377612.29, grad_norm=0.3103, duration=0.38s
Step 17423: loss=2.7210, lr=0.000247, tokens/sec=1372235.98, grad_norm=0.3172, duration=0.38s
Step 17424: loss=2.8069, lr=0.000247, tokens/sec=1375599.20, grad_norm=0.3004, duration=0.38s
Step 17425: loss=2.7450, lr=0.000247, tokens/sec=1375601.78, grad_norm=0.2892, duration=0.38s
Step 17426: loss=2.7521, lr=0.000247, tokens/sec=1377018.78, grad_norm=0.3003, duration=0.38s
Step 17427: loss=2.7359, lr=0.000247, tokens/sec=1371805.40, grad_norm=0.2977, duration=0.38s
Step 17428: loss=2.7461, lr=0.000247, tokens/sec=1373553.38, grad_norm=0.3052, duration=0.38s
Step 17429: loss=2.8168, lr=0.000247, tokens/sec=1376601.56, grad_norm=0.3216, duration=0.38s
Step 17430: loss=2.8145, lr=0.000247, tokens/sec=1374156.79, grad_norm=0.3177, duration=0.38s
Step 17431: loss=2.7667, lr=0.000247, tokens/sec=1368160.13, grad_norm=0.3106, duration=0.38s
Step 17432: loss=2.8243, lr=0.000247, tokens/sec=1376383.57, grad_norm=0.3311, duration=0.38s
Step 17433: loss=2.8017, lr=0.000247, tokens/sec=1376250.05, grad_norm=0.3243, duration=0.38s
Step 17434: loss=2.8598, lr=0.000247, tokens/sec=1372122.96, grad_norm=0.3419, duration=0.38s
Step 17435: loss=2.7998, lr=0.000247, tokens/sec=1373469.31, grad_norm=0.3178, duration=0.38s
Step 17436: loss=2.8283, lr=0.000247, tokens/sec=1372675.41, grad_norm=0.3212, duration=0.38s
Step 17437: loss=2.8481, lr=0.000247, tokens/sec=1376429.23, grad_norm=0.3372, duration=0.38s
Step 17438: loss=2.9227, lr=0.000247, tokens/sec=1377963.63, grad_norm=0.3655, duration=0.38s
Step 17439: loss=2.9017, lr=0.000247, tokens/sec=1374798.54, grad_norm=0.3439, duration=0.38s
Step 17440: loss=2.8794, lr=0.000247, tokens/sec=1372243.69, grad_norm=0.3166, duration=0.38s
Step 17441: loss=2.8153, lr=0.000247, tokens/sec=1374808.85, grad_norm=0.3152, duration=0.38s
Step 17442: loss=2.8451, lr=0.000247, tokens/sec=1374417.88, grad_norm=0.3389, duration=0.38s
Step 17443: loss=2.8008, lr=0.000247, tokens/sec=1377744.34, grad_norm=0.3336, duration=0.38s
Step 17444: loss=2.7779, lr=0.000247, tokens/sec=1373674.37, grad_norm=0.3380, duration=0.38s
Step 17445: loss=2.8738, lr=0.000247, tokens/sec=1376051.98, grad_norm=0.3186, duration=0.38s
Step 17446: loss=2.7620, lr=0.000247, tokens/sec=1371251.94, grad_norm=0.3345, duration=0.38s
Step 17447: loss=2.8521, lr=0.000247, tokens/sec=1371850.75, grad_norm=0.3568, duration=0.38s
Step 17448: loss=2.8749, lr=0.000247, tokens/sec=1373825.41, grad_norm=0.3381, duration=0.38s
Step 17449: loss=2.8221, lr=0.000247, tokens/sec=1376309.49, grad_norm=0.3254, duration=0.38s
Step 17450: loss=2.8103, lr=0.000247, tokens/sec=1375032.36, grad_norm=0.3082, duration=0.38s
Step 17451: loss=2.8322, lr=0.000247, tokens/sec=1375609.53, grad_norm=0.3566, duration=0.38s
Step 17452: loss=2.8712, lr=0.000247, tokens/sec=1376937.73, grad_norm=0.3313, duration=0.38s
Step 17453: loss=2.8551, lr=0.000247, tokens/sec=1380427.18, grad_norm=0.3199, duration=0.38s
Step 17454: loss=2.7423, lr=0.000247, tokens/sec=1376354.28, grad_norm=0.2972, duration=0.38s
Step 17455: loss=2.8297, lr=0.000247, tokens/sec=1364457.14, grad_norm=0.3046, duration=0.38s
Step 17456: loss=2.8242, lr=0.000247, tokens/sec=1376556.75, grad_norm=0.3068, duration=0.38s
Step 17457: loss=2.8292, lr=0.000247, tokens/sec=1368218.02, grad_norm=0.3165, duration=0.38s
Step 17458: loss=2.8732, lr=0.000247, tokens/sec=1377814.26, grad_norm=0.3158, duration=0.38s
Step 17459: loss=2.7856, lr=0.000247, tokens/sec=1374003.95, grad_norm=0.3071, duration=0.38s
Step 17460: loss=2.8110, lr=0.000247, tokens/sec=1377840.16, grad_norm=0.2946, duration=0.38s
Step 17461: loss=2.8105, lr=0.000247, tokens/sec=1376151.01, grad_norm=0.3010, duration=0.38s
Step 17462: loss=2.7637, lr=0.000247, tokens/sec=1375551.02, grad_norm=0.3086, duration=0.38s
Step 17463: loss=2.7909, lr=0.000247, tokens/sec=1378164.85, grad_norm=0.3108, duration=0.38s
Step 17464: loss=2.7613, lr=0.000247, tokens/sec=1375835.88, grad_norm=0.3148, duration=0.38s
Step 17465: loss=2.7690, lr=0.000247, tokens/sec=1375831.58, grad_norm=0.3060, duration=0.38s
Step 17466: loss=2.7439, lr=0.000247, tokens/sec=1376587.77, grad_norm=0.2979, duration=0.38s
Step 17467: loss=2.8068, lr=0.000247, tokens/sec=1373824.55, grad_norm=0.3103, duration=0.38s
Step 17468: loss=2.7545, lr=0.000247, tokens/sec=1374143.91, grad_norm=0.3076, duration=0.38s
Step 17469: loss=2.7433, lr=0.000247, tokens/sec=1373997.95, grad_norm=0.3060, duration=0.38s
Step 17470: loss=2.7228, lr=0.000247, tokens/sec=1372968.51, grad_norm=0.2984, duration=0.38s
Step 17471: loss=2.7224, lr=0.000247, tokens/sec=1374273.58, grad_norm=0.3122, duration=0.38s
Step 17472: loss=2.7728, lr=0.000247, tokens/sec=1373053.38, grad_norm=0.3074, duration=0.38s
Step 17473: loss=2.7473, lr=0.000247, tokens/sec=1374496.92, grad_norm=0.2973, duration=0.38s
Step 17474: loss=2.7598, lr=0.000247, tokens/sec=1374125.87, grad_norm=0.2901, duration=0.38s
Step 17475: loss=2.8079, lr=0.000247, tokens/sec=1373704.40, grad_norm=0.2976, duration=0.38s
Step 17476: loss=2.8151, lr=0.000247, tokens/sec=1375645.67, grad_norm=0.2993, duration=0.38s
Step 17477: loss=2.7499, lr=0.000247, tokens/sec=1375264.55, grad_norm=0.3109, duration=0.38s
Step 17478: loss=2.7841, lr=0.000247, tokens/sec=1374323.40, grad_norm=0.3119, duration=0.38s
Step 17479: loss=2.8109, lr=0.000247, tokens/sec=1379306.76, grad_norm=0.3044, duration=0.38s
Step 17480: loss=2.8215, lr=0.000247, tokens/sec=1376310.35, grad_norm=0.2983, duration=0.38s
Step 17481: loss=2.8447, lr=0.000247, tokens/sec=1373615.16, grad_norm=0.3047, duration=0.38s
Step 17482: loss=2.8485, lr=0.000247, tokens/sec=1372753.38, grad_norm=0.3148, duration=0.38s
Step 17483: loss=2.9116, lr=0.000247, tokens/sec=1375847.07, grad_norm=0.3104, duration=0.38s
Step 17484: loss=2.8512, lr=0.000247, tokens/sec=1376314.65, grad_norm=0.2975, duration=0.38s
Step 17485: loss=2.8527, lr=0.000247, tokens/sec=1376776.52, grad_norm=0.3111, duration=0.38s
Step 17486: loss=2.8697, lr=0.000247, tokens/sec=1375967.60, grad_norm=0.3292, duration=0.38s
Step 17487: loss=2.8585, lr=0.000247, tokens/sec=1375463.26, grad_norm=0.3139, duration=0.38s
Step 17488: loss=2.8699, lr=0.000247, tokens/sec=1374369.78, grad_norm=0.3089, duration=0.38s
Step 17489: loss=2.8423, lr=0.000247, tokens/sec=1378024.07, grad_norm=0.3162, duration=0.38s
Step 17490: loss=2.8846, lr=0.000247, tokens/sec=1375314.43, grad_norm=0.3205, duration=0.38s
Step 17491: loss=2.8217, lr=0.000247, tokens/sec=1373527.65, grad_norm=0.3117, duration=0.38s
Step 17492: loss=2.8505, lr=0.000247, tokens/sec=1379071.48, grad_norm=0.3170, duration=0.38s
Step 17493: loss=2.7832, lr=0.000247, tokens/sec=1378849.25, grad_norm=0.3198, duration=0.38s
Step 17494: loss=2.8428, lr=0.000247, tokens/sec=1374221.19, grad_norm=0.3352, duration=0.38s
Step 17495: loss=2.8442, lr=0.000247, tokens/sec=1372281.37, grad_norm=0.3380, duration=0.38s
Step 17496: loss=2.7480, lr=0.000247, tokens/sec=1370916.83, grad_norm=0.3285, duration=0.38s
Step 17497: loss=2.8443, lr=0.000247, tokens/sec=1372035.64, grad_norm=0.3221, duration=0.38s
Step 17498: loss=2.7258, lr=0.000247, tokens/sec=1370756.17, grad_norm=0.3268, duration=0.38s
Step 17499: loss=2.7307, lr=0.000247, tokens/sec=1372087.86, grad_norm=0.3952, duration=0.38s
Step 17500/19073 (91.8%), Elapsed time: 6923.82s, Steps per hour: 9099.02, Estimated hours remaining: 0.17
Validation loss at step 17500: 3.8446567058563232
Step 17500: loss=2.7676, lr=0.000247, tokens/sec=153493.00, grad_norm=0.3184, duration=3.42s
Step 17501: loss=2.7756, lr=0.000247, tokens/sec=1372207.72, grad_norm=0.3047, duration=0.38s
Step 17502: loss=2.8799, lr=0.000247, tokens/sec=1371870.44, grad_norm=0.3232, duration=0.38s
Step 17503: loss=2.7115, lr=0.000247, tokens/sec=1377563.10, grad_norm=0.3290, duration=0.38s
Step 17504: loss=2.8456, lr=0.000247, tokens/sec=1370901.45, grad_norm=0.3201, duration=0.38s
Step 17505: loss=2.8501, lr=0.000247, tokens/sec=1372419.26, grad_norm=0.3027, duration=0.38s
Step 17506: loss=2.7887, lr=0.000247, tokens/sec=1369556.71, grad_norm=0.3073, duration=0.38s
Step 17507: loss=2.8112, lr=0.000247, tokens/sec=1374683.37, grad_norm=0.3266, duration=0.38s
Step 17508: loss=2.8583, lr=0.000247, tokens/sec=1372569.16, grad_norm=0.3099, duration=0.38s
Step 17509: loss=2.7947, lr=0.000247, tokens/sec=1376719.63, grad_norm=0.2862, duration=0.38s
Step 17510: loss=2.7608, lr=0.000247, tokens/sec=1376815.31, grad_norm=0.3048, duration=0.38s
Step 17511: loss=2.8065, lr=0.000247, tokens/sec=1371925.21, grad_norm=0.3316, duration=0.38s
Step 17512: loss=2.8047, lr=0.000247, tokens/sec=1375548.43, grad_norm=0.3234, duration=0.38s
Step 17513: loss=2.8236, lr=0.000247, tokens/sec=1375358.30, grad_norm=0.3055, duration=0.38s
Step 17514: loss=2.7865, lr=0.000247, tokens/sec=1374346.59, grad_norm=0.3112, duration=0.38s
Step 17515: loss=2.7456, lr=0.000247, tokens/sec=1377179.18, grad_norm=0.3389, duration=0.38s
Step 17516: loss=2.7615, lr=0.000247, tokens/sec=1374284.75, grad_norm=0.3214, duration=0.38s
Step 17517: loss=2.7047, lr=0.000246, tokens/sec=1370438.39, grad_norm=0.3086, duration=0.38s
Step 17518: loss=2.7161, lr=0.000246, tokens/sec=1373957.60, grad_norm=0.3212, duration=0.38s
Step 17519: loss=2.7254, lr=0.000246, tokens/sec=1377397.43, grad_norm=0.3209, duration=0.38s
Step 17520: loss=2.8021, lr=0.000246, tokens/sec=1368635.28, grad_norm=0.3344, duration=0.38s
Step 17521: loss=2.7942, lr=0.000246, tokens/sec=1374877.62, grad_norm=0.3202, duration=0.38s
Step 17522: loss=2.7586, lr=0.000246, tokens/sec=1375228.42, grad_norm=0.3168, duration=0.38s
Step 17523: loss=2.7812, lr=0.000246, tokens/sec=1376880.83, grad_norm=0.3211, duration=0.38s
Step 17524: loss=2.7228, lr=0.000246, tokens/sec=1378201.12, grad_norm=0.3113, duration=0.38s
Step 17525: loss=2.7629, lr=0.000246, tokens/sec=1373465.88, grad_norm=0.3114, duration=0.38s
Step 17526: loss=2.7888, lr=0.000246, tokens/sec=1375889.25, grad_norm=0.3025, duration=0.38s
Step 17527: loss=2.8369, lr=0.000246, tokens/sec=1374563.93, grad_norm=0.3463, duration=0.38s
Step 17528: loss=2.9095, lr=0.000246, tokens/sec=1376797.21, grad_norm=0.3377, duration=0.38s
Step 17529: loss=2.8522, lr=0.000246, tokens/sec=1376269.00, grad_norm=0.3269, duration=0.38s
Step 17530: loss=2.8110, lr=0.000246, tokens/sec=1376904.11, grad_norm=0.3172, duration=0.38s
Step 17531: loss=2.8700, lr=0.000246, tokens/sec=1372683.12, grad_norm=0.3241, duration=0.38s
Step 17532: loss=2.8295, lr=0.000246, tokens/sec=1366579.53, grad_norm=0.3340, duration=0.38s
Step 17533: loss=2.8283, lr=0.000246, tokens/sec=1375998.59, grad_norm=0.3209, duration=0.38s
Step 17534: loss=2.8190, lr=0.000246, tokens/sec=1371313.51, grad_norm=0.3096, duration=0.38s
Step 17535: loss=2.7972, lr=0.000246, tokens/sec=1370991.19, grad_norm=0.3163, duration=0.38s
Step 17536: loss=2.8191, lr=0.000246, tokens/sec=1373181.99, grad_norm=0.3174, duration=0.38s
Step 17537: loss=2.7831, lr=0.000246, tokens/sec=1367247.38, grad_norm=0.3233, duration=0.38s
Step 17538: loss=2.7766, lr=0.000246, tokens/sec=1370647.67, grad_norm=0.3155, duration=0.38s
Step 17539: loss=2.7991, lr=0.000246, tokens/sec=1371735.23, grad_norm=0.3057, duration=0.38s
Step 17540: loss=2.7947, lr=0.000246, tokens/sec=1374602.60, grad_norm=0.2970, duration=0.38s
Step 17541: loss=2.7398, lr=0.000246, tokens/sec=1374029.71, grad_norm=0.3045, duration=0.38s
Step 17542: loss=2.7693, lr=0.000246, tokens/sec=1374631.81, grad_norm=0.3193, duration=0.38s
Step 17543: loss=2.8070, lr=0.000246, tokens/sec=1372264.24, grad_norm=0.3183, duration=0.38s
Step 17544: loss=2.8101, lr=0.000246, tokens/sec=1374751.27, grad_norm=0.3130, duration=0.38s
Step 17545: loss=2.8333, lr=0.000246, tokens/sec=1376016.67, grad_norm=0.3042, duration=0.38s
Step 17546: loss=2.8004, lr=0.000246, tokens/sec=1374914.58, grad_norm=0.3019, duration=0.38s
Step 17547: loss=2.8658, lr=0.000246, tokens/sec=1374912.00, grad_norm=0.3120, duration=0.38s
Step 17548: loss=2.8584, lr=0.000246, tokens/sec=1374308.79, grad_norm=0.3113, duration=0.38s
Step 17549: loss=2.8260, lr=0.000246, tokens/sec=1373374.95, grad_norm=0.2999, duration=0.38s
Step 17550: loss=2.8218, lr=0.000246, tokens/sec=1373148.55, grad_norm=0.2999, duration=0.38s
Step 17551: loss=2.8265, lr=0.000246, tokens/sec=1375169.08, grad_norm=0.3041, duration=0.38s
Step 17552: loss=2.8081, lr=0.000246, tokens/sec=1370177.95, grad_norm=0.2985, duration=0.38s
Step 17553: loss=2.8106, lr=0.000246, tokens/sec=1369532.83, grad_norm=0.3075, duration=0.38s
Step 17554: loss=2.8200, lr=0.000246, tokens/sec=1373980.78, grad_norm=0.3070, duration=0.38s
Step 17555: loss=2.8173, lr=0.000246, tokens/sec=1373199.14, grad_norm=0.3071, duration=0.38s
Step 17556: loss=2.8262, lr=0.000246, tokens/sec=1375231.00, grad_norm=0.3000, duration=0.38s
Step 17557: loss=2.7705, lr=0.000246, tokens/sec=1375907.33, grad_norm=0.2932, duration=0.38s
Step 17558: loss=2.7984, lr=0.000246, tokens/sec=1375597.48, grad_norm=0.2956, duration=0.38s
Step 17559: loss=2.8178, lr=0.000246, tokens/sec=1372501.49, grad_norm=0.3101, duration=0.38s
Step 17560: loss=2.8289, lr=0.000246, tokens/sec=1373888.07, grad_norm=0.3095, duration=0.38s
Step 17561: loss=2.7336, lr=0.000246, tokens/sec=1375145.01, grad_norm=0.3079, duration=0.38s
Step 17562: loss=2.7868, lr=0.000246, tokens/sec=1373722.42, grad_norm=0.3019, duration=0.38s
Step 17563: loss=2.7453, lr=0.000246, tokens/sec=1372706.25, grad_norm=0.3084, duration=0.38s
Step 17564: loss=2.7504, lr=0.000246, tokens/sec=1372986.51, grad_norm=0.3024, duration=0.38s
Step 17565: loss=2.7205, lr=0.000246, tokens/sec=1371516.21, grad_norm=0.3028, duration=0.38s
Step 17566: loss=2.7350, lr=0.000246, tokens/sec=1375709.36, grad_norm=0.3007, duration=0.38s
Step 17567: loss=2.7353, lr=0.000246, tokens/sec=1372587.16, grad_norm=0.3072, duration=0.38s
Step 17568: loss=2.7052, lr=0.000246, tokens/sec=1375511.44, grad_norm=0.3168, duration=0.38s
Step 17569: loss=2.7259, lr=0.000246, tokens/sec=1373745.59, grad_norm=0.3046, duration=0.38s
Step 17570: loss=2.7121, lr=0.000246, tokens/sec=1373446.15, grad_norm=0.2995, duration=0.38s
Step 17571: loss=2.7683, lr=0.000246, tokens/sec=1371564.97, grad_norm=0.3002, duration=0.38s
Step 17572: loss=2.7228, lr=0.000246, tokens/sec=1372254.82, grad_norm=0.3064, duration=0.38s
Step 17573: loss=2.7596, lr=0.000246, tokens/sec=1375175.96, grad_norm=0.3107, duration=0.38s
Step 17574: loss=2.8457, lr=0.000246, tokens/sec=1374708.30, grad_norm=0.3032, duration=0.38s
Step 17575: loss=2.8531, lr=0.000246, tokens/sec=1376891.17, grad_norm=0.3078, duration=0.38s
Step 17576: loss=2.8461, lr=0.000246, tokens/sec=1375782.51, grad_norm=0.3050, duration=0.38s
Step 17577: loss=2.8348, lr=0.000246, tokens/sec=1375551.02, grad_norm=0.3011, duration=0.38s
Step 17578: loss=2.7819, lr=0.000246, tokens/sec=1373390.39, grad_norm=0.3003, duration=0.38s
Step 17579: loss=2.8169, lr=0.000246, tokens/sec=1375563.92, grad_norm=0.3139, duration=0.38s
Step 17580: loss=2.8031, lr=0.000246, tokens/sec=1377498.38, grad_norm=0.3309, duration=0.38s
Step 17581: loss=2.8261, lr=0.000246, tokens/sec=1372314.77, grad_norm=0.3090, duration=0.38s
Step 17582: loss=2.8407, lr=0.000246, tokens/sec=1373540.51, grad_norm=0.3084, duration=0.38s
Step 17583: loss=2.8224, lr=0.000246, tokens/sec=1375718.82, grad_norm=0.3077, duration=0.38s
Step 17584: loss=2.8517, lr=0.000246, tokens/sec=1378006.80, grad_norm=0.3144, duration=0.38s
Step 17585: loss=2.8310, lr=0.000246, tokens/sec=1374733.22, grad_norm=0.3300, duration=0.38s
Step 17586: loss=2.8200, lr=0.000246, tokens/sec=1374759.00, grad_norm=0.3150, duration=0.38s
Step 17587: loss=2.8251, lr=0.000246, tokens/sec=1375704.19, grad_norm=0.3054, duration=0.38s
Step 17588: loss=2.8531, lr=0.000246, tokens/sec=1374573.38, grad_norm=0.2985, duration=0.38s
Step 17589: loss=2.7370, lr=0.000246, tokens/sec=1374234.07, grad_norm=0.3121, duration=0.38s
Step 17590: loss=2.8310, lr=0.000246, tokens/sec=1372649.70, grad_norm=0.3249, duration=0.38s
Step 17591: loss=2.8220, lr=0.000246, tokens/sec=1375625.02, grad_norm=0.3201, duration=0.38s
Step 17592: loss=2.8587, lr=0.000246, tokens/sec=1374456.54, grad_norm=0.3119, duration=0.38s
Step 17593: loss=2.7496, lr=0.000246, tokens/sec=1371511.93, grad_norm=0.3044, duration=0.38s
Step 17594: loss=2.8364, lr=0.000246, tokens/sec=1373155.41, grad_norm=0.3194, duration=0.38s
Step 17595: loss=2.7196, lr=0.000246, tokens/sec=1373441.00, grad_norm=0.3782, duration=0.38s
Step 17596: loss=2.8484, lr=0.000246, tokens/sec=1377567.41, grad_norm=0.3324, duration=0.38s
Step 17597: loss=2.8157, lr=0.000246, tokens/sec=1374905.12, grad_norm=0.3218, duration=0.38s
Step 17598: loss=2.8188, lr=0.000246, tokens/sec=1375228.42, grad_norm=0.3233, duration=0.38s
Step 17599: loss=2.8147, lr=0.000246, tokens/sec=1372577.73, grad_norm=0.2973, duration=0.38s
Step 17600/19073 (92.3%), Elapsed time: 6965.10s, Steps per hour: 9096.78, Estimated hours remaining: 0.16
Step 17600: loss=2.8064, lr=0.000246, tokens/sec=1378672.03, grad_norm=0.3013, duration=0.38s
Step 17601: loss=2.8093, lr=0.000246, tokens/sec=1374184.27, grad_norm=0.3274, duration=0.38s
Step 17602: loss=2.8063, lr=0.000246, tokens/sec=1374315.67, grad_norm=0.3343, duration=0.38s
Step 17603: loss=2.7959, lr=0.000246, tokens/sec=1373416.13, grad_norm=0.3281, duration=0.38s
Step 17604: loss=2.8419, lr=0.000246, tokens/sec=1375916.80, grad_norm=0.3088, duration=0.38s
Step 17605: loss=2.8062, lr=0.000246, tokens/sec=1368900.24, grad_norm=0.3162, duration=0.38s
Step 17606: loss=2.7920, lr=0.000246, tokens/sec=1369029.78, grad_norm=0.3301, duration=0.38s
Step 17607: loss=2.7952, lr=0.000246, tokens/sec=1374526.99, grad_norm=0.3295, duration=0.38s
Step 17608: loss=2.8052, lr=0.000246, tokens/sec=1375832.44, grad_norm=0.3261, duration=0.38s
Step 17609: loss=2.7672, lr=0.000246, tokens/sec=1375003.99, grad_norm=0.3068, duration=0.38s
Step 17610: loss=2.7714, lr=0.000246, tokens/sec=1374463.41, grad_norm=0.3154, duration=0.38s
Step 17611: loss=2.7011, lr=0.000246, tokens/sec=1376136.37, grad_norm=0.3146, duration=0.38s
Step 17612: loss=2.7171, lr=0.000246, tokens/sec=1374139.61, grad_norm=0.3158, duration=0.38s
Step 17613: loss=2.7427, lr=0.000246, tokens/sec=1377538.93, grad_norm=0.3189, duration=0.38s
Step 17614: loss=2.7783, lr=0.000246, tokens/sec=1373854.59, grad_norm=0.3055, duration=0.38s
Step 17615: loss=2.7713, lr=0.000246, tokens/sec=1377593.30, grad_norm=0.2970, duration=0.38s
Step 17616: loss=2.7445, lr=0.000246, tokens/sec=1376622.24, grad_norm=0.3009, duration=0.38s
Step 17617: loss=2.7154, lr=0.000246, tokens/sec=1374465.99, grad_norm=0.3124, duration=0.38s
Step 17618: loss=2.7793, lr=0.000246, tokens/sec=1375924.55, grad_norm=0.3066, duration=0.38s
Step 17619: loss=2.8007, lr=0.000246, tokens/sec=1370968.97, grad_norm=0.3089, duration=0.38s
Step 17620: loss=2.7451, lr=0.000246, tokens/sec=1375544.13, grad_norm=0.3409, duration=0.38s
Step 17621: loss=2.8181, lr=0.000246, tokens/sec=1375252.51, grad_norm=0.3228, duration=0.38s
Step 17622: loss=2.8225, lr=0.000246, tokens/sec=1372942.80, grad_norm=0.3265, duration=0.38s
Step 17623: loss=2.8315, lr=0.000246, tokens/sec=1374624.94, grad_norm=0.3224, duration=0.38s
Step 17624: loss=2.8496, lr=0.000246, tokens/sec=1377044.65, grad_norm=0.3360, duration=0.38s
Step 17625: loss=2.7992, lr=0.000246, tokens/sec=1374338.86, grad_norm=0.3339, duration=0.38s
Step 17626: loss=2.8039, lr=0.000246, tokens/sec=1375953.82, grad_norm=0.3215, duration=0.38s
Step 17627: loss=2.8617, lr=0.000246, tokens/sec=1375603.50, grad_norm=0.3226, duration=0.38s
Step 17628: loss=2.9526, lr=0.000246, tokens/sec=1373820.26, grad_norm=0.3516, duration=0.38s
Step 17629: loss=2.8897, lr=0.000246, tokens/sec=1377247.32, grad_norm=0.3466, duration=0.38s
Step 17630: loss=2.8635, lr=0.000246, tokens/sec=1380239.16, grad_norm=0.3126, duration=0.38s
Step 17631: loss=2.8127, lr=0.000246, tokens/sec=1376082.98, grad_norm=0.2998, duration=0.38s
Step 17632: loss=2.8369, lr=0.000246, tokens/sec=1375192.30, grad_norm=0.3221, duration=0.38s
Step 17633: loss=2.8048, lr=0.000246, tokens/sec=1376300.87, grad_norm=0.3291, duration=0.38s
Step 17634: loss=2.7803, lr=0.000246, tokens/sec=1375513.16, grad_norm=0.3307, duration=0.38s
Step 17635: loss=2.8449, lr=0.000246, tokens/sec=1374569.09, grad_norm=0.3066, duration=0.38s
Step 17636: loss=2.7683, lr=0.000246, tokens/sec=1373808.24, grad_norm=0.3152, duration=0.38s
Step 17637: loss=2.8526, lr=0.000246, tokens/sec=1373909.53, grad_norm=0.3403, duration=0.38s
Step 17638: loss=2.8659, lr=0.000246, tokens/sec=1376092.45, grad_norm=0.3443, duration=0.38s
Step 17639: loss=2.8499, lr=0.000246, tokens/sec=1375700.75, grad_norm=0.3114, duration=0.38s
Step 17640: loss=2.7805, lr=0.000246, tokens/sec=1377868.65, grad_norm=0.2988, duration=0.38s
Step 17641: loss=2.8361, lr=0.000246, tokens/sec=1377136.92, grad_norm=0.3405, duration=0.38s
Step 17642: loss=2.8845, lr=0.000245, tokens/sec=1373731.86, grad_norm=0.3460, duration=0.38s
Step 17643: loss=2.8130, lr=0.000245, tokens/sec=1373624.60, grad_norm=0.3336, duration=0.38s
Step 17644: loss=2.7757, lr=0.000245, tokens/sec=1373882.06, grad_norm=0.3156, duration=0.38s
Step 17645: loss=2.8166, lr=0.000245, tokens/sec=1371334.03, grad_norm=0.2948, duration=0.38s
Step 17646: loss=2.8047, lr=0.000245, tokens/sec=1376418.89, grad_norm=0.2946, duration=0.38s
Step 17647: loss=2.8429, lr=0.000245, tokens/sec=1375434.87, grad_norm=0.3238, duration=0.38s
Step 17648: loss=2.8617, lr=0.000245, tokens/sec=1370691.24, grad_norm=0.3330, duration=0.38s
Step 17649: loss=2.7985, lr=0.000245, tokens/sec=1373191.43, grad_norm=0.3163, duration=0.38s
Step 17650: loss=2.8075, lr=0.000245, tokens/sec=1376745.49, grad_norm=0.2943, duration=0.38s
Step 17651: loss=2.8014, lr=0.000245, tokens/sec=1374772.75, grad_norm=0.3032, duration=0.38s
Step 17652: loss=2.7499, lr=0.000245, tokens/sec=1375665.46, grad_norm=0.3192, duration=0.38s
Step 17653: loss=2.7924, lr=0.000245, tokens/sec=1373450.44, grad_norm=0.3412, duration=0.38s
Step 17654: loss=2.7840, lr=0.000245, tokens/sec=1376334.47, grad_norm=0.3274, duration=0.38s
Step 17655: loss=2.7369, lr=0.000245, tokens/sec=1372716.54, grad_norm=0.3072, duration=0.38s
Step 17656: loss=2.7456, lr=0.000245, tokens/sec=1372410.69, grad_norm=0.3058, duration=0.38s
Step 17657: loss=2.7993, lr=0.000245, tokens/sec=1376035.62, grad_norm=0.3189, duration=0.38s
Step 17658: loss=2.7596, lr=0.000245, tokens/sec=1374618.92, grad_norm=0.3215, duration=0.38s
Step 17659: loss=2.7461, lr=0.000245, tokens/sec=1374262.42, grad_norm=0.3125, duration=0.38s
Step 17660: loss=2.6932, lr=0.000245, tokens/sec=1376164.79, grad_norm=0.3005, duration=0.38s
Step 17661: loss=2.7343, lr=0.000245, tokens/sec=1375377.23, grad_norm=0.3060, duration=0.38s
Step 17662: loss=2.7664, lr=0.000245, tokens/sec=1377826.35, grad_norm=0.3105, duration=0.38s
Step 17663: loss=2.7422, lr=0.000245, tokens/sec=1377115.36, grad_norm=0.3046, duration=0.38s
Step 17664: loss=2.7655, lr=0.000245, tokens/sec=1375011.73, grad_norm=0.2908, duration=0.38s
Step 17665: loss=2.8128, lr=0.000245, tokens/sec=1373126.26, grad_norm=0.3017, duration=0.38s
Step 17666: loss=2.7458, lr=0.000245, tokens/sec=1373269.46, grad_norm=0.3099, duration=0.38s
Step 17667: loss=2.8094, lr=0.000245, tokens/sec=1376766.18, grad_norm=0.3100, duration=0.38s
Step 17668: loss=2.7807, lr=0.000245, tokens/sec=1373500.19, grad_norm=0.3098, duration=0.38s
Step 17669: loss=2.8437, lr=0.000245, tokens/sec=1377054.13, grad_norm=0.2974, duration=0.38s
Step 17670: loss=2.8117, lr=0.000245, tokens/sec=1374110.42, grad_norm=0.3050, duration=0.38s
Step 17671: loss=2.8425, lr=0.000245, tokens/sec=1371829.36, grad_norm=0.3081, duration=0.38s
Step 17672: loss=2.8410, lr=0.000245, tokens/sec=1370074.66, grad_norm=0.3170, duration=0.38s
Step 17673: loss=2.9132, lr=0.000245, tokens/sec=1377989.53, grad_norm=0.3160, duration=0.38s
Step 17674: loss=2.8319, lr=0.000245, tokens/sec=1374771.89, grad_norm=0.2930, duration=0.38s
Step 17675: loss=2.8243, lr=0.000245, tokens/sec=1377917.87, grad_norm=0.3048, duration=0.38s
Step 17676: loss=2.9151, lr=0.000245, tokens/sec=1371368.24, grad_norm=0.3246, duration=0.38s
Step 17677: loss=2.8522, lr=0.000245, tokens/sec=1374531.28, grad_norm=0.3227, duration=0.38s
Step 17678: loss=2.8598, lr=0.000245, tokens/sec=1373079.96, grad_norm=0.3126, duration=0.38s
Step 17679: loss=2.8658, lr=0.000245, tokens/sec=1376954.97, grad_norm=0.3102, duration=0.38s
Step 17680: loss=2.8481, lr=0.000245, tokens/sec=1374740.95, grad_norm=0.3076, duration=0.38s
Step 17681: loss=2.8391, lr=0.000245, tokens/sec=1374628.38, grad_norm=0.3221, duration=0.38s
Step 17682: loss=2.8380, lr=0.000245, tokens/sec=1373668.36, grad_norm=0.3279, duration=0.38s
Step 17683: loss=2.7685, lr=0.000245, tokens/sec=1370608.37, grad_norm=0.3234, duration=0.38s
Step 17684: loss=2.8587, lr=0.000245, tokens/sec=1374779.63, grad_norm=0.3141, duration=0.38s
Step 17685: loss=2.7865, lr=0.000245, tokens/sec=1372754.24, grad_norm=0.3744, duration=0.38s
Step 17686: loss=2.7783, lr=0.000245, tokens/sec=1377167.11, grad_norm=0.3351, duration=0.38s
Step 17687: loss=2.8079, lr=0.000245, tokens/sec=1376011.51, grad_norm=0.3071, duration=0.38s
Step 17688: loss=2.7591, lr=0.000245, tokens/sec=1374159.36, grad_norm=0.3092, duration=0.38s
Step 17689: loss=2.7225, lr=0.000245, tokens/sec=1375536.39, grad_norm=0.3539, duration=0.38s
Step 17690: loss=2.7330, lr=0.000245, tokens/sec=1377906.64, grad_norm=0.3486, duration=0.38s
Step 17691: loss=2.8027, lr=0.000245, tokens/sec=1377393.98, grad_norm=0.3318, duration=0.38s
Step 17692: loss=2.8651, lr=0.000245, tokens/sec=1376220.77, grad_norm=0.3108, duration=0.38s
Step 17693: loss=2.7325, lr=0.000245, tokens/sec=1375927.13, grad_norm=0.3070, duration=0.38s
Step 17694: loss=2.8567, lr=0.000245, tokens/sec=1374949.83, grad_norm=0.3237, duration=0.38s
Step 17695: loss=2.8467, lr=0.000245, tokens/sec=1373570.54, grad_norm=0.3293, duration=0.38s
Step 17696: loss=2.7714, lr=0.000245, tokens/sec=1376590.36, grad_norm=0.3040, duration=0.38s
Step 17697: loss=2.8325, lr=0.000245, tokens/sec=1374939.51, grad_norm=0.3119, duration=0.38s
Step 17698: loss=2.8142, lr=0.000245, tokens/sec=1372009.10, grad_norm=0.3098, duration=0.38s
Step 17699: loss=2.7986, lr=0.000245, tokens/sec=1375953.82, grad_norm=0.3052, duration=0.38s
Step 17700/19073 (92.8%), Elapsed time: 7003.32s, Steps per hour: 9098.54, Estimated hours remaining: 0.15
Step 17700: loss=2.7704, lr=0.000245, tokens/sec=1374216.90, grad_norm=0.3057, duration=0.38s
Step 17701: loss=2.7996, lr=0.000245, tokens/sec=1378908.04, grad_norm=0.3082, duration=0.38s
Step 17702: loss=2.7848, lr=0.000245, tokens/sec=1377021.37, grad_norm=0.3209, duration=0.38s
Step 17703: loss=2.8385, lr=0.000245, tokens/sec=1377801.32, grad_norm=0.3072, duration=0.38s
Step 17704: loss=2.7961, lr=0.000245, tokens/sec=1376919.62, grad_norm=0.3192, duration=0.38s
Step 17705: loss=2.7179, lr=0.000245, tokens/sec=1379116.46, grad_norm=0.3220, duration=0.38s
Step 17706: loss=2.7502, lr=0.000245, tokens/sec=1375512.30, grad_norm=0.3277, duration=0.38s
Step 17707: loss=2.7199, lr=0.000245, tokens/sec=1377211.96, grad_norm=0.3154, duration=0.38s
Step 17708: loss=2.7065, lr=0.000245, tokens/sec=1375457.23, grad_norm=0.3366, duration=0.38s
Step 17709: loss=2.7876, lr=0.000245, tokens/sec=1375184.56, grad_norm=0.3358, duration=0.38s
Step 17710: loss=2.7894, lr=0.000245, tokens/sec=1372247.11, grad_norm=0.3318, duration=0.38s
Step 17711: loss=2.7540, lr=0.000245, tokens/sec=1372749.10, grad_norm=0.3340, duration=0.38s
Step 17712: loss=2.7612, lr=0.000245, tokens/sec=1371508.51, grad_norm=0.3361, duration=0.38s
Step 17713: loss=2.7460, lr=0.000245, tokens/sec=1374689.39, grad_norm=0.3316, duration=0.38s
Step 17714: loss=2.7656, lr=0.000245, tokens/sec=1373322.63, grad_norm=0.3252, duration=0.38s
Step 17715: loss=2.7745, lr=0.000245, tokens/sec=1374913.72, grad_norm=0.3252, duration=0.38s
Step 17716: loss=2.7716, lr=0.000245, tokens/sec=1377272.34, grad_norm=0.3222, duration=0.38s
Step 17717: loss=2.8670, lr=0.000245, tokens/sec=1371865.30, grad_norm=0.3448, duration=0.38s
Step 17718: loss=2.8909, lr=0.000245, tokens/sec=1373036.24, grad_norm=0.3487, duration=0.38s
Step 17719: loss=2.8178, lr=0.000245, tokens/sec=1375111.47, grad_norm=0.3340, duration=0.38s
Step 17720: loss=2.8448, lr=0.000245, tokens/sec=1375307.55, grad_norm=0.3251, duration=0.38s
Step 17721: loss=2.8363, lr=0.000245, tokens/sec=1370945.04, grad_norm=0.3115, duration=0.38s
Step 17722: loss=2.8246, lr=0.000245, tokens/sec=1371020.25, grad_norm=0.3311, duration=0.38s
Step 17723: loss=2.8514, lr=0.000245, tokens/sec=1376552.44, grad_norm=0.3421, duration=0.38s
Step 17724: loss=2.8046, lr=0.000245, tokens/sec=1374639.55, grad_norm=0.3230, duration=0.38s
Step 17725: loss=2.7913, lr=0.000245, tokens/sec=1377174.01, grad_norm=0.3154, duration=0.38s
Step 17726: loss=2.8001, lr=0.000245, tokens/sec=1375764.44, grad_norm=0.3342, duration=0.38s
Step 17727: loss=2.7803, lr=0.000245, tokens/sec=1375323.04, grad_norm=0.3264, duration=0.38s
Step 17728: loss=2.7992, lr=0.000245, tokens/sec=1376398.22, grad_norm=0.3111, duration=0.38s
Step 17729: loss=2.7943, lr=0.000245, tokens/sec=1375803.17, grad_norm=0.3140, duration=0.38s
Step 17730: loss=2.8136, lr=0.000245, tokens/sec=1372552.89, grad_norm=0.3238, duration=0.38s
Step 17731: loss=2.7122, lr=0.000245, tokens/sec=1378275.41, grad_norm=0.3182, duration=0.38s
Step 17732: loss=2.7637, lr=0.000245, tokens/sec=1375072.77, grad_norm=0.3145, duration=0.38s
Step 17733: loss=2.8177, lr=0.000245, tokens/sec=1374447.09, grad_norm=0.3278, duration=0.38s
Step 17734: loss=2.7964, lr=0.000245, tokens/sec=1376847.21, grad_norm=0.3129, duration=0.38s
Step 17735: loss=2.8504, lr=0.000245, tokens/sec=1376278.48, grad_norm=0.3087, duration=0.38s
Step 17736: loss=2.7929, lr=0.000245, tokens/sec=1374820.88, grad_norm=0.3017, duration=0.38s
Step 17737: loss=2.8903, lr=0.000245, tokens/sec=1373734.44, grad_norm=0.3286, duration=0.38s
Step 17738: loss=2.8338, lr=0.000245, tokens/sec=1376436.12, grad_norm=0.3083, duration=0.38s
Step 17739: loss=2.8147, lr=0.000245, tokens/sec=1371591.49, grad_norm=0.2979, duration=0.38s
Step 17740: loss=2.8188, lr=0.000245, tokens/sec=1375369.49, grad_norm=0.3044, duration=0.38s
Step 17741: loss=2.8304, lr=0.000245, tokens/sec=1377098.11, grad_norm=0.3088, duration=0.38s
Step 17742: loss=2.7887, lr=0.000245, tokens/sec=1374228.06, grad_norm=0.3033, duration=0.38s
Step 17743: loss=2.8152, lr=0.000245, tokens/sec=1378480.17, grad_norm=0.2953, duration=0.38s
Step 17744: loss=2.8247, lr=0.000245, tokens/sec=1375741.20, grad_norm=0.2925, duration=0.38s
Step 17745: loss=2.8166, lr=0.000245, tokens/sec=1374106.98, grad_norm=0.3028, duration=0.38s
Step 17746: loss=2.8101, lr=0.000245, tokens/sec=1376157.90, grad_norm=0.3106, duration=0.38s
Step 17747: loss=2.7589, lr=0.000245, tokens/sec=1375513.16, grad_norm=0.2892, duration=0.38s
Step 17748: loss=2.8271, lr=0.000245, tokens/sec=1373471.03, grad_norm=0.2826, duration=0.38s
Step 17749: loss=2.8145, lr=0.000245, tokens/sec=1372201.73, grad_norm=0.2930, duration=0.38s
Validation loss at step 17750: 3.850010633468628
Step 17750: loss=2.8021, lr=0.000245, tokens/sec=155474.17, grad_norm=0.3003, duration=3.37s
Step 17751: loss=2.7233, lr=0.000245, tokens/sec=1378812.07, grad_norm=0.3082, duration=0.38s
Step 17752: loss=2.7948, lr=0.000245, tokens/sec=1374933.49, grad_norm=0.2925, duration=0.38s
Step 17753: loss=2.7370, lr=0.000245, tokens/sec=1376286.23, grad_norm=0.2896, duration=0.38s
Step 17754: loss=2.7651, lr=0.000245, tokens/sec=1375001.41, grad_norm=0.3031, duration=0.38s
Step 17755: loss=2.7033, lr=0.000245, tokens/sec=1375258.53, grad_norm=0.3064, duration=0.38s
Step 17756: loss=2.7136, lr=0.000245, tokens/sec=1374670.48, grad_norm=0.3017, duration=0.38s
Step 17757: loss=2.7502, lr=0.000245, tokens/sec=1375285.19, grad_norm=0.3038, duration=0.38s
Step 17758: loss=2.7026, lr=0.000245, tokens/sec=1372165.77, grad_norm=0.3013, duration=0.38s
Step 17759: loss=2.7303, lr=0.000245, tokens/sec=1378513.01, grad_norm=0.3079, duration=0.38s
Step 17760: loss=2.7106, lr=0.000245, tokens/sec=1371718.11, grad_norm=0.3076, duration=0.38s
Step 17761: loss=2.7368, lr=0.000245, tokens/sec=1376118.28, grad_norm=0.3059, duration=0.38s
Step 17762: loss=2.7372, lr=0.000245, tokens/sec=1374328.55, grad_norm=0.3091, duration=0.38s
Step 17763: loss=2.8006, lr=0.000245, tokens/sec=1373917.25, grad_norm=0.3147, duration=0.38s
Step 17764: loss=2.8495, lr=0.000245, tokens/sec=1376474.89, grad_norm=0.3071, duration=0.38s
Step 17765: loss=2.8397, lr=0.000245, tokens/sec=1374121.58, grad_norm=0.3247, duration=0.38s
Step 17766: loss=2.8325, lr=0.000245, tokens/sec=1370253.08, grad_norm=0.3192, duration=0.38s
Step 17767: loss=2.8484, lr=0.000245, tokens/sec=1375928.86, grad_norm=0.3033, duration=0.38s
Step 17768: loss=2.7653, lr=0.000245, tokens/sec=1376425.78, grad_norm=0.2994, duration=0.38s
Step 17769: loss=2.8341, lr=0.000245, tokens/sec=1377103.29, grad_norm=0.3106, duration=0.38s
Step 17770: loss=2.7616, lr=0.000245, tokens/sec=1377073.11, grad_norm=0.3272, duration=0.38s
Step 17771: loss=2.8471, lr=0.000245, tokens/sec=1373329.50, grad_norm=0.3134, duration=0.38s
Step 17772: loss=2.8354, lr=0.000245, tokens/sec=1376962.73, grad_norm=0.3123, duration=0.38s
Step 17773: loss=2.8481, lr=0.000245, tokens/sec=1378788.73, grad_norm=0.3149, duration=0.38s
Step 17774: loss=2.8568, lr=0.000245, tokens/sec=1375120.07, grad_norm=0.3207, duration=0.38s
Step 17775: loss=2.8034, lr=0.000245, tokens/sec=1377538.07, grad_norm=0.3167, duration=0.38s
Step 17776: loss=2.8131, lr=0.000245, tokens/sec=1374507.23, grad_norm=0.3077, duration=0.38s
Step 17777: loss=2.8493, lr=0.000245, tokens/sec=1373414.41, grad_norm=0.2994, duration=0.38s
Step 17778: loss=2.7846, lr=0.000245, tokens/sec=1377627.82, grad_norm=0.3097, duration=0.38s
Step 17779: loss=2.7808, lr=0.000244, tokens/sec=1373511.35, grad_norm=0.3071, duration=0.38s
Step 17780: loss=2.8315, lr=0.000244, tokens/sec=1373076.53, grad_norm=0.3003, duration=0.38s
Step 17781: loss=2.8379, lr=0.000244, tokens/sec=1376425.78, grad_norm=0.3172, duration=0.38s
Step 17782: loss=2.8090, lr=0.000244, tokens/sec=1374647.28, grad_norm=0.3169, duration=0.38s
Step 17783: loss=2.7686, lr=0.000244, tokens/sec=1378357.48, grad_norm=0.3136, duration=0.38s
Step 17784: loss=2.7677, lr=0.000244, tokens/sec=1375406.48, grad_norm=0.3243, duration=0.38s
Step 17785: loss=2.7888, lr=0.000244, tokens/sec=1375331.64, grad_norm=0.3045, duration=0.38s
Step 17786: loss=2.8427, lr=0.000244, tokens/sec=1373652.91, grad_norm=0.3148, duration=0.38s
Step 17787: loss=2.7960, lr=0.000244, tokens/sec=1376345.67, grad_norm=0.3260, duration=0.38s
Step 17788: loss=2.8346, lr=0.000244, tokens/sec=1373700.97, grad_norm=0.3293, duration=0.38s
Step 17789: loss=2.7954, lr=0.000244, tokens/sec=1375465.84, grad_norm=0.3019, duration=0.38s
Step 17790: loss=2.8077, lr=0.000244, tokens/sec=1379308.49, grad_norm=0.2936, duration=0.38s
Step 17791: loss=2.7988, lr=0.000244, tokens/sec=1376006.34, grad_norm=0.3111, duration=0.38s
Step 17792: loss=2.8137, lr=0.000244, tokens/sec=1377405.19, grad_norm=0.3244, duration=0.38s
Step 17793: loss=2.8199, lr=0.000244, tokens/sec=1373219.72, grad_norm=0.3217, duration=0.38s
Step 17794: loss=2.8034, lr=0.000244, tokens/sec=1373348.36, grad_norm=0.3038, duration=0.38s
Step 17795: loss=2.7985, lr=0.000244, tokens/sec=1375808.34, grad_norm=0.3096, duration=0.38s
Step 17796: loss=2.8034, lr=0.000244, tokens/sec=1377527.72, grad_norm=0.3114, duration=0.38s
Step 17797: loss=2.8044, lr=0.000244, tokens/sec=1377080.87, grad_norm=0.3168, duration=0.38s
Step 17798: loss=2.8189, lr=0.000244, tokens/sec=1375984.82, grad_norm=0.3377, duration=0.38s
Step 17799: loss=2.7359, lr=0.000244, tokens/sec=1376046.81, grad_norm=0.3200, duration=0.38s
Step 17800/19073 (93.3%), Elapsed time: 7044.53s, Steps per hour: 9096.42, Estimated hours remaining: 0.14
Step 17800: loss=2.7774, lr=0.000244, tokens/sec=1373977.34, grad_norm=0.3045, duration=0.38s
Step 17801: loss=2.6689, lr=0.000244, tokens/sec=1374614.63, grad_norm=0.3114, duration=0.38s
Step 17802: loss=2.7393, lr=0.000244, tokens/sec=1376006.34, grad_norm=0.3313, duration=0.38s
Step 17803: loss=2.7141, lr=0.000244, tokens/sec=1377898.01, grad_norm=0.3390, duration=0.38s
Step 17804: loss=2.8012, lr=0.000244, tokens/sec=1376664.47, grad_norm=0.3140, duration=0.38s
Step 17805: loss=2.7627, lr=0.000244, tokens/sec=1372549.46, grad_norm=0.2907, duration=0.38s
Step 17806: loss=2.7222, lr=0.000244, tokens/sec=1377105.01, grad_norm=0.3084, duration=0.38s
Step 17807: loss=2.7499, lr=0.000244, tokens/sec=1378044.80, grad_norm=0.3353, duration=0.38s
Step 17808: loss=2.7657, lr=0.000244, tokens/sec=1375044.40, grad_norm=0.3240, duration=0.38s
Step 17809: loss=2.7354, lr=0.000244, tokens/sec=1375417.66, grad_norm=0.3291, duration=0.38s
Step 17810: loss=2.7950, lr=0.000244, tokens/sec=1373125.40, grad_norm=0.3312, duration=0.38s
Step 17811: loss=2.8155, lr=0.000244, tokens/sec=1375498.53, grad_norm=0.3474, duration=0.38s
Step 17812: loss=2.8466, lr=0.000244, tokens/sec=1377626.96, grad_norm=0.3472, duration=0.38s
Step 17813: loss=2.8179, lr=0.000244, tokens/sec=1377315.47, grad_norm=0.3268, duration=0.38s
Step 17814: loss=2.8503, lr=0.000244, tokens/sec=1380084.97, grad_norm=0.3486, duration=0.38s
Step 17815: loss=2.7770, lr=0.000244, tokens/sec=1374478.02, grad_norm=0.3614, duration=0.38s
Step 17816: loss=2.8129, lr=0.000244, tokens/sec=1377657.17, grad_norm=0.3392, duration=0.38s
Step 17817: loss=2.8903, lr=0.000244, tokens/sec=1373307.20, grad_norm=0.3285, duration=0.38s
Step 17818: loss=2.9394, lr=0.000244, tokens/sec=1375938.33, grad_norm=0.3415, duration=0.38s
Step 17819: loss=2.8700, lr=0.000244, tokens/sec=1376928.25, grad_norm=0.3628, duration=0.38s
Step 17820: loss=2.8565, lr=0.000244, tokens/sec=1377202.47, grad_norm=0.3508, duration=0.38s
Step 17821: loss=2.8072, lr=0.000244, tokens/sec=1379606.17, grad_norm=0.3094, duration=0.38s
Step 17822: loss=2.8385, lr=0.000244, tokens/sec=1376462.83, grad_norm=0.3277, duration=0.38s
Step 17823: loss=2.8079, lr=0.000244, tokens/sec=1376374.96, grad_norm=0.3420, duration=0.38s
Step 17824: loss=2.7547, lr=0.000244, tokens/sec=1373525.07, grad_norm=0.3600, duration=0.38s
Step 17825: loss=2.8524, lr=0.000244, tokens/sec=1377123.99, grad_norm=0.3267, duration=0.38s
Step 17826: loss=2.7685, lr=0.000244, tokens/sec=1377603.66, grad_norm=0.3158, duration=0.38s
Step 17827: loss=2.8429, lr=0.000244, tokens/sec=1377456.96, grad_norm=0.3446, duration=0.38s
Step 17828: loss=2.8974, lr=0.000244, tokens/sec=1378009.39, grad_norm=0.3669, duration=0.38s
Step 17829: loss=2.8232, lr=0.000244, tokens/sec=1378077.61, grad_norm=0.3458, duration=0.38s
Step 17830: loss=2.7846, lr=0.000244, tokens/sec=1376592.08, grad_norm=0.3126, duration=0.38s
Step 17831: loss=2.8513, lr=0.000244, tokens/sec=1373180.28, grad_norm=0.3554, duration=0.38s
Step 17832: loss=2.8427, lr=0.000244, tokens/sec=1376748.08, grad_norm=0.3420, duration=0.38s
Step 17833: loss=2.8459, lr=0.000244, tokens/sec=1378707.47, grad_norm=0.3422, duration=0.38s
Step 17834: loss=2.7580, lr=0.000244, tokens/sec=1375286.91, grad_norm=0.3279, duration=0.38s
Step 17835: loss=2.8012, lr=0.000244, tokens/sec=1376567.09, grad_norm=0.3178, duration=0.38s
Step 17836: loss=2.8209, lr=0.000244, tokens/sec=1378026.66, grad_norm=0.3036, duration=0.38s
Step 17837: loss=2.8303, lr=0.000244, tokens/sec=1377343.07, grad_norm=0.3158, duration=0.38s
Step 17838: loss=2.8715, lr=0.000244, tokens/sec=1375908.19, grad_norm=0.3366, duration=0.38s
Step 17839: loss=2.7958, lr=0.000244, tokens/sec=1376054.56, grad_norm=0.3488, duration=0.38s
Step 17840: loss=2.7980, lr=0.000244, tokens/sec=1377780.60, grad_norm=0.3245, duration=0.38s
Step 17841: loss=2.7868, lr=0.000244, tokens/sec=1374125.87, grad_norm=0.2971, duration=0.38s
Step 17842: loss=2.7514, lr=0.000244, tokens/sec=1378895.94, grad_norm=0.3092, duration=0.38s
Step 17843: loss=2.8157, lr=0.000244, tokens/sec=1372057.04, grad_norm=0.3279, duration=0.38s
Step 17844: loss=2.7529, lr=0.000244, tokens/sec=1377719.31, grad_norm=0.3480, duration=0.38s
Step 17845: loss=2.7399, lr=0.000244, tokens/sec=1377563.10, grad_norm=0.3286, duration=0.38s
Step 17846: loss=2.7377, lr=0.000244, tokens/sec=1373155.41, grad_norm=0.3233, duration=0.38s
Step 17847: loss=2.8026, lr=0.000244, tokens/sec=1374347.45, grad_norm=0.3245, duration=0.38s
Step 17848: loss=2.7621, lr=0.000244, tokens/sec=1370461.45, grad_norm=0.3220, duration=0.38s
Step 17849: loss=2.7154, lr=0.000244, tokens/sec=1372732.82, grad_norm=0.3265, duration=0.38s
Step 17850: loss=2.7042, lr=0.000244, tokens/sec=1376648.10, grad_norm=0.3042, duration=0.38s
Step 17851: loss=2.7260, lr=0.000244, tokens/sec=1375007.43, grad_norm=0.3225, duration=0.38s
Step 17852: loss=2.7599, lr=0.000244, tokens/sec=1377180.91, grad_norm=0.3279, duration=0.38s
Step 17853: loss=2.7462, lr=0.000244, tokens/sec=1378839.74, grad_norm=0.3338, duration=0.38s
Step 17854: loss=2.7680, lr=0.000244, tokens/sec=1377371.54, grad_norm=0.3102, duration=0.38s
Step 17855: loss=2.7433, lr=0.000244, tokens/sec=1374895.67, grad_norm=0.2994, duration=0.38s
Step 17856: loss=2.7984, lr=0.000244, tokens/sec=1377544.97, grad_norm=0.3151, duration=0.38s
Step 17857: loss=2.8045, lr=0.000244, tokens/sec=1374696.26, grad_norm=0.3324, duration=0.38s
Step 17858: loss=2.8152, lr=0.000244, tokens/sec=1377752.11, grad_norm=0.3598, duration=0.38s
Step 17859: loss=2.8340, lr=0.000244, tokens/sec=1376663.61, grad_norm=0.3431, duration=0.38s
Step 17860: loss=2.8072, lr=0.000244, tokens/sec=1376283.65, grad_norm=0.3204, duration=0.38s
Step 17861: loss=2.8374, lr=0.000244, tokens/sec=1378295.28, grad_norm=0.3114, duration=0.38s
Step 17862: loss=2.8407, lr=0.000244, tokens/sec=1376646.37, grad_norm=0.3457, duration=0.38s
Step 17863: loss=2.8939, lr=0.000244, tokens/sec=1372331.04, grad_norm=0.3758, duration=0.38s
Step 17864: loss=2.8046, lr=0.000244, tokens/sec=1374924.04, grad_norm=0.3369, duration=0.38s
Step 17865: loss=2.8700, lr=0.000244, tokens/sec=1374569.95, grad_norm=0.3199, duration=0.38s
Step 17866: loss=2.9109, lr=0.000244, tokens/sec=1377882.47, grad_norm=0.3184, duration=0.38s
Step 17867: loss=2.8451, lr=0.000244, tokens/sec=1376076.95, grad_norm=0.3305, duration=0.38s
Step 17868: loss=2.8849, lr=0.000244, tokens/sec=1375177.68, grad_norm=0.3589, duration=0.38s
Step 17869: loss=2.8332, lr=0.000244, tokens/sec=1377037.75, grad_norm=0.3411, duration=0.38s
Step 17870: loss=2.8664, lr=0.000244, tokens/sec=1377826.35, grad_norm=0.3277, duration=0.38s
Step 17871: loss=2.8285, lr=0.000244, tokens/sec=1378577.83, grad_norm=0.3280, duration=0.38s
Step 17872: loss=2.8261, lr=0.000244, tokens/sec=1374757.28, grad_norm=0.3315, duration=0.38s
Step 17873: loss=2.7858, lr=0.000244, tokens/sec=1372456.94, grad_norm=0.3127, duration=0.38s
Step 17874: loss=2.8060, lr=0.000244, tokens/sec=1377610.56, grad_norm=0.3129, duration=0.38s
Step 17875: loss=2.8171, lr=0.000244, tokens/sec=1379815.66, grad_norm=0.3751, duration=0.38s
Step 17876: loss=2.7414, lr=0.000244, tokens/sec=1374540.73, grad_norm=0.3639, duration=0.38s
Step 17877: loss=2.8426, lr=0.000244, tokens/sec=1377290.45, grad_norm=0.3493, duration=0.38s
Step 17878: loss=2.7550, lr=0.000244, tokens/sec=1378049.98, grad_norm=0.3311, duration=0.38s
Step 17879: loss=2.6867, lr=0.000244, tokens/sec=1375317.87, grad_norm=0.3635, duration=0.38s
Step 17880: loss=2.7596, lr=0.000244, tokens/sec=1375389.27, grad_norm=0.3371, duration=0.38s
Step 17881: loss=2.7897, lr=0.000244, tokens/sec=1372763.67, grad_norm=0.3482, duration=0.38s
Step 17882: loss=2.8831, lr=0.000244, tokens/sec=1374874.18, grad_norm=0.3358, duration=0.38s
Step 17883: loss=2.7450, lr=0.000244, tokens/sec=1374994.53, grad_norm=0.3294, duration=0.38s
Step 17884: loss=2.8553, lr=0.000244, tokens/sec=1377140.37, grad_norm=0.3274, duration=0.38s
Step 17885: loss=2.8293, lr=0.000244, tokens/sec=1377944.63, grad_norm=0.3215, duration=0.38s
Step 17886: loss=2.7931, lr=0.000244, tokens/sec=1376120.87, grad_norm=0.3088, duration=0.38s
Step 17887: loss=2.7902, lr=0.000244, tokens/sec=1380906.55, grad_norm=0.3266, duration=0.38s
Step 17888: loss=2.8170, lr=0.000244, tokens/sec=1375562.20, grad_norm=0.3163, duration=0.38s
Step 17889: loss=2.8106, lr=0.000244, tokens/sec=1373633.18, grad_norm=0.3142, duration=0.38s
Step 17890: loss=2.7649, lr=0.000244, tokens/sec=1371900.39, grad_norm=0.3196, duration=0.38s
Step 17891: loss=2.7785, lr=0.000244, tokens/sec=1377298.22, grad_norm=0.3288, duration=0.38s
Step 17892: loss=2.8017, lr=0.000244, tokens/sec=1379794.88, grad_norm=0.3254, duration=0.38s
Step 17893: loss=2.8452, lr=0.000244, tokens/sec=1375682.68, grad_norm=0.3140, duration=0.38s
Step 17894: loss=2.7692, lr=0.000244, tokens/sec=1374149.92, grad_norm=0.3164, duration=0.38s
Step 17895: loss=2.7104, lr=0.000244, tokens/sec=1378229.63, grad_norm=0.3262, duration=0.38s
Step 17896: loss=2.7657, lr=0.000244, tokens/sec=1377507.01, grad_norm=0.3388, duration=0.38s
Step 17897: loss=2.7103, lr=0.000244, tokens/sec=1374097.54, grad_norm=0.3354, duration=0.38s
Step 17898: loss=2.7718, lr=0.000244, tokens/sec=1373603.15, grad_norm=0.3421, duration=0.38s
Step 17899: loss=2.7735, lr=0.000244, tokens/sec=1375720.54, grad_norm=0.3289, duration=0.38s
Step 17900/19073 (93.8%), Elapsed time: 7082.70s, Steps per hour: 9098.22, Estimated hours remaining: 0.13
Step 17900: loss=2.7476, lr=0.000244, tokens/sec=1375912.50, grad_norm=0.3156, duration=0.38s
Step 17901: loss=2.7594, lr=0.000244, tokens/sec=1375187.14, grad_norm=0.3483, duration=0.38s
Step 17902: loss=2.7274, lr=0.000244, tokens/sec=1378697.10, grad_norm=0.3285, duration=0.38s
Step 17903: loss=2.7862, lr=0.000244, tokens/sec=1374920.60, grad_norm=0.3330, duration=0.38s
Step 17904: loss=2.7766, lr=0.000244, tokens/sec=1377596.75, grad_norm=0.3354, duration=0.38s
Step 17905: loss=2.7598, lr=0.000244, tokens/sec=1378371.30, grad_norm=0.3427, duration=0.38s
Step 17906: loss=2.8048, lr=0.000244, tokens/sec=1376606.73, grad_norm=0.3146, duration=0.38s
Step 17907: loss=2.8505, lr=0.000244, tokens/sec=1376088.14, grad_norm=0.3235, duration=0.38s
Step 17908: loss=2.8602, lr=0.000244, tokens/sec=1376092.45, grad_norm=0.3293, duration=0.38s
Step 17909: loss=2.8533, lr=0.000244, tokens/sec=1377660.62, grad_norm=0.3591, duration=0.38s
Step 17910: loss=2.8109, lr=0.000244, tokens/sec=1378793.06, grad_norm=0.3313, duration=0.38s
Step 17911: loss=2.8304, lr=0.000244, tokens/sec=1379115.59, grad_norm=0.3242, duration=0.38s
Step 17912: loss=2.8477, lr=0.000244, tokens/sec=1374270.15, grad_norm=0.3248, duration=0.38s
Step 17913: loss=2.8361, lr=0.000244, tokens/sec=1376579.16, grad_norm=0.3290, duration=0.38s
Step 17914: loss=2.7998, lr=0.000244, tokens/sec=1374893.09, grad_norm=0.3225, duration=0.38s
Step 17915: loss=2.7709, lr=0.000244, tokens/sec=1376379.26, grad_norm=0.3519, duration=0.38s
Step 17916: loss=2.7962, lr=0.000244, tokens/sec=1376621.38, grad_norm=0.3443, duration=0.38s
Step 17917: loss=2.8025, lr=0.000244, tokens/sec=1375767.02, grad_norm=0.3242, duration=0.38s
Step 17918: loss=2.7930, lr=0.000244, tokens/sec=1375857.40, grad_norm=0.3176, duration=0.38s
Step 17919: loss=2.8137, lr=0.000244, tokens/sec=1377678.74, grad_norm=0.3297, duration=0.38s
Step 17920: loss=2.7835, lr=0.000244, tokens/sec=1371979.14, grad_norm=0.3303, duration=0.38s
Step 17921: loss=2.7070, lr=0.000244, tokens/sec=1379333.58, grad_norm=0.3326, duration=0.38s
Step 17922: loss=2.7746, lr=0.000244, tokens/sec=1375593.18, grad_norm=0.3217, duration=0.38s
Step 17923: loss=2.8024, lr=0.000244, tokens/sec=1374563.07, grad_norm=0.3172, duration=0.38s
Step 17924: loss=2.8138, lr=0.000244, tokens/sec=1379575.88, grad_norm=0.3279, duration=0.38s
Step 17925: loss=2.8453, lr=0.000244, tokens/sec=1376826.52, grad_norm=0.3278, duration=0.38s
Step 17926: loss=2.8147, lr=0.000244, tokens/sec=1374718.61, grad_norm=0.3161, duration=0.38s
Step 17927: loss=2.8692, lr=0.000244, tokens/sec=1373821.97, grad_norm=0.3234, duration=0.38s
Step 17928: loss=2.8298, lr=0.000244, tokens/sec=1378338.47, grad_norm=0.3380, duration=0.38s
Step 17929: loss=2.8134, lr=0.000244, tokens/sec=1376501.60, grad_norm=0.3163, duration=0.38s
Step 17930: loss=2.8223, lr=0.000244, tokens/sec=1376725.67, grad_norm=0.3082, duration=0.38s
Step 17931: loss=2.8105, lr=0.000244, tokens/sec=1376948.08, grad_norm=0.3030, duration=0.38s
Step 17932: loss=2.7955, lr=0.000243, tokens/sec=1378059.48, grad_norm=0.3105, duration=0.38s
Step 17933: loss=2.8190, lr=0.000243, tokens/sec=1373644.33, grad_norm=0.3016, duration=0.38s
Step 17934: loss=2.8238, lr=0.000243, tokens/sec=1374655.01, grad_norm=0.2978, duration=0.38s
Step 17935: loss=2.8062, lr=0.000243, tokens/sec=1377393.11, grad_norm=0.3020, duration=0.38s
Step 17936: loss=2.8025, lr=0.000243, tokens/sec=1373695.82, grad_norm=0.3064, duration=0.38s
Step 17937: loss=2.7887, lr=0.000243, tokens/sec=1377421.58, grad_norm=0.3026, duration=0.38s
Step 17938: loss=2.8230, lr=0.000243, tokens/sec=1375882.37, grad_norm=0.2980, duration=0.38s
Step 17939: loss=2.7874, lr=0.000243, tokens/sec=1373473.60, grad_norm=0.2902, duration=0.38s
Step 17940: loss=2.7916, lr=0.000243, tokens/sec=1378267.64, grad_norm=0.2943, duration=0.38s
Step 17941: loss=2.7345, lr=0.000243, tokens/sec=1377776.28, grad_norm=0.3046, duration=0.38s
Step 17942: loss=2.7882, lr=0.000243, tokens/sec=1377450.92, grad_norm=0.2980, duration=0.38s
Step 17943: loss=2.7548, lr=0.000243, tokens/sec=1371754.91, grad_norm=0.2953, duration=0.38s
Step 17944: loss=2.7467, lr=0.000243, tokens/sec=1366882.79, grad_norm=0.2976, duration=0.38s
Step 17945: loss=2.6821, lr=0.000243, tokens/sec=1373450.44, grad_norm=0.2983, duration=0.38s
Step 17946: loss=2.7287, lr=0.000243, tokens/sec=1375744.64, grad_norm=0.3027, duration=0.38s
Step 17947: loss=2.7495, lr=0.000243, tokens/sec=1375777.35, grad_norm=0.3015, duration=0.38s
Step 17948: loss=2.7067, lr=0.000243, tokens/sec=1374833.78, grad_norm=0.2949, duration=0.38s
Step 17949: loss=2.7285, lr=0.000243, tokens/sec=1375970.18, grad_norm=0.2986, duration=0.38s
Step 17950: loss=2.6798, lr=0.000243, tokens/sec=1374477.16, grad_norm=0.2989, duration=0.38s
Step 17951: loss=2.7512, lr=0.000243, tokens/sec=1374988.51, grad_norm=0.3043, duration=0.38s
Step 17952: loss=2.7789, lr=0.000243, tokens/sec=1375961.57, grad_norm=0.3082, duration=0.38s
Step 17953: loss=2.8012, lr=0.000243, tokens/sec=1379213.33, grad_norm=0.3120, duration=0.38s
Step 17954: loss=2.8308, lr=0.000243, tokens/sec=1375825.55, grad_norm=0.3077, duration=0.38s
Step 17955: loss=2.8205, lr=0.000243, tokens/sec=1376039.06, grad_norm=0.3005, duration=0.38s
Step 17956: loss=2.8431, lr=0.000243, tokens/sec=1372740.53, grad_norm=0.3143, duration=0.38s
Step 17957: loss=2.8302, lr=0.000243, tokens/sec=1375723.99, grad_norm=0.3156, duration=0.38s
Step 17958: loss=2.7854, lr=0.000243, tokens/sec=1377042.92, grad_norm=0.3202, duration=0.38s
Step 17959: loss=2.7914, lr=0.000243, tokens/sec=1378117.34, grad_norm=0.3085, duration=0.38s
Step 17960: loss=2.7829, lr=0.000243, tokens/sec=1379190.84, grad_norm=0.3247, duration=0.38s
Step 17961: loss=2.8423, lr=0.000243, tokens/sec=1375689.56, grad_norm=0.3196, duration=0.38s
Step 17962: loss=2.8588, lr=0.000243, tokens/sec=1377986.94, grad_norm=0.3039, duration=0.38s
Step 17963: loss=2.8537, lr=0.000243, tokens/sec=1376645.51, grad_norm=0.3044, duration=0.38s
Step 17964: loss=2.8328, lr=0.000243, tokens/sec=1377821.17, grad_norm=0.3127, duration=0.38s
Step 17965: loss=2.7986, lr=0.000243, tokens/sec=1374679.94, grad_norm=0.3160, duration=0.38s
Step 17966: loss=2.8390, lr=0.000243, tokens/sec=1372323.33, grad_norm=0.3103, duration=0.38s
Step 17967: loss=2.7804, lr=0.000243, tokens/sec=1376614.49, grad_norm=0.3001, duration=0.38s
Step 17968: loss=2.8268, lr=0.000243, tokens/sec=1372822.80, grad_norm=0.3009, duration=0.38s
Step 17969: loss=2.7804, lr=0.000243, tokens/sec=1373016.52, grad_norm=0.3002, duration=0.38s
Step 17970: loss=2.8470, lr=0.000243, tokens/sec=1375103.73, grad_norm=0.3102, duration=0.38s
Step 17971: loss=2.7891, lr=0.000243, tokens/sec=1376648.10, grad_norm=0.3079, duration=0.38s
Step 17972: loss=2.8266, lr=0.000243, tokens/sec=1377566.55, grad_norm=0.3091, duration=0.38s
Step 17973: loss=2.7012, lr=0.000243, tokens/sec=1372524.62, grad_norm=0.3557, duration=0.38s
Step 17974: loss=2.8360, lr=0.000243, tokens/sec=1377487.16, grad_norm=0.3148, duration=0.38s
Step 17975: loss=2.7832, lr=0.000243, tokens/sec=1378776.63, grad_norm=0.3134, duration=0.38s
Step 17976: loss=2.8228, lr=0.000243, tokens/sec=1373682.09, grad_norm=0.3177, duration=0.38s
Step 17977: loss=2.8116, lr=0.000243, tokens/sec=1375773.05, grad_norm=0.3128, duration=0.38s
Step 17978: loss=2.8137, lr=0.000243, tokens/sec=1376033.03, grad_norm=0.3224, duration=0.38s
Step 17979: loss=2.8003, lr=0.000243, tokens/sec=1376557.61, grad_norm=0.3109, duration=0.38s
Step 17980: loss=2.7989, lr=0.000243, tokens/sec=1377532.03, grad_norm=0.2927, duration=0.38s
Step 17981: loss=2.8080, lr=0.000243, tokens/sec=1376711.88, grad_norm=0.3020, duration=0.38s
Step 17982: loss=2.8397, lr=0.000243, tokens/sec=1375862.57, grad_norm=0.3136, duration=0.38s
Step 17983: loss=2.7829, lr=0.000243, tokens/sec=1369314.51, grad_norm=0.3181, duration=0.38s
Step 17984: loss=2.7973, lr=0.000243, tokens/sec=1371745.50, grad_norm=0.3141, duration=0.38s
Step 17985: loss=2.8080, lr=0.000243, tokens/sec=1375574.25, grad_norm=0.3022, duration=0.38s
Step 17986: loss=2.8113, lr=0.000243, tokens/sec=1377614.88, grad_norm=0.3031, duration=0.38s
Step 17987: loss=2.8163, lr=0.000243, tokens/sec=1375231.00, grad_norm=0.3156, duration=0.38s
Step 17988: loss=2.7861, lr=0.000243, tokens/sec=1373511.35, grad_norm=0.3226, duration=0.38s
Step 17989: loss=2.7417, lr=0.000243, tokens/sec=1376988.60, grad_norm=0.3177, duration=0.38s
Step 17990: loss=2.7442, lr=0.000243, tokens/sec=1376293.98, grad_norm=0.3105, duration=0.38s
Step 17991: loss=2.6901, lr=0.000243, tokens/sec=1374210.03, grad_norm=0.3124, duration=0.38s
Step 17992: loss=2.7101, lr=0.000243, tokens/sec=1372486.92, grad_norm=0.3074, duration=0.38s
Step 17993: loss=2.7391, lr=0.000243, tokens/sec=1377627.82, grad_norm=0.3223, duration=0.38s
Step 17994: loss=2.7963, lr=0.000243, tokens/sec=1374858.71, grad_norm=0.3141, duration=0.38s
Step 17995: loss=2.7404, lr=0.000243, tokens/sec=1375618.99, grad_norm=0.3046, duration=0.38s
Step 17996: loss=2.7579, lr=0.000243, tokens/sec=1374902.55, grad_norm=0.3025, duration=0.38s
Step 17997: loss=2.7346, lr=0.000243, tokens/sec=1377444.88, grad_norm=0.3090, duration=0.38s
Step 17998: loss=2.6994, lr=0.000243, tokens/sec=1376180.29, grad_norm=0.3147, duration=0.38s
Step 17999: loss=2.7842, lr=0.000243, tokens/sec=1375675.79, grad_norm=0.3337, duration=0.38s
Step 18000/19073 (94.4%), Elapsed time: 7120.89s, Steps per hour: 9099.99, Estimated hours remaining: 0.12
Validation loss at step 18000: 3.8518683910369873
Step 18000: loss=2.7933, lr=0.000243, tokens/sec=153412.18, grad_norm=0.3362, duration=3.42s
Step 18001: loss=2.8391, lr=0.000243, tokens/sec=1374883.63, grad_norm=0.3298, duration=0.38s
Step 18002: loss=2.8381, lr=0.000243, tokens/sec=1377790.96, grad_norm=0.3400, duration=0.38s
Step 18003: loss=2.8199, lr=0.000243, tokens/sec=1378433.51, grad_norm=0.3355, duration=0.38s
Step 18004: loss=2.8249, lr=0.000243, tokens/sec=1380121.35, grad_norm=0.3508, duration=0.38s
Step 18005: loss=2.7873, lr=0.000243, tokens/sec=1375884.95, grad_norm=0.3496, duration=0.38s
Step 18006: loss=2.8456, lr=0.000243, tokens/sec=1379965.46, grad_norm=0.3443, duration=0.38s
Step 18007: loss=2.8790, lr=0.000243, tokens/sec=1375501.97, grad_norm=0.3319, duration=0.38s
Step 18008: loss=2.9221, lr=0.000243, tokens/sec=1376888.59, grad_norm=0.3459, duration=0.38s
Step 18009: loss=2.8663, lr=0.000243, tokens/sec=1372164.06, grad_norm=0.3591, duration=0.38s
Step 18010: loss=2.8495, lr=0.000243, tokens/sec=1373190.57, grad_norm=0.3484, duration=0.38s
Step 18011: loss=2.8082, lr=0.000243, tokens/sec=1374967.88, grad_norm=0.3141, duration=0.38s
Step 18012: loss=2.8436, lr=0.000243, tokens/sec=1376967.04, grad_norm=0.3172, duration=0.38s
Step 18013: loss=2.7849, lr=0.000243, tokens/sec=1375903.89, grad_norm=0.3236, duration=0.38s
Step 18014: loss=2.7627, lr=0.000243, tokens/sec=1373459.02, grad_norm=0.3626, duration=0.38s
Step 18015: loss=2.8535, lr=0.000243, tokens/sec=1369601.07, grad_norm=0.3535, duration=0.38s
Step 18016: loss=2.7623, lr=0.000243, tokens/sec=1377055.00, grad_norm=0.3233, duration=0.38s
Step 18017: loss=2.8732, lr=0.000243, tokens/sec=1374108.70, grad_norm=0.3323, duration=0.38s
Step 18018: loss=2.8672, lr=0.000243, tokens/sec=1372068.17, grad_norm=0.3657, duration=0.38s
Step 18019: loss=2.8283, lr=0.000243, tokens/sec=1373613.44, grad_norm=0.3647, duration=0.38s
Step 18020: loss=2.8000, lr=0.000243, tokens/sec=1372433.82, grad_norm=0.3394, duration=0.38s
Step 18021: loss=2.8082, lr=0.000243, tokens/sec=1365821.57, grad_norm=0.3313, duration=0.38s
Step 18022: loss=2.8714, lr=0.000243, tokens/sec=1370309.44, grad_norm=0.3263, duration=0.38s
Step 18023: loss=2.8315, lr=0.000243, tokens/sec=1372327.61, grad_norm=0.3507, duration=0.38s
Step 18024: loss=2.7453, lr=0.000243, tokens/sec=1372183.75, grad_norm=0.3475, duration=0.38s
Step 18025: loss=2.8146, lr=0.000243, tokens/sec=1371076.67, grad_norm=0.3419, duration=0.38s
Step 18026: loss=2.8078, lr=0.000243, tokens/sec=1371683.89, grad_norm=0.3066, duration=0.38s
Step 18027: loss=2.8418, lr=0.000243, tokens/sec=1373991.94, grad_norm=0.3102, duration=0.38s
Step 18028: loss=2.8667, lr=0.000243, tokens/sec=1377456.10, grad_norm=0.3296, duration=0.38s
Step 18029: loss=2.7832, lr=0.000243, tokens/sec=1371951.75, grad_norm=0.3505, duration=0.38s
Step 18030: loss=2.7844, lr=0.000243, tokens/sec=1375250.79, grad_norm=0.3378, duration=0.38s
Step 18031: loss=2.7913, lr=0.000243, tokens/sec=1374818.31, grad_norm=0.3268, duration=0.38s
Step 18032: loss=2.7771, lr=0.000243, tokens/sec=1373198.29, grad_norm=0.3073, duration=0.38s
Step 18033: loss=2.7814, lr=0.000243, tokens/sec=1375366.04, grad_norm=0.3027, duration=0.38s
Step 18034: loss=2.7532, lr=0.000243, tokens/sec=1374789.94, grad_norm=0.3198, duration=0.38s
Step 18035: loss=2.7305, lr=0.000243, tokens/sec=1373452.15, grad_norm=0.3304, duration=0.38s
Step 18036: loss=2.7420, lr=0.000243, tokens/sec=1374320.82, grad_norm=0.3273, duration=0.38s
Step 18037: loss=2.8048, lr=0.000243, tokens/sec=1372734.53, grad_norm=0.3159, duration=0.38s
Step 18038: loss=2.7294, lr=0.000243, tokens/sec=1373888.92, grad_norm=0.3154, duration=0.38s
Step 18039: loss=2.7266, lr=0.000243, tokens/sec=1374301.06, grad_norm=0.3107, duration=0.38s
Step 18040: loss=2.6967, lr=0.000243, tokens/sec=1375004.85, grad_norm=0.3037, duration=0.38s
Step 18041: loss=2.7187, lr=0.000243, tokens/sec=1375878.06, grad_norm=0.3153, duration=0.38s
Step 18042: loss=2.7651, lr=0.000243, tokens/sec=1376819.62, grad_norm=0.3186, duration=0.38s
Step 18043: loss=2.7502, lr=0.000243, tokens/sec=1372325.90, grad_norm=0.3315, duration=0.38s
Step 18044: loss=2.6994, lr=0.000243, tokens/sec=1375863.43, grad_norm=0.3128, duration=0.38s
Step 18045: loss=2.8018, lr=0.000243, tokens/sec=1375927.99, grad_norm=0.3123, duration=0.38s
Step 18046: loss=2.7976, lr=0.000243, tokens/sec=1375879.78, grad_norm=0.3081, duration=0.38s
Step 18047: loss=2.8368, lr=0.000243, tokens/sec=1372064.74, grad_norm=0.3121, duration=0.38s
Step 18048: loss=2.8057, lr=0.000243, tokens/sec=1376555.03, grad_norm=0.3304, duration=0.38s
Step 18049: loss=2.8303, lr=0.000243, tokens/sec=1374416.16, grad_norm=0.3360, duration=0.38s
Step 18050: loss=2.7993, lr=0.000243, tokens/sec=1376751.52, grad_norm=0.3300, duration=0.38s
Step 18051: loss=2.8358, lr=0.000243, tokens/sec=1377998.17, grad_norm=0.3075, duration=0.38s
Step 18052: loss=2.8219, lr=0.000243, tokens/sec=1377276.65, grad_norm=0.3220, duration=0.38s
Step 18053: loss=2.8653, lr=0.000243, tokens/sec=1375000.55, grad_norm=0.3387, duration=0.38s
Step 18054: loss=2.8489, lr=0.000243, tokens/sec=1377702.91, grad_norm=0.3273, duration=0.38s
Step 18055: loss=2.8656, lr=0.000243, tokens/sec=1375132.11, grad_norm=0.3107, duration=0.38s
Step 18056: loss=2.9010, lr=0.000243, tokens/sec=1375506.27, grad_norm=0.3150, duration=0.38s
Step 18057: loss=2.8696, lr=0.000243, tokens/sec=1374924.04, grad_norm=0.3211, duration=0.38s
Step 18058: loss=2.8496, lr=0.000243, tokens/sec=1379772.37, grad_norm=0.3209, duration=0.38s
Step 18059: loss=2.8504, lr=0.000243, tokens/sec=1376010.65, grad_norm=0.3260, duration=0.38s
Step 18060: loss=2.8544, lr=0.000243, tokens/sec=1374157.65, grad_norm=0.3118, duration=0.38s
Step 18061: loss=2.8133, lr=0.000243, tokens/sec=1373606.58, grad_norm=0.3100, duration=0.38s
Step 18062: loss=2.8432, lr=0.000243, tokens/sec=1377503.55, grad_norm=0.3271, duration=0.38s
Step 18063: loss=2.7312, lr=0.000243, tokens/sec=1375884.09, grad_norm=0.3113, duration=0.38s
Step 18064: loss=2.8334, lr=0.000243, tokens/sec=1375733.45, grad_norm=0.3032, duration=0.38s
Step 18065: loss=2.7818, lr=0.000243, tokens/sec=1379278.21, grad_norm=0.3485, duration=0.38s
Step 18066: loss=2.7722, lr=0.000243, tokens/sec=1375372.93, grad_norm=0.3273, duration=0.38s
Step 18067: loss=2.8362, lr=0.000243, tokens/sec=1378034.44, grad_norm=0.3274, duration=0.38s
Step 18068: loss=2.7164, lr=0.000243, tokens/sec=1377117.95, grad_norm=0.3345, duration=0.38s
Step 18069: loss=2.7142, lr=0.000243, tokens/sec=1375551.88, grad_norm=0.4033, duration=0.38s
Step 18070: loss=2.7437, lr=0.000243, tokens/sec=1375660.30, grad_norm=0.3167, duration=0.38s
Step 18071: loss=2.8065, lr=0.000243, tokens/sec=1373057.67, grad_norm=0.3246, duration=0.38s
Step 18072: loss=2.8935, lr=0.000243, tokens/sec=1375483.04, grad_norm=0.3245, duration=0.38s
Step 18073: loss=2.7431, lr=0.000243, tokens/sec=1376450.77, grad_norm=0.3177, duration=0.38s
Step 18074: loss=2.8374, lr=0.000243, tokens/sec=1378078.48, grad_norm=0.3185, duration=0.38s
Step 18075: loss=2.8512, lr=0.000243, tokens/sec=1375348.84, grad_norm=0.3150, duration=0.38s
Step 18076: loss=2.7499, lr=0.000243, tokens/sec=1371938.05, grad_norm=0.3153, duration=0.38s
Step 18077: loss=2.7935, lr=0.000243, tokens/sec=1371849.90, grad_norm=0.3095, duration=0.38s
Step 18078: loss=2.8263, lr=0.000243, tokens/sec=1373967.90, grad_norm=0.2901, duration=0.38s
Step 18079: loss=2.8001, lr=0.000243, tokens/sec=1375643.95, grad_norm=0.3027, duration=0.38s
Step 18080: loss=2.7424, lr=0.000243, tokens/sec=1375979.65, grad_norm=0.3153, duration=0.38s
Step 18081: loss=2.7961, lr=0.000243, tokens/sec=1372657.41, grad_norm=0.3240, duration=0.38s
Step 18082: loss=2.8144, lr=0.000243, tokens/sec=1374801.12, grad_norm=0.3256, duration=0.38s
Step 18083: loss=2.8222, lr=0.000243, tokens/sec=1374286.46, grad_norm=0.3025, duration=0.38s
Step 18084: loss=2.7593, lr=0.000243, tokens/sec=1379706.58, grad_norm=0.3003, duration=0.38s
Step 18085: loss=2.7205, lr=0.000243, tokens/sec=1376213.02, grad_norm=0.3097, duration=0.38s
Step 18086: loss=2.7547, lr=0.000243, tokens/sec=1374646.42, grad_norm=0.3182, duration=0.38s
Step 18087: loss=2.7743, lr=0.000243, tokens/sec=1373732.72, grad_norm=0.3384, duration=0.38s
Step 18088: loss=2.7563, lr=0.000243, tokens/sec=1377226.62, grad_norm=0.3457, duration=0.38s
Step 18089: loss=2.7320, lr=0.000243, tokens/sec=1377663.21, grad_norm=0.3244, duration=0.38s
Step 18090: loss=2.7534, lr=0.000243, tokens/sec=1372239.41, grad_norm=0.3280, duration=0.38s
Step 18091: loss=2.7242, lr=0.000243, tokens/sec=1372096.42, grad_norm=0.3331, duration=0.38s
Step 18092: loss=2.7674, lr=0.000243, tokens/sec=1373342.36, grad_norm=0.3295, duration=0.38s
Step 18093: loss=2.7969, lr=0.000243, tokens/sec=1374922.32, grad_norm=0.3149, duration=0.38s
Step 18094: loss=2.7576, lr=0.000243, tokens/sec=1376075.23, grad_norm=0.3268, duration=0.38s
Step 18095: loss=2.7866, lr=0.000243, tokens/sec=1375882.37, grad_norm=0.3347, duration=0.38s
Step 18096: loss=2.7876, lr=0.000243, tokens/sec=1376010.65, grad_norm=0.3209, duration=0.38s
Step 18097: loss=2.8181, lr=0.000243, tokens/sec=1377902.32, grad_norm=0.3273, duration=0.38s
Step 18098: loss=2.8918, lr=0.000243, tokens/sec=1380611.78, grad_norm=0.3174, duration=0.38s
Step 18099: loss=2.8189, lr=0.000243, tokens/sec=1373930.13, grad_norm=0.3149, duration=0.38s
Step 18100/19073 (94.9%), Elapsed time: 7162.14s, Steps per hour: 9097.83, Estimated hours remaining: 0.11
Step 18100: loss=2.8023, lr=0.000243, tokens/sec=1375267.99, grad_norm=0.3208, duration=0.38s
Step 18101: loss=2.8520, lr=0.000243, tokens/sec=1379867.61, grad_norm=0.3350, duration=0.38s
Step 18102: loss=2.8287, lr=0.000243, tokens/sec=1377279.24, grad_norm=0.3266, duration=0.38s
Step 18103: loss=2.8280, lr=0.000243, tokens/sec=1374818.31, grad_norm=0.3151, duration=0.38s
Step 18104: loss=2.7750, lr=0.000243, tokens/sec=1375451.21, grad_norm=0.3186, duration=0.38s
Step 18105: loss=2.7663, lr=0.000243, tokens/sec=1375343.68, grad_norm=0.3427, duration=0.38s
Step 18106: loss=2.8157, lr=0.000243, tokens/sec=1367977.14, grad_norm=0.3259, duration=0.38s
Step 18107: loss=2.7954, lr=0.000243, tokens/sec=1372547.75, grad_norm=0.3254, duration=0.38s
Step 18108: loss=2.8126, lr=0.000243, tokens/sec=1375581.99, grad_norm=0.3209, duration=0.38s
Step 18109: loss=2.7839, lr=0.000242, tokens/sec=1375406.48, grad_norm=0.3223, duration=0.38s
Step 18110: loss=2.7790, lr=0.000242, tokens/sec=1379193.44, grad_norm=0.3163, duration=0.38s
Step 18111: loss=2.7153, lr=0.000242, tokens/sec=1376585.19, grad_norm=0.3222, duration=0.38s
Step 18112: loss=2.7587, lr=0.000242, tokens/sec=1373138.26, grad_norm=0.3118, duration=0.38s
Step 18113: loss=2.8235, lr=0.000242, tokens/sec=1375858.26, grad_norm=0.3262, duration=0.38s
Step 18114: loss=2.8076, lr=0.000242, tokens/sec=1377151.58, grad_norm=0.3335, duration=0.38s
Step 18115: loss=2.8661, lr=0.000242, tokens/sec=1373734.44, grad_norm=0.3176, duration=0.38s
Step 18116: loss=2.7911, lr=0.000242, tokens/sec=1377280.96, grad_norm=0.3042, duration=0.38s
Step 18117: loss=2.8590, lr=0.000242, tokens/sec=1373507.91, grad_norm=0.3149, duration=0.38s
Step 18118: loss=2.8248, lr=0.000242, tokens/sec=1381467.83, grad_norm=0.3156, duration=0.38s
Step 18119: loss=2.8165, lr=0.000242, tokens/sec=1375598.34, grad_norm=0.3147, duration=0.38s
Step 18120: loss=2.8023, lr=0.000242, tokens/sec=1374021.13, grad_norm=0.3087, duration=0.38s
Step 18121: loss=2.8153, lr=0.000242, tokens/sec=1375840.19, grad_norm=0.3063, duration=0.38s
Step 18122: loss=2.7980, lr=0.000242, tokens/sec=1374398.12, grad_norm=0.3039, duration=0.38s
Step 18123: loss=2.8200, lr=0.000242, tokens/sec=1374854.41, grad_norm=0.3068, duration=0.38s
Step 18124: loss=2.8095, lr=0.000242, tokens/sec=1371254.50, grad_norm=0.2968, duration=0.38s
Step 18125: loss=2.7939, lr=0.000242, tokens/sec=1370620.33, grad_norm=0.2959, duration=0.38s
Step 18126: loss=2.8286, lr=0.000242, tokens/sec=1371079.24, grad_norm=0.2973, duration=0.38s
Step 18127: loss=2.7828, lr=0.000242, tokens/sec=1372674.55, grad_norm=0.2977, duration=0.38s
Step 18128: loss=2.7966, lr=0.000242, tokens/sec=1375112.33, grad_norm=0.3039, duration=0.38s
Step 18129: loss=2.7768, lr=0.000242, tokens/sec=1374288.18, grad_norm=0.3060, duration=0.38s
Step 18130: loss=2.8015, lr=0.000242, tokens/sec=1375258.53, grad_norm=0.3043, duration=0.38s
Step 18131: loss=2.7222, lr=0.000242, tokens/sec=1374533.86, grad_norm=0.2974, duration=0.38s
Step 18132: loss=2.8013, lr=0.000242, tokens/sec=1374769.31, grad_norm=0.2924, duration=0.38s
Step 18133: loss=2.7342, lr=0.000242, tokens/sec=1374169.67, grad_norm=0.3015, duration=0.38s
Step 18134: loss=2.7249, lr=0.000242, tokens/sec=1375521.76, grad_norm=0.3023, duration=0.38s
Step 18135: loss=2.6940, lr=0.000242, tokens/sec=1375698.17, grad_norm=0.2959, duration=0.38s
Step 18136: loss=2.7257, lr=0.000242, tokens/sec=1375553.60, grad_norm=0.2925, duration=0.38s
Step 18137: loss=2.7501, lr=0.000242, tokens/sec=1376278.48, grad_norm=0.2978, duration=0.38s
Step 18138: loss=2.7075, lr=0.000242, tokens/sec=1374008.25, grad_norm=0.2961, duration=0.38s
Step 18139: loss=2.6985, lr=0.000242, tokens/sec=1374928.33, grad_norm=0.2900, duration=0.38s
Step 18140: loss=2.6956, lr=0.000242, tokens/sec=1373619.45, grad_norm=0.2943, duration=0.38s
Step 18141: loss=2.7934, lr=0.000242, tokens/sec=1368508.37, grad_norm=0.2930, duration=0.38s
Step 18142: loss=2.7796, lr=0.000242, tokens/sec=1369908.22, grad_norm=0.3125, duration=0.38s
Step 18143: loss=2.7829, lr=0.000242, tokens/sec=1374205.73, grad_norm=0.3173, duration=0.38s
Step 18144: loss=2.8130, lr=0.000242, tokens/sec=1376038.20, grad_norm=0.3011, duration=0.38s
Step 18145: loss=2.8333, lr=0.000242, tokens/sec=1373032.81, grad_norm=0.2973, duration=0.38s
Step 18146: loss=2.8251, lr=0.000242, tokens/sec=1371607.74, grad_norm=0.3114, duration=0.38s
Step 18147: loss=2.8499, lr=0.000242, tokens/sec=1373499.34, grad_norm=0.3210, duration=0.38s
Step 18148: loss=2.7417, lr=0.000242, tokens/sec=1377252.50, grad_norm=0.3165, duration=0.38s
Step 18149: loss=2.8117, lr=0.000242, tokens/sec=1374671.34, grad_norm=0.3122, duration=0.38s
Step 18150: loss=2.7775, lr=0.000242, tokens/sec=1373630.60, grad_norm=0.3115, duration=0.38s
Step 18151: loss=2.8666, lr=0.000242, tokens/sec=1373425.56, grad_norm=0.3076, duration=0.38s
Step 18152: loss=2.8639, lr=0.000242, tokens/sec=1376196.65, grad_norm=0.3194, duration=0.38s
Step 18153: loss=2.8304, lr=0.000242, tokens/sec=1372961.65, grad_norm=0.3116, duration=0.38s
Step 18154: loss=2.8275, lr=0.000242, tokens/sec=1373646.91, grad_norm=0.3120, duration=0.38s
Step 18155: loss=2.8234, lr=0.000242, tokens/sec=1375152.74, grad_norm=0.3178, duration=0.38s
Step 18156: loss=2.7700, lr=0.000242, tokens/sec=1371487.13, grad_norm=0.3135, duration=0.38s
Step 18157: loss=2.8210, lr=0.000242, tokens/sec=1374350.02, grad_norm=0.3141, duration=0.38s
Step 18158: loss=2.8257, lr=0.000242, tokens/sec=1377389.66, grad_norm=0.3065, duration=0.38s
Step 18159: loss=2.7946, lr=0.000242, tokens/sec=1373458.16, grad_norm=0.3199, duration=0.38s
Step 18160: loss=2.8004, lr=0.000242, tokens/sec=1373693.24, grad_norm=0.3123, duration=0.38s
Step 18161: loss=2.8097, lr=0.000242, tokens/sec=1373402.40, grad_norm=0.3064, duration=0.38s
Step 18162: loss=2.7593, lr=0.000242, tokens/sec=1372251.40, grad_norm=0.3508, duration=0.38s
Step 18163: loss=2.7693, lr=0.000242, tokens/sec=1373939.57, grad_norm=0.3217, duration=0.38s
Step 18164: loss=2.8300, lr=0.000242, tokens/sec=1369441.57, grad_norm=0.3264, duration=0.38s
Step 18165: loss=2.7614, lr=0.000242, tokens/sec=1372317.34, grad_norm=0.3200, duration=0.38s
Step 18166: loss=2.8350, lr=0.000242, tokens/sec=1374709.15, grad_norm=0.3170, duration=0.38s
Step 18167: loss=2.7877, lr=0.000242, tokens/sec=1376103.64, grad_norm=0.3169, duration=0.38s
Step 18168: loss=2.8115, lr=0.000242, tokens/sec=1373551.67, grad_norm=0.3171, duration=0.38s
Step 18169: loss=2.7874, lr=0.000242, tokens/sec=1374292.48, grad_norm=0.3053, duration=0.38s
Step 18170: loss=2.8065, lr=0.000242, tokens/sec=1378097.48, grad_norm=0.2981, duration=0.38s
Step 18171: loss=2.8311, lr=0.000242, tokens/sec=1372447.52, grad_norm=0.3045, duration=0.38s
Step 18172: loss=2.7992, lr=0.000242, tokens/sec=1373262.60, grad_norm=0.3114, duration=0.38s
Step 18173: loss=2.7758, lr=0.000242, tokens/sec=1371155.32, grad_norm=0.3307, duration=0.38s
Step 18174: loss=2.8056, lr=0.000242, tokens/sec=1371048.46, grad_norm=0.3090, duration=0.38s
Step 18175: loss=2.8190, lr=0.000242, tokens/sec=1372547.75, grad_norm=0.3094, duration=0.38s
Step 18176: loss=2.8243, lr=0.000242, tokens/sec=1373715.56, grad_norm=0.3243, duration=0.38s
Step 18177: loss=2.7852, lr=0.000242, tokens/sec=1375240.46, grad_norm=0.3117, duration=0.38s
Step 18178: loss=2.7926, lr=0.000242, tokens/sec=1375420.24, grad_norm=0.3164, duration=0.38s
Step 18179: loss=2.7102, lr=0.000242, tokens/sec=1374259.84, grad_norm=0.3134, duration=0.38s
Step 18180: loss=2.7668, lr=0.000242, tokens/sec=1375966.74, grad_norm=0.3035, duration=0.38s
Step 18181: loss=2.6623, lr=0.000242, tokens/sec=1375419.38, grad_norm=0.3039, duration=0.38s
Step 18182: loss=2.7348, lr=0.000242, tokens/sec=1375808.34, grad_norm=0.3089, duration=0.38s
Step 18183: loss=2.7316, lr=0.000242, tokens/sec=1371300.68, grad_norm=0.3122, duration=0.38s
Step 18184: loss=2.7730, lr=0.000242, tokens/sec=1371093.77, grad_norm=0.3143, duration=0.38s
Step 18185: loss=2.7747, lr=0.000242, tokens/sec=1374630.95, grad_norm=0.3023, duration=0.38s
Step 18186: loss=2.7438, lr=0.000242, tokens/sec=1372079.30, grad_norm=0.2953, duration=0.38s
Step 18187: loss=2.6677, lr=0.000242, tokens/sec=1376169.09, grad_norm=0.3037, duration=0.38s
Step 18188: loss=2.7471, lr=0.000242, tokens/sec=1373935.28, grad_norm=0.3115, duration=0.38s
Step 18189: loss=2.7802, lr=0.000242, tokens/sec=1373350.94, grad_norm=0.3345, duration=0.38s
Step 18190: loss=2.8191, lr=0.000242, tokens/sec=1377425.90, grad_norm=0.3558, duration=0.38s
Step 18191: loss=2.8276, lr=0.000242, tokens/sec=1374646.42, grad_norm=0.3306, duration=0.38s
Step 18192: loss=2.8353, lr=0.000242, tokens/sec=1373607.44, grad_norm=0.3280, duration=0.38s
Step 18193: loss=2.7985, lr=0.000242, tokens/sec=1374220.33, grad_norm=0.3377, duration=0.38s
Step 18194: loss=2.8365, lr=0.000242, tokens/sec=1373654.63, grad_norm=0.3701, duration=0.38s
Step 18195: loss=2.8201, lr=0.000242, tokens/sec=1370487.07, grad_norm=0.3654, duration=0.38s
Step 18196: loss=2.8322, lr=0.000242, tokens/sec=1371037.35, grad_norm=0.3311, duration=0.38s
Step 18197: loss=2.8610, lr=0.000242, tokens/sec=1374295.05, grad_norm=0.3180, duration=0.38s
Step 18198: loss=2.9180, lr=0.000242, tokens/sec=1373519.92, grad_norm=0.3484, duration=0.38s
Step 18199: loss=2.8611, lr=0.000242, tokens/sec=1375331.64, grad_norm=0.3650, duration=0.38s
Step 18200/19073 (95.4%), Elapsed time: 7200.38s, Steps per hour: 9099.51, Estimated hours remaining: 0.10
Step 18200: loss=2.8538, lr=0.000242, tokens/sec=1372596.58, grad_norm=0.3473, duration=0.38s
Step 18201: loss=2.8139, lr=0.000242, tokens/sec=1371954.32, grad_norm=0.3192, duration=0.38s
Step 18202: loss=2.8200, lr=0.000242, tokens/sec=1372665.98, grad_norm=0.3234, duration=0.38s
Step 18203: loss=2.7900, lr=0.000242, tokens/sec=1369950.89, grad_norm=0.3296, duration=0.38s
Step 18204: loss=2.7622, lr=0.000242, tokens/sec=1375502.83, grad_norm=0.3580, duration=0.38s
Step 18205: loss=2.8457, lr=0.000242, tokens/sec=1373479.60, grad_norm=0.3526, duration=0.38s
Step 18206: loss=2.7913, lr=0.000242, tokens/sec=1372438.10, grad_norm=0.3243, duration=0.38s
Step 18207: loss=2.8444, lr=0.000242, tokens/sec=1370427.29, grad_norm=0.3226, duration=0.38s
Step 18208: loss=2.8730, lr=0.000242, tokens/sec=1374737.52, grad_norm=0.3466, duration=0.38s
Step 18209: loss=2.8419, lr=0.000242, tokens/sec=1372570.88, grad_norm=0.3531, duration=0.38s
Step 18210: loss=2.7584, lr=0.000242, tokens/sec=1370460.60, grad_norm=0.3332, duration=0.38s
Step 18211: loss=2.8400, lr=0.000242, tokens/sec=1375254.23, grad_norm=0.3290, duration=0.38s
Step 18212: loss=2.8601, lr=0.000242, tokens/sec=1374169.67, grad_norm=0.3255, duration=0.38s
Step 18213: loss=2.8155, lr=0.000242, tokens/sec=1372557.17, grad_norm=0.3203, duration=0.38s
Step 18214: loss=2.7600, lr=0.000242, tokens/sec=1371210.90, grad_norm=0.3368, duration=0.38s
Step 18215: loss=2.8017, lr=0.000242, tokens/sec=1374082.08, grad_norm=0.3364, duration=0.38s
Step 18216: loss=2.8212, lr=0.000242, tokens/sec=1375575.11, grad_norm=0.3163, duration=0.38s
Step 18217: loss=2.8357, lr=0.000242, tokens/sec=1375792.84, grad_norm=0.3007, duration=0.38s
Step 18218: loss=2.8556, lr=0.000242, tokens/sec=1375316.15, grad_norm=0.3117, duration=0.38s
Step 18219: loss=2.7721, lr=0.000242, tokens/sec=1369352.88, grad_norm=0.3299, duration=0.38s
Step 18220: loss=2.7874, lr=0.000242, tokens/sec=1368874.68, grad_norm=0.3423, duration=0.38s
Step 18221: loss=2.8140, lr=0.000242, tokens/sec=1375559.62, grad_norm=0.3223, duration=0.38s
Step 18222: loss=2.7431, lr=0.000242, tokens/sec=1371035.64, grad_norm=0.3127, duration=0.38s
Step 18223: loss=2.7852, lr=0.000242, tokens/sec=1372379.86, grad_norm=0.3039, duration=0.38s
Step 18224: loss=2.7447, lr=0.000242, tokens/sec=1371005.72, grad_norm=0.3209, duration=0.38s
Step 18225: loss=2.7362, lr=0.000242, tokens/sec=1373641.76, grad_norm=0.3235, duration=0.38s
Step 18226: loss=2.7457, lr=0.000242, tokens/sec=1362965.33, grad_norm=0.3310, duration=0.38s
Step 18227: loss=2.7752, lr=0.000242, tokens/sec=1374312.23, grad_norm=0.3232, duration=0.38s
Step 18228: loss=2.7419, lr=0.000242, tokens/sec=1372193.17, grad_norm=0.3127, duration=0.38s
Step 18229: loss=2.7193, lr=0.000242, tokens/sec=1372325.90, grad_norm=0.3074, duration=0.38s
Step 18230: loss=2.6906, lr=0.000242, tokens/sec=1375425.40, grad_norm=0.3066, duration=0.38s
Step 18231: loss=2.7247, lr=0.000242, tokens/sec=1376660.16, grad_norm=0.3264, duration=0.38s
Step 18232: loss=2.7694, lr=0.000242, tokens/sec=1376334.47, grad_norm=0.3157, duration=0.38s
Step 18233: loss=2.6818, lr=0.000242, tokens/sec=1373099.68, grad_norm=0.3249, duration=0.38s
Step 18234: loss=2.7567, lr=0.000242, tokens/sec=1371655.65, grad_norm=0.3240, duration=0.38s
Step 18235: loss=2.7970, lr=0.000242, tokens/sec=1375491.65, grad_norm=0.3131, duration=0.38s
Step 18236: loss=2.8330, lr=0.000242, tokens/sec=1367327.29, grad_norm=0.3138, duration=0.38s
Step 18237: loss=2.8285, lr=0.000242, tokens/sec=1373062.81, grad_norm=0.3136, duration=0.38s
Step 18238: loss=2.8004, lr=0.000242, tokens/sec=1371161.31, grad_norm=0.3325, duration=0.38s
Step 18239: loss=2.8218, lr=0.000242, tokens/sec=1375696.45, grad_norm=0.3519, duration=0.38s
Step 18240: loss=2.8003, lr=0.000242, tokens/sec=1373636.61, grad_norm=0.3370, duration=0.38s
Step 18241: loss=2.8137, lr=0.000242, tokens/sec=1374801.98, grad_norm=0.3172, duration=0.38s
Step 18242: loss=2.7947, lr=0.000242, tokens/sec=1372968.51, grad_norm=0.3116, duration=0.38s
Step 18243: loss=2.9104, lr=0.000242, tokens/sec=1371654.80, grad_norm=0.3639, duration=0.38s
Step 18244: loss=2.8450, lr=0.000242, tokens/sec=1367330.69, grad_norm=0.3789, duration=0.38s
Step 18245: loss=2.8570, lr=0.000242, tokens/sec=1370253.94, grad_norm=0.3445, duration=0.38s
Step 18246: loss=2.9271, lr=0.000242, tokens/sec=1374501.21, grad_norm=0.3280, duration=0.38s
Step 18247: loss=2.8350, lr=0.000242, tokens/sec=1370407.64, grad_norm=0.3076, duration=0.38s
Step 18248: loss=2.8673, lr=0.000242, tokens/sec=1373194.86, grad_norm=0.3323, duration=0.38s
Step 18249: loss=2.8381, lr=0.000242, tokens/sec=1377922.18, grad_norm=0.3578, duration=0.38s
Validation loss at step 18250: 3.8557863235473633
Step 18250: loss=2.8398, lr=0.000242, tokens/sec=152540.36, grad_norm=0.3339, duration=3.44s
Step 18251: loss=2.8303, lr=0.000242, tokens/sec=1375247.35, grad_norm=0.3266, duration=0.38s
Step 18252: loss=2.7868, lr=0.000242, tokens/sec=1374980.78, grad_norm=0.3167, duration=0.38s
Step 18253: loss=2.7608, lr=0.000242, tokens/sec=1373161.41, grad_norm=0.3239, duration=0.38s
Step 18254: loss=2.8002, lr=0.000242, tokens/sec=1368740.06, grad_norm=0.3203, duration=0.38s
Step 18255: loss=2.8154, lr=0.000242, tokens/sec=1375071.05, grad_norm=0.3636, duration=0.38s
Step 18256: loss=2.7674, lr=0.000242, tokens/sec=1371665.06, grad_norm=0.3129, duration=0.38s
Step 18257: loss=2.7972, lr=0.000242, tokens/sec=1373191.43, grad_norm=0.3377, duration=0.38s
Step 18258: loss=2.7439, lr=0.000242, tokens/sec=1374005.67, grad_norm=0.3388, duration=0.38s
Step 18259: loss=2.6967, lr=0.000242, tokens/sec=1374227.20, grad_norm=0.3522, duration=0.38s
Step 18260: loss=2.7624, lr=0.000242, tokens/sec=1374282.17, grad_norm=0.3303, duration=0.38s
Step 18261: loss=2.8207, lr=0.000242, tokens/sec=1366898.08, grad_norm=0.3378, duration=0.38s
Step 18262: loss=2.8907, lr=0.000242, tokens/sec=1370155.75, grad_norm=0.3338, duration=0.38s
Step 18263: loss=2.7254, lr=0.000242, tokens/sec=1370774.12, grad_norm=0.3216, duration=0.38s
Step 18264: loss=2.8610, lr=0.000242, tokens/sec=1375138.99, grad_norm=0.3140, duration=0.38s
Step 18265: loss=2.8103, lr=0.000242, tokens/sec=1370160.02, grad_norm=0.3142, duration=0.38s
Step 18266: loss=2.7527, lr=0.000242, tokens/sec=1372210.29, grad_norm=0.3212, duration=0.38s
Step 18267: loss=2.8030, lr=0.000242, tokens/sec=1375116.63, grad_norm=0.3292, duration=0.38s
Step 18268: loss=2.8210, lr=0.000242, tokens/sec=1374851.83, grad_norm=0.3227, duration=0.38s
Step 18269: loss=2.7791, lr=0.000242, tokens/sec=1372893.94, grad_norm=0.3089, duration=0.38s
Step 18270: loss=2.7584, lr=0.000242, tokens/sec=1372424.39, grad_norm=0.3063, duration=0.38s
Step 18271: loss=2.8021, lr=0.000242, tokens/sec=1371162.16, grad_norm=0.3238, duration=0.38s
Step 18272: loss=2.7798, lr=0.000242, tokens/sec=1371227.14, grad_norm=0.3286, duration=0.38s
Step 18273: loss=2.8108, lr=0.000242, tokens/sec=1373246.31, grad_norm=0.3212, duration=0.38s
Step 18274: loss=2.7721, lr=0.000242, tokens/sec=1372662.55, grad_norm=0.3179, duration=0.38s
Step 18275: loss=2.7109, lr=0.000242, tokens/sec=1375601.78, grad_norm=0.3174, duration=0.38s
Step 18276: loss=2.8169, lr=0.000242, tokens/sec=1373596.28, grad_norm=0.3202, duration=0.38s
Step 18277: loss=2.7582, lr=0.000242, tokens/sec=1376311.21, grad_norm=0.3301, duration=0.38s
Step 18278: loss=2.7152, lr=0.000242, tokens/sec=1370410.21, grad_norm=0.3322, duration=0.38s
Step 18279: loss=2.7374, lr=0.000242, tokens/sec=1371222.01, grad_norm=0.3341, duration=0.38s
Step 18280: loss=2.7165, lr=0.000242, tokens/sec=1375754.11, grad_norm=0.3228, duration=0.38s
Step 18281: loss=2.7636, lr=0.000242, tokens/sec=1374543.31, grad_norm=0.3338, duration=0.38s
Step 18282: loss=2.7793, lr=0.000242, tokens/sec=1375145.86, grad_norm=0.3267, duration=0.38s
Step 18283: loss=2.7776, lr=0.000242, tokens/sec=1376967.04, grad_norm=0.3139, duration=0.38s
Step 18284: loss=2.7890, lr=0.000242, tokens/sec=1377580.36, grad_norm=0.3096, duration=0.38s
Step 18285: loss=2.7705, lr=0.000242, tokens/sec=1373363.80, grad_norm=0.3303, duration=0.38s
Step 18286: loss=2.7540, lr=0.000242, tokens/sec=1375261.97, grad_norm=0.3192, duration=0.38s
Step 18287: loss=2.8520, lr=0.000242, tokens/sec=1375587.16, grad_norm=0.3418, duration=0.38s
Step 18288: loss=2.8596, lr=0.000242, tokens/sec=1374470.28, grad_norm=0.3129, duration=0.38s
Step 18289: loss=2.8105, lr=0.000242, tokens/sec=1371590.63, grad_norm=0.3087, duration=0.38s
Step 18290: loss=2.8279, lr=0.000242, tokens/sec=1375693.00, grad_norm=0.3216, duration=0.38s
Step 18291: loss=2.8377, lr=0.000242, tokens/sec=1375683.54, grad_norm=0.3320, duration=0.38s
Step 18292: loss=2.8266, lr=0.000242, tokens/sec=1372958.23, grad_norm=0.3234, duration=0.38s
Step 18293: loss=2.8063, lr=0.000242, tokens/sec=1379278.21, grad_norm=0.3213, duration=0.38s
Step 18294: loss=2.7730, lr=0.000242, tokens/sec=1375527.78, grad_norm=0.3216, duration=0.38s
Step 18295: loss=2.7884, lr=0.000242, tokens/sec=1373233.44, grad_norm=0.3504, duration=0.38s
Step 18296: loss=2.8101, lr=0.000242, tokens/sec=1379157.97, grad_norm=0.3394, duration=0.38s
Step 18297: loss=2.8126, lr=0.000242, tokens/sec=1373370.67, grad_norm=0.3214, duration=0.38s
Step 18298: loss=2.7838, lr=0.000242, tokens/sec=1374478.88, grad_norm=0.3168, duration=0.38s
Step 18299: loss=2.7823, lr=0.000242, tokens/sec=1377217.13, grad_norm=0.3186, duration=0.38s
Step 18300/19073 (95.9%), Elapsed time: 7241.70s, Steps per hour: 9097.31, Estimated hours remaining: 0.08
Step 18300: loss=2.7890, lr=0.000242, tokens/sec=1376217.32, grad_norm=0.3392, duration=0.38s
Step 18301: loss=2.7017, lr=0.000242, tokens/sec=1378812.07, grad_norm=0.3350, duration=0.38s
Step 18302: loss=2.7793, lr=0.000242, tokens/sec=1375419.38, grad_norm=0.3112, duration=0.38s
Step 18303: loss=2.8135, lr=0.000242, tokens/sec=1377903.19, grad_norm=0.3224, duration=0.38s
Step 18304: loss=2.8325, lr=0.000242, tokens/sec=1376993.77, grad_norm=0.3402, duration=0.38s
Step 18305: loss=2.8442, lr=0.000242, tokens/sec=1372712.25, grad_norm=0.3473, duration=0.38s
Step 18306: loss=2.7847, lr=0.000242, tokens/sec=1375041.82, grad_norm=0.3202, duration=0.38s
Step 18307: loss=2.8576, lr=0.000242, tokens/sec=1373266.03, grad_norm=0.3188, duration=0.38s
Step 18308: loss=2.8306, lr=0.000242, tokens/sec=1372459.51, grad_norm=0.3235, duration=0.38s
Step 18309: loss=2.8001, lr=0.000242, tokens/sec=1378621.90, grad_norm=0.3377, duration=0.38s
Step 18310: loss=2.8062, lr=0.000242, tokens/sec=1377440.57, grad_norm=0.3238, duration=0.38s
Step 18311: loss=2.8200, lr=0.000242, tokens/sec=1378782.68, grad_norm=0.3118, duration=0.38s
Step 18312: loss=2.7969, lr=0.000242, tokens/sec=1374733.22, grad_norm=0.3080, duration=0.38s
Step 18313: loss=2.8079, lr=0.000242, tokens/sec=1374475.44, grad_norm=0.3103, duration=0.38s
Step 18314: loss=2.7993, lr=0.000242, tokens/sec=1376050.26, grad_norm=0.3088, duration=0.38s
Step 18315: loss=2.8220, lr=0.000242, tokens/sec=1379346.56, grad_norm=0.3173, duration=0.38s
Step 18316: loss=2.8264, lr=0.000242, tokens/sec=1376486.10, grad_norm=0.3051, duration=0.38s
Step 18317: loss=2.7546, lr=0.000242, tokens/sec=1377844.48, grad_norm=0.2955, duration=0.38s
Step 18318: loss=2.7853, lr=0.000242, tokens/sec=1378691.05, grad_norm=0.3026, duration=0.38s
Step 18319: loss=2.7845, lr=0.000242, tokens/sec=1375286.05, grad_norm=0.3042, duration=0.38s
Step 18320: loss=2.7905, lr=0.000242, tokens/sec=1375507.99, grad_norm=0.3026, duration=0.38s
Step 18321: loss=2.7391, lr=0.000242, tokens/sec=1376606.73, grad_norm=0.3024, duration=0.38s
Step 18322: loss=2.7824, lr=0.000242, tokens/sec=1376211.29, grad_norm=0.3017, duration=0.38s
Step 18323: loss=2.7135, lr=0.000242, tokens/sec=1376600.70, grad_norm=0.2977, duration=0.38s
Step 18324: loss=2.7406, lr=0.000242, tokens/sec=1377974.85, grad_norm=0.3017, duration=0.38s
Step 18325: loss=2.6931, lr=0.000242, tokens/sec=1378755.88, grad_norm=0.2987, duration=0.38s
Step 18326: loss=2.7318, lr=0.000241, tokens/sec=1372113.54, grad_norm=0.2912, duration=0.38s
Step 18327: loss=2.7499, lr=0.000241, tokens/sec=1377524.26, grad_norm=0.2964, duration=0.38s
Step 18328: loss=2.6736, lr=0.000241, tokens/sec=1376931.69, grad_norm=0.3055, duration=0.38s
Step 18329: loss=2.7133, lr=0.000241, tokens/sec=1374997.11, grad_norm=0.3079, duration=0.38s
Step 18330: loss=2.7382, lr=0.000241, tokens/sec=1372916.22, grad_norm=0.2898, duration=0.38s
Step 18331: loss=2.7966, lr=0.000241, tokens/sec=1375565.64, grad_norm=0.2859, duration=0.38s
Step 18332: loss=2.7626, lr=0.000241, tokens/sec=1374799.40, grad_norm=0.3125, duration=0.38s
Step 18333: loss=2.7676, lr=0.000241, tokens/sec=1380114.42, grad_norm=0.3331, duration=0.38s
Step 18334: loss=2.8277, lr=0.000241, tokens/sec=1375668.91, grad_norm=0.3242, duration=0.38s
Step 18335: loss=2.8159, lr=0.000241, tokens/sec=1374992.81, grad_norm=0.3025, duration=0.38s
Step 18336: loss=2.8470, lr=0.000241, tokens/sec=1376022.70, grad_norm=0.3045, duration=0.38s
Step 18337: loss=2.8055, lr=0.000241, tokens/sec=1378616.72, grad_norm=0.3111, duration=0.38s
Step 18338: loss=2.7639, lr=0.000241, tokens/sec=1375482.18, grad_norm=0.3355, duration=0.38s
Step 18339: loss=2.8066, lr=0.000241, tokens/sec=1373870.90, grad_norm=0.3164, duration=0.38s
Step 18340: loss=2.8024, lr=0.000241, tokens/sec=1377579.49, grad_norm=0.3165, duration=0.38s
Step 18341: loss=2.8735, lr=0.000241, tokens/sec=1377759.88, grad_norm=0.3252, duration=0.38s
Step 18342: loss=2.8398, lr=0.000241, tokens/sec=1375958.99, grad_norm=0.3101, duration=0.38s
Step 18343: loss=2.8260, lr=0.000241, tokens/sec=1376738.59, grad_norm=0.3114, duration=0.38s
Step 18344: loss=2.8542, lr=0.000241, tokens/sec=1376548.13, grad_norm=0.3246, duration=0.38s
Step 18345: loss=2.7555, lr=0.000241, tokens/sec=1376152.73, grad_norm=0.3191, duration=0.38s
Step 18346: loss=2.8111, lr=0.000241, tokens/sec=1377359.47, grad_norm=0.3220, duration=0.38s
Step 18347: loss=2.8214, lr=0.000241, tokens/sec=1370527.22, grad_norm=0.3126, duration=0.38s
Step 18348: loss=2.8397, lr=0.000241, tokens/sec=1374565.65, grad_norm=0.3128, duration=0.38s
Step 18349: loss=2.7455, lr=0.000241, tokens/sec=1376639.48, grad_norm=0.3222, duration=0.38s
Step 18350: loss=2.8186, lr=0.000241, tokens/sec=1375849.66, grad_norm=0.3207, duration=0.38s
Step 18351: loss=2.7429, lr=0.000241, tokens/sec=1375090.83, grad_norm=0.3611, duration=0.38s
Step 18352: loss=2.8286, lr=0.000241, tokens/sec=1376289.67, grad_norm=0.3111, duration=0.38s
Step 18353: loss=2.7653, lr=0.000241, tokens/sec=1373697.53, grad_norm=0.3205, duration=0.38s
Step 18354: loss=2.8103, lr=0.000241, tokens/sec=1373466.74, grad_norm=0.3246, duration=0.38s
Step 18355: loss=2.7800, lr=0.000241, tokens/sec=1375390.13, grad_norm=0.3286, duration=0.38s
Step 18356: loss=2.8142, lr=0.000241, tokens/sec=1374307.08, grad_norm=0.3153, duration=0.38s
Step 18357: loss=2.7904, lr=0.000241, tokens/sec=1376811.00, grad_norm=0.3181, duration=0.38s
Step 18358: loss=2.8031, lr=0.000241, tokens/sec=1375720.54, grad_norm=0.3284, duration=0.38s
Step 18359: loss=2.7976, lr=0.000241, tokens/sec=1375402.17, grad_norm=0.3171, duration=0.38s
Step 18360: loss=2.8322, lr=0.000241, tokens/sec=1371533.32, grad_norm=0.3103, duration=0.38s
Step 18361: loss=2.7942, lr=0.000241, tokens/sec=1375331.64, grad_norm=0.3022, duration=0.38s
Step 18362: loss=2.7931, lr=0.000241, tokens/sec=1376839.45, grad_norm=0.3181, duration=0.38s
Step 18363: loss=2.7876, lr=0.000241, tokens/sec=1372791.09, grad_norm=0.3209, duration=0.38s
Step 18364: loss=2.8172, lr=0.000241, tokens/sec=1373664.07, grad_norm=0.3109, duration=0.38s
Step 18365: loss=2.8324, lr=0.000241, tokens/sec=1373641.76, grad_norm=0.3093, duration=0.38s
Step 18366: loss=2.7922, lr=0.000241, tokens/sec=1371865.30, grad_norm=0.3061, duration=0.38s
Step 18367: loss=2.7931, lr=0.000241, tokens/sec=1373582.55, grad_norm=0.3181, duration=0.38s
Step 18368: loss=2.7603, lr=0.000241, tokens/sec=1374988.51, grad_norm=0.3216, duration=0.38s
Step 18369: loss=2.7328, lr=0.000241, tokens/sec=1373332.93, grad_norm=0.3270, duration=0.38s
Step 18370: loss=2.7389, lr=0.000241, tokens/sec=1377039.48, grad_norm=0.3203, duration=0.38s
Step 18371: loss=2.6873, lr=0.000241, tokens/sec=1377472.49, grad_norm=0.3116, duration=0.38s
Step 18372: loss=2.7276, lr=0.000241, tokens/sec=1376824.79, grad_norm=0.3215, duration=0.38s
Step 18373: loss=2.7107, lr=0.000241, tokens/sec=1373312.34, grad_norm=0.3222, duration=0.38s
Step 18374: loss=2.8107, lr=0.000241, tokens/sec=1374806.27, grad_norm=0.3330, duration=0.38s
Step 18375: loss=2.7619, lr=0.000241, tokens/sec=1375701.61, grad_norm=0.3136, duration=0.38s
Step 18376: loss=2.6765, lr=0.000241, tokens/sec=1372953.94, grad_norm=0.3069, duration=0.38s
Step 18377: loss=2.7165, lr=0.000241, tokens/sec=1378121.66, grad_norm=0.3003, duration=0.38s
Step 18378: loss=2.7473, lr=0.000241, tokens/sec=1374319.96, grad_norm=0.3297, duration=0.38s
Step 18379: loss=2.8066, lr=0.000241, tokens/sec=1377415.54, grad_norm=0.3518, duration=0.38s
Step 18380: loss=2.8090, lr=0.000241, tokens/sec=1376140.67, grad_norm=0.3576, duration=0.38s
Step 18381: loss=2.8305, lr=0.000241, tokens/sec=1377282.69, grad_norm=0.3366, duration=0.38s
Step 18382: loss=2.8119, lr=0.000241, tokens/sec=1373638.33, grad_norm=0.3163, duration=0.38s
Step 18383: loss=2.8094, lr=0.000241, tokens/sec=1375104.59, grad_norm=0.3294, duration=0.38s
Step 18384: loss=2.8690, lr=0.000241, tokens/sec=1375931.44, grad_norm=0.3815, duration=0.38s
Step 18385: loss=2.8080, lr=0.000241, tokens/sec=1373410.98, grad_norm=0.3656, duration=0.38s
Step 18386: loss=2.8171, lr=0.000241, tokens/sec=1376298.29, grad_norm=0.3389, duration=0.38s
Step 18387: loss=2.8569, lr=0.000241, tokens/sec=1375452.93, grad_norm=0.3154, duration=0.38s
Step 18388: loss=2.9116, lr=0.000241, tokens/sec=1375975.35, grad_norm=0.3323, duration=0.38s
Step 18389: loss=2.8634, lr=0.000241, tokens/sec=1375058.16, grad_norm=0.3676, duration=0.38s
Step 18390: loss=2.8587, lr=0.000241, tokens/sec=1376002.04, grad_norm=0.3687, duration=0.38s
Step 18391: loss=2.7913, lr=0.000241, tokens/sec=1374626.66, grad_norm=0.3349, duration=0.38s
Step 18392: loss=2.8280, lr=0.000241, tokens/sec=1381277.79, grad_norm=0.3139, duration=0.38s
Step 18393: loss=2.7890, lr=0.000241, tokens/sec=1372856.23, grad_norm=0.3244, duration=0.38s
Step 18394: loss=2.7523, lr=0.000241, tokens/sec=1373672.65, grad_norm=0.3611, duration=0.38s
Step 18395: loss=2.8765, lr=0.000241, tokens/sec=1374744.39, grad_norm=0.3636, duration=0.38s
Step 18396: loss=2.7621, lr=0.000241, tokens/sec=1374355.18, grad_norm=0.3344, duration=0.38s
Step 18397: loss=2.8502, lr=0.000241, tokens/sec=1375054.72, grad_norm=0.3258, duration=0.38s
Step 18398: loss=2.8846, lr=0.000241, tokens/sec=1376672.23, grad_norm=0.3474, duration=0.38s
Step 18399: loss=2.7985, lr=0.000241, tokens/sec=1374984.22, grad_norm=0.3562, duration=0.38s
Step 18400/19073 (96.5%), Elapsed time: 7279.90s, Steps per hour: 9099.02, Estimated hours remaining: 0.07
Step 18400: loss=2.7876, lr=0.000241, tokens/sec=1373459.87, grad_norm=0.3310, duration=0.38s
Step 18401: loss=2.8257, lr=0.000241, tokens/sec=1374862.14, grad_norm=0.3371, duration=0.38s
Step 18402: loss=2.8432, lr=0.000241, tokens/sec=1374458.26, grad_norm=0.3233, duration=0.38s
Step 18403: loss=2.8296, lr=0.000241, tokens/sec=1378552.76, grad_norm=0.3265, duration=0.38s
Step 18404: loss=2.7464, lr=0.000241, tokens/sec=1377948.09, grad_norm=0.3250, duration=0.38s
Step 18405: loss=2.8153, lr=0.000241, tokens/sec=1375138.13, grad_norm=0.3354, duration=0.38s
Step 18406: loss=2.8157, lr=0.000241, tokens/sec=1375835.88, grad_norm=0.3171, duration=0.38s
Step 18407: loss=2.8269, lr=0.000241, tokens/sec=1375255.09, grad_norm=0.3209, duration=0.38s
Step 18408: loss=2.8448, lr=0.000241, tokens/sec=1378044.80, grad_norm=0.3268, duration=0.38s
Step 18409: loss=2.7726, lr=0.000241, tokens/sec=1378083.66, grad_norm=0.3270, duration=0.38s
Step 18410: loss=2.8109, lr=0.000241, tokens/sec=1376198.38, grad_norm=0.3200, duration=0.38s
Step 18411: loss=2.7803, lr=0.000241, tokens/sec=1376245.75, grad_norm=0.3157, duration=0.38s
Step 18412: loss=2.7453, lr=0.000241, tokens/sec=1373605.72, grad_norm=0.3265, duration=0.38s
Step 18413: loss=2.7772, lr=0.000241, tokens/sec=1377074.83, grad_norm=0.3377, duration=0.38s
Step 18414: loss=2.7520, lr=0.000241, tokens/sec=1376213.88, grad_norm=0.3163, duration=0.38s
Step 18415: loss=2.7377, lr=0.000241, tokens/sec=1373913.82, grad_norm=0.3055, duration=0.38s
Step 18416: loss=2.7157, lr=0.000241, tokens/sec=1374113.85, grad_norm=0.3151, duration=0.38s
Step 18417: loss=2.7880, lr=0.000241, tokens/sec=1378515.60, grad_norm=0.3349, duration=0.38s
Step 18418: loss=2.7365, lr=0.000241, tokens/sec=1377223.17, grad_norm=0.3330, duration=0.38s
Step 18419: loss=2.7140, lr=0.000241, tokens/sec=1377773.69, grad_norm=0.3236, duration=0.38s
Step 18420: loss=2.6950, lr=0.000241, tokens/sec=1379442.61, grad_norm=0.3133, duration=0.38s
Step 18421: loss=2.7293, lr=0.000241, tokens/sec=1377264.57, grad_norm=0.3218, duration=0.38s
Step 18422: loss=2.6996, lr=0.000241, tokens/sec=1375573.39, grad_norm=0.3173, duration=0.38s
Step 18423: loss=2.7378, lr=0.000241, tokens/sec=1373636.61, grad_norm=0.3294, duration=0.38s
Step 18424: loss=2.7538, lr=0.000241, tokens/sec=1375389.27, grad_norm=0.3246, duration=0.38s
Step 18425: loss=2.8342, lr=0.000241, tokens/sec=1375990.84, grad_norm=0.3333, duration=0.38s
Step 18426: loss=2.8247, lr=0.000241, tokens/sec=1378645.24, grad_norm=0.3253, duration=0.38s
Step 18427: loss=2.8237, lr=0.000241, tokens/sec=1376789.45, grad_norm=0.3245, duration=0.38s
Step 18428: loss=2.7947, lr=0.000241, tokens/sec=1378100.93, grad_norm=0.3292, duration=0.38s
Step 18429: loss=2.8229, lr=0.000241, tokens/sec=1378455.12, grad_norm=0.3432, duration=0.38s
Step 18430: loss=2.7808, lr=0.000241, tokens/sec=1371509.37, grad_norm=0.3615, duration=0.38s
Step 18431: loss=2.7883, lr=0.000241, tokens/sec=1377155.90, grad_norm=0.3336, duration=0.38s
Step 18432: loss=2.8397, lr=0.000241, tokens/sec=1376353.42, grad_norm=0.3151, duration=0.38s
Step 18433: loss=2.9081, lr=0.000241, tokens/sec=1377250.77, grad_norm=0.3492, duration=0.38s
Step 18434: loss=2.8384, lr=0.000241, tokens/sec=1374207.45, grad_norm=0.3615, duration=0.38s
Step 18435: loss=2.8832, lr=0.000241, tokens/sec=1375704.19, grad_norm=0.3497, duration=0.38s
Step 18436: loss=2.8909, lr=0.000241, tokens/sec=1377341.35, grad_norm=0.3499, duration=0.38s
Step 18437: loss=2.8534, lr=0.000241, tokens/sec=1375527.78, grad_norm=0.3120, duration=0.38s
Step 18438: loss=2.8580, lr=0.000241, tokens/sec=1378022.35, grad_norm=0.3246, duration=0.38s
Step 18439: loss=2.8263, lr=0.000241, tokens/sec=1377985.21, grad_norm=0.3431, duration=0.38s
Step 18440: loss=2.8596, lr=0.000241, tokens/sec=1373507.06, grad_norm=0.3395, duration=0.38s
Step 18441: loss=2.7748, lr=0.000241, tokens/sec=1377271.48, grad_norm=0.3260, duration=0.38s
Step 18442: loss=2.8144, lr=0.000241, tokens/sec=1377789.23, grad_norm=0.3235, duration=0.38s
Step 18443: loss=2.7247, lr=0.000241, tokens/sec=1376261.25, grad_norm=0.3200, duration=0.38s
Step 18444: loss=2.8336, lr=0.000241, tokens/sec=1375780.79, grad_norm=0.3249, duration=0.38s
Step 18445: loss=2.8085, lr=0.000241, tokens/sec=1377160.21, grad_norm=0.3673, duration=0.38s
Step 18446: loss=2.7323, lr=0.000241, tokens/sec=1373891.50, grad_norm=0.3318, duration=0.38s
Step 18447: loss=2.8262, lr=0.000241, tokens/sec=1379570.68, grad_norm=0.3320, duration=0.38s
Step 18448: loss=2.7256, lr=0.000241, tokens/sec=1377806.49, grad_norm=0.3287, duration=0.38s
Step 18449: loss=2.7185, lr=0.000241, tokens/sec=1374030.57, grad_norm=0.3981, duration=0.38s
Step 18450: loss=2.7742, lr=0.000241, tokens/sec=1372828.80, grad_norm=0.3245, duration=0.38s
Step 18451: loss=2.8148, lr=0.000241, tokens/sec=1376163.06, grad_norm=0.3334, duration=0.38s
Step 18452: loss=2.8760, lr=0.000241, tokens/sec=1378452.52, grad_norm=0.3424, duration=0.38s
Step 18453: loss=2.7491, lr=0.000241, tokens/sec=1377582.94, grad_norm=0.3361, duration=0.38s
Step 18454: loss=2.8155, lr=0.000241, tokens/sec=1373958.46, grad_norm=0.3194, duration=0.38s
Step 18455: loss=2.8132, lr=0.000241, tokens/sec=1379626.94, grad_norm=0.3165, duration=0.38s
Step 18456: loss=2.7655, lr=0.000241, tokens/sec=1373806.53, grad_norm=0.3076, duration=0.38s
Step 18457: loss=2.7975, lr=0.000241, tokens/sec=1373756.75, grad_norm=0.3284, duration=0.38s
Step 18458: loss=2.7987, lr=0.000241, tokens/sec=1375874.62, grad_norm=0.3375, duration=0.38s
Step 18459: loss=2.7974, lr=0.000241, tokens/sec=1375600.92, grad_norm=0.3232, duration=0.38s
Step 18460: loss=2.7666, lr=0.000241, tokens/sec=1378236.54, grad_norm=0.3336, duration=0.38s
Step 18461: loss=2.7782, lr=0.000241, tokens/sec=1377373.27, grad_norm=0.3251, duration=0.38s
Step 18462: loss=2.7747, lr=0.000241, tokens/sec=1374773.61, grad_norm=0.3371, duration=0.38s
Step 18463: loss=2.8256, lr=0.000241, tokens/sec=1377109.33, grad_norm=0.3153, duration=0.38s
Step 18464: loss=2.7670, lr=0.000241, tokens/sec=1380020.88, grad_norm=0.3299, duration=0.38s
Step 18465: loss=2.7741, lr=0.000241, tokens/sec=1375681.82, grad_norm=0.3285, duration=0.38s
Step 18466: loss=2.8034, lr=0.000241, tokens/sec=1375449.49, grad_norm=0.3294, duration=0.38s
Step 18467: loss=2.7173, lr=0.000241, tokens/sec=1375043.54, grad_norm=0.3134, duration=0.38s
Step 18468: loss=2.7185, lr=0.000241, tokens/sec=1378473.26, grad_norm=0.3290, duration=0.38s
Step 18469: loss=2.6995, lr=0.000241, tokens/sec=1374652.44, grad_norm=0.3425, duration=0.38s
Step 18470: loss=2.7575, lr=0.000241, tokens/sec=1376404.25, grad_norm=0.3386, duration=0.38s
Step 18471: loss=2.7760, lr=0.000241, tokens/sec=1375071.91, grad_norm=0.3395, duration=0.38s
Step 18472: loss=2.7639, lr=0.000241, tokens/sec=1379428.76, grad_norm=0.3386, duration=0.38s
Step 18473: loss=2.8133, lr=0.000241, tokens/sec=1376492.13, grad_norm=0.3423, duration=0.38s
Step 18474: loss=2.7717, lr=0.000241, tokens/sec=1378235.67, grad_norm=0.3150, duration=0.38s
Step 18475: loss=2.7393, lr=0.000241, tokens/sec=1373451.30, grad_norm=0.3080, duration=0.38s
Step 18476: loss=2.7868, lr=0.000241, tokens/sec=1373798.80, grad_norm=0.3037, duration=0.38s
Step 18477: loss=2.8174, lr=0.000241, tokens/sec=1375422.82, grad_norm=0.3305, duration=0.38s
Step 18478: loss=2.8525, lr=0.000241, tokens/sec=1374404.14, grad_norm=0.3401, duration=0.38s
Step 18479: loss=2.8347, lr=0.000241, tokens/sec=1378274.55, grad_norm=0.3236, duration=0.38s
Step 18480: loss=2.8108, lr=0.000241, tokens/sec=1376711.88, grad_norm=0.3080, duration=0.38s
Step 18481: loss=2.8316, lr=0.000241, tokens/sec=1377339.62, grad_norm=0.3324, duration=0.38s
Step 18482: loss=2.8042, lr=0.000241, tokens/sec=1373815.97, grad_norm=0.3295, duration=0.38s
Step 18483: loss=2.8029, lr=0.000241, tokens/sec=1378148.43, grad_norm=0.3212, duration=0.38s
Step 18484: loss=2.7965, lr=0.000241, tokens/sec=1373073.96, grad_norm=0.3218, duration=0.38s
Step 18485: loss=2.7835, lr=0.000241, tokens/sec=1378283.18, grad_norm=0.3505, duration=0.38s
Step 18486: loss=2.8292, lr=0.000241, tokens/sec=1375424.54, grad_norm=0.3477, duration=0.38s
Step 18487: loss=2.7846, lr=0.000241, tokens/sec=1377261.12, grad_norm=0.3244, duration=0.38s
Step 18488: loss=2.7768, lr=0.000241, tokens/sec=1376943.77, grad_norm=0.3169, duration=0.38s
Step 18489: loss=2.7883, lr=0.000241, tokens/sec=1375731.73, grad_norm=0.3137, duration=0.38s
Step 18490: loss=2.7743, lr=0.000241, tokens/sec=1376567.09, grad_norm=0.3485, duration=0.38s
Step 18491: loss=2.7200, lr=0.000241, tokens/sec=1377564.82, grad_norm=0.3506, duration=0.38s
Step 18492: loss=2.7678, lr=0.000241, tokens/sec=1374685.95, grad_norm=0.3135, duration=0.38s
Step 18493: loss=2.8358, lr=0.000241, tokens/sec=1376308.63, grad_norm=0.3072, duration=0.38s
Step 18494: loss=2.8076, lr=0.000241, tokens/sec=1374034.86, grad_norm=0.3367, duration=0.38s
Step 18495: loss=2.8376, lr=0.000241, tokens/sec=1380371.72, grad_norm=0.3557, duration=0.38s
Step 18496: loss=2.7828, lr=0.000241, tokens/sec=1372564.88, grad_norm=0.3241, duration=0.38s
Step 18497: loss=2.8598, lr=0.000241, tokens/sec=1376679.12, grad_norm=0.3127, duration=0.38s
Step 18498: loss=2.8077, lr=0.000241, tokens/sec=1375129.53, grad_norm=0.3092, duration=0.38s
Step 18499: loss=2.8048, lr=0.000241, tokens/sec=1374762.44, grad_norm=0.3327, duration=0.38s
Step 18500/19073 (97.0%), Elapsed time: 7318.08s, Steps per hour: 9100.75, Estimated hours remaining: 0.06
Validation loss at step 18500: 3.854417324066162
Step 18500: loss=2.8135, lr=0.000241, tokens/sec=154104.04, grad_norm=0.3412, duration=3.40s
Step 18501: loss=2.8187, lr=0.000241, tokens/sec=1375349.70, grad_norm=0.3203, duration=0.38s
Step 18502: loss=2.7834, lr=0.000241, tokens/sec=1376468.86, grad_norm=0.3104, duration=0.38s
Step 18503: loss=2.7935, lr=0.000241, tokens/sec=1376771.35, grad_norm=0.3032, duration=0.38s
Step 18504: loss=2.8248, lr=0.000241, tokens/sec=1378266.77, grad_norm=0.3136, duration=0.38s
Step 18505: loss=2.8173, lr=0.000241, tokens/sec=1375489.07, grad_norm=0.3178, duration=0.38s
Step 18506: loss=2.7994, lr=0.000241, tokens/sec=1375672.35, grad_norm=0.3165, duration=0.38s
Step 18507: loss=2.7485, lr=0.000241, tokens/sec=1375767.02, grad_norm=0.3079, duration=0.38s
Step 18508: loss=2.7942, lr=0.000241, tokens/sec=1376520.56, grad_norm=0.3017, duration=0.38s
Step 18509: loss=2.7770, lr=0.000241, tokens/sec=1376339.64, grad_norm=0.2992, duration=0.38s
Step 18510: loss=2.8067, lr=0.000241, tokens/sec=1376229.38, grad_norm=0.3063, duration=0.38s
Step 18511: loss=2.7214, lr=0.000241, tokens/sec=1371951.75, grad_norm=0.3116, duration=0.38s
Step 18512: loss=2.7648, lr=0.000241, tokens/sec=1375796.29, grad_norm=0.3033, duration=0.38s
Step 18513: loss=2.7286, lr=0.000241, tokens/sec=1375979.65, grad_norm=0.2987, duration=0.38s
Step 18514: loss=2.7368, lr=0.000241, tokens/sec=1371374.23, grad_norm=0.2899, duration=0.38s
Step 18515: loss=2.6971, lr=0.000241, tokens/sec=1372290.79, grad_norm=0.3016, duration=0.38s
Step 18516: loss=2.7304, lr=0.000241, tokens/sec=1373769.62, grad_norm=0.3106, duration=0.38s
Step 18517: loss=2.7182, lr=0.000241, tokens/sec=1373013.95, grad_norm=0.3048, duration=0.38s
Step 18518: loss=2.6915, lr=0.000241, tokens/sec=1374382.66, grad_norm=0.2978, duration=0.38s
Step 18519: loss=2.7552, lr=0.000241, tokens/sec=1372430.39, grad_norm=0.3165, duration=0.38s
Step 18520: loss=2.7381, lr=0.000241, tokens/sec=1376576.57, grad_norm=0.3137, duration=0.38s
Step 18521: loss=2.7773, lr=0.000241, tokens/sec=1377898.87, grad_norm=0.2975, duration=0.38s
Step 18522: loss=2.7468, lr=0.000241, tokens/sec=1373457.30, grad_norm=0.3004, duration=0.38s
Step 18523: loss=2.7835, lr=0.000241, tokens/sec=1373951.59, grad_norm=0.3225, duration=0.38s
Step 18524: loss=2.8138, lr=0.000241, tokens/sec=1374389.53, grad_norm=0.3343, duration=0.38s
Step 18525: loss=2.8368, lr=0.000241, tokens/sec=1374600.02, grad_norm=0.3247, duration=0.38s
Step 18526: loss=2.8028, lr=0.000241, tokens/sec=1374392.97, grad_norm=0.3137, duration=0.38s
Step 18527: loss=2.8288, lr=0.000241, tokens/sec=1373718.99, grad_norm=0.3174, duration=0.38s
Step 18528: loss=2.7582, lr=0.000241, tokens/sec=1377696.01, grad_norm=0.3363, duration=0.38s
Step 18529: loss=2.8344, lr=0.000241, tokens/sec=1375773.05, grad_norm=0.3435, duration=0.38s
Step 18530: loss=2.8069, lr=0.000241, tokens/sec=1375095.13, grad_norm=0.3394, duration=0.38s
Step 18531: loss=2.8512, lr=0.000241, tokens/sec=1374818.31, grad_norm=0.3237, duration=0.38s
Step 18532: loss=2.8388, lr=0.000241, tokens/sec=1373617.73, grad_norm=0.3213, duration=0.38s
Step 18533: loss=2.8518, lr=0.000241, tokens/sec=1374439.36, grad_norm=0.3303, duration=0.38s
Step 18534: loss=2.7849, lr=0.000241, tokens/sec=1375903.89, grad_norm=0.3281, duration=0.38s
Step 18535: loss=2.7981, lr=0.000241, tokens/sec=1372186.32, grad_norm=0.3324, duration=0.38s
Step 18536: loss=2.8122, lr=0.000241, tokens/sec=1371318.64, grad_norm=0.3360, duration=0.38s
Step 18537: loss=2.8386, lr=0.000241, tokens/sec=1371552.14, grad_norm=0.3304, duration=0.38s
Step 18538: loss=2.7958, lr=0.000241, tokens/sec=1376243.16, grad_norm=0.3247, duration=0.38s
Step 18539: loss=2.7651, lr=0.000241, tokens/sec=1375067.61, grad_norm=0.3209, duration=0.38s
Step 18540: loss=2.7522, lr=0.000241, tokens/sec=1373211.15, grad_norm=0.3591, duration=0.38s
Step 18541: loss=2.8124, lr=0.000241, tokens/sec=1379436.55, grad_norm=0.3409, duration=0.38s
Step 18542: loss=2.8267, lr=0.000241, tokens/sec=1375414.22, grad_norm=0.3197, duration=0.38s
Step 18543: loss=2.7440, lr=0.000241, tokens/sec=1378462.03, grad_norm=0.3269, duration=0.38s
Step 18544: loss=2.8279, lr=0.000241, tokens/sec=1376965.32, grad_norm=0.3411, duration=0.38s
Step 18545: loss=2.7567, lr=0.000241, tokens/sec=1370893.76, grad_norm=0.3264, duration=0.38s
Step 18546: loss=2.8152, lr=0.000241, tokens/sec=1380908.28, grad_norm=0.3115, duration=0.38s
Step 18547: loss=2.7792, lr=0.000241, tokens/sec=1377633.00, grad_norm=0.3097, duration=0.38s
Step 18548: loss=2.8131, lr=0.000241, tokens/sec=1373520.78, grad_norm=0.3352, duration=0.38s
Step 18549: loss=2.8212, lr=0.000241, tokens/sec=1377733.98, grad_norm=0.3441, duration=0.38s
Step 18550: loss=2.7949, lr=0.000241, tokens/sec=1375918.52, grad_norm=0.3288, duration=0.38s
Step 18551: loss=2.7900, lr=0.000241, tokens/sec=1377517.36, grad_norm=0.3045, duration=0.38s
Step 18552: loss=2.8030, lr=0.000241, tokens/sec=1375976.21, grad_norm=0.3040, duration=0.38s
Step 18553: loss=2.7968, lr=0.000241, tokens/sec=1376982.57, grad_norm=0.3204, duration=0.38s
Step 18554: loss=2.8301, lr=0.000241, tokens/sec=1377055.86, grad_norm=0.3222, duration=0.38s
Step 18555: loss=2.7998, lr=0.000241, tokens/sec=1376186.32, grad_norm=0.3090, duration=0.38s
Step 18556: loss=2.7982, lr=0.000241, tokens/sec=1378377.35, grad_norm=0.3187, duration=0.38s
Step 18557: loss=2.7594, lr=0.000241, tokens/sec=1376030.45, grad_norm=0.3239, duration=0.38s
Step 18558: loss=2.7828, lr=0.000241, tokens/sec=1375048.70, grad_norm=0.3264, duration=0.38s
Step 18559: loss=2.7059, lr=0.000241, tokens/sec=1377968.81, grad_norm=0.3164, duration=0.38s
Step 18560: loss=2.7635, lr=0.000241, tokens/sec=1376312.07, grad_norm=0.3236, duration=0.38s
Step 18561: loss=2.6816, lr=0.000241, tokens/sec=1374714.31, grad_norm=0.3241, duration=0.38s
Step 18562: loss=2.7063, lr=0.000241, tokens/sec=1376992.91, grad_norm=0.3266, duration=0.38s
Step 18563: loss=2.7455, lr=0.000241, tokens/sec=1375495.95, grad_norm=0.3221, duration=0.38s
Step 18564: loss=2.7951, lr=0.000241, tokens/sec=1374502.07, grad_norm=0.3233, duration=0.38s
Step 18565: loss=2.6957, lr=0.000241, tokens/sec=1378879.51, grad_norm=0.3338, duration=0.38s
Step 18566: loss=2.7249, lr=0.000241, tokens/sec=1377669.25, grad_norm=0.3219, duration=0.38s
Step 18567: loss=2.7121, lr=0.000241, tokens/sec=1376113.12, grad_norm=0.2992, duration=0.38s
Step 18568: loss=2.7724, lr=0.000241, tokens/sec=1375341.96, grad_norm=0.3149, duration=0.38s
Step 18569: loss=2.7978, lr=0.000241, tokens/sec=1372589.73, grad_norm=0.3490, duration=0.38s
Step 18570: loss=2.8069, lr=0.000241, tokens/sec=1376076.09, grad_norm=0.3809, duration=0.38s
Step 18571: loss=2.8093, lr=0.000241, tokens/sec=1372026.22, grad_norm=0.3517, duration=0.38s
Step 18572: loss=2.8231, lr=0.000241, tokens/sec=1375149.30, grad_norm=0.3198, duration=0.38s
Step 18573: loss=2.8408, lr=0.000241, tokens/sec=1374234.07, grad_norm=0.3186, duration=0.38s
Step 18574: loss=2.8550, lr=0.000241, tokens/sec=1375980.51, grad_norm=0.3491, duration=0.38s
Step 18575: loss=2.7904, lr=0.000241, tokens/sec=1378930.53, grad_norm=0.3822, duration=0.38s
Step 18576: loss=2.8140, lr=0.000241, tokens/sec=1377612.29, grad_norm=0.3697, duration=0.38s
Step 18577: loss=2.8521, lr=0.000241, tokens/sec=1373866.61, grad_norm=0.3241, duration=0.38s
Step 18578: loss=2.9139, lr=0.000241, tokens/sec=1372979.66, grad_norm=0.3181, duration=0.38s
Step 18579: loss=2.8673, lr=0.000241, tokens/sec=1376347.39, grad_norm=0.3530, duration=0.38s
Step 18580: loss=2.8357, lr=0.000241, tokens/sec=1376436.12, grad_norm=0.3713, duration=0.38s
Step 18581: loss=2.7973, lr=0.000241, tokens/sec=1376673.09, grad_norm=0.3598, duration=0.38s
Step 18582: loss=2.8280, lr=0.000241, tokens/sec=1375746.36, grad_norm=0.3302, duration=0.38s
Step 18583: loss=2.7810, lr=0.000241, tokens/sec=1376267.28, grad_norm=0.2996, duration=0.38s
Step 18584: loss=2.7846, lr=0.000241, tokens/sec=1376549.00, grad_norm=0.3399, duration=0.38s
Step 18585: loss=2.8471, lr=0.000241, tokens/sec=1374979.06, grad_norm=0.3687, duration=0.38s
Step 18586: loss=2.7693, lr=0.000241, tokens/sec=1374429.91, grad_norm=0.3722, duration=0.38s
Step 18587: loss=2.8628, lr=0.000241, tokens/sec=1374972.18, grad_norm=0.3559, duration=0.38s
Step 18588: loss=2.8440, lr=0.000241, tokens/sec=1378012.85, grad_norm=0.3484, duration=0.38s
Step 18589: loss=2.8300, lr=0.000241, tokens/sec=1377450.06, grad_norm=0.3457, duration=0.38s
Step 18590: loss=2.7772, lr=0.000241, tokens/sec=1379330.99, grad_norm=0.3349, duration=0.38s
Step 18591: loss=2.8125, lr=0.000241, tokens/sec=1378809.48, grad_norm=0.3537, duration=0.38s
Step 18592: loss=2.8583, lr=0.000241, tokens/sec=1375323.90, grad_norm=0.3425, duration=0.38s
Step 18593: loss=2.8187, lr=0.000241, tokens/sec=1373602.29, grad_norm=0.3369, duration=0.38s
Step 18594: loss=2.7586, lr=0.000241, tokens/sec=1373025.09, grad_norm=0.3334, duration=0.38s
Step 18595: loss=2.8105, lr=0.000241, tokens/sec=1375459.81, grad_norm=0.3375, duration=0.38s
Step 18596: loss=2.8080, lr=0.000241, tokens/sec=1373440.14, grad_norm=0.3281, duration=0.38s
Step 18597: loss=2.8167, lr=0.000241, tokens/sec=1371325.48, grad_norm=0.3443, duration=0.38s
Step 18598: loss=2.8481, lr=0.000241, tokens/sec=1374730.64, grad_norm=0.3500, duration=0.38s
Step 18599: loss=2.7976, lr=0.000241, tokens/sec=1375625.88, grad_norm=0.3416, duration=0.38s
Step 18600/19073 (97.5%), Elapsed time: 7359.30s, Steps per hour: 9098.69, Estimated hours remaining: 0.05
Step 18600: loss=2.7774, lr=0.000241, tokens/sec=1380159.46, grad_norm=0.3283, duration=0.38s
Step 18601: loss=2.7830, lr=0.000241, tokens/sec=1377150.72, grad_norm=0.3119, duration=0.38s
Step 18602: loss=2.7359, lr=0.000241, tokens/sec=1375558.76, grad_norm=0.3178, duration=0.38s
Step 18603: loss=2.7829, lr=0.000241, tokens/sec=1376941.18, grad_norm=0.3465, duration=0.38s
Step 18604: loss=2.7550, lr=0.000241, tokens/sec=1377362.92, grad_norm=0.3460, duration=0.38s
Step 18605: loss=2.7089, lr=0.000241, tokens/sec=1374090.67, grad_norm=0.3342, duration=0.38s
Step 18606: loss=2.7254, lr=0.000241, tokens/sec=1375551.88, grad_norm=0.3134, duration=0.38s
Step 18607: loss=2.7816, lr=0.000241, tokens/sec=1374293.33, grad_norm=0.3168, duration=0.38s
Step 18608: loss=2.7306, lr=0.000241, tokens/sec=1374838.94, grad_norm=0.3273, duration=0.38s
Step 18609: loss=2.7196, lr=0.000241, tokens/sec=1376503.33, grad_norm=0.3465, duration=0.38s
Step 18610: loss=2.7001, lr=0.000241, tokens/sec=1374626.66, grad_norm=0.3475, duration=0.38s
Step 18611: loss=2.6580, lr=0.000241, tokens/sec=1373428.99, grad_norm=0.3377, duration=0.38s
Step 18612: loss=2.7562, lr=0.000241, tokens/sec=1374794.24, grad_norm=0.3115, duration=0.38s
Step 18613: loss=2.7348, lr=0.000241, tokens/sec=1376390.46, grad_norm=0.3127, duration=0.38s
Step 18614: loss=2.7901, lr=0.000241, tokens/sec=1377683.06, grad_norm=0.3377, duration=0.38s
Step 18615: loss=2.8264, lr=0.000241, tokens/sec=1376385.29, grad_norm=0.3543, duration=0.38s
Step 18616: loss=2.8229, lr=0.000241, tokens/sec=1378220.99, grad_norm=0.3505, duration=0.38s
Step 18617: loss=2.8186, lr=0.000241, tokens/sec=1371996.26, grad_norm=0.3157, duration=0.38s
Step 18618: loss=2.7936, lr=0.000241, tokens/sec=1372905.94, grad_norm=0.3009, duration=0.38s
Step 18619: loss=2.8049, lr=0.000241, tokens/sec=1372415.83, grad_norm=0.3377, duration=0.38s
Step 18620: loss=2.7529, lr=0.000241, tokens/sec=1375844.49, grad_norm=0.3745, duration=0.38s
Step 18621: loss=2.8329, lr=0.000241, tokens/sec=1374855.27, grad_norm=0.3619, duration=0.38s
Step 18622: loss=2.8378, lr=0.000241, tokens/sec=1378402.41, grad_norm=0.3420, duration=0.38s
Step 18623: loss=2.9001, lr=0.000241, tokens/sec=1374766.74, grad_norm=0.3256, duration=0.38s
Step 18624: loss=2.8635, lr=0.000241, tokens/sec=1376196.65, grad_norm=0.3608, duration=0.38s
Step 18625: loss=2.8479, lr=0.000241, tokens/sec=1375185.42, grad_norm=0.3951, duration=0.38s
Step 18626: loss=2.9106, lr=0.000241, tokens/sec=1377102.43, grad_norm=0.3749, duration=0.38s
Step 18627: loss=2.8426, lr=0.000241, tokens/sec=1379133.75, grad_norm=0.3329, duration=0.38s
Step 18628: loss=2.8435, lr=0.000241, tokens/sec=1374979.06, grad_norm=0.3289, duration=0.38s
Step 18629: loss=2.8433, lr=0.000241, tokens/sec=1373854.59, grad_norm=0.3447, duration=0.38s
Step 18630: loss=2.8043, lr=0.000241, tokens/sec=1378258.13, grad_norm=0.3421, duration=0.38s
Step 18631: loss=2.8053, lr=0.000241, tokens/sec=1375340.24, grad_norm=0.3363, duration=0.38s
Step 18632: loss=2.7787, lr=0.000241, tokens/sec=1373663.21, grad_norm=0.3357, duration=0.38s
Step 18633: loss=2.7593, lr=0.000241, tokens/sec=1375506.27, grad_norm=0.3385, duration=0.38s
Step 18634: loss=2.8276, lr=0.000241, tokens/sec=1374167.95, grad_norm=0.3301, duration=0.38s
Step 18635: loss=2.7767, lr=0.000241, tokens/sec=1375756.69, grad_norm=0.3604, duration=0.38s
Step 18636: loss=2.7604, lr=0.000241, tokens/sec=1373314.06, grad_norm=0.3269, duration=0.38s
Step 18637: loss=2.8130, lr=0.000241, tokens/sec=1373058.53, grad_norm=0.3413, duration=0.38s
Step 18638: loss=2.7463, lr=0.000241, tokens/sec=1377584.67, grad_norm=0.3573, duration=0.38s
Step 18639: loss=2.7322, lr=0.000241, tokens/sec=1377384.49, grad_norm=0.4377, duration=0.38s
Step 18640: loss=2.7698, lr=0.000241, tokens/sec=1378276.27, grad_norm=0.3223, duration=0.38s
Step 18641: loss=2.7993, lr=0.000241, tokens/sec=1374294.19, grad_norm=0.3256, duration=0.38s
Step 18642: loss=2.8977, lr=0.000240, tokens/sec=1376611.04, grad_norm=0.3567, duration=0.38s
Step 18643: loss=2.7049, lr=0.000240, tokens/sec=1375411.64, grad_norm=0.3498, duration=0.38s
Step 18644: loss=2.8211, lr=0.000240, tokens/sec=1377181.77, grad_norm=0.3302, duration=0.38s
Step 18645: loss=2.8244, lr=0.000240, tokens/sec=1374572.52, grad_norm=0.3199, duration=0.38s
Step 18646: loss=2.7575, lr=0.000240, tokens/sec=1378087.11, grad_norm=0.3371, duration=0.38s
Step 18647: loss=2.7760, lr=0.000240, tokens/sec=1375826.41, grad_norm=0.3298, duration=0.38s
Step 18648: loss=2.8150, lr=0.000240, tokens/sec=1374428.19, grad_norm=0.3331, duration=0.38s
Step 18649: loss=2.8030, lr=0.000240, tokens/sec=1372758.53, grad_norm=0.3334, duration=0.38s
Step 18650: loss=2.7393, lr=0.000240, tokens/sec=1373489.90, grad_norm=0.3200, duration=0.38s
Step 18651: loss=2.7647, lr=0.000240, tokens/sec=1376737.73, grad_norm=0.3335, duration=0.38s
Step 18652: loss=2.7881, lr=0.000240, tokens/sec=1378359.21, grad_norm=0.3489, duration=0.38s
Step 18653: loss=2.8161, lr=0.000240, tokens/sec=1379129.43, grad_norm=0.3413, duration=0.38s
Step 18654: loss=2.8266, lr=0.000240, tokens/sec=1373306.34, grad_norm=0.3259, duration=0.38s
Step 18655: loss=2.7626, lr=0.000240, tokens/sec=1378792.19, grad_norm=0.3181, duration=0.38s
Step 18656: loss=2.7609, lr=0.000240, tokens/sec=1373678.66, grad_norm=0.3232, duration=0.38s
Step 18657: loss=2.7203, lr=0.000240, tokens/sec=1377186.95, grad_norm=0.3493, duration=0.38s
Step 18658: loss=2.6822, lr=0.000240, tokens/sec=1374453.96, grad_norm=0.3365, duration=0.38s
Step 18659: loss=2.7398, lr=0.000240, tokens/sec=1377215.41, grad_norm=0.3286, duration=0.38s
Step 18660: loss=2.7688, lr=0.000240, tokens/sec=1372284.79, grad_norm=0.3152, duration=0.38s
Step 18661: loss=2.7551, lr=0.000240, tokens/sec=1378857.90, grad_norm=0.3248, duration=0.38s
Step 18662: loss=2.7933, lr=0.000240, tokens/sec=1371954.32, grad_norm=0.3415, duration=0.38s
Step 18663: loss=2.7920, lr=0.000240, tokens/sec=1373840.00, grad_norm=0.3510, duration=0.38s
Step 18664: loss=2.7404, lr=0.000240, tokens/sec=1376577.43, grad_norm=0.3289, duration=0.38s
Step 18665: loss=2.7699, lr=0.000240, tokens/sec=1376907.55, grad_norm=0.3239, duration=0.38s
Step 18666: loss=2.7509, lr=0.000240, tokens/sec=1375045.26, grad_norm=0.3177, duration=0.38s
Step 18667: loss=2.8082, lr=0.000240, tokens/sec=1375102.87, grad_norm=0.3358, duration=0.38s
Step 18668: loss=2.8746, lr=0.000240, tokens/sec=1377960.17, grad_norm=0.3252, duration=0.38s
Step 18669: loss=2.8173, lr=0.000240, tokens/sec=1377470.76, grad_norm=0.3309, duration=0.38s
Step 18670: loss=2.8060, lr=0.000240, tokens/sec=1370780.95, grad_norm=0.3383, duration=0.38s
Step 18671: loss=2.8100, lr=0.000240, tokens/sec=1374368.06, grad_norm=0.3406, duration=0.38s
Step 18672: loss=2.8013, lr=0.000240, tokens/sec=1372056.18, grad_norm=0.3298, duration=0.38s
Step 18673: loss=2.8243, lr=0.000240, tokens/sec=1376549.86, grad_norm=0.3277, duration=0.38s
Step 18674: loss=2.7875, lr=0.000240, tokens/sec=1376475.76, grad_norm=0.3229, duration=0.38s
Step 18675: loss=2.8000, lr=0.000240, tokens/sec=1376639.48, grad_norm=0.3352, duration=0.38s
Step 18676: loss=2.7998, lr=0.000240, tokens/sec=1376271.59, grad_norm=0.3380, duration=0.38s
Step 18677: loss=2.7802, lr=0.000240, tokens/sec=1376184.60, grad_norm=0.3406, duration=0.38s
Step 18678: loss=2.7856, lr=0.000240, tokens/sec=1374935.21, grad_norm=0.3534, duration=0.38s
Step 18679: loss=2.7749, lr=0.000240, tokens/sec=1376144.98, grad_norm=0.3202, duration=0.38s
Step 18680: loss=2.7959, lr=0.000240, tokens/sec=1375983.96, grad_norm=0.3125, duration=0.38s
Step 18681: loss=2.7128, lr=0.000240, tokens/sec=1371752.34, grad_norm=0.3602, duration=0.38s
Step 18682: loss=2.7916, lr=0.000240, tokens/sec=1374187.70, grad_norm=0.3531, duration=0.38s
Step 18683: loss=2.8135, lr=0.000240, tokens/sec=1371056.15, grad_norm=0.3407, duration=0.38s
Step 18684: loss=2.8007, lr=0.000240, tokens/sec=1373475.32, grad_norm=0.3203, duration=0.38s
Step 18685: loss=2.8327, lr=0.000240, tokens/sec=1374561.35, grad_norm=0.3372, duration=0.38s
Step 18686: loss=2.7858, lr=0.000240, tokens/sec=1374919.74, grad_norm=0.3450, duration=0.38s
Step 18687: loss=2.8406, lr=0.000240, tokens/sec=1376987.74, grad_norm=0.3501, duration=0.38s
Step 18688: loss=2.8143, lr=0.000240, tokens/sec=1373262.60, grad_norm=0.3349, duration=0.38s
Step 18689: loss=2.8084, lr=0.000240, tokens/sec=1376619.66, grad_norm=0.3193, duration=0.38s
Step 18690: loss=2.8110, lr=0.000240, tokens/sec=1376748.94, grad_norm=0.3215, duration=0.38s
Step 18691: loss=2.8047, lr=0.000240, tokens/sec=1376505.91, grad_norm=0.3302, duration=0.38s
Step 18692: loss=2.7726, lr=0.000240, tokens/sec=1376329.30, grad_norm=0.3314, duration=0.38s
Step 18693: loss=2.8207, lr=0.000240, tokens/sec=1376666.20, grad_norm=0.3301, duration=0.38s
Step 18694: loss=2.8223, lr=0.000240, tokens/sec=1376722.22, grad_norm=0.3179, duration=0.38s
Step 18695: loss=2.7894, lr=0.000240, tokens/sec=1372998.52, grad_norm=0.3136, duration=0.38s
Step 18696: loss=2.7909, lr=0.000240, tokens/sec=1377834.12, grad_norm=0.3238, duration=0.38s
Step 18697: loss=2.7566, lr=0.000240, tokens/sec=1373483.04, grad_norm=0.3232, duration=0.38s
Step 18698: loss=2.7877, lr=0.000240, tokens/sec=1375917.66, grad_norm=0.3202, duration=0.38s
Step 18699: loss=2.7925, lr=0.000240, tokens/sec=1376654.99, grad_norm=0.2898, duration=0.38s
Step 18700/19073 (98.0%), Elapsed time: 7397.51s, Steps per hour: 9100.36, Estimated hours remaining: 0.04
Step 18700: loss=2.7877, lr=0.000240, tokens/sec=1376665.33, grad_norm=0.2910, duration=0.38s
Step 18701: loss=2.7009, lr=0.000240, tokens/sec=1375353.14, grad_norm=0.3150, duration=0.38s
Step 18702: loss=2.7761, lr=0.000240, tokens/sec=1375720.54, grad_norm=0.3104, duration=0.38s
Step 18703: loss=2.7263, lr=0.000240, tokens/sec=1378144.98, grad_norm=0.3086, duration=0.38s
Step 18704: loss=2.7417, lr=0.000240, tokens/sec=1378849.25, grad_norm=0.2908, duration=0.38s
Step 18705: loss=2.6964, lr=0.000240, tokens/sec=1374534.72, grad_norm=0.3075, duration=0.38s
Step 18706: loss=2.6976, lr=0.000240, tokens/sec=1376259.53, grad_norm=0.3062, duration=0.38s
Step 18707: loss=2.7366, lr=0.000240, tokens/sec=1377601.07, grad_norm=0.3067, duration=0.38s
Step 18708: loss=2.7319, lr=0.000240, tokens/sec=1377062.76, grad_norm=0.3176, duration=0.38s
Step 18709: loss=2.7597, lr=0.000240, tokens/sec=1377040.34, grad_norm=0.3176, duration=0.38s
Step 18710: loss=2.7222, lr=0.000240, tokens/sec=1377831.53, grad_norm=0.3114, duration=0.38s
Step 18711: loss=2.7631, lr=0.000240, tokens/sec=1376543.83, grad_norm=0.3045, duration=0.38s
Step 18712: loss=2.7616, lr=0.000240, tokens/sec=1377269.75, grad_norm=0.3068, duration=0.38s
Step 18713: loss=2.7668, lr=0.000240, tokens/sec=1378151.89, grad_norm=0.3248, duration=0.38s
Step 18714: loss=2.8329, lr=0.000240, tokens/sec=1376666.20, grad_norm=0.3196, duration=0.38s
Step 18715: loss=2.7952, lr=0.000240, tokens/sec=1378568.32, grad_norm=0.3332, duration=0.38s
Step 18716: loss=2.8246, lr=0.000240, tokens/sec=1375409.92, grad_norm=0.3159, duration=0.38s
Step 18717: loss=2.8205, lr=0.000240, tokens/sec=1373718.99, grad_norm=0.3198, duration=0.38s
Step 18718: loss=2.7835, lr=0.000240, tokens/sec=1375588.02, grad_norm=0.3386, duration=0.38s
Step 18719: loss=2.8390, lr=0.000240, tokens/sec=1378248.63, grad_norm=0.3496, duration=0.38s
Step 18720: loss=2.7845, lr=0.000240, tokens/sec=1377526.85, grad_norm=0.3320, duration=0.38s
Step 18721: loss=2.8436, lr=0.000240, tokens/sec=1374019.41, grad_norm=0.3285, duration=0.38s
Step 18722: loss=2.8651, lr=0.000240, tokens/sec=1378671.17, grad_norm=0.3344, duration=0.38s
Step 18723: loss=2.7845, lr=0.000240, tokens/sec=1374701.42, grad_norm=0.3195, duration=0.38s
Step 18724: loss=2.8285, lr=0.000240, tokens/sec=1376300.01, grad_norm=0.3204, duration=0.38s
Step 18725: loss=2.7986, lr=0.000240, tokens/sec=1378387.72, grad_norm=0.3188, duration=0.38s
Step 18726: loss=2.8289, lr=0.000240, tokens/sec=1373852.02, grad_norm=0.3351, duration=0.38s
Step 18727: loss=2.7913, lr=0.000240, tokens/sec=1373601.43, grad_norm=0.3306, duration=0.38s
Step 18728: loss=2.8119, lr=0.000240, tokens/sec=1374765.02, grad_norm=0.3180, duration=0.38s
Step 18729: loss=2.6972, lr=0.000240, tokens/sec=1377149.86, grad_norm=0.3473, duration=0.38s
Step 18730: loss=2.8215, lr=0.000240, tokens/sec=1375772.19, grad_norm=0.3250, duration=0.38s
Step 18731: loss=2.8067, lr=0.000240, tokens/sec=1374301.06, grad_norm=0.3404, duration=0.38s
Step 18732: loss=2.8058, lr=0.000240, tokens/sec=1376510.22, grad_norm=0.3234, duration=0.38s
Step 18733: loss=2.7617, lr=0.000240, tokens/sec=1379817.39, grad_norm=0.3319, duration=0.38s
Step 18734: loss=2.8056, lr=0.000240, tokens/sec=1375865.15, grad_norm=0.3373, duration=0.38s
Step 18735: loss=2.7622, lr=0.000240, tokens/sec=1375254.23, grad_norm=0.3365, duration=0.38s
Step 18736: loss=2.8062, lr=0.000240, tokens/sec=1375524.34, grad_norm=0.3245, duration=0.38s
Step 18737: loss=2.7858, lr=0.000240, tokens/sec=1376602.42, grad_norm=0.3129, duration=0.38s
Step 18738: loss=2.8351, lr=0.000240, tokens/sec=1373516.49, grad_norm=0.3228, duration=0.38s
Step 18739: loss=2.7845, lr=0.000240, tokens/sec=1373100.54, grad_norm=0.3304, duration=0.38s
Step 18740: loss=2.7871, lr=0.000240, tokens/sec=1374651.58, grad_norm=0.3234, duration=0.38s
Step 18741: loss=2.7998, lr=0.000240, tokens/sec=1375610.39, grad_norm=0.3007, duration=0.38s
Step 18742: loss=2.8147, lr=0.000240, tokens/sec=1378387.72, grad_norm=0.3032, duration=0.38s
Step 18743: loss=2.8069, lr=0.000240, tokens/sec=1377298.22, grad_norm=0.3156, duration=0.38s
Step 18744: loss=2.7987, lr=0.000240, tokens/sec=1375820.39, grad_norm=0.3140, duration=0.38s
Step 18745: loss=2.8070, lr=0.000240, tokens/sec=1374557.92, grad_norm=0.3153, duration=0.38s
Step 18746: loss=2.7666, lr=0.000240, tokens/sec=1373441.86, grad_norm=0.3091, duration=0.38s
Step 18747: loss=2.7808, lr=0.000240, tokens/sec=1378411.91, grad_norm=0.3172, duration=0.38s
Step 18748: loss=2.7536, lr=0.000240, tokens/sec=1376345.67, grad_norm=0.3299, duration=0.38s
Step 18749: loss=2.7311, lr=0.000240, tokens/sec=1376983.43, grad_norm=0.3248, duration=0.38s
Validation loss at step 18750: 3.85919189453125
Step 18750: loss=2.7574, lr=0.000240, tokens/sec=154830.15, grad_norm=0.3176, duration=3.39s
Step 18751: loss=2.6602, lr=0.000240, tokens/sec=1376886.00, grad_norm=0.3155, duration=0.38s
Step 18752: loss=2.7394, lr=0.000240, tokens/sec=1375014.31, grad_norm=0.3258, duration=0.38s
Step 18753: loss=2.7326, lr=0.000240, tokens/sec=1375232.72, grad_norm=0.3215, duration=0.38s
Step 18754: loss=2.7279, lr=0.000240, tokens/sec=1375059.02, grad_norm=0.3174, duration=0.38s
Step 18755: loss=2.7467, lr=0.000240, tokens/sec=1379535.20, grad_norm=0.3226, duration=0.38s
Step 18756: loss=2.7218, lr=0.000240, tokens/sec=1375278.31, grad_norm=0.3260, duration=0.38s
Step 18757: loss=2.7387, lr=0.000240, tokens/sec=1374008.25, grad_norm=0.3036, duration=0.38s
Step 18758: loss=2.7626, lr=0.000240, tokens/sec=1376623.97, grad_norm=0.2952, duration=0.38s
Step 18759: loss=2.7979, lr=0.000240, tokens/sec=1380600.51, grad_norm=0.3347, duration=0.38s
Step 18760: loss=2.7870, lr=0.000240, tokens/sec=1373768.76, grad_norm=0.3691, duration=0.38s
Step 18761: loss=2.8174, lr=0.000240, tokens/sec=1372608.58, grad_norm=0.3551, duration=0.38s
Step 18762: loss=2.8537, lr=0.000240, tokens/sec=1374673.06, grad_norm=0.3186, duration=0.38s
Step 18763: loss=2.8249, lr=0.000240, tokens/sec=1379358.67, grad_norm=0.3076, duration=0.38s
Step 18764: loss=2.8368, lr=0.000240, tokens/sec=1378273.68, grad_norm=0.3332, duration=0.38s
Step 18765: loss=2.7881, lr=0.000240, tokens/sec=1372315.62, grad_norm=0.3678, duration=0.38s
Step 18766: loss=2.8062, lr=0.000240, tokens/sec=1375811.78, grad_norm=0.3691, duration=0.38s
Step 18767: loss=2.8547, lr=0.000240, tokens/sec=1377900.60, grad_norm=0.3316, duration=0.38s
Step 18768: loss=2.9178, lr=0.000240, tokens/sec=1372191.46, grad_norm=0.3112, duration=0.38s
Step 18769: loss=2.8447, lr=0.000240, tokens/sec=1376349.97, grad_norm=0.3259, duration=0.38s
Step 18770: loss=2.8419, lr=0.000240, tokens/sec=1372183.75, grad_norm=0.3617, duration=0.38s
Step 18771: loss=2.7969, lr=0.000240, tokens/sec=1374718.61, grad_norm=0.3525, duration=0.38s
Step 18772: loss=2.8191, lr=0.000240, tokens/sec=1375011.73, grad_norm=0.3459, duration=0.38s
Step 18773: loss=2.8100, lr=0.000240, tokens/sec=1376051.12, grad_norm=0.2966, duration=0.38s
Step 18774: loss=2.7529, lr=0.000240, tokens/sec=1372704.54, grad_norm=0.3134, duration=0.38s
Step 18775: loss=2.8503, lr=0.000240, tokens/sec=1376812.73, grad_norm=0.3519, duration=0.38s
Step 18776: loss=2.7799, lr=0.000240, tokens/sec=1374838.94, grad_norm=0.3664, duration=0.38s
Step 18777: loss=2.8220, lr=0.000240, tokens/sec=1376279.34, grad_norm=0.3693, duration=0.38s
Step 18778: loss=2.8742, lr=0.000240, tokens/sec=1375136.41, grad_norm=0.3409, duration=0.38s
Step 18779: loss=2.8181, lr=0.000240, tokens/sec=1375850.52, grad_norm=0.3339, duration=0.38s
Step 18780: loss=2.7587, lr=0.000240, tokens/sec=1374650.72, grad_norm=0.3273, duration=0.38s
Step 18781: loss=2.8264, lr=0.000240, tokens/sec=1374800.26, grad_norm=0.3521, duration=0.38s
Step 18782: loss=2.8454, lr=0.000240, tokens/sec=1372902.51, grad_norm=0.3287, duration=0.38s
Step 18783: loss=2.8309, lr=0.000240, tokens/sec=1375084.81, grad_norm=0.3376, duration=0.38s
Step 18784: loss=2.7537, lr=0.000240, tokens/sec=1376876.52, grad_norm=0.3329, duration=0.38s
Step 18785: loss=2.8006, lr=0.000240, tokens/sec=1375126.09, grad_norm=0.3163, duration=0.38s
Step 18786: loss=2.7963, lr=0.000240, tokens/sec=1375961.57, grad_norm=0.3173, duration=0.38s
Step 18787: loss=2.8185, lr=0.000240, tokens/sec=1371795.13, grad_norm=0.3330, duration=0.38s
Step 18788: loss=2.8711, lr=0.000240, tokens/sec=1373608.30, grad_norm=0.3456, duration=0.38s
Step 18789: loss=2.7630, lr=0.000240, tokens/sec=1372194.02, grad_norm=0.3468, duration=0.38s
Step 18790: loss=2.7789, lr=0.000240, tokens/sec=1374426.47, grad_norm=0.3222, duration=0.38s
Step 18791: loss=2.7734, lr=0.000240, tokens/sec=1373331.21, grad_norm=0.3077, duration=0.38s
Step 18792: loss=2.7423, lr=0.000240, tokens/sec=1372566.59, grad_norm=0.3083, duration=0.38s
Step 18793: loss=2.7859, lr=0.000240, tokens/sec=1377814.26, grad_norm=0.3259, duration=0.38s
Step 18794: loss=2.7233, lr=0.000240, tokens/sec=1375934.88, grad_norm=0.3401, duration=0.38s
Step 18795: loss=2.7210, lr=0.000240, tokens/sec=1374060.62, grad_norm=0.3481, duration=0.38s
Step 18796: loss=2.7200, lr=0.000240, tokens/sec=1371787.43, grad_norm=0.3226, duration=0.38s
Step 18797: loss=2.7735, lr=0.000240, tokens/sec=1376928.25, grad_norm=0.3012, duration=0.38s
Step 18798: loss=2.7367, lr=0.000240, tokens/sec=1377498.38, grad_norm=0.3076, duration=0.38s
Step 18799: loss=2.7217, lr=0.000240, tokens/sec=1378481.04, grad_norm=0.3232, duration=0.38s
Step 18800/19073 (98.6%), Elapsed time: 7438.72s, Steps per hour: 9098.34, Estimated hours remaining: 0.03
Step 18800: loss=2.6306, lr=0.000240, tokens/sec=1373404.12, grad_norm=0.3332, duration=0.38s
Step 18801: loss=2.7141, lr=0.000240, tokens/sec=1374622.36, grad_norm=0.3332, duration=0.38s
Step 18802: loss=2.7523, lr=0.000240, tokens/sec=1369630.07, grad_norm=0.3134, duration=0.38s
Step 18803: loss=2.7686, lr=0.000240, tokens/sec=1378549.31, grad_norm=0.2957, duration=0.38s
Step 18804: loss=2.7807, lr=0.000240, tokens/sec=1374990.23, grad_norm=0.3107, duration=0.38s
Step 18805: loss=2.8232, lr=0.000240, tokens/sec=1373648.62, grad_norm=0.3377, duration=0.38s
Step 18806: loss=2.8137, lr=0.000240, tokens/sec=1378380.81, grad_norm=0.3407, duration=0.38s
Step 18807: loss=2.8177, lr=0.000240, tokens/sec=1375965.88, grad_norm=0.3192, duration=0.38s
Step 18808: loss=2.7749, lr=0.000240, tokens/sec=1377343.07, grad_norm=0.3045, duration=0.38s
Step 18809: loss=2.7754, lr=0.000240, tokens/sec=1374162.80, grad_norm=0.3029, duration=0.38s
Step 18810: loss=2.7984, lr=0.000240, tokens/sec=1377339.62, grad_norm=0.3308, duration=0.38s
Step 18811: loss=2.8302, lr=0.000240, tokens/sec=1374463.41, grad_norm=0.3534, duration=0.38s
Step 18812: loss=2.8270, lr=0.000240, tokens/sec=1375492.51, grad_norm=0.3472, duration=0.38s
Step 18813: loss=2.9232, lr=0.000240, tokens/sec=1376146.70, grad_norm=0.3280, duration=0.38s
Step 18814: loss=2.8264, lr=0.000240, tokens/sec=1376001.18, grad_norm=0.3091, duration=0.38s
Step 18815: loss=2.8648, lr=0.000240, tokens/sec=1376734.29, grad_norm=0.3432, duration=0.38s
Step 18816: loss=2.8979, lr=0.000240, tokens/sec=1373565.40, grad_norm=0.3817, duration=0.38s
Step 18817: loss=2.8304, lr=0.000240, tokens/sec=1376058.87, grad_norm=0.3551, duration=0.38s
Step 18818: loss=2.8607, lr=0.000240, tokens/sec=1376833.41, grad_norm=0.3208, duration=0.38s
Step 18819: loss=2.7885, lr=0.000240, tokens/sec=1373902.66, grad_norm=0.3200, duration=0.38s
Step 18820: loss=2.8333, lr=0.000240, tokens/sec=1378261.59, grad_norm=0.3266, duration=0.38s
Step 18821: loss=2.7682, lr=0.000240, tokens/sec=1375060.74, grad_norm=0.3215, duration=0.38s
Step 18822: loss=2.8112, lr=0.000240, tokens/sec=1376037.34, grad_norm=0.3309, duration=0.38s
Step 18823: loss=2.7531, lr=0.000240, tokens/sec=1376807.55, grad_norm=0.3276, duration=0.38s
Step 18824: loss=2.7910, lr=0.000240, tokens/sec=1374502.07, grad_norm=0.3325, duration=0.38s
Step 18825: loss=2.7988, lr=0.000240, tokens/sec=1375638.79, grad_norm=0.3544, duration=0.38s
Step 18826: loss=2.7455, lr=0.000240, tokens/sec=1377463.86, grad_norm=0.3197, duration=0.38s
Step 18827: loss=2.8306, lr=0.000240, tokens/sec=1377670.11, grad_norm=0.3225, duration=0.38s
Step 18828: loss=2.7569, lr=0.000240, tokens/sec=1377285.28, grad_norm=0.3465, duration=0.38s
Step 18829: loss=2.7268, lr=0.000240, tokens/sec=1374046.88, grad_norm=0.3988, duration=0.38s
Step 18830: loss=2.7523, lr=0.000240, tokens/sec=1375693.86, grad_norm=0.3146, duration=0.38s
Step 18831: loss=2.8189, lr=0.000240, tokens/sec=1374701.42, grad_norm=0.3217, duration=0.38s
Step 18832: loss=2.8528, lr=0.000240, tokens/sec=1375253.37, grad_norm=0.3431, duration=0.38s
Step 18833: loss=2.7077, lr=0.000240, tokens/sec=1377333.59, grad_norm=0.3493, duration=0.38s
Step 18834: loss=2.8308, lr=0.000240, tokens/sec=1373028.52, grad_norm=0.3318, duration=0.38s
Step 18835: loss=2.8167, lr=0.000240, tokens/sec=1378313.42, grad_norm=0.3288, duration=0.38s
Step 18836: loss=2.7376, lr=0.000240, tokens/sec=1377253.36, grad_norm=0.3156, duration=0.38s
Step 18837: loss=2.7925, lr=0.000240, tokens/sec=1374997.11, grad_norm=0.3268, duration=0.38s
Step 18838: loss=2.8237, lr=0.000240, tokens/sec=1376464.56, grad_norm=0.3180, duration=0.38s
Step 18839: loss=2.7771, lr=0.000240, tokens/sec=1374064.91, grad_norm=0.3224, duration=0.38s
Step 18840: loss=2.7292, lr=0.000240, tokens/sec=1376928.25, grad_norm=0.3235, duration=0.38s
Step 18841: loss=2.7790, lr=0.000240, tokens/sec=1374372.36, grad_norm=0.3234, duration=0.38s
Step 18842: loss=2.7741, lr=0.000240, tokens/sec=1377224.90, grad_norm=0.3424, duration=0.38s
Step 18843: loss=2.8764, lr=0.000240, tokens/sec=1375121.79, grad_norm=0.3347, duration=0.38s
Step 18844: loss=2.8137, lr=0.000240, tokens/sec=1376149.29, grad_norm=0.3223, duration=0.38s
Step 18845: loss=2.7222, lr=0.000240, tokens/sec=1379818.26, grad_norm=0.3213, duration=0.38s
Step 18846: loss=2.7667, lr=0.000240, tokens/sec=1374144.77, grad_norm=0.3184, duration=0.38s
Step 18847: loss=2.6866, lr=0.000240, tokens/sec=1375715.38, grad_norm=0.3223, duration=0.38s
Step 18848: loss=2.7224, lr=0.000240, tokens/sec=1374669.62, grad_norm=0.3369, duration=0.38s
Step 18849: loss=2.7499, lr=0.000240, tokens/sec=1378661.66, grad_norm=0.3344, duration=0.38s
Step 18850: loss=2.7511, lr=0.000240, tokens/sec=1375497.67, grad_norm=0.3273, duration=0.38s
Step 18851: loss=2.7876, lr=0.000240, tokens/sec=1377590.71, grad_norm=0.3491, duration=0.38s
Step 18852: loss=2.7737, lr=0.000240, tokens/sec=1376458.52, grad_norm=0.3415, duration=0.38s
Step 18853: loss=2.7633, lr=0.000240, tokens/sec=1379620.02, grad_norm=0.3389, duration=0.38s
Step 18854: loss=2.7745, lr=0.000240, tokens/sec=1376524.01, grad_norm=0.3295, duration=0.38s
Step 18855: loss=2.7386, lr=0.000240, tokens/sec=1381308.16, grad_norm=0.3256, duration=0.38s
Step 18856: loss=2.7437, lr=0.000240, tokens/sec=1376507.64, grad_norm=0.3106, duration=0.38s
Step 18857: loss=2.8317, lr=0.000240, tokens/sec=1377421.58, grad_norm=0.3434, duration=0.38s
Step 18858: loss=2.8589, lr=0.000240, tokens/sec=1377982.62, grad_norm=0.3356, duration=0.38s
Step 18859: loss=2.8100, lr=0.000240, tokens/sec=1378615.85, grad_norm=0.3335, duration=0.38s
Step 18860: loss=2.7824, lr=0.000240, tokens/sec=1372689.97, grad_norm=0.3371, duration=0.38s
Step 18861: loss=2.8084, lr=0.000240, tokens/sec=1372561.45, grad_norm=0.3413, duration=0.38s
Step 18862: loss=2.8224, lr=0.000240, tokens/sec=1371735.23, grad_norm=0.3437, duration=0.38s
Step 18863: loss=2.8191, lr=0.000240, tokens/sec=1375659.44, grad_norm=0.3389, duration=0.38s
Step 18864: loss=2.8046, lr=0.000240, tokens/sec=1372076.73, grad_norm=0.3132, duration=0.38s
Step 18865: loss=2.7679, lr=0.000240, tokens/sec=1376101.06, grad_norm=0.3169, duration=0.38s
Step 18866: loss=2.7948, lr=0.000240, tokens/sec=1371577.80, grad_norm=0.3408, duration=0.38s
Step 18867: loss=2.7873, lr=0.000240, tokens/sec=1374131.03, grad_norm=0.3552, duration=0.38s
Step 18868: loss=2.7727, lr=0.000240, tokens/sec=1377860.88, grad_norm=0.3404, duration=0.38s
Step 18869: loss=2.7926, lr=0.000240, tokens/sec=1375691.28, grad_norm=0.3106, duration=0.38s
Step 18870: loss=2.7839, lr=0.000240, tokens/sec=1379681.48, grad_norm=0.3236, duration=0.38s
Step 18871: loss=2.7352, lr=0.000240, tokens/sec=1375496.81, grad_norm=0.3360, duration=0.38s
Step 18872: loss=2.7709, lr=0.000240, tokens/sec=1377324.96, grad_norm=0.3486, duration=0.38s
Step 18873: loss=2.8076, lr=0.000240, tokens/sec=1374886.21, grad_norm=0.3384, duration=0.38s
Step 18874: loss=2.7975, lr=0.000240, tokens/sec=1371640.25, grad_norm=0.3211, duration=0.38s
Step 18875: loss=2.8372, lr=0.000240, tokens/sec=1375221.54, grad_norm=0.3363, duration=0.38s
Step 18876: loss=2.7654, lr=0.000240, tokens/sec=1378271.09, grad_norm=0.3412, duration=0.38s
Step 18877: loss=2.8457, lr=0.000240, tokens/sec=1376421.48, grad_norm=0.3467, duration=0.38s
Step 18878: loss=2.8183, lr=0.000240, tokens/sec=1377380.17, grad_norm=0.3326, duration=0.38s
Step 18879: loss=2.8085, lr=0.000240, tokens/sec=1370158.31, grad_norm=0.3271, duration=0.38s
Step 18880: loss=2.7984, lr=0.000240, tokens/sec=1375384.11, grad_norm=0.3286, duration=0.38s
Step 18881: loss=2.7962, lr=0.000240, tokens/sec=1374564.79, grad_norm=0.3322, duration=0.38s
Step 18882: loss=2.7998, lr=0.000240, tokens/sec=1375149.30, grad_norm=0.3284, duration=0.38s
Step 18883: loss=2.8168, lr=0.000240, tokens/sec=1371717.26, grad_norm=0.3285, duration=0.38s
Step 18884: loss=2.7947, lr=0.000240, tokens/sec=1376512.81, grad_norm=0.3241, duration=0.38s
Step 18885: loss=2.7813, lr=0.000240, tokens/sec=1378526.84, grad_norm=0.3030, duration=0.38s
Step 18886: loss=2.7964, lr=0.000240, tokens/sec=1373934.42, grad_norm=0.3164, duration=0.38s
Step 18887: loss=2.7481, lr=0.000240, tokens/sec=1373968.76, grad_norm=0.3266, duration=0.38s
Step 18888: loss=2.8040, lr=0.000240, tokens/sec=1374124.16, grad_norm=0.3223, duration=0.38s
Step 18889: loss=2.7751, lr=0.000240, tokens/sec=1372801.38, grad_norm=0.3078, duration=0.38s
Step 18890: loss=2.7679, lr=0.000240, tokens/sec=1359498.77, grad_norm=0.2991, duration=0.39s
Step 18891: loss=2.7126, lr=0.000240, tokens/sec=1374606.04, grad_norm=0.3026, duration=0.38s
Step 18892: loss=2.7758, lr=0.000240, tokens/sec=1373749.88, grad_norm=0.3136, duration=0.38s
Step 18893: loss=2.7319, lr=0.000240, tokens/sec=1375310.99, grad_norm=0.3287, duration=0.38s
Step 18894: loss=2.7394, lr=0.000240, tokens/sec=1377387.07, grad_norm=0.3087, duration=0.38s
Step 18895: loss=2.6653, lr=0.000240, tokens/sec=1373418.70, grad_norm=0.2968, duration=0.38s
Step 18896: loss=2.7151, lr=0.000240, tokens/sec=1375335.08, grad_norm=0.3082, duration=0.38s
Step 18897: loss=2.7774, lr=0.000240, tokens/sec=1379935.15, grad_norm=0.3240, duration=0.38s
Step 18898: loss=2.7353, lr=0.000240, tokens/sec=1376214.74, grad_norm=0.3312, duration=0.38s
Step 18899: loss=2.7410, lr=0.000240, tokens/sec=1373507.06, grad_norm=0.3250, duration=0.38s
Step 18900/19073 (99.1%), Elapsed time: 7476.92s, Steps per hour: 9100.01, Estimated hours remaining: 0.02
Step 18900: loss=2.7061, lr=0.000240, tokens/sec=1375908.19, grad_norm=0.3144, duration=0.38s
Step 18901: loss=2.7770, lr=0.000240, tokens/sec=1376822.21, grad_norm=0.3028, duration=0.38s
Step 18902: loss=2.7447, lr=0.000240, tokens/sec=1375279.17, grad_norm=0.3088, duration=0.38s
Step 18903: loss=2.7867, lr=0.000240, tokens/sec=1374050.32, grad_norm=0.3249, duration=0.38s
Step 18904: loss=2.7913, lr=0.000240, tokens/sec=1373849.44, grad_norm=0.3235, duration=0.38s
Step 18905: loss=2.8162, lr=0.000240, tokens/sec=1371975.71, grad_norm=0.3327, duration=0.38s
Step 18906: loss=2.8194, lr=0.000240, tokens/sec=1377606.25, grad_norm=0.3184, duration=0.38s
Step 18907: loss=2.8484, lr=0.000240, tokens/sec=1376558.47, grad_norm=0.3166, duration=0.38s
Step 18908: loss=2.7897, lr=0.000240, tokens/sec=1372047.62, grad_norm=0.3344, duration=0.38s
Step 18909: loss=2.8139, lr=0.000240, tokens/sec=1379282.54, grad_norm=0.3465, duration=0.38s
Step 18910: loss=2.7794, lr=0.000240, tokens/sec=1373206.86, grad_norm=0.3361, duration=0.38s
Step 18911: loss=2.8719, lr=0.000240, tokens/sec=1378275.41, grad_norm=0.3196, duration=0.38s
Step 18912: loss=2.7957, lr=0.000240, tokens/sec=1372923.08, grad_norm=0.3184, duration=0.38s
Step 18913: loss=2.8267, lr=0.000240, tokens/sec=1373838.28, grad_norm=0.3276, duration=0.38s
Step 18914: loss=2.8323, lr=0.000240, tokens/sec=1375599.20, grad_norm=0.3301, duration=0.38s
Step 18915: loss=2.8142, lr=0.000240, tokens/sec=1375279.17, grad_norm=0.3257, duration=0.38s
Step 18916: loss=2.7804, lr=0.000240, tokens/sec=1377749.52, grad_norm=0.3403, duration=0.38s
Step 18917: loss=2.8082, lr=0.000240, tokens/sec=1379280.81, grad_norm=0.3258, duration=0.38s
Step 18918: loss=2.7451, lr=0.000240, tokens/sec=1376946.35, grad_norm=0.3530, duration=0.38s
Step 18919: loss=2.7659, lr=0.000240, tokens/sec=1374041.73, grad_norm=0.3251, duration=0.38s
Step 18920: loss=2.8146, lr=0.000240, tokens/sec=1378412.78, grad_norm=0.3405, duration=0.38s
Step 18921: loss=2.7839, lr=0.000240, tokens/sec=1377401.74, grad_norm=0.3433, duration=0.38s
Step 18922: loss=2.8205, lr=0.000240, tokens/sec=1373333.78, grad_norm=0.3308, duration=0.38s
Step 18923: loss=2.7414, lr=0.000240, tokens/sec=1378114.75, grad_norm=0.3285, duration=0.38s
Step 18924: loss=2.8118, lr=0.000240, tokens/sec=1376794.62, grad_norm=0.3456, duration=0.38s
Step 18925: loss=2.7493, lr=0.000240, tokens/sec=1374979.92, grad_norm=0.3376, duration=0.38s
Step 18926: loss=2.8137, lr=0.000240, tokens/sec=1375548.43, grad_norm=0.3308, duration=0.38s
Step 18927: loss=2.8130, lr=0.000240, tokens/sec=1377211.10, grad_norm=0.3084, duration=0.38s
Step 18928: loss=2.7977, lr=0.000240, tokens/sec=1375048.70, grad_norm=0.3227, duration=0.38s
Step 18929: loss=2.7746, lr=0.000240, tokens/sec=1375878.92, grad_norm=0.3371, duration=0.38s
Step 18930: loss=2.7983, lr=0.000240, tokens/sec=1375820.39, grad_norm=0.3249, duration=0.38s
Step 18931: loss=2.8081, lr=0.000240, tokens/sec=1373864.89, grad_norm=0.3082, duration=0.38s
Step 18932: loss=2.8271, lr=0.000240, tokens/sec=1374246.96, grad_norm=0.3095, duration=0.38s
Step 18933: loss=2.7781, lr=0.000240, tokens/sec=1376539.52, grad_norm=0.3347, duration=0.38s
Step 18934: loss=2.8047, lr=0.000240, tokens/sec=1375971.04, grad_norm=0.3256, duration=0.38s
Step 18935: loss=2.7766, lr=0.000240, tokens/sec=1375169.94, grad_norm=0.3177, duration=0.38s
Step 18936: loss=2.7905, lr=0.000240, tokens/sec=1377137.79, grad_norm=0.3074, duration=0.38s
Step 18937: loss=2.7562, lr=0.000240, tokens/sec=1375116.63, grad_norm=0.3303, duration=0.38s
Step 18938: loss=2.7805, lr=0.000240, tokens/sec=1377308.57, grad_norm=0.3349, duration=0.38s
Step 18939: loss=2.7221, lr=0.000240, tokens/sec=1371888.41, grad_norm=0.3118, duration=0.38s
Step 18940: loss=2.7339, lr=0.000240, tokens/sec=1373525.07, grad_norm=0.3124, duration=0.38s
Step 18941: loss=2.6948, lr=0.000240, tokens/sec=1378071.57, grad_norm=0.3186, duration=0.38s
Step 18942: loss=2.7269, lr=0.000240, tokens/sec=1375739.48, grad_norm=0.3353, duration=0.38s
Step 18943: loss=2.6661, lr=0.000240, tokens/sec=1374228.06, grad_norm=0.3288, duration=0.38s
Step 18944: loss=2.7763, lr=0.000240, tokens/sec=1375411.64, grad_norm=0.2991, duration=0.38s
Step 18945: loss=2.7424, lr=0.000240, tokens/sec=1379404.53, grad_norm=0.3208, duration=0.38s
Step 18946: loss=2.7499, lr=0.000240, tokens/sec=1376464.56, grad_norm=0.3336, duration=0.38s
Step 18947: loss=2.7285, lr=0.000240, tokens/sec=1374642.13, grad_norm=0.3137, duration=0.38s
Step 18948: loss=2.7616, lr=0.000240, tokens/sec=1374146.48, grad_norm=0.3113, duration=0.38s
Step 18949: loss=2.7750, lr=0.000240, tokens/sec=1378698.83, grad_norm=0.3180, duration=0.38s
Step 18950: loss=2.7974, lr=0.000240, tokens/sec=1374094.10, grad_norm=0.3342, duration=0.38s
Step 18951: loss=2.8518, lr=0.000240, tokens/sec=1377123.99, grad_norm=0.3471, duration=0.38s
Step 18952: loss=2.8450, lr=0.000240, tokens/sec=1377055.00, grad_norm=0.3408, duration=0.38s
Step 18953: loss=2.8087, lr=0.000240, tokens/sec=1374362.05, grad_norm=0.3168, duration=0.38s
Step 18954: loss=2.8327, lr=0.000240, tokens/sec=1376453.35, grad_norm=0.3224, duration=0.38s
Step 18955: loss=2.7793, lr=0.000240, tokens/sec=1375786.82, grad_norm=0.3404, duration=0.38s
Step 18956: loss=2.8082, lr=0.000240, tokens/sec=1376154.45, grad_norm=0.3732, duration=0.38s
Step 18957: loss=2.8604, lr=0.000240, tokens/sec=1373885.49, grad_norm=0.3716, duration=0.38s
Step 18958: loss=2.8937, lr=0.000240, tokens/sec=1375835.02, grad_norm=0.3341, duration=0.38s
Step 18959: loss=2.8493, lr=0.000240, tokens/sec=1375879.78, grad_norm=0.3213, duration=0.38s
Step 18960: loss=2.8406, lr=0.000240, tokens/sec=1374762.44, grad_norm=0.3337, duration=0.38s
Step 18961: loss=2.7904, lr=0.000240, tokens/sec=1375503.69, grad_norm=0.3517, duration=0.38s
Step 18962: loss=2.8490, lr=0.000240, tokens/sec=1375949.52, grad_norm=0.3608, duration=0.38s
Step 18963: loss=2.7814, lr=0.000240, tokens/sec=1379642.52, grad_norm=0.3318, duration=0.38s
Step 18964: loss=2.7607, lr=0.000240, tokens/sec=1374809.71, grad_norm=0.3123, duration=0.38s
Step 18965: loss=2.8644, lr=0.000240, tokens/sec=1375638.79, grad_norm=0.3284, duration=0.38s
Step 18966: loss=2.7380, lr=0.000240, tokens/sec=1376314.65, grad_norm=0.3485, duration=0.38s
Step 18967: loss=2.8528, lr=0.000240, tokens/sec=1375243.90, grad_norm=0.3733, duration=0.38s
Step 18968: loss=2.8612, lr=0.000240, tokens/sec=1372524.62, grad_norm=0.3687, duration=0.38s
Step 18969: loss=2.8019, lr=0.000240, tokens/sec=1376188.04, grad_norm=0.3424, duration=0.38s
Step 18970: loss=2.7733, lr=0.000240, tokens/sec=1376643.79, grad_norm=0.3167, duration=0.38s
Step 18971: loss=2.8136, lr=0.000240, tokens/sec=1379360.40, grad_norm=0.3381, duration=0.38s
Step 18972: loss=2.8580, lr=0.000240, tokens/sec=1376699.81, grad_norm=0.3275, duration=0.38s
Step 18973: loss=2.8250, lr=0.000240, tokens/sec=1375369.49, grad_norm=0.3346, duration=0.38s
Step 18974: loss=2.7440, lr=0.000240, tokens/sec=1372470.65, grad_norm=0.3356, duration=0.38s
Step 18975: loss=2.7866, lr=0.000240, tokens/sec=1374074.36, grad_norm=0.3347, duration=0.38s
Step 18976: loss=2.7988, lr=0.000240, tokens/sec=1372984.80, grad_norm=0.3189, duration=0.38s
Step 18977: loss=2.8421, lr=0.000240, tokens/sec=1375550.16, grad_norm=0.3194, duration=0.38s
Step 18978: loss=2.8389, lr=0.000240, tokens/sec=1374571.67, grad_norm=0.3365, duration=0.38s
Step 18979: loss=2.7652, lr=0.000240, tokens/sec=1371402.45, grad_norm=0.3473, duration=0.38s
Step 18980: loss=2.7701, lr=0.000240, tokens/sec=1376262.11, grad_norm=0.3479, duration=0.38s
Step 18981: loss=2.7801, lr=0.000240, tokens/sec=1372089.57, grad_norm=0.3355, duration=0.38s
Step 18982: loss=2.7433, lr=0.000240, tokens/sec=1371262.20, grad_norm=0.3118, duration=0.38s
Step 18983: loss=2.7576, lr=0.000240, tokens/sec=1376198.38, grad_norm=0.3128, duration=0.38s
Step 18984: loss=2.7347, lr=0.000240, tokens/sec=1374486.61, grad_norm=0.3468, duration=0.38s
Step 18985: loss=2.7144, lr=0.000240, tokens/sec=1378461.16, grad_norm=0.3588, duration=0.38s
Step 18986: loss=2.7146, lr=0.000240, tokens/sec=1375935.74, grad_norm=0.3554, duration=0.38s
Step 18987: loss=2.7783, lr=0.000240, tokens/sec=1374938.65, grad_norm=0.3350, duration=0.38s
Step 18988: loss=2.7372, lr=0.000240, tokens/sec=1375316.15, grad_norm=0.3055, duration=0.38s
Step 18989: loss=2.6534, lr=0.000240, tokens/sec=1378070.71, grad_norm=0.3092, duration=0.38s
Step 18990: loss=2.6881, lr=0.000240, tokens/sec=1374718.61, grad_norm=0.3341, duration=0.38s
Step 18991: loss=2.7133, lr=0.000240, tokens/sec=1375274.01, grad_norm=0.3598, duration=0.38s
Step 18992: loss=2.7887, lr=0.000240, tokens/sec=1377818.58, grad_norm=0.3441, duration=0.38s
Step 18993: loss=2.7605, lr=0.000240, tokens/sec=1377071.38, grad_norm=0.3130, duration=0.38s
Step 18994: loss=2.7744, lr=0.000240, tokens/sec=1373392.11, grad_norm=0.2973, duration=0.38s
Step 18995: loss=2.8136, lr=0.000240, tokens/sec=1377861.75, grad_norm=0.3261, duration=0.38s
Step 18996: loss=2.8153, lr=0.000240, tokens/sec=1374788.22, grad_norm=0.3582, duration=0.38s
Step 18997: loss=2.7992, lr=0.000240, tokens/sec=1376462.83, grad_norm=0.3360, duration=0.38s
Step 18998: loss=2.7508, lr=0.000240, tokens/sec=1375757.55, grad_norm=0.3132, duration=0.38s
Step 18999: loss=2.8223, lr=0.000240, tokens/sec=1377967.95, grad_norm=0.3098, duration=0.38s
Step 19000/19073 (99.6%), Elapsed time: 7515.12s, Steps per hour: 9101.65, Estimated hours remaining: 0.01
Validation loss at step 19000: 3.8534185886383057
Step 19000: loss=2.7937, lr=0.000240, tokens/sec=155941.38, grad_norm=0.3340, duration=3.36s
Step 19001: loss=2.8185, lr=0.000240, tokens/sec=1379634.73, grad_norm=0.3369, duration=0.38s
Step 19002: loss=2.8539, lr=0.000240, tokens/sec=1376218.18, grad_norm=0.3487, duration=0.38s
Step 19003: loss=2.8882, lr=0.000240, tokens/sec=1376176.84, grad_norm=0.3431, duration=0.38s
Step 19004: loss=2.8450, lr=0.000240, tokens/sec=1376792.90, grad_norm=0.3202, duration=0.38s
Step 19005: loss=2.8540, lr=0.000240, tokens/sec=1375525.20, grad_norm=0.3294, duration=0.38s
Step 19006: loss=2.8846, lr=0.000240, tokens/sec=1374606.89, grad_norm=0.3457, duration=0.38s
Step 19007: loss=2.8479, lr=0.000240, tokens/sec=1373816.82, grad_norm=0.3402, duration=0.38s
Step 19008: loss=2.8069, lr=0.000240, tokens/sec=1366951.61, grad_norm=0.3585, duration=0.38s
Step 19009: loss=2.8171, lr=0.000240, tokens/sec=1376447.32, grad_norm=0.3344, duration=0.38s
Step 19010: loss=2.7940, lr=0.000240, tokens/sec=1374961.86, grad_norm=0.3249, duration=0.38s
Step 19011: loss=2.8027, lr=0.000240, tokens/sec=1377481.98, grad_norm=0.3120, duration=0.38s
Step 19012: loss=2.8074, lr=0.000240, tokens/sec=1376269.00, grad_norm=0.3183, duration=0.38s
Step 19013: loss=2.7158, lr=0.000240, tokens/sec=1373343.22, grad_norm=0.3317, duration=0.38s
Step 19014: loss=2.8170, lr=0.000240, tokens/sec=1369360.56, grad_norm=0.3499, duration=0.38s
Step 19015: loss=2.7861, lr=0.000240, tokens/sec=1372907.65, grad_norm=0.3704, duration=0.38s
Step 19016: loss=2.7632, lr=0.000240, tokens/sec=1375299.81, grad_norm=0.3144, duration=0.38s
Step 19017: loss=2.8433, lr=0.000240, tokens/sec=1373109.11, grad_norm=0.3191, duration=0.38s
Step 19018: loss=2.7562, lr=0.000240, tokens/sec=1375513.16, grad_norm=0.3560, duration=0.38s
Step 19019: loss=2.7095, lr=0.000240, tokens/sec=1372679.69, grad_norm=0.4079, duration=0.38s
Step 19020: loss=2.7762, lr=0.000240, tokens/sec=1372354.16, grad_norm=0.3279, duration=0.38s
Step 19021: loss=2.7772, lr=0.000240, tokens/sec=1374752.13, grad_norm=0.3155, duration=0.38s
Step 19022: loss=2.8569, lr=0.000240, tokens/sec=1377983.49, grad_norm=0.3429, duration=0.38s
Step 19023: loss=2.7178, lr=0.000240, tokens/sec=1375356.58, grad_norm=0.3601, duration=0.38s
Step 19024: loss=2.8227, lr=0.000240, tokens/sec=1375157.90, grad_norm=0.3473, duration=0.38s
Step 19025: loss=2.7928, lr=0.000240, tokens/sec=1378505.23, grad_norm=0.3180, duration=0.38s
Step 19026: loss=2.7521, lr=0.000240, tokens/sec=1378881.24, grad_norm=0.3201, duration=0.38s
Step 19027: loss=2.8017, lr=0.000240, tokens/sec=1373479.60, grad_norm=0.3397, duration=0.38s
Step 19028: loss=2.7967, lr=0.000240, tokens/sec=1377122.26, grad_norm=0.3419, duration=0.38s
Step 19029: loss=2.7672, lr=0.000240, tokens/sec=1372306.20, grad_norm=0.3157, duration=0.38s
Step 19030: loss=2.7435, lr=0.000240, tokens/sec=1376917.90, grad_norm=0.3158, duration=0.38s
Step 19031: loss=2.7710, lr=0.000240, tokens/sec=1376747.21, grad_norm=0.3189, duration=0.38s
Step 19032: loss=2.8377, lr=0.000240, tokens/sec=1378042.21, grad_norm=0.3564, duration=0.38s
Step 19033: loss=2.8662, lr=0.000240, tokens/sec=1376808.42, grad_norm=0.3523, duration=0.38s
Step 19034: loss=2.7722, lr=0.000240, tokens/sec=1377382.76, grad_norm=0.3358, duration=0.38s
Step 19035: loss=2.7259, lr=0.000240, tokens/sec=1376638.62, grad_norm=0.3256, duration=0.38s
Step 19036: loss=2.7304, lr=0.000240, tokens/sec=1377849.66, grad_norm=0.3188, duration=0.38s
Step 19037: loss=2.7248, lr=0.000240, tokens/sec=1374874.18, grad_norm=0.3432, duration=0.38s
Step 19038: loss=2.7352, lr=0.000240, tokens/sec=1376159.62, grad_norm=0.3364, duration=0.38s
Step 19039: loss=2.7337, lr=0.000240, tokens/sec=1376812.73, grad_norm=0.3464, duration=0.38s
Step 19040: loss=2.7803, lr=0.000240, tokens/sec=1374723.76, grad_norm=0.3466, duration=0.38s
Step 19041: loss=2.7709, lr=0.000240, tokens/sec=1375229.28, grad_norm=0.3339, duration=0.38s
Step 19042: loss=2.7411, lr=0.000240, tokens/sec=1378316.01, grad_norm=0.3252, duration=0.38s
Step 19043: loss=2.7975, lr=0.000240, tokens/sec=1376291.40, grad_norm=0.3529, duration=0.38s
Step 19044: loss=2.7403, lr=0.000240, tokens/sec=1377146.41, grad_norm=0.3600, duration=0.38s
Step 19045: loss=2.7311, lr=0.000240, tokens/sec=1374562.21, grad_norm=0.3424, duration=0.38s
Step 19046: loss=2.7691, lr=0.000240, tokens/sec=1375884.95, grad_norm=0.3060, duration=0.38s
Step 19047: loss=2.8175, lr=0.000240, tokens/sec=1379127.70, grad_norm=0.3387, duration=0.38s
Step 19048: loss=2.8530, lr=0.000240, tokens/sec=1374731.50, grad_norm=0.3388, duration=0.38s
Step 19049: loss=2.7912, lr=0.000240, tokens/sec=1375057.30, grad_norm=0.3404, duration=0.38s
Step 19050: loss=2.7812, lr=0.000240, tokens/sec=1377388.80, grad_norm=0.3345, duration=0.38s
Step 19051: loss=2.8263, lr=0.000240, tokens/sec=1374234.93, grad_norm=0.3382, duration=0.38s
Step 19052: loss=2.8169, lr=0.000240, tokens/sec=1377698.59, grad_norm=0.3491, duration=0.38s
Step 19053: loss=2.8378, lr=0.000240, tokens/sec=1377610.56, grad_norm=0.3366, duration=0.38s
Step 19054: loss=2.7807, lr=0.000240, tokens/sec=1377082.59, grad_norm=0.3216, duration=0.38s
Step 19055: loss=2.7657, lr=0.000240, tokens/sec=1373088.53, grad_norm=0.3177, duration=0.38s
Step 19056: loss=2.8034, lr=0.000240, tokens/sec=1376107.09, grad_norm=0.3473, duration=0.38s
Step 19057: loss=2.7746, lr=0.000240, tokens/sec=1372508.34, grad_norm=0.3569, duration=0.38s
Step 19058: loss=2.7930, lr=0.000240, tokens/sec=1374398.98, grad_norm=0.3449, duration=0.38s
Step 19059: loss=2.7847, lr=0.000240, tokens/sec=1372325.90, grad_norm=0.3174, duration=0.38s
Step 19060: loss=2.8084, lr=0.000240, tokens/sec=1375370.35, grad_norm=0.3119, duration=0.38s
Step 19061: loss=2.7140, lr=0.000240, tokens/sec=1374863.00, grad_norm=0.3385, duration=0.38s
Step 19062: loss=2.7646, lr=0.000240, tokens/sec=1375839.33, grad_norm=0.3364, duration=0.38s
Step 19063: loss=2.8055, lr=0.000240, tokens/sec=1375057.30, grad_norm=0.3317, duration=0.38s
Step 19064: loss=2.7995, lr=0.000240, tokens/sec=1374076.07, grad_norm=0.3313, duration=0.38s
Step 19065: loss=2.8175, lr=0.000240, tokens/sec=1375352.28, grad_norm=0.3354, duration=0.38s
Step 19066: loss=2.7734, lr=0.000240, tokens/sec=1376566.23, grad_norm=0.3373, duration=0.38s
Step 19067: loss=2.8502, lr=0.000240, tokens/sec=1375388.41, grad_norm=0.3404, duration=0.38s
Step 19068: loss=2.8170, lr=0.000240, tokens/sec=1376306.90, grad_norm=0.3319, duration=0.38s
Step 19069: loss=2.7930, lr=0.000240, tokens/sec=1373323.49, grad_norm=0.3312, duration=0.38s
Step 19070: loss=2.7871, lr=0.000240, tokens/sec=1373414.41, grad_norm=0.3380, duration=0.38s
Step 19071: loss=2.8219, lr=0.000240, tokens/sec=1375245.63, grad_norm=0.3391, duration=0.38s
Validation loss at step 19072: 3.8599724769592285
Checkpoint saved to model_checkpoints/model_19072.pt
Step 19072: loss=2.7963, lr=0.000240, tokens/sec=94995.54, grad_norm=0.3361, duration=5.52s
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:          perf/duration ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñà
wandb:         perf/grad_norm ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb: perf/tokens_per_second ‚ñà‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ
wandb:      perf/total_tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    train/learning_rate ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:             train/loss ‚ñà‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/perplexity ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:             train/step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:               val/loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:         val/perplexity ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:          perf/duration 5.51908
wandb:         perf/grad_norm 0.33609
wandb: perf/tokens_per_second 94995.53995
wandb:      perf/total_tokens 9999745024
wandb:    train/learning_rate 0.00024
wandb:             train/loss 2.79627
wandb:       train/perplexity 16.3835
wandb:             train/step 19072
wandb:               val/loss 3.85997
wandb:         val/perplexity 47.46405
wandb: 
wandb: üöÄ View run wise-sponge-4 at: https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining/runs/by6p52jo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250621_184634-by6p52jo/logs
