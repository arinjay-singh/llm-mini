W0621 18:05:35.616000 17871 torch/distributed/run.py:791] 
W0621 18:05:35.616000 17871 torch/distributed/run.py:791] *****************************************
W0621 18:05:35.616000 17871 torch/distributed/run.py:791] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0621 18:05:35.616000 17871 torch/distributed/run.py:791] *****************************************
Total desired batch size: 524,288
=> calculated gradient accumulation steps: 1
found 99 shards for split train
found 1 shards for split val
Total parameters: 124,475,904
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
num of decayed parameter tensors: 74, with 124354560 total parameters
num of non-decayed parameter tensors: 122, with 121344 total parameters
using fused AdamW: True
wandb: Currently logged in as: arinjay-singh (arinjay-singh-northeastern-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/ubuntu/llm/wandb/run-20250621_180551-k1h7rjd0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-haze-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining
wandb: üöÄ View run at https://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining/runs/k1h7rjd0
Wandb logging initialized
Validation loss at step 0: 10.961261749267578
Step 0: loss=10.9660, lr=0.000024, tokens/sec=8613.95, grad_norm=2.8407, duration=60.87s
Step 1: loss=10.6926, lr=0.000024, tokens/sec=1407576.67, grad_norm=2.6768, duration=0.37s
Step 2: loss=10.4523, lr=0.000024, tokens/sec=1410489.24, grad_norm=2.6229, duration=0.37s
Step 3: loss=10.2479, lr=0.000024, tokens/sec=1412345.44, grad_norm=2.5387, duration=0.37s
Step 4: loss=10.0980, lr=0.000024, tokens/sec=1412267.43, grad_norm=2.4194, duration=0.37s
Step 5: loss=9.9744, lr=0.000024, tokens/sec=1409692.64, grad_norm=2.3614, duration=0.37s
Step 6: loss=9.8908, lr=0.000024, tokens/sec=1410843.98, grad_norm=2.2938, duration=0.37s
Step 7: loss=9.8195, lr=0.000024, tokens/sec=1411816.80, grad_norm=2.2162, duration=0.37s
Step 8: loss=9.7484, lr=0.000024, tokens/sec=1408355.55, grad_norm=2.2043, duration=0.37s
Step 9: loss=9.7104, lr=0.000024, tokens/sec=1409725.18, grad_norm=2.1462, duration=0.37s
Step 10: loss=9.6577, lr=0.000024, tokens/sec=1410993.35, grad_norm=2.1241, duration=0.37s
Step 11: loss=9.6363, lr=0.000024, tokens/sec=1410520.00, grad_norm=2.0712, duration=0.37s
Step 12: loss=9.6097, lr=0.000024, tokens/sec=1406472.04, grad_norm=2.0430, duration=0.37s
Step 13: loss=9.5867, lr=0.000024, tokens/sec=1407866.85, grad_norm=2.0209, duration=0.37s
Step 14: loss=9.5313, lr=0.000024, tokens/sec=1406864.36, grad_norm=2.0642, duration=0.37s
Step 15: loss=9.5144, lr=0.000024, tokens/sec=1410950.80, grad_norm=2.0520, duration=0.37s
Step 16: loss=9.5029, lr=0.000024, tokens/sec=1409166.89, grad_norm=2.0269, duration=0.37s
Step 17: loss=9.4763, lr=0.000025, tokens/sec=1410476.58, grad_norm=2.0256, duration=0.37s
Step 18: loss=9.5250, lr=0.000025, tokens/sec=1410422.30, grad_norm=1.9396, duration=0.37s
Step 19: loss=9.5190, lr=0.000025, tokens/sec=1408012.88, grad_norm=1.9597, duration=0.37s
Step 20: loss=9.4130, lr=0.000025, tokens/sec=1406248.09, grad_norm=2.0139, duration=0.37s
Step 21: loss=9.3783, lr=0.000025, tokens/sec=1409594.15, grad_norm=2.0436, duration=0.37s
Step 22: loss=9.3627, lr=0.000025, tokens/sec=1407029.99, grad_norm=2.0172, duration=0.37s
Step 23: loss=9.3560, lr=0.000025, tokens/sec=1410655.73, grad_norm=1.9927, duration=0.37s
Step 24: loss=9.2984, lr=0.000025, tokens/sec=1409416.17, grad_norm=2.0233, duration=0.37s
Step 25: loss=9.2734, lr=0.000025, tokens/sec=1404093.12, grad_norm=2.0040, duration=0.37s
Step 26: loss=9.2631, lr=0.000025, tokens/sec=1406597.99, grad_norm=1.9712, duration=0.37s
Step 27: loss=9.2233, lr=0.000025, tokens/sec=1407828.09, grad_norm=1.9751, duration=0.37s
Step 28: loss=9.2032, lr=0.000025, tokens/sec=1408957.42, grad_norm=1.9524, duration=0.37s
Step 29: loss=9.1640, lr=0.000025, tokens/sec=1398548.59, grad_norm=1.9528, duration=0.37s
Step 30: loss=9.2052, lr=0.000026, tokens/sec=1404129.88, grad_norm=1.8559, duration=0.37s
Step 31: loss=9.1415, lr=0.000026, tokens/sec=1407690.21, grad_norm=1.8921, duration=0.37s
Step 32: loss=9.1312, lr=0.000026, tokens/sec=1406504.43, grad_norm=1.8629, duration=0.37s
Step 33: loss=9.1048, lr=0.000026, tokens/sec=1407454.15, grad_norm=1.8865, duration=0.37s
Step 34: loss=9.0190, lr=0.000026, tokens/sec=1405379.02, grad_norm=1.9424, duration=0.37s
Step 35: loss=9.0408, lr=0.000026, tokens/sec=1403985.55, grad_norm=1.8467, duration=0.37s
Step 36: loss=9.0387, lr=0.000026, tokens/sec=1406416.27, grad_norm=1.8217, duration=0.37s
Step 37: loss=8.9801, lr=0.000026, tokens/sec=1408011.08, grad_norm=1.8414, duration=0.37s
Step 38: loss=8.9685, lr=0.000026, tokens/sec=1406051.17, grad_norm=1.8567, duration=0.37s
Step 39: loss=8.9639, lr=0.000027, tokens/sec=1397587.74, grad_norm=1.8100, duration=0.38s
Step 40: loss=8.9446, lr=0.000027, tokens/sec=1405912.73, grad_norm=1.7808, duration=0.37s
Step 41: loss=8.8821, lr=0.000027, tokens/sec=1402906.23, grad_norm=1.8420, duration=0.37s
Step 42: loss=8.8614, lr=0.000027, tokens/sec=1405231.74, grad_norm=1.8119, duration=0.37s
Step 43: loss=8.8578, lr=0.000027, tokens/sec=1403633.36, grad_norm=1.7878, duration=0.37s
Step 44: loss=8.8417, lr=0.000027, tokens/sec=1407072.31, grad_norm=1.7347, duration=0.37s
Step 45: loss=8.8375, lr=0.000027, tokens/sec=1401397.08, grad_norm=1.7016, duration=0.37s
Step 46: loss=8.7955, lr=0.000027, tokens/sec=1401228.31, grad_norm=1.7154, duration=0.37s
Step 47: loss=8.7809, lr=0.000028, tokens/sec=1405755.45, grad_norm=1.7004, duration=0.37s
Step 48: loss=8.7735, lr=0.000028, tokens/sec=1401503.37, grad_norm=1.6647, duration=0.37s
Step 49: loss=8.7190, lr=0.000028, tokens/sec=1401699.91, grad_norm=1.6893, duration=0.37s
Step 50: loss=8.7371, lr=0.000028, tokens/sec=1400814.14, grad_norm=1.6278, duration=0.37s
Step 51: loss=8.6972, lr=0.000028, tokens/sec=1393901.54, grad_norm=1.6305, duration=0.38s
Step 52: loss=8.6761, lr=0.000028, tokens/sec=1397475.84, grad_norm=1.5979, duration=0.38s
Step 53: loss=8.6423, lr=0.000029, tokens/sec=1398083.56, grad_norm=1.5968, duration=0.38s
Step 54: loss=8.6527, lr=0.000029, tokens/sec=1396043.03, grad_norm=1.5463, duration=0.38s
Step 55: loss=8.6232, lr=0.000029, tokens/sec=1397782.30, grad_norm=1.5539, duration=0.38s
Step 56: loss=8.6005, lr=0.000029, tokens/sec=1394802.47, grad_norm=1.5419, duration=0.38s
Step 57: loss=8.5885, lr=0.000029, tokens/sec=1394445.14, grad_norm=1.5206, duration=0.38s
Step 58: loss=8.5489, lr=0.000029, tokens/sec=1395761.26, grad_norm=1.5266, duration=0.38s
Step 59: loss=8.5291, lr=0.000030, tokens/sec=1397908.47, grad_norm=1.5369, duration=0.38s
Step 60: loss=8.5223, lr=0.000030, tokens/sec=1398367.16, grad_norm=1.5183, duration=0.37s
Step 61: loss=8.5238, lr=0.000030, tokens/sec=1395869.35, grad_norm=1.5031, duration=0.38s
Step 62: loss=8.4505, lr=0.000030, tokens/sec=1393265.67, grad_norm=1.5329, duration=0.38s
Step 63: loss=8.4272, lr=0.000030, tokens/sec=1395892.38, grad_norm=1.5047, duration=0.38s
Step 64: loss=8.4018, lr=0.000031, tokens/sec=1396449.07, grad_norm=1.4838, duration=0.38s
Step 65: loss=8.4020, lr=0.000031, tokens/sec=1400310.15, grad_norm=1.4733, duration=0.37s
Step 66: loss=8.3597, lr=0.000031, tokens/sec=1395717.85, grad_norm=1.4456, duration=0.38s
Step 67: loss=8.3605, lr=0.000031, tokens/sec=1397238.75, grad_norm=1.3903, duration=0.38s
Step 68: loss=8.3395, lr=0.000031, tokens/sec=1395219.29, grad_norm=1.3695, duration=0.38s
Step 69: loss=8.2719, lr=0.000032, tokens/sec=1397230.76, grad_norm=1.4107, duration=0.38s
Step 70: loss=8.2391, lr=0.000032, tokens/sec=1401331.00, grad_norm=1.3738, duration=0.37s
Step 71: loss=8.2592, lr=0.000032, tokens/sec=1390556.49, grad_norm=1.3051, duration=0.38s
Step 72: loss=8.2006, lr=0.000032, tokens/sec=1393851.18, grad_norm=1.3177, duration=0.38s
Step 73: loss=8.1803, lr=0.000033, tokens/sec=1396268.18, grad_norm=1.3303, duration=0.38s
Step 74: loss=8.1421, lr=0.000033, tokens/sec=1392221.28, grad_norm=1.3077, duration=0.38s
Step 75: loss=8.1557, lr=0.000033, tokens/sec=1391206.61, grad_norm=1.2984, duration=0.38s
Step 76: loss=8.1040, lr=0.000033, tokens/sec=1393926.28, grad_norm=1.3074, duration=0.38s
Step 77: loss=8.1189, lr=0.000033, tokens/sec=1394027.02, grad_norm=1.2685, duration=0.38s
Step 78: loss=8.0845, lr=0.000034, tokens/sec=1394447.79, grad_norm=1.2655, duration=0.38s
Step 79: loss=8.0518, lr=0.000034, tokens/sec=1393638.29, grad_norm=1.2711, duration=0.38s
Step 80: loss=8.0232, lr=0.000034, tokens/sec=1395700.13, grad_norm=1.2618, duration=0.38s
Step 81: loss=8.0302, lr=0.000034, tokens/sec=1395846.31, grad_norm=1.2122, duration=0.38s
Step 82: loss=7.9857, lr=0.000035, tokens/sec=1393415.76, grad_norm=1.2167, duration=0.38s
Step 83: loss=7.9436, lr=0.000035, tokens/sec=1395082.09, grad_norm=1.2181, duration=0.38s
Step 84: loss=7.9423, lr=0.000035, tokens/sec=1393625.93, grad_norm=1.1742, duration=0.38s
Step 85: loss=7.9071, lr=0.000036, tokens/sec=1387858.70, grad_norm=1.1694, duration=0.38s
Step 86: loss=7.9011, lr=0.000036, tokens/sec=1390546.82, grad_norm=1.1275, duration=0.38s
Step 87: loss=7.8952, lr=0.000036, tokens/sec=1394515.89, grad_norm=1.1174, duration=0.38s
Step 88: loss=7.8816, lr=0.000036, tokens/sec=1392753.86, grad_norm=1.1003, duration=0.38s
Step 89: loss=7.9069, lr=0.000037, tokens/sec=1391321.92, grad_norm=1.0691, duration=0.38s
Step 90: loss=7.8319, lr=0.000037, tokens/sec=1390141.58, grad_norm=1.0709, duration=0.38s
Step 91: loss=7.8208, lr=0.000037, tokens/sec=1393399.86, grad_norm=1.0359, duration=0.38s
Step 92: loss=7.8298, lr=0.000037, tokens/sec=1391751.64, grad_norm=1.0193, duration=0.38s
Step 93: loss=7.8690, lr=0.000038, tokens/sec=1397479.39, grad_norm=0.9713, duration=0.38s
Step 94: loss=7.8650, lr=0.000038, tokens/sec=1393196.82, grad_norm=0.9689, duration=0.38s
Step 95: loss=7.8564, lr=0.000038, tokens/sec=1392248.60, grad_norm=0.9423, duration=0.38s
Step 96: loss=7.8178, lr=0.000039, tokens/sec=1390575.84, grad_norm=0.9379, duration=0.38s
Step 97: loss=7.7682, lr=0.000039, tokens/sec=1394001.39, grad_norm=0.9470, duration=0.38s
Step 98: loss=7.7818, lr=0.000039, tokens/sec=1392089.08, grad_norm=0.9268, duration=0.38s
Step 99: loss=7.7369, lr=0.000040, tokens/sec=1388736.05, grad_norm=0.9009, duration=0.38s
Step 100/19073 (0.5%), Elapsed time: 98.24s, Steps per hour: 3664.32, Estimated hours remaining: 5.18
Step 100: loss=7.7795, lr=0.000040, tokens/sec=1392738.87, grad_norm=0.8222, duration=0.38s
Step 101: loss=7.7193, lr=0.000040, tokens/sec=1391203.09, grad_norm=0.8404, duration=0.38s
Step 102: loss=7.7017, lr=0.000040, tokens/sec=1389459.09, grad_norm=0.8158, duration=0.38s
Step 103: loss=7.6090, lr=0.000041, tokens/sec=1392551.89, grad_norm=0.8399, duration=0.38s
Step 104: loss=7.6791, lr=0.000041, tokens/sec=1391035.89, grad_norm=0.7967, duration=0.38s
Step 105: loss=7.6880, lr=0.000041, tokens/sec=1389484.55, grad_norm=0.7671, duration=0.38s
Step 106: loss=7.6323, lr=0.000042, tokens/sec=1390398.23, grad_norm=0.8049, duration=0.38s
Step 107: loss=7.5943, lr=0.000042, tokens/sec=1388881.65, grad_norm=0.7717, duration=0.38s
Step 108: loss=7.6322, lr=0.000042, tokens/sec=1389884.14, grad_norm=0.6935, duration=0.38s
Step 109: loss=7.6510, lr=0.000043, tokens/sec=1388807.09, grad_norm=0.6509, duration=0.38s
Step 110: loss=7.5785, lr=0.000043, tokens/sec=1388217.92, grad_norm=0.6690, duration=0.38s
Step 111: loss=7.5633, lr=0.000043, tokens/sec=1389602.20, grad_norm=0.6752, duration=0.38s
Step 112: loss=7.5294, lr=0.000044, tokens/sec=1388511.57, grad_norm=0.6843, duration=0.38s
Step 113: loss=7.5788, lr=0.000044, tokens/sec=1387414.76, grad_norm=0.6556, duration=0.38s
Step 114: loss=7.5682, lr=0.000044, tokens/sec=1389957.05, grad_norm=0.6182, duration=0.38s
Step 115: loss=7.5351, lr=0.000045, tokens/sec=1384935.82, grad_norm=0.6120, duration=0.38s
Step 116: loss=7.4632, lr=0.000045, tokens/sec=1385040.50, grad_norm=0.6230, duration=0.38s
Step 117: loss=7.3831, lr=0.000046, tokens/sec=1386099.48, grad_norm=0.6487, duration=0.38s
Step 118: loss=7.4132, lr=0.000046, tokens/sec=1388836.03, grad_norm=0.6300, duration=0.38s
Step 119: loss=7.4519, lr=0.000046, tokens/sec=1382206.78, grad_norm=0.5471, duration=0.38s
Step 120: loss=7.4094, lr=0.000047, tokens/sec=1387582.85, grad_norm=0.5508, duration=0.38s
Step 121: loss=7.3978, lr=0.000047, tokens/sec=1387073.45, grad_norm=0.5329, duration=0.38s
Step 122: loss=7.3802, lr=0.000047, tokens/sec=1383586.06, grad_norm=0.5627, duration=0.38s
Step 123: loss=7.3629, lr=0.000048, tokens/sec=1386995.59, grad_norm=0.5329, duration=0.38s
Step 124: loss=7.3826, lr=0.000048, tokens/sec=1385611.25, grad_norm=0.4822, duration=0.38s
Step 125: loss=7.3663, lr=0.000049, tokens/sec=1385474.19, grad_norm=0.4650, duration=0.38s
Step 126: loss=7.3623, lr=0.000049, tokens/sec=1386635.26, grad_norm=0.4782, duration=0.38s
Step 127: loss=7.3147, lr=0.000049, tokens/sec=1385475.07, grad_norm=0.4861, duration=0.38s
Step 128: loss=7.2864, lr=0.000050, tokens/sec=1386534.71, grad_norm=0.4728, duration=0.38s
Step 129: loss=7.2655, lr=0.000050, tokens/sec=1385029.16, grad_norm=0.4665, duration=0.38s
Step 130: loss=7.3192, lr=0.000050, tokens/sec=1386179.86, grad_norm=0.4212, duration=0.38s
Step 131: loss=7.2768, lr=0.000051, tokens/sec=1384495.49, grad_norm=0.4029, duration=0.38s
Step 132: loss=7.2775, lr=0.000051, tokens/sec=1384922.74, grad_norm=0.4340, duration=0.38s
Step 133: loss=7.1884, lr=0.000052, tokens/sec=1378523.38, grad_norm=0.4385, duration=0.38s
Step 134: loss=7.1856, lr=0.000052, tokens/sec=1378564.86, grad_norm=0.4673, duration=0.38s
Step 135: loss=7.2597, lr=0.000052, tokens/sec=1383229.23, grad_norm=0.3802, duration=0.38s
Step 136: loss=7.2347, lr=0.000053, tokens/sec=1384746.57, grad_norm=0.3946, duration=0.38s
Step 137: loss=7.2508, lr=0.000053, tokens/sec=1378846.66, grad_norm=0.3250, duration=0.38s
Step 138: loss=7.1867, lr=0.000054, tokens/sec=1381996.56, grad_norm=0.3768, duration=0.38s
Step 139: loss=7.2549, lr=0.000054, tokens/sec=1383233.58, grad_norm=0.4050, duration=0.38s
Step 140: loss=7.3457, lr=0.000055, tokens/sec=1385929.13, grad_norm=0.3154, duration=0.38s
Step 141: loss=7.2359, lr=0.000055, tokens/sec=1382382.30, grad_norm=0.3288, duration=0.38s
Step 142: loss=7.3019, lr=0.000055, tokens/sec=1386456.03, grad_norm=0.3483, duration=0.38s
Step 143: loss=7.2355, lr=0.000056, tokens/sec=1380604.85, grad_norm=0.3131, duration=0.38s
Step 144: loss=7.2395, lr=0.000056, tokens/sec=1383665.28, grad_norm=0.3242, duration=0.38s
Step 145: loss=7.2397, lr=0.000057, tokens/sec=1384474.57, grad_norm=0.2883, duration=0.38s
Step 146: loss=7.2365, lr=0.000057, tokens/sec=1380346.59, grad_norm=0.3390, duration=0.38s
Step 147: loss=7.2291, lr=0.000058, tokens/sec=1385324.94, grad_norm=0.2834, duration=0.38s
Step 148: loss=7.2222, lr=0.000058, tokens/sec=1377118.81, grad_norm=0.2929, duration=0.38s
Step 149: loss=7.2252, lr=0.000059, tokens/sec=1385105.93, grad_norm=0.2701, duration=0.38s
Step 150: loss=7.2119, lr=0.000059, tokens/sec=1375837.60, grad_norm=0.2559, duration=0.38s
Step 151: loss=7.1337, lr=0.000059, tokens/sec=1380447.11, grad_norm=0.3027, duration=0.38s
Step 152: loss=7.1500, lr=0.000060, tokens/sec=1383856.85, grad_norm=0.3002, duration=0.38s
Step 153: loss=7.2254, lr=0.000060, tokens/sec=1385523.08, grad_norm=0.3175, duration=0.38s
Step 154: loss=7.1247, lr=0.000061, tokens/sec=1380877.93, grad_norm=0.2905, duration=0.38s
Step 155: loss=7.0844, lr=0.000061, tokens/sec=1381927.08, grad_norm=0.3451, duration=0.38s
Step 156: loss=7.1627, lr=0.000062, tokens/sec=1381381.92, grad_norm=0.2865, duration=0.38s
Step 157: loss=7.1947, lr=0.000062, tokens/sec=1384734.37, grad_norm=0.3382, duration=0.38s
Step 158: loss=7.1676, lr=0.000063, tokens/sec=1383365.85, grad_norm=0.3275, duration=0.38s
Step 159: loss=7.1015, lr=0.000063, tokens/sec=1384039.75, grad_norm=0.2974, duration=0.38s
Step 160: loss=7.0512, lr=0.000064, tokens/sec=1385605.14, grad_norm=0.3170, duration=0.38s
Step 161: loss=7.1213, lr=0.000064, tokens/sec=1382508.31, grad_norm=0.3080, duration=0.38s
Step 162: loss=7.0204, lr=0.000065, tokens/sec=1378539.80, grad_norm=0.3075, duration=0.38s
Step 163: loss=6.9777, lr=0.000065, tokens/sec=1381128.58, grad_norm=0.2492, duration=0.38s
Step 164: loss=7.0234, lr=0.000066, tokens/sec=1381217.93, grad_norm=0.2703, duration=0.38s
Step 165: loss=7.0425, lr=0.000066, tokens/sec=1382341.45, grad_norm=0.3108, duration=0.38s
Step 166: loss=6.8907, lr=0.000067, tokens/sec=1382511.79, grad_norm=0.3809, duration=0.38s
Step 167: loss=6.9759, lr=0.000067, tokens/sec=1383972.68, grad_norm=0.3263, duration=0.38s
Step 168: loss=6.9442, lr=0.000068, tokens/sec=1384230.55, grad_norm=0.2742, duration=0.38s
Step 169: loss=6.9932, lr=0.000068, tokens/sec=1382685.65, grad_norm=0.4352, duration=0.38s
Step 170: loss=6.9552, lr=0.000069, tokens/sec=1382126.85, grad_norm=0.2636, duration=0.38s
Step 171: loss=6.9764, lr=0.000069, tokens/sec=1381381.05, grad_norm=0.3285, duration=0.38s
Step 172: loss=6.9717, lr=0.000070, tokens/sec=1384087.66, grad_norm=0.2692, duration=0.38s
Step 173: loss=6.9065, lr=0.000070, tokens/sec=1384113.80, grad_norm=0.2618, duration=0.38s
Step 174: loss=6.8845, lr=0.000071, tokens/sec=1384237.52, grad_norm=0.2272, duration=0.38s
Step 175: loss=6.9530, lr=0.000071, tokens/sec=1384165.20, grad_norm=0.2774, duration=0.38s
Step 176: loss=6.8892, lr=0.000072, tokens/sec=1383084.82, grad_norm=0.2536, duration=0.38s
Step 177: loss=6.9130, lr=0.000072, tokens/sec=1374053.75, grad_norm=0.2629, duration=0.38s
Step 178: loss=6.8541, lr=0.000073, tokens/sec=1376195.79, grad_norm=0.2604, duration=0.38s
Step 179: loss=6.8311, lr=0.000073, tokens/sec=1377839.30, grad_norm=0.3097, duration=0.38s
Step 180: loss=6.8207, lr=0.000074, tokens/sec=1375861.71, grad_norm=0.2115, duration=0.38s
Step 181: loss=6.8541, lr=0.000074, tokens/sec=1379842.50, grad_norm=0.2620, duration=0.38s
Step 182: loss=6.8403, lr=0.000075, tokens/sec=1375587.16, grad_norm=0.2408, duration=0.38s
Step 183: loss=6.8312, lr=0.000075, tokens/sec=1376037.34, grad_norm=0.2167, duration=0.38s
Step 184: loss=6.8426, lr=0.000076, tokens/sec=1381269.12, grad_norm=0.2752, duration=0.38s
Step 185: loss=6.8903, lr=0.000077, tokens/sec=1387399.88, grad_norm=0.2862, duration=0.38s
Step 186: loss=6.9041, lr=0.000077, tokens/sec=1381269.99, grad_norm=0.2360, duration=0.38s
Step 187: loss=6.9668, lr=0.000078, tokens/sec=1384432.73, grad_norm=0.2886, duration=0.38s
Step 188: loss=6.9212, lr=0.000078, tokens/sec=1381196.24, grad_norm=0.2550, duration=0.38s
Step 189: loss=6.8687, lr=0.000079, tokens/sec=1384827.67, grad_norm=0.2984, duration=0.38s
Step 190: loss=6.9253, lr=0.000079, tokens/sec=1380310.20, grad_norm=0.2962, duration=0.38s
Step 191: loss=6.9347, lr=0.000080, tokens/sec=1381269.12, grad_norm=0.3481, duration=0.38s
Step 192: loss=6.8797, lr=0.000080, tokens/sec=1381356.75, grad_norm=0.2441, duration=0.38s
Step 193: loss=6.8422, lr=0.000081, tokens/sec=1377917.00, grad_norm=0.3245, duration=0.38s
Step 194: loss=6.9004, lr=0.000082, tokens/sec=1382198.96, grad_norm=0.2277, duration=0.38s
Step 195: loss=6.8655, lr=0.000082, tokens/sec=1379809.60, grad_norm=0.3961, duration=0.38s
Step 196: loss=6.9095, lr=0.000083, tokens/sec=1379723.03, grad_norm=0.4041, duration=0.38s
Step 197: loss=6.8810, lr=0.000083, tokens/sec=1377343.07, grad_norm=0.2691, duration=0.38s
Step 198: loss=6.8604, lr=0.000084, tokens/sec=1383184.86, grad_norm=0.4514, duration=0.38s
Step 199: loss=6.7984, lr=0.000084, tokens/sec=1382783.90, grad_norm=0.3169, duration=0.38s
Step 200/19073 (1.0%), Elapsed time: 136.21s, Steps per hour: 5285.87, Estimated hours remaining: 3.57
Step 200: loss=6.8219, lr=0.000085, tokens/sec=1384879.13, grad_norm=0.3597, duration=0.38s
Step 201: loss=6.8220, lr=0.000086, tokens/sec=1383223.14, grad_norm=0.2537, duration=0.38s
Step 202: loss=6.8444, lr=0.000086, tokens/sec=1382480.50, grad_norm=0.4054, duration=0.38s
Step 203: loss=6.7376, lr=0.000087, tokens/sec=1384253.20, grad_norm=0.2995, duration=0.38s
Step 204: loss=6.7304, lr=0.000087, tokens/sec=1386364.26, grad_norm=0.3212, duration=0.38s
Step 205: loss=6.7746, lr=0.000088, tokens/sec=1380355.26, grad_norm=0.4156, duration=0.38s
Step 206: loss=6.7828, lr=0.000089, tokens/sec=1376148.42, grad_norm=0.2454, duration=0.38s
Step 207: loss=6.9406, lr=0.000089, tokens/sec=1379536.07, grad_norm=0.3839, duration=0.38s
Step 208: loss=6.7876, lr=0.000090, tokens/sec=1378710.07, grad_norm=0.3166, duration=0.38s
Step 209: loss=6.9667, lr=0.000090, tokens/sec=1381801.17, grad_norm=0.6411, duration=0.38s
Step 210: loss=6.7712, lr=0.000091, tokens/sec=1383118.74, grad_norm=0.3642, duration=0.38s
Step 211: loss=6.7241, lr=0.000092, tokens/sec=1375368.63, grad_norm=0.3627, duration=0.38s
Step 212: loss=6.7120, lr=0.000092, tokens/sec=1380825.04, grad_norm=0.2918, duration=0.38s
Step 213: loss=6.7396, lr=0.000093, tokens/sec=1378770.58, grad_norm=0.4249, duration=0.38s
Step 214: loss=6.6368, lr=0.000093, tokens/sec=1383297.10, grad_norm=0.2989, duration=0.38s
Step 215: loss=6.5950, lr=0.000094, tokens/sec=1380156.87, grad_norm=0.3696, duration=0.38s
Step 216: loss=6.6438, lr=0.000095, tokens/sec=1379212.47, grad_norm=0.3232, duration=0.38s
Step 217: loss=6.5769, lr=0.000095, tokens/sec=1379619.15, grad_norm=0.3904, duration=0.38s
Step 218: loss=6.5902, lr=0.000096, tokens/sec=1382658.70, grad_norm=0.2935, duration=0.38s
Step 219: loss=6.5605, lr=0.000097, tokens/sec=1381519.90, grad_norm=0.3299, duration=0.38s
Step 220: loss=6.6792, lr=0.000097, tokens/sec=1383291.88, grad_norm=0.2713, duration=0.38s
Step 221: loss=6.6452, lr=0.000098, tokens/sec=1385795.50, grad_norm=0.3777, duration=0.38s
Step 222: loss=6.6109, lr=0.000098, tokens/sec=1379357.81, grad_norm=0.2850, duration=0.38s
Step 223: loss=6.6162, lr=0.000099, tokens/sec=1382474.42, grad_norm=0.3472, duration=0.38s
Step 224: loss=6.5322, lr=0.000100, tokens/sec=1380743.54, grad_norm=0.3066, duration=0.38s
Step 225: loss=6.5288, lr=0.000100, tokens/sec=1379788.82, grad_norm=0.2689, duration=0.38s
Step 226: loss=6.5411, lr=0.000101, tokens/sec=1382111.22, grad_norm=0.4188, duration=0.38s
Step 227: loss=6.5437, lr=0.000102, tokens/sec=1378142.39, grad_norm=0.2532, duration=0.38s
Step 228: loss=6.4868, lr=0.000102, tokens/sec=1375437.45, grad_norm=0.3853, duration=0.38s
Step 229: loss=6.5390, lr=0.000103, tokens/sec=1377651.12, grad_norm=0.2556, duration=0.38s
Step 230: loss=6.5375, lr=0.000104, tokens/sec=1380936.03, grad_norm=0.4013, duration=0.38s
Step 231: loss=6.4858, lr=0.000104, tokens/sec=1380000.96, grad_norm=0.2723, duration=0.38s
Step 232: loss=6.4797, lr=0.000105, tokens/sec=1381960.95, grad_norm=0.5833, duration=0.38s
Step 233: loss=6.5076, lr=0.000106, tokens/sec=1383674.86, grad_norm=0.2782, duration=0.38s
Step 234: loss=6.5185, lr=0.000106, tokens/sec=1381751.68, grad_norm=0.3991, duration=0.38s
Step 235: loss=6.5686, lr=0.000107, tokens/sec=1377541.52, grad_norm=0.3012, duration=0.38s
Step 236: loss=6.4909, lr=0.000108, tokens/sec=1381940.11, grad_norm=0.3394, duration=0.38s
Step 237: loss=6.6158, lr=0.000108, tokens/sec=1377951.54, grad_norm=0.2847, duration=0.38s
Step 238: loss=6.6260, lr=0.000109, tokens/sec=1377609.70, grad_norm=0.3379, duration=0.38s
Step 239: loss=6.5664, lr=0.000110, tokens/sec=1381440.93, grad_norm=0.3217, duration=0.38s
Step 240: loss=6.6127, lr=0.000110, tokens/sec=1377651.12, grad_norm=0.3453, duration=0.38s
Step 241: loss=6.6037, lr=0.000111, tokens/sec=1383011.75, grad_norm=0.2847, duration=0.38s
Step 242: loss=6.5945, lr=0.000112, tokens/sec=1380847.59, grad_norm=0.2721, duration=0.38s
Step 243: loss=6.6080, lr=0.000112, tokens/sec=1375788.54, grad_norm=0.3070, duration=0.38s
Step 244: loss=6.5536, lr=0.000113, tokens/sec=1377400.88, grad_norm=0.3189, duration=0.38s
Step 245: loss=6.5923, lr=0.000114, tokens/sec=1382957.82, grad_norm=0.3319, duration=0.38s
Step 246: loss=6.5558, lr=0.000114, tokens/sec=1375863.43, grad_norm=0.3156, duration=0.38s
Step 247: loss=6.6029, lr=0.000115, tokens/sec=1380826.78, grad_norm=0.2698, duration=0.38s
Step 248: loss=6.5026, lr=0.000116, tokens/sec=1380029.54, grad_norm=0.2736, duration=0.38s
Step 249: loss=6.4770, lr=0.000116, tokens/sec=1376331.02, grad_norm=0.2630, duration=0.38s
Validation loss at step 250: 6.488459587097168
Step 250: loss=6.5616, lr=0.000117, tokens/sec=156836.70, grad_norm=0.2584, duration=3.34s
Step 251: loss=6.5294, lr=0.000118, tokens/sec=1380876.20, grad_norm=0.2458, duration=0.38s
Step 252: loss=6.4798, lr=0.000119, tokens/sec=1383203.13, grad_norm=0.2712, duration=0.38s
Step 253: loss=6.4880, lr=0.000119, tokens/sec=1380463.57, grad_norm=0.2900, duration=0.38s
Step 254: loss=6.4899, lr=0.000120, tokens/sec=1382502.23, grad_norm=0.2540, duration=0.38s
Step 255: loss=6.5187, lr=0.000121, tokens/sec=1383719.26, grad_norm=0.2707, duration=0.38s
Step 256: loss=6.4597, lr=0.000121, tokens/sec=1383972.68, grad_norm=0.3281, duration=0.38s
Step 257: loss=6.5140, lr=0.000122, tokens/sec=1381958.35, grad_norm=0.2975, duration=0.38s
Step 258: loss=6.4781, lr=0.000123, tokens/sec=1381701.33, grad_norm=0.2772, duration=0.38s
Step 259: loss=6.3901, lr=0.000124, tokens/sec=1381168.48, grad_norm=0.3194, duration=0.38s
Step 260: loss=6.4326, lr=0.000124, tokens/sec=1377956.72, grad_norm=0.2578, duration=0.38s
Step 261: loss=6.4042, lr=0.000125, tokens/sec=1379964.59, grad_norm=0.2553, duration=0.38s
Step 262: loss=6.4321, lr=0.000126, tokens/sec=1382250.22, grad_norm=0.3738, duration=0.38s
Step 263: loss=6.4053, lr=0.000126, tokens/sec=1378272.82, grad_norm=0.4102, duration=0.38s
Step 264: loss=6.3527, lr=0.000127, tokens/sec=1378260.72, grad_norm=0.3373, duration=0.38s
Step 265: loss=6.4312, lr=0.000128, tokens/sec=1378966.84, grad_norm=0.3955, duration=0.38s
Step 266: loss=6.3544, lr=0.000129, tokens/sec=1378375.62, grad_norm=0.3323, duration=0.38s
Step 267: loss=6.3847, lr=0.000129, tokens/sec=1374804.55, grad_norm=0.3465, duration=0.38s
Step 268: loss=6.3644, lr=0.000130, tokens/sec=1381478.24, grad_norm=0.3383, duration=0.38s
Step 269: loss=6.3737, lr=0.000131, tokens/sec=1375265.41, grad_norm=0.3101, duration=0.38s
Step 270: loss=6.3650, lr=0.000132, tokens/sec=1375757.55, grad_norm=0.3837, duration=0.38s
Step 271: loss=6.3236, lr=0.000132, tokens/sec=1377572.59, grad_norm=0.3098, duration=0.38s
Step 272: loss=6.3116, lr=0.000133, tokens/sec=1378426.60, grad_norm=0.3145, duration=0.38s
Step 273: loss=6.3143, lr=0.000134, tokens/sec=1380922.16, grad_norm=0.2823, duration=0.38s
Step 274: loss=6.2855, lr=0.000135, tokens/sec=1379110.40, grad_norm=0.2744, duration=0.38s
Step 275: loss=6.2841, lr=0.000135, tokens/sec=1375785.96, grad_norm=0.2809, duration=0.38s
Step 276: loss=6.2793, lr=0.000136, tokens/sec=1377689.10, grad_norm=0.2818, duration=0.38s
Step 277: loss=6.2979, lr=0.000137, tokens/sec=1376451.63, grad_norm=0.2957, duration=0.38s
Step 278: loss=6.3517, lr=0.000138, tokens/sec=1376187.18, grad_norm=0.2689, duration=0.38s
Step 279: loss=6.2997, lr=0.000138, tokens/sec=1380233.96, grad_norm=0.3430, duration=0.38s
Step 280: loss=6.2858, lr=0.000139, tokens/sec=1377168.83, grad_norm=0.3704, duration=0.38s
Step 281: loss=6.3203, lr=0.000140, tokens/sec=1376341.36, grad_norm=0.4606, duration=0.38s
Step 282: loss=6.3851, lr=0.000141, tokens/sec=1378625.36, grad_norm=0.4475, duration=0.38s
Step 283: loss=6.4208, lr=0.000141, tokens/sec=1376657.58, grad_norm=0.4101, duration=0.38s
Step 284: loss=6.4181, lr=0.000142, tokens/sec=1373992.79, grad_norm=0.3304, duration=0.38s
Step 285: loss=6.4401, lr=0.000143, tokens/sec=1377155.90, grad_norm=0.3497, duration=0.38s
Step 286: loss=6.3902, lr=0.000144, tokens/sec=1374460.83, grad_norm=0.4186, duration=0.38s
Step 287: loss=6.3910, lr=0.000144, tokens/sec=1378105.25, grad_norm=0.5786, duration=0.38s
Step 288: loss=6.4070, lr=0.000145, tokens/sec=1382536.13, grad_norm=0.5302, duration=0.38s
Step 289: loss=6.3962, lr=0.000146, tokens/sec=1377906.64, grad_norm=0.3355, duration=0.38s
Step 290: loss=6.4023, lr=0.000147, tokens/sec=1382430.09, grad_norm=0.3240, duration=0.38s
Step 291: loss=6.3729, lr=0.000148, tokens/sec=1383700.98, grad_norm=0.5554, duration=0.38s
Step 292: loss=6.3341, lr=0.000148, tokens/sec=1379321.47, grad_norm=0.3726, duration=0.38s
Step 293: loss=6.2606, lr=0.000149, tokens/sec=1376698.09, grad_norm=0.3827, duration=0.38s
Step 294: loss=6.3715, lr=0.000150, tokens/sec=1380078.91, grad_norm=0.3594, duration=0.38s
Step 295: loss=6.3422, lr=0.000151, tokens/sec=1379824.32, grad_norm=0.2866, duration=0.38s
Step 296: loss=6.3263, lr=0.000151, tokens/sec=1379825.18, grad_norm=0.3162, duration=0.38s
Step 297: loss=6.2786, lr=0.000152, tokens/sec=1377309.43, grad_norm=0.2953, duration=0.38s
Step 298: loss=6.3371, lr=0.000153, tokens/sec=1379132.89, grad_norm=0.2645, duration=0.38s
Step 299: loss=6.3793, lr=0.000154, tokens/sec=1379092.24, grad_norm=0.3539, duration=0.38s
Step 300/19073 (1.6%), Elapsed time: 177.25s, Steps per hour: 6093.14, Estimated hours remaining: 3.08
Step 300: loss=6.2966, lr=0.000155, tokens/sec=1371982.56, grad_norm=0.3293, duration=0.38s
Step 301: loss=6.2909, lr=0.000155, tokens/sec=1382250.22, grad_norm=0.3402, duration=0.38s
Step 302: loss=6.2941, lr=0.000156, tokens/sec=1382326.68, grad_norm=0.3537, duration=0.38s
Step 303: loss=6.2920, lr=0.000157, tokens/sec=1381934.03, grad_norm=0.3024, duration=0.38s
Step 304: loss=6.3590, lr=0.000158, tokens/sec=1377551.88, grad_norm=0.3228, duration=0.38s
Step 305: loss=6.2810, lr=0.000159, tokens/sec=1378390.31, grad_norm=0.3167, duration=0.38s
Step 306: loss=6.2250, lr=0.000159, tokens/sec=1384001.42, grad_norm=0.3654, duration=0.38s
Step 307: loss=6.1971, lr=0.000160, tokens/sec=1380737.48, grad_norm=0.3169, duration=0.38s
Step 308: loss=6.2453, lr=0.000161, tokens/sec=1378795.65, grad_norm=0.2918, duration=0.38s
Step 309: loss=6.2376, lr=0.000162, tokens/sec=1380333.60, grad_norm=0.2930, duration=0.38s
Step 310: loss=6.2071, lr=0.000163, tokens/sec=1380384.72, grad_norm=0.3047, duration=0.38s
Step 311: loss=6.2104, lr=0.000163, tokens/sec=1378943.50, grad_norm=0.3323, duration=0.38s
Step 312: loss=6.1557, lr=0.000164, tokens/sec=1378842.33, grad_norm=0.3135, duration=0.38s
Step 313: loss=6.2517, lr=0.000165, tokens/sec=1379478.95, grad_norm=0.4128, duration=0.38s
Step 314: loss=6.2024, lr=0.000166, tokens/sec=1377671.84, grad_norm=0.3424, duration=0.38s
Step 315: loss=6.2191, lr=0.000167, tokens/sec=1379367.33, grad_norm=0.4155, duration=0.38s
Step 316: loss=6.2104, lr=0.000168, tokens/sec=1371669.34, grad_norm=0.3722, duration=0.38s
Step 317: loss=6.1455, lr=0.000168, tokens/sec=1376542.10, grad_norm=0.4210, duration=0.38s
Step 318: loss=6.1393, lr=0.000169, tokens/sec=1370132.70, grad_norm=0.3116, duration=0.38s
Step 319: loss=6.1466, lr=0.000170, tokens/sec=1382916.95, grad_norm=0.2586, duration=0.38s
Step 320: loss=6.1828, lr=0.000171, tokens/sec=1380691.53, grad_norm=0.4001, duration=0.38s
Step 321: loss=6.1667, lr=0.000172, tokens/sec=1379829.51, grad_norm=0.5162, duration=0.38s
Step 322: loss=6.1399, lr=0.000173, tokens/sec=1380392.52, grad_norm=0.4432, duration=0.38s
Step 323: loss=6.0751, lr=0.000173, tokens/sec=1376472.31, grad_norm=0.3571, duration=0.38s
Step 324: loss=6.1212, lr=0.000174, tokens/sec=1382696.08, grad_norm=0.4038, duration=0.38s
Step 325: loss=6.1515, lr=0.000175, tokens/sec=1377503.55, grad_norm=0.5262, duration=0.38s
Step 326: loss=6.1178, lr=0.000176, tokens/sec=1380377.79, grad_norm=0.3082, duration=0.38s
Step 327: loss=6.1433, lr=0.000177, tokens/sec=1379434.82, grad_norm=0.4409, duration=0.38s
Step 328: loss=6.1654, lr=0.000178, tokens/sec=1381685.70, grad_norm=0.4376, duration=0.38s
Step 329: loss=6.1997, lr=0.000178, tokens/sec=1381526.85, grad_norm=0.4027, duration=0.38s
Step 330: loss=6.3006, lr=0.000179, tokens/sec=1382079.08, grad_norm=0.4389, duration=0.38s
Step 331: loss=6.1820, lr=0.000180, tokens/sec=1378360.07, grad_norm=0.4731, duration=0.38s
Step 332: loss=6.2576, lr=0.000181, tokens/sec=1379588.86, grad_norm=0.4310, duration=0.38s
Step 333: loss=6.2134, lr=0.000182, tokens/sec=1381620.59, grad_norm=0.5457, duration=0.38s
Step 334: loss=6.1938, lr=0.000183, tokens/sec=1379485.01, grad_norm=0.4393, duration=0.38s
Step 335: loss=6.2285, lr=0.000183, tokens/sec=1376435.26, grad_norm=0.4601, duration=0.38s
Step 336: loss=6.2122, lr=0.000184, tokens/sec=1380450.58, grad_norm=0.3129, duration=0.38s
Step 337: loss=6.2276, lr=0.000185, tokens/sec=1380224.43, grad_norm=0.3847, duration=0.38s
Step 338: loss=6.2530, lr=0.000186, tokens/sec=1380930.83, grad_norm=0.3140, duration=0.38s
Step 339: loss=6.1941, lr=0.000187, tokens/sec=1380537.24, grad_norm=0.3559, duration=0.38s
Step 340: loss=6.2009, lr=0.000188, tokens/sec=1380989.80, grad_norm=0.2835, duration=0.38s
Step 341: loss=6.1362, lr=0.000189, tokens/sec=1380657.72, grad_norm=0.3319, duration=0.38s
Step 342: loss=6.1692, lr=0.000189, tokens/sec=1377303.39, grad_norm=0.3397, duration=0.38s
Step 343: loss=6.1981, lr=0.000190, tokens/sec=1377675.29, grad_norm=0.2850, duration=0.38s
Step 344: loss=6.1219, lr=0.000191, tokens/sec=1381268.25, grad_norm=0.4683, duration=0.38s
Step 345: loss=6.1164, lr=0.000192, tokens/sec=1383876.00, grad_norm=0.5000, duration=0.38s
Step 346: loss=6.1887, lr=0.000193, tokens/sec=1382377.95, grad_norm=0.4278, duration=0.38s
Step 347: loss=6.2092, lr=0.000194, tokens/sec=1381956.61, grad_norm=0.3416, duration=0.38s
Step 348: loss=6.2195, lr=0.000195, tokens/sec=1378276.27, grad_norm=0.3521, duration=0.38s
Step 349: loss=6.1250, lr=0.000195, tokens/sec=1373278.04, grad_norm=0.4056, duration=0.38s
Step 350: loss=6.1044, lr=0.000196, tokens/sec=1377027.40, grad_norm=0.3827, duration=0.38s
Step 351: loss=6.1662, lr=0.000197, tokens/sec=1381762.10, grad_norm=0.3816, duration=0.38s
Step 352: loss=6.0753, lr=0.000198, tokens/sec=1382002.64, grad_norm=0.3674, duration=0.38s
Step 353: loss=6.0498, lr=0.000199, tokens/sec=1383873.39, grad_norm=0.3310, duration=0.38s
Step 354: loss=6.0652, lr=0.000200, tokens/sec=1378696.24, grad_norm=0.4020, duration=0.38s
Step 355: loss=6.0543, lr=0.000201, tokens/sec=1379367.33, grad_norm=0.3215, duration=0.38s
Step 356: loss=6.0015, lr=0.000202, tokens/sec=1376306.04, grad_norm=0.3625, duration=0.38s
Step 357: loss=6.0527, lr=0.000202, tokens/sec=1380714.07, grad_norm=0.3240, duration=0.38s
Step 358: loss=6.0622, lr=0.000203, tokens/sec=1380923.03, grad_norm=0.3399, duration=0.38s
Step 359: loss=6.0515, lr=0.000204, tokens/sec=1381810.72, grad_norm=0.3954, duration=0.38s
Step 360: loss=6.0479, lr=0.000205, tokens/sec=1379523.95, grad_norm=0.4696, duration=0.38s
Step 361: loss=6.0655, lr=0.000206, tokens/sec=1383277.09, grad_norm=0.4896, duration=0.38s
Step 362: loss=6.0454, lr=0.000207, tokens/sec=1370915.12, grad_norm=0.6569, duration=0.38s
Step 363: loss=6.0153, lr=0.000208, tokens/sec=1374258.12, grad_norm=0.5406, duration=0.38s
Step 364: loss=5.9768, lr=0.000209, tokens/sec=1374851.83, grad_norm=0.4445, duration=0.38s
Step 365: loss=6.0283, lr=0.000210, tokens/sec=1373533.65, grad_norm=0.4743, duration=0.38s
Step 366: loss=5.9947, lr=0.000210, tokens/sec=1382869.12, grad_norm=0.5503, duration=0.38s
Step 367: loss=6.0017, lr=0.000211, tokens/sec=1378865.68, grad_norm=0.5495, duration=0.38s
Step 368: loss=5.9603, lr=0.000212, tokens/sec=1377728.80, grad_norm=0.4080, duration=0.38s
Step 369: loss=5.9582, lr=0.000213, tokens/sec=1375364.32, grad_norm=0.4805, duration=0.38s
Step 370: loss=5.9370, lr=0.000214, tokens/sec=1381963.56, grad_norm=0.4964, duration=0.38s
Step 371: loss=5.9806, lr=0.000215, tokens/sec=1381703.93, grad_norm=0.4913, duration=0.38s
Step 372: loss=5.9687, lr=0.000216, tokens/sec=1379379.44, grad_norm=0.5485, duration=0.38s
Step 373: loss=5.9704, lr=0.000217, tokens/sec=1377860.02, grad_norm=0.4032, duration=0.38s
Step 374: loss=6.0132, lr=0.000218, tokens/sec=1380924.76, grad_norm=0.4895, duration=0.38s
Step 375: loss=6.0450, lr=0.000218, tokens/sec=1370458.89, grad_norm=0.3976, duration=0.38s
Step 376: loss=6.0666, lr=0.000219, tokens/sec=1379851.16, grad_norm=0.4442, duration=0.38s
Step 377: loss=6.1467, lr=0.000220, tokens/sec=1378163.98, grad_norm=0.3251, duration=0.38s
Step 378: loss=6.0502, lr=0.000221, tokens/sec=1380548.51, grad_norm=0.4105, duration=0.38s
Step 379: loss=6.0585, lr=0.000222, tokens/sec=1383428.51, grad_norm=0.3473, duration=0.38s
Step 380: loss=6.0836, lr=0.000223, tokens/sec=1379373.38, grad_norm=0.3539, duration=0.38s
Step 381: loss=6.1117, lr=0.000224, tokens/sec=1376530.90, grad_norm=0.4031, duration=0.38s
Step 382: loss=6.0327, lr=0.000225, tokens/sec=1379496.26, grad_norm=0.4478, duration=0.38s
Step 383: loss=6.1004, lr=0.000226, tokens/sec=1379900.51, grad_norm=0.5689, duration=0.38s
Step 384: loss=6.0688, lr=0.000227, tokens/sec=1376226.80, grad_norm=0.6240, duration=0.38s
Step 385: loss=6.0699, lr=0.000227, tokens/sec=1376686.88, grad_norm=0.5647, duration=0.38s
Step 386: loss=6.1105, lr=0.000228, tokens/sec=1378699.69, grad_norm=0.4309, duration=0.38s
Step 387: loss=6.0691, lr=0.000229, tokens/sec=1375965.88, grad_norm=0.4706, duration=0.38s
Step 388: loss=6.0710, lr=0.000230, tokens/sec=1374935.21, grad_norm=0.4876, duration=0.38s
Step 389: loss=6.0210, lr=0.000231, tokens/sec=1377266.30, grad_norm=0.5117, duration=0.38s
Step 390: loss=6.0786, lr=0.000232, tokens/sec=1382586.54, grad_norm=0.4667, duration=0.38s
Step 391: loss=6.0313, lr=0.000233, tokens/sec=1377733.12, grad_norm=0.3938, duration=0.38s
Step 392: loss=5.9910, lr=0.000234, tokens/sec=1378427.47, grad_norm=0.4305, duration=0.38s
Step 393: loss=6.0116, lr=0.000235, tokens/sec=1378571.78, grad_norm=0.4242, duration=0.38s
Step 394: loss=5.9905, lr=0.000236, tokens/sec=1376792.90, grad_norm=0.6293, duration=0.38s
Step 395: loss=6.0145, lr=0.000237, tokens/sec=1372494.63, grad_norm=0.8065, duration=0.38s
Step 396: loss=6.1860, lr=0.000238, tokens/sec=1376807.55, grad_norm=0.7165, duration=0.38s
Step 397: loss=6.0186, lr=0.000238, tokens/sec=1379858.08, grad_norm=0.8386, duration=0.38s
Step 398: loss=6.0560, lr=0.000239, tokens/sec=1376761.87, grad_norm=0.5902, duration=0.38s
Step 399: loss=6.0022, lr=0.000240, tokens/sec=1379494.53, grad_norm=0.8850, duration=0.38s
Step 400/19073 (2.1%), Elapsed time: 215.35s, Steps per hour: 6686.88, Estimated hours remaining: 2.79
Step 400: loss=6.0110, lr=0.000241, tokens/sec=1378922.74, grad_norm=0.4800, duration=0.38s
Step 401: loss=5.9727, lr=0.000242, tokens/sec=1377004.98, grad_norm=0.5754, duration=0.38s
Step 402: loss=5.9719, lr=0.000243, tokens/sec=1378883.83, grad_norm=0.5059, duration=0.38s
Step 403: loss=5.9818, lr=0.000244, tokens/sec=1374745.25, grad_norm=0.5702, duration=0.38s
Step 404: loss=5.9270, lr=0.000245, tokens/sec=1373085.96, grad_norm=0.4733, duration=0.38s
Step 405: loss=5.8985, lr=0.000246, tokens/sec=1366040.47, grad_norm=0.5920, duration=0.38s
Step 406: loss=5.9020, lr=0.000247, tokens/sec=1370754.47, grad_norm=0.4559, duration=0.38s
Step 407: loss=5.8554, lr=0.000248, tokens/sec=1372475.79, grad_norm=0.5437, duration=0.38s
Step 408: loss=5.8973, lr=0.000249, tokens/sec=1376680.85, grad_norm=0.5605, duration=0.38s
Step 409: loss=5.8522, lr=0.000250, tokens/sec=1377100.70, grad_norm=0.5151, duration=0.38s
Step 410: loss=5.9784, lr=0.000250, tokens/sec=1378377.35, grad_norm=0.4735, duration=0.38s
Step 411: loss=5.9052, lr=0.000251, tokens/sec=1377004.12, grad_norm=0.5242, duration=0.38s
Step 412: loss=5.8770, lr=0.000252, tokens/sec=1376467.14, grad_norm=0.4670, duration=0.38s
Step 413: loss=5.9236, lr=0.000253, tokens/sec=1375754.11, grad_norm=0.4577, duration=0.38s
Step 414: loss=5.8369, lr=0.000254, tokens/sec=1379490.20, grad_norm=0.4494, duration=0.38s
Step 415: loss=5.7934, lr=0.000255, tokens/sec=1374864.72, grad_norm=0.4894, duration=0.38s
Step 416: loss=5.8892, lr=0.000256, tokens/sec=1378265.91, grad_norm=0.4628, duration=0.38s
Step 417: loss=5.8205, lr=0.000257, tokens/sec=1380722.74, grad_norm=0.4404, duration=0.38s
Step 418: loss=5.7692, lr=0.000258, tokens/sec=1373373.24, grad_norm=0.4714, duration=0.38s
Step 419: loss=5.8393, lr=0.000259, tokens/sec=1376353.42, grad_norm=0.5089, duration=0.38s
Step 420: loss=5.8184, lr=0.000260, tokens/sec=1371912.38, grad_norm=0.5722, duration=0.38s
Step 421: loss=5.7968, lr=0.000261, tokens/sec=1374840.65, grad_norm=0.6778, duration=0.38s
Step 422: loss=5.7685, lr=0.000262, tokens/sec=1378609.80, grad_norm=0.6268, duration=0.38s
Step 423: loss=5.8392, lr=0.000263, tokens/sec=1382390.99, grad_norm=0.5465, duration=0.38s
Step 424: loss=5.8760, lr=0.000263, tokens/sec=1375373.79, grad_norm=0.5283, duration=0.38s
Step 425: loss=5.8460, lr=0.000264, tokens/sec=1379454.72, grad_norm=0.5305, duration=0.38s
Step 426: loss=5.8317, lr=0.000265, tokens/sec=1380123.95, grad_norm=0.4300, duration=0.38s
Step 427: loss=5.9464, lr=0.000266, tokens/sec=1380125.68, grad_norm=0.4160, duration=0.38s
Step 428: loss=5.9503, lr=0.000267, tokens/sec=1380854.52, grad_norm=0.4779, duration=0.38s
Step 429: loss=5.8848, lr=0.000268, tokens/sec=1377137.79, grad_norm=0.6215, duration=0.38s
Step 430: loss=5.9499, lr=0.000269, tokens/sec=1377804.77, grad_norm=0.9961, duration=0.38s
Step 431: loss=5.9444, lr=0.000270, tokens/sec=1378800.84, grad_norm=1.3229, duration=0.38s
Step 432: loss=5.9894, lr=0.000271, tokens/sec=1379961.13, grad_norm=0.7327, duration=0.38s
Step 433: loss=5.9294, lr=0.000272, tokens/sec=1372765.38, grad_norm=1.0789, duration=0.38s
Step 434: loss=5.8798, lr=0.000273, tokens/sec=1377458.68, grad_norm=0.7210, duration=0.38s
Step 435: loss=5.9175, lr=0.000274, tokens/sec=1379588.86, grad_norm=0.8276, duration=0.38s
Step 436: loss=5.8990, lr=0.000275, tokens/sec=1380078.04, grad_norm=0.4469, duration=0.38s
Step 437: loss=5.9219, lr=0.000276, tokens/sec=1376942.90, grad_norm=0.7215, duration=0.38s
Step 438: loss=5.8216, lr=0.000277, tokens/sec=1376813.59, grad_norm=0.5081, duration=0.38s
Step 439: loss=5.8524, lr=0.000278, tokens/sec=1380039.07, grad_norm=0.5121, duration=0.38s
Step 440: loss=5.8730, lr=0.000279, tokens/sec=1376821.35, grad_norm=0.5201, duration=0.38s
Step 441: loss=5.8324, lr=0.000279, tokens/sec=1379153.65, grad_norm=0.4840, duration=0.38s
Step 442: loss=5.8374, lr=0.000280, tokens/sec=1376401.66, grad_norm=0.5702, duration=0.38s
Step 443: loss=5.8519, lr=0.000281, tokens/sec=1374903.40, grad_norm=0.5335, duration=0.38s
Step 444: loss=5.8312, lr=0.000282, tokens/sec=1375606.95, grad_norm=0.4884, duration=0.38s
Step 445: loss=5.8633, lr=0.000283, tokens/sec=1379942.94, grad_norm=0.4508, duration=0.38s
Step 446: loss=5.8188, lr=0.000284, tokens/sec=1380643.85, grad_norm=0.5434, duration=0.38s
Step 447: loss=5.8538, lr=0.000285, tokens/sec=1376632.59, grad_norm=0.5719, duration=0.38s
Step 448: loss=5.8099, lr=0.000286, tokens/sec=1379823.45, grad_norm=0.4899, duration=0.38s
Step 449: loss=5.7627, lr=0.000287, tokens/sec=1379658.10, grad_norm=0.4791, duration=0.38s
Step 450: loss=5.7572, lr=0.000288, tokens/sec=1375440.89, grad_norm=0.5097, duration=0.38s
Step 451: loss=5.7863, lr=0.000289, tokens/sec=1379231.50, grad_norm=0.7891, duration=0.38s
Step 452: loss=5.7921, lr=0.000290, tokens/sec=1378244.31, grad_norm=1.3070, duration=0.38s
Step 453: loss=5.7717, lr=0.000291, tokens/sec=1380149.94, grad_norm=0.7079, duration=0.38s
Step 454: loss=5.7457, lr=0.000292, tokens/sec=1378715.25, grad_norm=0.8272, duration=0.38s
Step 455: loss=5.7907, lr=0.000293, tokens/sec=1380136.94, grad_norm=0.7756, duration=0.38s
Step 456: loss=5.7018, lr=0.000294, tokens/sec=1375281.75, grad_norm=0.5929, duration=0.38s
Step 457: loss=5.7147, lr=0.000295, tokens/sec=1381079.14, grad_norm=0.4904, duration=0.38s
Step 458: loss=5.7536, lr=0.000296, tokens/sec=1370423.87, grad_norm=0.4587, duration=0.38s
Step 459: loss=5.7405, lr=0.000297, tokens/sec=1378794.78, grad_norm=0.5385, duration=0.38s
Step 460: loss=5.7019, lr=0.000297, tokens/sec=1379406.26, grad_norm=0.4834, duration=0.38s
Step 461: loss=5.6546, lr=0.000298, tokens/sec=1381573.72, grad_norm=0.4289, duration=0.38s
Step 462: loss=5.7000, lr=0.000299, tokens/sec=1376898.93, grad_norm=0.5139, duration=0.38s
Step 463: loss=5.6474, lr=0.000300, tokens/sec=1377262.85, grad_norm=0.5328, duration=0.38s
Step 464: loss=5.6459, lr=0.000301, tokens/sec=1378373.90, grad_norm=0.6399, duration=0.38s
Step 465: loss=5.6472, lr=0.000302, tokens/sec=1379668.49, grad_norm=0.6691, duration=0.38s
Step 466: loss=5.6278, lr=0.000303, tokens/sec=1380009.62, grad_norm=0.6563, duration=0.38s
Step 467: loss=5.7091, lr=0.000304, tokens/sec=1381055.72, grad_norm=0.6844, duration=0.38s
Step 468: loss=5.6617, lr=0.000305, tokens/sec=1378128.57, grad_norm=0.7911, duration=0.38s
Step 469: loss=5.6537, lr=0.000306, tokens/sec=1380717.54, grad_norm=0.7590, duration=0.38s
Step 470: loss=5.6628, lr=0.000307, tokens/sec=1378062.93, grad_norm=0.5529, duration=0.38s
Step 471: loss=5.7218, lr=0.000308, tokens/sec=1377684.78, grad_norm=0.6916, duration=0.38s
Step 472: loss=5.7378, lr=0.000309, tokens/sec=1377635.59, grad_norm=0.6523, duration=0.38s
Step 473: loss=5.7592, lr=0.000310, tokens/sec=1377708.95, grad_norm=0.7213, duration=0.38s
Step 474: loss=5.7932, lr=0.000311, tokens/sec=1378916.69, grad_norm=0.7822, duration=0.38s
Step 475: loss=5.7973, lr=0.000312, tokens/sec=1380172.46, grad_norm=0.8047, duration=0.38s
Step 476: loss=5.7718, lr=0.000313, tokens/sec=1374875.04, grad_norm=0.6553, duration=0.38s
Step 477: loss=5.7419, lr=0.000314, tokens/sec=1382221.55, grad_norm=0.5458, duration=0.38s
Step 478: loss=5.7766, lr=0.000315, tokens/sec=1381956.61, grad_norm=0.6389, duration=0.38s
Step 479: loss=5.7502, lr=0.000315, tokens/sec=1378852.71, grad_norm=0.6721, duration=0.38s
Step 480: loss=5.7715, lr=0.000316, tokens/sec=1375808.34, grad_norm=0.6833, duration=0.38s
Step 481: loss=5.7196, lr=0.000317, tokens/sec=1382402.28, grad_norm=0.6415, duration=0.38s
Step 482: loss=5.7125, lr=0.000318, tokens/sec=1382442.26, grad_norm=0.6031, duration=0.38s
Step 483: loss=5.6419, lr=0.000319, tokens/sec=1380448.84, grad_norm=0.5645, duration=0.38s
Step 484: loss=5.7288, lr=0.000320, tokens/sec=1382282.37, grad_norm=0.4939, duration=0.38s
Step 485: loss=5.6958, lr=0.000321, tokens/sec=1377462.14, grad_norm=0.4892, duration=0.38s
Step 486: loss=5.6791, lr=0.000322, tokens/sec=1379981.05, grad_norm=0.6180, duration=0.38s
Step 487: loss=5.6442, lr=0.000323, tokens/sec=1376114.84, grad_norm=0.6582, duration=0.38s
Step 488: loss=5.7294, lr=0.000324, tokens/sec=1370725.41, grad_norm=0.7235, duration=0.38s
Step 489: loss=5.7058, lr=0.000325, tokens/sec=1375463.26, grad_norm=0.7584, duration=0.38s
Step 490: loss=5.6737, lr=0.000326, tokens/sec=1380450.58, grad_norm=0.7868, duration=0.38s
Step 491: loss=5.6631, lr=0.000327, tokens/sec=1375162.20, grad_norm=0.7020, duration=0.38s
Step 492: loss=5.6613, lr=0.000328, tokens/sec=1375946.07, grad_norm=0.8312, duration=0.38s
Step 493: loss=5.6757, lr=0.000329, tokens/sec=1373757.61, grad_norm=0.8580, duration=0.38s
Step 494: loss=5.7149, lr=0.000330, tokens/sec=1376524.87, grad_norm=0.8359, duration=0.38s
Step 495: loss=5.6611, lr=0.000331, tokens/sec=1374952.41, grad_norm=0.8790, duration=0.38s
Step 496: loss=5.6511, lr=0.000332, tokens/sec=1374340.57, grad_norm=0.8162, duration=0.38s
Step 497: loss=5.6209, lr=0.000333, tokens/sec=1379697.06, grad_norm=0.6269, duration=0.38s
Step 498: loss=5.6349, lr=0.000334, tokens/sec=1377197.30, grad_norm=0.5732, duration=0.38s
Step 499: loss=5.5984, lr=0.000334, tokens/sec=1378352.30, grad_norm=0.4752, duration=0.38s
Step 500/19073 (2.6%), Elapsed time: 253.48s, Steps per hour: 7101.19, Estimated hours remaining: 2.62
Validation loss at step 500: 5.610202789306641
Step 500: loss=5.5980, lr=0.000335, tokens/sec=153676.16, grad_norm=0.4448, duration=3.41s
Step 501: loss=5.5339, lr=0.000336, tokens/sec=1377898.87, grad_norm=0.4835, duration=0.38s
Step 502: loss=5.5817, lr=0.000337, tokens/sec=1379285.13, grad_norm=0.5074, duration=0.38s
Step 503: loss=5.6000, lr=0.000338, tokens/sec=1380835.45, grad_norm=0.5231, duration=0.38s
Step 504: loss=5.5799, lr=0.000339, tokens/sec=1370382.02, grad_norm=0.5846, duration=0.38s
Step 505: loss=5.6011, lr=0.000340, tokens/sec=1377549.29, grad_norm=0.6392, duration=0.38s
Step 506: loss=5.5558, lr=0.000341, tokens/sec=1376371.51, grad_norm=0.6426, duration=0.38s
Step 507: loss=5.5369, lr=0.000342, tokens/sec=1376173.40, grad_norm=0.8894, duration=0.38s
Step 508: loss=5.5561, lr=0.000343, tokens/sec=1377726.22, grad_norm=1.1521, duration=0.38s
Step 509: loss=5.5138, lr=0.000344, tokens/sec=1375893.56, grad_norm=0.8416, duration=0.38s
Step 510: loss=5.5923, lr=0.000345, tokens/sec=1374690.25, grad_norm=0.7467, duration=0.38s
Step 511: loss=5.5188, lr=0.000346, tokens/sec=1373950.73, grad_norm=0.6967, duration=0.38s
Step 512: loss=5.5159, lr=0.000347, tokens/sec=1367886.94, grad_norm=0.6921, duration=0.38s
Step 513: loss=5.4699, lr=0.000348, tokens/sec=1378240.86, grad_norm=0.5858, duration=0.38s
Step 514: loss=5.5113, lr=0.000349, tokens/sec=1375794.56, grad_norm=0.6007, duration=0.38s
Step 515: loss=5.5236, lr=0.000350, tokens/sec=1369547.33, grad_norm=0.5238, duration=0.38s
Step 516: loss=5.4689, lr=0.000351, tokens/sec=1376160.48, grad_norm=0.4340, duration=0.38s
Step 517: loss=5.5564, lr=0.000351, tokens/sec=1376219.91, grad_norm=0.4508, duration=0.38s
Step 518: loss=5.5371, lr=0.000352, tokens/sec=1377029.99, grad_norm=0.4720, duration=0.38s
Step 519: loss=5.5682, lr=0.000353, tokens/sec=1374952.41, grad_norm=0.5898, duration=0.38s
Step 520: loss=5.6676, lr=0.000354, tokens/sec=1376637.76, grad_norm=0.9477, duration=0.38s
Step 521: loss=5.6014, lr=0.000355, tokens/sec=1378917.56, grad_norm=0.9870, duration=0.38s
Step 522: loss=5.6654, lr=0.000356, tokens/sec=1375326.48, grad_norm=0.9899, duration=0.38s
Step 523: loss=5.6335, lr=0.000357, tokens/sec=1375516.60, grad_norm=1.2074, duration=0.38s
Step 524: loss=5.6471, lr=0.000358, tokens/sec=1375794.56, grad_norm=0.9390, duration=0.38s
Step 525: loss=5.6256, lr=0.000359, tokens/sec=1374574.24, grad_norm=0.8849, duration=0.38s
Step 526: loss=5.6089, lr=0.000360, tokens/sec=1377960.17, grad_norm=0.6636, duration=0.38s
Step 527: loss=5.6475, lr=0.000361, tokens/sec=1376696.36, grad_norm=0.6944, duration=0.38s
Step 528: loss=5.6613, lr=0.000362, tokens/sec=1377885.92, grad_norm=0.5419, duration=0.38s
Step 529: loss=5.6008, lr=0.000363, tokens/sec=1378448.20, grad_norm=0.5674, duration=0.38s
Step 530: loss=5.6069, lr=0.000364, tokens/sec=1380235.70, grad_norm=0.5137, duration=0.38s
Step 531: loss=5.5325, lr=0.000365, tokens/sec=1378727.36, grad_norm=0.4605, duration=0.38s
Step 532: loss=5.5490, lr=0.000366, tokens/sec=1382736.08, grad_norm=0.5217, duration=0.38s
Step 533: loss=5.5683, lr=0.000366, tokens/sec=1379029.97, grad_norm=0.7596, duration=0.38s
Step 534: loss=5.5114, lr=0.000367, tokens/sec=1381443.53, grad_norm=0.9896, duration=0.38s
Step 535: loss=5.5758, lr=0.000368, tokens/sec=1382068.65, grad_norm=0.9429, duration=0.38s
Step 536: loss=5.5480, lr=0.000369, tokens/sec=1376702.39, grad_norm=0.5338, duration=0.38s
Step 537: loss=5.6021, lr=0.000370, tokens/sec=1375773.91, grad_norm=0.6003, duration=0.38s
Step 538: loss=5.6240, lr=0.000371, tokens/sec=1374270.15, grad_norm=0.5233, duration=0.38s
Step 539: loss=5.5423, lr=0.000372, tokens/sec=1373069.67, grad_norm=0.4248, duration=0.38s
Step 540: loss=5.4643, lr=0.000373, tokens/sec=1374449.67, grad_norm=0.4949, duration=0.38s
Step 541: loss=5.5512, lr=0.000374, tokens/sec=1377304.25, grad_norm=0.5605, duration=0.38s
Step 542: loss=5.5009, lr=0.000375, tokens/sec=1381696.98, grad_norm=0.8348, duration=0.38s
Step 543: loss=5.4888, lr=0.000376, tokens/sec=1376600.70, grad_norm=1.0246, duration=0.38s
Step 544: loss=5.4289, lr=0.000377, tokens/sec=1374358.61, grad_norm=0.8713, duration=0.38s
Step 545: loss=5.4970, lr=0.000378, tokens/sec=1378335.02, grad_norm=0.7296, duration=0.38s
Step 546: loss=5.4300, lr=0.000379, tokens/sec=1378062.93, grad_norm=0.8911, duration=0.38s
Step 547: loss=5.4764, lr=0.000379, tokens/sec=1373919.83, grad_norm=0.6697, duration=0.38s
Step 548: loss=5.4869, lr=0.000380, tokens/sec=1380136.08, grad_norm=0.6370, duration=0.38s
Step 549: loss=5.4691, lr=0.000381, tokens/sec=1375841.91, grad_norm=0.8375, duration=0.38s
Step 550: loss=5.4733, lr=0.000382, tokens/sec=1365400.93, grad_norm=0.9582, duration=0.38s
Step 551: loss=5.4674, lr=0.000383, tokens/sec=1377305.98, grad_norm=0.7713, duration=0.38s
Step 552: loss=5.4109, lr=0.000384, tokens/sec=1374676.50, grad_norm=0.6011, duration=0.38s
Step 553: loss=5.4294, lr=0.000385, tokens/sec=1372265.95, grad_norm=0.5355, duration=0.38s
Step 554: loss=5.3630, lr=0.000386, tokens/sec=1375590.60, grad_norm=0.5145, duration=0.38s
Step 555: loss=5.4082, lr=0.000387, tokens/sec=1379148.46, grad_norm=0.5402, duration=0.38s
Step 556: loss=5.3611, lr=0.000388, tokens/sec=1378898.53, grad_norm=0.6474, duration=0.38s
Step 557: loss=5.3953, lr=0.000389, tokens/sec=1381519.04, grad_norm=0.5212, duration=0.38s
Step 558: loss=5.3646, lr=0.000390, tokens/sec=1378509.56, grad_norm=0.4757, duration=0.38s
Step 559: loss=5.3328, lr=0.000390, tokens/sec=1377127.44, grad_norm=0.4655, duration=0.38s
Step 560: loss=5.3043, lr=0.000391, tokens/sec=1377035.16, grad_norm=0.4911, duration=0.38s
Step 561: loss=5.3540, lr=0.000392, tokens/sec=1376221.63, grad_norm=0.4565, duration=0.38s
Step 562: loss=5.3373, lr=0.000393, tokens/sec=1375145.86, grad_norm=0.4914, duration=0.38s
Step 563: loss=5.3893, lr=0.000394, tokens/sec=1380251.29, grad_norm=0.8942, duration=0.38s
Step 564: loss=5.4555, lr=0.000395, tokens/sec=1382475.29, grad_norm=1.2859, duration=0.38s
Step 565: loss=5.4573, lr=0.000396, tokens/sec=1378585.60, grad_norm=0.7938, duration=0.38s
Step 566: loss=5.5156, lr=0.000397, tokens/sec=1378613.26, grad_norm=0.8325, duration=0.38s
Step 567: loss=5.5223, lr=0.000398, tokens/sec=1380175.06, grad_norm=0.6988, duration=0.38s
Step 568: loss=5.4636, lr=0.000399, tokens/sec=1380156.00, grad_norm=0.6040, duration=0.38s
Step 569: loss=5.4655, lr=0.000400, tokens/sec=1380079.78, grad_norm=0.6240, duration=0.38s
Step 570: loss=5.4880, lr=0.000400, tokens/sec=1379148.46, grad_norm=0.7012, duration=0.38s
Step 571: loss=5.4819, lr=0.000401, tokens/sec=1380204.51, grad_norm=0.7107, duration=0.38s
Step 572: loss=5.5204, lr=0.000402, tokens/sec=1375443.47, grad_norm=0.7001, duration=0.38s
Step 573: loss=5.4959, lr=0.000403, tokens/sec=1380363.92, grad_norm=0.6738, duration=0.38s
Step 574: loss=5.4938, lr=0.000404, tokens/sec=1382160.73, grad_norm=0.9056, duration=0.38s
Step 575: loss=5.5010, lr=0.000405, tokens/sec=1378691.92, grad_norm=0.9100, duration=0.38s
Step 576: loss=5.5501, lr=0.000406, tokens/sec=1381272.59, grad_norm=0.8769, duration=0.38s
Step 577: loss=5.4811, lr=0.000407, tokens/sec=1379539.53, grad_norm=0.7342, duration=0.38s
Step 578: loss=5.5005, lr=0.000408, tokens/sec=1376110.53, grad_norm=0.6593, duration=0.38s
Step 579: loss=5.4492, lr=0.000409, tokens/sec=1376661.03, grad_norm=0.7589, duration=0.38s
Step 580: loss=5.4968, lr=0.000409, tokens/sec=1374550.19, grad_norm=0.6470, duration=0.38s
Step 581: loss=5.3721, lr=0.000410, tokens/sec=1378204.58, grad_norm=0.6404, duration=0.38s
Step 582: loss=5.4649, lr=0.000411, tokens/sec=1376692.05, grad_norm=0.6955, duration=0.38s
Step 583: loss=5.3968, lr=0.000412, tokens/sec=1379163.16, grad_norm=0.5929, duration=0.38s
Step 584: loss=5.3757, lr=0.000413, tokens/sec=1376679.12, grad_norm=0.4854, duration=0.38s
Step 585: loss=5.5888, lr=0.000414, tokens/sec=1377368.96, grad_norm=0.6548, duration=0.38s
Step 586: loss=5.4192, lr=0.000415, tokens/sec=1378908.91, grad_norm=1.0260, duration=0.38s
Step 587: loss=5.4716, lr=0.000416, tokens/sec=1380847.59, grad_norm=1.1786, duration=0.38s
Step 588: loss=5.4504, lr=0.000417, tokens/sec=1375984.82, grad_norm=0.7564, duration=0.38s
Step 589: loss=5.2775, lr=0.000417, tokens/sec=1380805.10, grad_norm=0.9248, duration=0.38s
Step 590: loss=5.3887, lr=0.000418, tokens/sec=1378778.36, grad_norm=0.5256, duration=0.38s
Step 591: loss=5.3461, lr=0.000419, tokens/sec=1373628.03, grad_norm=0.5609, duration=0.38s
Step 592: loss=5.3832, lr=0.000420, tokens/sec=1377502.69, grad_norm=0.5699, duration=0.38s
Step 593: loss=5.3590, lr=0.000421, tokens/sec=1376824.79, grad_norm=0.5866, duration=0.38s
Step 594: loss=5.3544, lr=0.000422, tokens/sec=1374747.83, grad_norm=0.5798, duration=0.38s
Step 595: loss=5.2944, lr=0.000423, tokens/sec=1378139.80, grad_norm=0.6154, duration=0.38s
Step 596: loss=5.2926, lr=0.000424, tokens/sec=1374302.78, grad_norm=0.7541, duration=0.38s
Step 597: loss=5.2829, lr=0.000424, tokens/sec=1371837.92, grad_norm=0.8669, duration=0.38s
Step 598: loss=5.3139, lr=0.000425, tokens/sec=1372139.23, grad_norm=0.8097, duration=0.38s
Step 599: loss=5.3137, lr=0.000426, tokens/sec=1377509.59, grad_norm=0.8756, duration=0.38s
Step 600/19073 (3.1%), Elapsed time: 294.66s, Steps per hour: 7330.58, Estimated hours remaining: 2.52
Step 600: loss=5.3681, lr=0.000427, tokens/sec=1380022.61, grad_norm=0.8461, duration=0.38s
Step 601: loss=5.3262, lr=0.000428, tokens/sec=1375507.99, grad_norm=0.7738, duration=0.38s
Step 602: loss=5.3203, lr=0.000429, tokens/sec=1380826.78, grad_norm=0.9446, duration=0.38s
Step 603: loss=5.3763, lr=0.000430, tokens/sec=1373932.70, grad_norm=0.7185, duration=0.38s
Step 604: loss=5.2311, lr=0.000431, tokens/sec=1378240.86, grad_norm=0.6748, duration=0.38s
Step 605: loss=5.2501, lr=0.000431, tokens/sec=1376477.48, grad_norm=0.6823, duration=0.38s
Step 606: loss=5.2972, lr=0.000432, tokens/sec=1376577.43, grad_norm=0.8041, duration=0.38s
Step 607: loss=5.2285, lr=0.000433, tokens/sec=1382068.65, grad_norm=0.8590, duration=0.38s
Step 608: loss=5.2306, lr=0.000434, tokens/sec=1378837.15, grad_norm=0.8708, duration=0.38s
Step 609: loss=5.2461, lr=0.000435, tokens/sec=1378316.88, grad_norm=0.6755, duration=0.38s
Step 610: loss=5.2113, lr=0.000436, tokens/sec=1374559.64, grad_norm=0.6503, duration=0.38s
Step 611: loss=5.2168, lr=0.000437, tokens/sec=1383421.55, grad_norm=0.6257, duration=0.38s
Step 612: loss=5.1935, lr=0.000437, tokens/sec=1377507.01, grad_norm=0.5773, duration=0.38s
Step 613: loss=5.2871, lr=0.000438, tokens/sec=1381323.78, grad_norm=0.5744, duration=0.38s
Step 614: loss=5.2643, lr=0.000439, tokens/sec=1379995.77, grad_norm=0.5700, duration=0.38s
Step 615: loss=5.2424, lr=0.000440, tokens/sec=1379363.87, grad_norm=0.4686, duration=0.38s
Step 616: loss=5.2467, lr=0.000441, tokens/sec=1379171.81, grad_norm=0.4273, duration=0.38s
Step 617: loss=5.3396, lr=0.000442, tokens/sec=1380251.29, grad_norm=0.5310, duration=0.38s
Step 618: loss=5.3735, lr=0.000443, tokens/sec=1376166.51, grad_norm=0.6138, duration=0.38s
Step 619: loss=5.3138, lr=0.000443, tokens/sec=1377565.68, grad_norm=0.7142, duration=0.38s
Step 620: loss=5.3487, lr=0.000444, tokens/sec=1375323.04, grad_norm=0.7230, duration=0.38s
Step 621: loss=5.3844, lr=0.000445, tokens/sec=1382432.70, grad_norm=0.6465, duration=0.38s
Step 622: loss=5.3641, lr=0.000446, tokens/sec=1378033.57, grad_norm=0.6776, duration=0.38s
Step 623: loss=5.3488, lr=0.000447, tokens/sec=1380603.11, grad_norm=0.8835, duration=0.38s
Step 624: loss=5.2931, lr=0.000448, tokens/sec=1380443.64, grad_norm=1.0356, duration=0.38s
Step 625: loss=5.3277, lr=0.000448, tokens/sec=1372839.94, grad_norm=0.8324, duration=0.38s
Step 626: loss=5.3108, lr=0.000449, tokens/sec=1377167.97, grad_norm=0.6210, duration=0.38s
Step 627: loss=5.3125, lr=0.000450, tokens/sec=1377402.60, grad_norm=0.5894, duration=0.38s
Step 628: loss=5.2625, lr=0.000451, tokens/sec=1377554.47, grad_norm=0.6759, duration=0.38s
Step 629: loss=5.2901, lr=0.000452, tokens/sec=1373789.36, grad_norm=0.9469, duration=0.38s
Step 630: loss=5.3124, lr=0.000453, tokens/sec=1377225.76, grad_norm=1.0298, duration=0.38s
Step 631: loss=5.2744, lr=0.000453, tokens/sec=1373098.82, grad_norm=0.8205, duration=0.38s
Step 632: loss=5.2928, lr=0.000454, tokens/sec=1373915.53, grad_norm=0.7471, duration=0.38s
Step 633: loss=5.2823, lr=0.000455, tokens/sec=1378826.77, grad_norm=0.5602, duration=0.38s
Step 634: loss=5.2626, lr=0.000456, tokens/sec=1371761.75, grad_norm=0.5312, duration=0.38s
Step 635: loss=5.3122, lr=0.000457, tokens/sec=1378256.41, grad_norm=0.6016, duration=0.38s
Step 636: loss=5.2077, lr=0.000458, tokens/sec=1380063.32, grad_norm=0.5121, duration=0.38s
Step 637: loss=5.2896, lr=0.000458, tokens/sec=1378168.30, grad_norm=0.5922, duration=0.38s
Step 638: loss=5.2762, lr=0.000459, tokens/sec=1378270.23, grad_norm=0.7346, duration=0.38s
Step 639: loss=5.1918, lr=0.000460, tokens/sec=1373598.86, grad_norm=0.9168, duration=0.38s
Step 640: loss=5.2424, lr=0.000461, tokens/sec=1377752.11, grad_norm=0.8947, duration=0.38s
Step 641: loss=5.2289, lr=0.000462, tokens/sec=1377042.92, grad_norm=0.7722, duration=0.38s
Step 642: loss=5.2084, lr=0.000462, tokens/sec=1378457.71, grad_norm=0.7590, duration=0.38s
Step 643: loss=5.2507, lr=0.000463, tokens/sec=1370296.63, grad_norm=0.9428, duration=0.38s
Step 644: loss=5.2043, lr=0.000464, tokens/sec=1376156.18, grad_norm=0.8171, duration=0.38s
Step 645: loss=5.2030, lr=0.000465, tokens/sec=1370106.24, grad_norm=0.7855, duration=0.38s
Step 646: loss=5.1521, lr=0.000466, tokens/sec=1377405.19, grad_norm=0.7716, duration=0.38s
Step 647: loss=5.1820, lr=0.000466, tokens/sec=1380484.37, grad_norm=0.6777, duration=0.38s
Step 648: loss=5.1994, lr=0.000467, tokens/sec=1376331.02, grad_norm=0.6608, duration=0.38s
Step 649: loss=5.1700, lr=0.000468, tokens/sec=1377171.42, grad_norm=0.6668, duration=0.38s
Step 650: loss=5.1285, lr=0.000469, tokens/sec=1382454.43, grad_norm=0.6144, duration=0.38s
Step 651: loss=5.1100, lr=0.000470, tokens/sec=1380707.13, grad_norm=0.5571, duration=0.38s
Step 652: loss=5.1240, lr=0.000470, tokens/sec=1377159.35, grad_norm=0.6497, duration=0.38s
Step 653: loss=5.0785, lr=0.000471, tokens/sec=1377527.72, grad_norm=0.7317, duration=0.38s
Step 654: loss=5.0963, lr=0.000472, tokens/sec=1379691.86, grad_norm=0.6820, duration=0.38s
Step 655: loss=5.0629, lr=0.000473, tokens/sec=1379951.60, grad_norm=0.6920, duration=0.38s
Step 656: loss=5.0831, lr=0.000474, tokens/sec=1380781.69, grad_norm=0.6634, duration=0.38s
Step 657: loss=5.0993, lr=0.000474, tokens/sec=1378631.41, grad_norm=0.5237, duration=0.38s
Step 658: loss=5.0654, lr=0.000475, tokens/sec=1374584.55, grad_norm=0.5074, duration=0.38s
Step 659: loss=5.0986, lr=0.000476, tokens/sec=1374819.17, grad_norm=0.4960, duration=0.38s
Step 660: loss=5.1181, lr=0.000477, tokens/sec=1378546.71, grad_norm=0.4839, duration=0.38s
Step 661: loss=5.1391, lr=0.000478, tokens/sec=1378489.68, grad_norm=0.5210, duration=0.38s
Step 662: loss=5.1327, lr=0.000478, tokens/sec=1379868.47, grad_norm=0.5770, duration=0.38s
Step 663: loss=5.1821, lr=0.000479, tokens/sec=1379505.78, grad_norm=0.6258, duration=0.38s
Step 664: loss=5.2169, lr=0.000480, tokens/sec=1377129.16, grad_norm=0.7080, duration=0.38s
Step 665: loss=5.2395, lr=0.000481, tokens/sec=1372227.42, grad_norm=0.7882, duration=0.38s
Step 666: loss=5.2361, lr=0.000481, tokens/sec=1372363.58, grad_norm=0.7734, duration=0.38s
Step 667: loss=5.2295, lr=0.000482, tokens/sec=1377717.58, grad_norm=0.9691, duration=0.38s
Step 668: loss=5.2500, lr=0.000483, tokens/sec=1379618.29, grad_norm=1.1334, duration=0.38s
Step 669: loss=5.2911, lr=0.000484, tokens/sec=1377489.75, grad_norm=1.1214, duration=0.38s
Step 670: loss=5.2471, lr=0.000484, tokens/sec=1382311.91, grad_norm=0.9957, duration=0.38s
Step 671: loss=5.2233, lr=0.000485, tokens/sec=1378089.71, grad_norm=0.7876, duration=0.38s
Step 672: loss=5.1993, lr=0.000486, tokens/sec=1377931.68, grad_norm=0.7608, duration=0.38s
Step 673: loss=5.1573, lr=0.000487, tokens/sec=1375532.95, grad_norm=0.7178, duration=0.38s
Step 674: loss=5.2278, lr=0.000487, tokens/sec=1366265.38, grad_norm=0.7528, duration=0.38s
Step 675: loss=5.1785, lr=0.000488, tokens/sec=1375378.09, grad_norm=0.8413, duration=0.38s
Step 676: loss=5.1699, lr=0.000489, tokens/sec=1374367.20, grad_norm=0.7326, duration=0.38s
Step 677: loss=5.1540, lr=0.000490, tokens/sec=1375137.27, grad_norm=0.8070, duration=0.38s
Step 678: loss=5.2046, lr=0.000490, tokens/sec=1371660.79, grad_norm=0.8633, duration=0.38s
Step 679: loss=5.1630, lr=0.000491, tokens/sec=1376480.93, grad_norm=0.8358, duration=0.38s
Step 680: loss=5.1501, lr=0.000492, tokens/sec=1373948.15, grad_norm=0.6324, duration=0.38s
Step 681: loss=5.1345, lr=0.000493, tokens/sec=1374606.04, grad_norm=0.6720, duration=0.38s
Step 682: loss=5.1571, lr=0.000493, tokens/sec=1375518.32, grad_norm=0.8474, duration=0.38s
Step 683: loss=5.1087, lr=0.000494, tokens/sec=1380455.78, grad_norm=0.7340, duration=0.38s
Step 684: loss=5.1781, lr=0.000495, tokens/sec=1375788.54, grad_norm=0.5949, duration=0.38s
Step 685: loss=5.1225, lr=0.000496, tokens/sec=1378963.38, grad_norm=0.4617, duration=0.38s
Step 686: loss=5.1331, lr=0.000496, tokens/sec=1378470.67, grad_norm=0.4553, duration=0.38s
Step 687: loss=5.0976, lr=0.000497, tokens/sec=1376290.54, grad_norm=0.5210, duration=0.38s
Step 688: loss=5.0661, lr=0.000498, tokens/sec=1374214.32, grad_norm=0.6555, duration=0.38s
Step 689: loss=5.0941, lr=0.000499, tokens/sec=1372102.41, grad_norm=0.6872, duration=0.38s
Step 690: loss=5.0316, lr=0.000499, tokens/sec=1371124.54, grad_norm=0.5898, duration=0.38s
Step 691: loss=5.0306, lr=0.000500, tokens/sec=1377747.79, grad_norm=0.6010, duration=0.38s
Step 692: loss=5.0486, lr=0.000501, tokens/sec=1376429.23, grad_norm=0.6469, duration=0.38s
Step 693: loss=5.0539, lr=0.000501, tokens/sec=1377739.16, grad_norm=0.6189, duration=0.38s
Step 694: loss=5.0650, lr=0.000502, tokens/sec=1373761.90, grad_norm=0.5613, duration=0.38s
Step 695: loss=5.0299, lr=0.000503, tokens/sec=1379094.83, grad_norm=0.6439, duration=0.38s
Step 696: loss=5.0462, lr=0.000504, tokens/sec=1381327.25, grad_norm=0.8235, duration=0.38s
Step 697: loss=5.0268, lr=0.000504, tokens/sec=1378416.23, grad_norm=0.8356, duration=0.38s
Step 698: loss=5.0062, lr=0.000505, tokens/sec=1376157.04, grad_norm=0.7394, duration=0.38s
Step 699: loss=5.0157, lr=0.000506, tokens/sec=1381139.86, grad_norm=0.6407, duration=0.38s
Step 700/19073 (3.7%), Elapsed time: 332.80s, Steps per hour: 7572.10, Estimated hours remaining: 2.43
Step 700: loss=5.0284, lr=0.000506, tokens/sec=1376519.70, grad_norm=0.6260, duration=0.38s
Step 701: loss=4.9584, lr=0.000507, tokens/sec=1379000.57, grad_norm=0.5761, duration=0.38s
Step 702: loss=4.9731, lr=0.000508, tokens/sec=1383669.63, grad_norm=0.5030, duration=0.38s
Step 703: loss=4.9326, lr=0.000508, tokens/sec=1376420.62, grad_norm=0.5944, duration=0.38s
Step 704: loss=4.9902, lr=0.000509, tokens/sec=1378981.54, grad_norm=0.6615, duration=0.38s
Step 705: loss=4.9806, lr=0.000510, tokens/sec=1380649.05, grad_norm=0.6220, duration=0.38s
Step 706: loss=4.9948, lr=0.000511, tokens/sec=1376606.73, grad_norm=0.6350, duration=0.38s
Step 707: loss=5.0080, lr=0.000511, tokens/sec=1378173.48, grad_norm=0.6151, duration=0.38s
Step 708: loss=4.9930, lr=0.000512, tokens/sec=1379455.59, grad_norm=0.5075, duration=0.38s
Step 709: loss=5.0137, lr=0.000513, tokens/sec=1379432.22, grad_norm=0.6160, duration=0.38s
Step 710: loss=5.1649, lr=0.000513, tokens/sec=1377776.28, grad_norm=1.0205, duration=0.38s
Step 711: loss=5.1213, lr=0.000514, tokens/sec=1381118.17, grad_norm=1.0517, duration=0.38s
Step 712: loss=5.1523, lr=0.000515, tokens/sec=1375873.76, grad_norm=0.8806, duration=0.38s
Step 713: loss=5.1351, lr=0.000515, tokens/sec=1379476.35, grad_norm=0.7927, duration=0.38s
Step 714: loss=5.1190, lr=0.000516, tokens/sec=1378827.64, grad_norm=0.7950, duration=0.38s
Step 715: loss=5.0910, lr=0.000517, tokens/sec=1379671.09, grad_norm=0.8456, duration=0.38s
Step 716: loss=5.1231, lr=0.000517, tokens/sec=1378544.99, grad_norm=1.0616, duration=0.38s
Step 717: loss=5.1537, lr=0.000518, tokens/sec=1378007.67, grad_norm=0.9521, duration=0.38s
Step 718: loss=5.2022, lr=0.000519, tokens/sec=1376842.90, grad_norm=0.9280, duration=0.38s
Step 719: loss=5.1368, lr=0.000519, tokens/sec=1379398.48, grad_norm=0.8241, duration=0.38s
Step 720: loss=5.1409, lr=0.000520, tokens/sec=1381787.28, grad_norm=0.7395, duration=0.38s
Step 721: loss=5.0332, lr=0.000521, tokens/sec=1377756.43, grad_norm=0.6909, duration=0.38s
Step 722: loss=5.0566, lr=0.000521, tokens/sec=1380508.64, grad_norm=0.6362, duration=0.38s
Step 723: loss=5.0515, lr=0.000522, tokens/sec=1371162.16, grad_norm=0.6584, duration=0.38s
Step 724: loss=5.0306, lr=0.000523, tokens/sec=1380077.18, grad_norm=0.6139, duration=0.38s
Step 725: loss=5.0418, lr=0.000523, tokens/sec=1380449.71, grad_norm=0.5565, duration=0.38s
Step 726: loss=5.0355, lr=0.000524, tokens/sec=1379407.99, grad_norm=0.6483, duration=0.38s
Step 727: loss=5.1052, lr=0.000525, tokens/sec=1378338.47, grad_norm=0.6899, duration=0.38s
Step 728: loss=5.1458, lr=0.000525, tokens/sec=1377676.15, grad_norm=0.7280, duration=0.38s
Step 729: loss=5.0344, lr=0.000526, tokens/sec=1376649.82, grad_norm=0.6878, duration=0.38s
Step 730: loss=4.9820, lr=0.000526, tokens/sec=1381125.98, grad_norm=0.8246, duration=0.38s
Step 731: loss=5.0802, lr=0.000527, tokens/sec=1376276.75, grad_norm=0.8509, duration=0.38s
Step 732: loss=5.0204, lr=0.000528, tokens/sec=1377458.68, grad_norm=0.6673, duration=0.38s
Step 733: loss=4.9335, lr=0.000528, tokens/sec=1379303.30, grad_norm=0.5132, duration=0.38s
Step 734: loss=4.9235, lr=0.000529, tokens/sec=1377890.24, grad_norm=0.5302, duration=0.38s
Step 735: loss=4.9633, lr=0.000530, tokens/sec=1379803.54, grad_norm=0.4536, duration=0.38s
Step 736: loss=4.9101, lr=0.000530, tokens/sec=1376986.01, grad_norm=0.5333, duration=0.38s
Step 737: loss=4.9510, lr=0.000531, tokens/sec=1379274.75, grad_norm=0.5700, duration=0.38s
Step 738: loss=4.9863, lr=0.000531, tokens/sec=1375916.80, grad_norm=0.4796, duration=0.38s
Step 739: loss=4.9243, lr=0.000532, tokens/sec=1380475.71, grad_norm=0.4616, duration=0.38s
Step 740: loss=4.9061, lr=0.000533, tokens/sec=1377982.62, grad_norm=0.4936, duration=0.38s
Step 741: loss=4.9195, lr=0.000533, tokens/sec=1378544.99, grad_norm=0.6311, duration=0.38s
Step 742: loss=4.8881, lr=0.000534, tokens/sec=1376741.18, grad_norm=0.8180, duration=0.38s
Step 743: loss=4.9111, lr=0.000534, tokens/sec=1377821.17, grad_norm=0.6941, duration=0.38s
Step 744: loss=4.8777, lr=0.000535, tokens/sec=1379058.51, grad_norm=0.8238, duration=0.38s
Step 745: loss=4.8776, lr=0.000536, tokens/sec=1376001.18, grad_norm=0.8853, duration=0.38s
Step 746: loss=4.8696, lr=0.000536, tokens/sec=1381673.54, grad_norm=0.8613, duration=0.38s
Step 747: loss=4.9431, lr=0.000537, tokens/sec=1380603.11, grad_norm=0.7653, duration=0.38s
Step 748: loss=4.8706, lr=0.000537, tokens/sec=1381068.73, grad_norm=0.8027, duration=0.38s
Step 749: loss=4.8523, lr=0.000538, tokens/sec=1376620.52, grad_norm=0.6913, duration=0.38s
Validation loss at step 750: 4.968682289123535
Step 750: loss=4.8179, lr=0.000539, tokens/sec=152686.17, grad_norm=0.6711, duration=3.43s
Step 751: loss=4.8520, lr=0.000539, tokens/sec=1377481.12, grad_norm=0.6461, duration=0.38s
Step 752: loss=4.9015, lr=0.000540, tokens/sec=1381269.12, grad_norm=0.7423, duration=0.38s
Step 753: loss=4.9052, lr=0.000540, tokens/sec=1378019.76, grad_norm=0.7354, duration=0.38s
Step 754: loss=4.9174, lr=0.000541, tokens/sec=1376677.40, grad_norm=0.5779, duration=0.38s
Step 755: loss=4.9848, lr=0.000542, tokens/sec=1378962.52, grad_norm=0.6096, duration=0.38s
Step 756: loss=4.9848, lr=0.000542, tokens/sec=1380485.24, grad_norm=0.7108, duration=0.38s
Step 757: loss=5.0373, lr=0.000543, tokens/sec=1379234.96, grad_norm=0.6708, duration=0.38s
Step 758: loss=4.9647, lr=0.000543, tokens/sec=1379420.97, grad_norm=0.6945, duration=0.38s
Step 759: loss=4.9895, lr=0.000544, tokens/sec=1380850.19, grad_norm=0.6069, duration=0.38s
Step 760: loss=4.9454, lr=0.000544, tokens/sec=1379141.54, grad_norm=0.5343, duration=0.38s
Step 761: loss=5.0548, lr=0.000545, tokens/sec=1375508.86, grad_norm=0.6181, duration=0.38s
Step 762: loss=4.9962, lr=0.000545, tokens/sec=1378340.20, grad_norm=0.6445, duration=0.38s
Step 763: loss=5.0376, lr=0.000546, tokens/sec=1377159.35, grad_norm=0.7199, duration=0.38s
Step 764: loss=4.9956, lr=0.000547, tokens/sec=1379156.24, grad_norm=0.6018, duration=0.38s
Step 765: loss=4.9929, lr=0.000547, tokens/sec=1376429.23, grad_norm=0.6622, duration=0.38s
Step 766: loss=5.0549, lr=0.000548, tokens/sec=1376620.52, grad_norm=0.6292, duration=0.38s
Step 767: loss=4.9934, lr=0.000548, tokens/sec=1375477.88, grad_norm=0.6288, duration=0.38s
Step 768: loss=5.0079, lr=0.000549, tokens/sec=1373767.05, grad_norm=0.6806, duration=0.38s
Step 769: loss=4.9801, lr=0.000549, tokens/sec=1374523.55, grad_norm=0.5703, duration=0.38s
Step 770: loss=4.9501, lr=0.000550, tokens/sec=1380142.14, grad_norm=0.6254, duration=0.38s
Step 771: loss=4.9400, lr=0.000550, tokens/sec=1376217.32, grad_norm=0.6836, duration=0.38s
Step 772: loss=4.9822, lr=0.000551, tokens/sec=1377251.64, grad_norm=0.8224, duration=0.38s
Step 773: loss=4.9151, lr=0.000551, tokens/sec=1376180.29, grad_norm=0.8999, duration=0.38s
Step 774: loss=5.1019, lr=0.000552, tokens/sec=1376786.00, grad_norm=0.9181, duration=0.38s
Step 775: loss=4.9985, lr=0.000552, tokens/sec=1378084.52, grad_norm=0.7787, duration=0.38s
Step 776: loss=4.9825, lr=0.000553, tokens/sec=1377828.08, grad_norm=0.7765, duration=0.38s
Step 777: loss=4.9717, lr=0.000554, tokens/sec=1379998.37, grad_norm=0.7292, duration=0.38s
Step 778: loss=4.9826, lr=0.000554, tokens/sec=1379897.05, grad_norm=0.6951, duration=0.38s
Step 779: loss=4.8084, lr=0.000555, tokens/sec=1377658.89, grad_norm=0.8389, duration=0.38s
Step 780: loss=4.9259, lr=0.000555, tokens/sec=1374946.39, grad_norm=0.6697, duration=0.38s
Step 781: loss=4.9034, lr=0.000556, tokens/sec=1377879.88, grad_norm=0.8192, duration=0.38s
Step 782: loss=4.9595, lr=0.000556, tokens/sec=1377927.36, grad_norm=0.8010, duration=0.38s
Step 783: loss=4.9229, lr=0.000557, tokens/sec=1379111.27, grad_norm=0.6782, duration=0.38s
Step 784: loss=4.8921, lr=0.000557, tokens/sec=1377166.25, grad_norm=0.5955, duration=0.38s
Step 785: loss=4.8444, lr=0.000558, tokens/sec=1382704.78, grad_norm=0.5973, duration=0.38s
Step 786: loss=4.8341, lr=0.000558, tokens/sec=1372759.38, grad_norm=0.5888, duration=0.38s
Step 787: loss=4.8285, lr=0.000559, tokens/sec=1380236.56, grad_norm=0.6657, duration=0.38s
Step 788: loss=4.8869, lr=0.000559, tokens/sec=1376941.18, grad_norm=0.6599, duration=0.38s
Step 789: loss=4.8037, lr=0.000560, tokens/sec=1375508.86, grad_norm=0.6142, duration=0.38s
Step 790: loss=4.8936, lr=0.000560, tokens/sec=1375042.68, grad_norm=0.6762, duration=0.38s
Step 791: loss=4.8293, lr=0.000561, tokens/sec=1378020.62, grad_norm=0.6222, duration=0.38s
Step 792: loss=4.8332, lr=0.000561, tokens/sec=1373518.21, grad_norm=0.6356, duration=0.38s
Step 793: loss=4.8627, lr=0.000561, tokens/sec=1374618.06, grad_norm=0.5358, duration=0.38s
Step 794: loss=4.7924, lr=0.000562, tokens/sec=1380319.73, grad_norm=0.4236, duration=0.38s
Step 795: loss=4.7217, lr=0.000562, tokens/sec=1380942.97, grad_norm=0.4307, duration=0.38s
Step 796: loss=4.7802, lr=0.000563, tokens/sec=1374331.98, grad_norm=0.5118, duration=0.38s
Step 797: loss=4.7558, lr=0.000563, tokens/sec=1378518.20, grad_norm=0.6405, duration=0.38s
Step 798: loss=4.7389, lr=0.000564, tokens/sec=1377625.23, grad_norm=0.7932, duration=0.38s
Step 799: loss=4.7428, lr=0.000564, tokens/sec=1373359.52, grad_norm=0.8813, duration=0.38s
Step 800/19073 (4.2%), Elapsed time: 373.98s, Steps per hour: 7700.92, Estimated hours remaining: 2.37
Step 800: loss=4.7576, lr=0.000565, tokens/sec=1373876.91, grad_norm=0.7957, duration=0.38s
Step 801: loss=4.7840, lr=0.000565, tokens/sec=1380266.02, grad_norm=0.8108, duration=0.38s
Step 802: loss=4.8035, lr=0.000566, tokens/sec=1379497.99, grad_norm=0.7586, duration=0.38s
Step 803: loss=4.8340, lr=0.000566, tokens/sec=1371974.86, grad_norm=0.7603, duration=0.38s
Step 804: loss=4.8450, lr=0.000567, tokens/sec=1374084.66, grad_norm=0.8168, duration=0.38s
Step 805: loss=4.8157, lr=0.000567, tokens/sec=1377136.06, grad_norm=0.8016, duration=0.38s
Step 806: loss=4.7976, lr=0.000567, tokens/sec=1375599.20, grad_norm=0.6423, duration=0.38s
Step 807: loss=4.9298, lr=0.000568, tokens/sec=1378953.87, grad_norm=0.5586, duration=0.38s
Step 808: loss=4.9338, lr=0.000568, tokens/sec=1376526.59, grad_norm=0.5839, duration=0.38s
Step 809: loss=4.8721, lr=0.000569, tokens/sec=1379459.05, grad_norm=0.7039, duration=0.38s
Step 810: loss=4.9688, lr=0.000569, tokens/sec=1380007.89, grad_norm=0.8175, duration=0.38s
Step 811: loss=4.9095, lr=0.000570, tokens/sec=1376472.31, grad_norm=0.6428, duration=0.38s
Step 812: loss=4.9355, lr=0.000570, tokens/sec=1379775.83, grad_norm=0.6106, duration=0.38s
Step 813: loss=4.8894, lr=0.000570, tokens/sec=1379013.54, grad_norm=0.6697, duration=0.38s
Step 814: loss=4.8158, lr=0.000571, tokens/sec=1377385.35, grad_norm=0.5944, duration=0.38s
Step 815: loss=4.8637, lr=0.000571, tokens/sec=1367425.92, grad_norm=0.6410, duration=0.38s
Step 816: loss=4.8516, lr=0.000572, tokens/sec=1375284.33, grad_norm=0.6358, duration=0.38s
Step 817: loss=4.8712, lr=0.000572, tokens/sec=1376966.18, grad_norm=0.6777, duration=0.38s
Step 818: loss=4.8211, lr=0.000572, tokens/sec=1377980.90, grad_norm=0.5970, duration=0.38s
Step 819: loss=4.8070, lr=0.000573, tokens/sec=1375795.43, grad_norm=0.6120, duration=0.38s
Step 820: loss=4.8495, lr=0.000573, tokens/sec=1377755.56, grad_norm=0.5640, duration=0.38s
Step 821: loss=4.8147, lr=0.000574, tokens/sec=1377781.46, grad_norm=0.5625, duration=0.38s
Step 822: loss=4.7989, lr=0.000574, tokens/sec=1376291.40, grad_norm=0.5078, duration=0.38s
Step 823: loss=4.8321, lr=0.000574, tokens/sec=1380783.42, grad_norm=0.5297, duration=0.38s
Step 824: loss=4.8119, lr=0.000575, tokens/sec=1380331.86, grad_norm=0.6993, duration=0.38s
Step 825: loss=4.8488, lr=0.000575, tokens/sec=1377951.54, grad_norm=0.6884, duration=0.38s
Step 826: loss=4.7735, lr=0.000576, tokens/sec=1376789.45, grad_norm=0.5930, duration=0.38s
Step 827: loss=4.8893, lr=0.000576, tokens/sec=1378638.33, grad_norm=0.6275, duration=0.38s
Step 828: loss=4.8187, lr=0.000576, tokens/sec=1378507.83, grad_norm=0.6681, duration=0.38s
Step 829: loss=4.7563, lr=0.000577, tokens/sec=1376736.01, grad_norm=0.4946, duration=0.38s
Step 830: loss=4.7567, lr=0.000577, tokens/sec=1373075.67, grad_norm=0.4691, duration=0.38s
Step 831: loss=4.7621, lr=0.000578, tokens/sec=1379445.20, grad_norm=0.4183, duration=0.38s
Step 832: loss=4.7459, lr=0.000578, tokens/sec=1378085.39, grad_norm=0.4541, duration=0.38s
Step 833: loss=4.7756, lr=0.000578, tokens/sec=1378724.76, grad_norm=0.5072, duration=0.38s
Step 834: loss=4.6964, lr=0.000579, tokens/sec=1373757.61, grad_norm=0.5438, duration=0.38s
Step 835: loss=4.7265, lr=0.000579, tokens/sec=1380875.33, grad_norm=0.5774, duration=0.38s
Step 836: loss=4.7240, lr=0.000579, tokens/sec=1378992.79, grad_norm=0.6913, duration=0.38s
Step 837: loss=4.7486, lr=0.000580, tokens/sec=1377588.99, grad_norm=0.8204, duration=0.38s
Step 838: loss=4.7658, lr=0.000580, tokens/sec=1378844.06, grad_norm=0.7916, duration=0.38s
Step 839: loss=4.7058, lr=0.000580, tokens/sec=1380585.78, grad_norm=0.5905, duration=0.38s
Step 840: loss=4.7400, lr=0.000581, tokens/sec=1378597.70, grad_norm=0.6112, duration=0.38s
Step 841: loss=4.6690, lr=0.000581, tokens/sec=1379099.16, grad_norm=0.6821, duration=0.38s
Step 842: loss=4.7067, lr=0.000581, tokens/sec=1376152.73, grad_norm=0.7102, duration=0.38s
Step 843: loss=4.6761, lr=0.000582, tokens/sec=1377661.48, grad_norm=0.8392, duration=0.38s
Step 844: loss=4.6785, lr=0.000582, tokens/sec=1368779.25, grad_norm=0.7572, duration=0.38s
Step 845: loss=4.6351, lr=0.000582, tokens/sec=1374572.52, grad_norm=0.6601, duration=0.38s
Step 846: loss=4.6470, lr=0.000583, tokens/sec=1380182.85, grad_norm=0.6362, duration=0.38s
Step 847: loss=4.6824, lr=0.000583, tokens/sec=1373608.30, grad_norm=0.6841, duration=0.38s
Step 848: loss=4.6926, lr=0.000583, tokens/sec=1375246.49, grad_norm=0.5236, duration=0.38s
Step 849: loss=4.7061, lr=0.000584, tokens/sec=1378330.70, grad_norm=0.5672, duration=0.38s
Step 850: loss=4.7154, lr=0.000584, tokens/sec=1371635.98, grad_norm=0.7064, duration=0.38s
Step 851: loss=4.7326, lr=0.000584, tokens/sec=1374875.90, grad_norm=0.7553, duration=0.38s
Step 852: loss=4.7353, lr=0.000585, tokens/sec=1375388.41, grad_norm=0.7221, duration=0.38s
Step 853: loss=4.7700, lr=0.000585, tokens/sec=1372953.94, grad_norm=0.7381, duration=0.38s
Step 854: loss=4.8289, lr=0.000585, tokens/sec=1377969.67, grad_norm=0.7165, duration=0.38s
Step 855: loss=4.8452, lr=0.000585, tokens/sec=1376508.50, grad_norm=0.6022, duration=0.38s
Step 856: loss=4.8340, lr=0.000586, tokens/sec=1378972.03, grad_norm=0.6684, duration=0.38s
Step 857: loss=4.7875, lr=0.000586, tokens/sec=1380547.64, grad_norm=0.6417, duration=0.38s
Step 858: loss=4.8157, lr=0.000586, tokens/sec=1375847.93, grad_norm=0.7060, duration=0.38s
Step 859: loss=4.8203, lr=0.000587, tokens/sec=1380154.27, grad_norm=0.9634, duration=0.38s
Step 860: loss=4.8173, lr=0.000587, tokens/sec=1376380.12, grad_norm=0.9597, duration=0.38s
Step 861: loss=4.8029, lr=0.000587, tokens/sec=1381477.38, grad_norm=0.6991, duration=0.38s
Step 862: loss=4.7806, lr=0.000588, tokens/sec=1381244.83, grad_norm=0.5898, duration=0.38s
Step 863: loss=4.7374, lr=0.000588, tokens/sec=1377539.80, grad_norm=0.4687, duration=0.38s
Step 864: loss=4.7523, lr=0.000588, tokens/sec=1378692.78, grad_norm=0.4207, duration=0.38s
Step 865: loss=4.7273, lr=0.000588, tokens/sec=1375404.76, grad_norm=0.4484, duration=0.38s
Step 866: loss=4.7336, lr=0.000589, tokens/sec=1379055.05, grad_norm=0.5182, duration=0.38s
Step 867: loss=4.7104, lr=0.000589, tokens/sec=1379141.54, grad_norm=0.6278, duration=0.38s
Step 868: loss=4.7538, lr=0.000589, tokens/sec=1375000.55, grad_norm=0.6219, duration=0.38s
Step 869: loss=4.7095, lr=0.000589, tokens/sec=1378593.38, grad_norm=0.6406, duration=0.38s
Step 870: loss=4.7402, lr=0.000590, tokens/sec=1376562.78, grad_norm=0.7219, duration=0.38s
Step 871: loss=4.7321, lr=0.000590, tokens/sec=1374690.25, grad_norm=0.7066, duration=0.38s
Step 872: loss=4.7108, lr=0.000590, tokens/sec=1372272.80, grad_norm=0.6698, duration=0.38s
Step 873: loss=4.6976, lr=0.000590, tokens/sec=1377548.43, grad_norm=0.7047, duration=0.38s
Step 874: loss=4.7584, lr=0.000591, tokens/sec=1374724.62, grad_norm=0.6098, duration=0.38s
Step 875: loss=4.7322, lr=0.000591, tokens/sec=1375684.40, grad_norm=0.5902, duration=0.38s
Step 876: loss=4.7457, lr=0.000591, tokens/sec=1376133.78, grad_norm=0.5999, duration=0.38s
Step 877: loss=4.6572, lr=0.000591, tokens/sec=1382730.86, grad_norm=0.5602, duration=0.38s
Step 878: loss=4.6873, lr=0.000592, tokens/sec=1378491.41, grad_norm=0.5670, duration=0.38s
Step 879: loss=4.6594, lr=0.000592, tokens/sec=1378427.47, grad_norm=0.6500, duration=0.38s
Step 880: loss=4.6513, lr=0.000592, tokens/sec=1378471.53, grad_norm=0.5637, duration=0.38s
Step 881: loss=4.6202, lr=0.000592, tokens/sec=1380540.71, grad_norm=0.5695, duration=0.38s
Step 882: loss=4.6505, lr=0.000592, tokens/sec=1378842.33, grad_norm=0.5656, duration=0.38s
Step 883: loss=4.6589, lr=0.000593, tokens/sec=1379170.95, grad_norm=0.5691, duration=0.38s
Step 884: loss=4.6181, lr=0.000593, tokens/sec=1378075.89, grad_norm=0.5417, duration=0.38s
Step 885: loss=4.6331, lr=0.000593, tokens/sec=1380935.17, grad_norm=0.5492, duration=0.38s
Step 886: loss=4.6289, lr=0.000593, tokens/sec=1379633.00, grad_norm=0.5329, duration=0.38s
Step 887: loss=4.5966, lr=0.000593, tokens/sec=1380555.44, grad_norm=0.5351, duration=0.38s
Step 888: loss=4.6334, lr=0.000594, tokens/sec=1378683.27, grad_norm=0.4634, duration=0.38s
Step 889: loss=4.5628, lr=0.000594, tokens/sec=1379790.55, grad_norm=0.5187, duration=0.38s
Step 890: loss=4.5786, lr=0.000594, tokens/sec=1380824.18, grad_norm=0.4906, duration=0.38s
Step 891: loss=4.5361, lr=0.000594, tokens/sec=1378312.56, grad_norm=0.5334, duration=0.38s
Step 892: loss=4.5545, lr=0.000594, tokens/sec=1376862.72, grad_norm=0.5314, duration=0.38s
Step 893: loss=4.5266, lr=0.000595, tokens/sec=1380584.04, grad_norm=0.5140, duration=0.38s
Step 894: loss=4.5865, lr=0.000595, tokens/sec=1379920.43, grad_norm=0.6897, duration=0.38s
Step 895: loss=4.6483, lr=0.000595, tokens/sec=1378532.89, grad_norm=0.6856, duration=0.38s
Step 896: loss=4.6051, lr=0.000595, tokens/sec=1379234.96, grad_norm=0.5967, duration=0.38s
Step 897: loss=4.5944, lr=0.000595, tokens/sec=1377083.45, grad_norm=0.6458, duration=0.38s
Step 898: loss=4.5896, lr=0.000596, tokens/sec=1379116.46, grad_norm=0.6416, duration=0.38s
Step 899: loss=4.6409, lr=0.000596, tokens/sec=1378083.66, grad_norm=0.5591, duration=0.38s
Step 900/19073 (4.7%), Elapsed time: 412.12s, Steps per hour: 7861.83, Estimated hours remaining: 2.31
Step 900: loss=4.7365, lr=0.000596, tokens/sec=1374813.15, grad_norm=0.7091, duration=0.38s
Step 901: loss=4.7038, lr=0.000596, tokens/sec=1373089.39, grad_norm=0.8318, duration=0.38s
Step 902: loss=4.7688, lr=0.000596, tokens/sec=1374513.24, grad_norm=0.7378, duration=0.38s
Step 903: loss=4.7063, lr=0.000596, tokens/sec=1375983.10, grad_norm=0.6021, duration=0.38s
Step 904: loss=4.6987, lr=0.000596, tokens/sec=1376415.45, grad_norm=0.6300, duration=0.38s
Step 905: loss=4.6738, lr=0.000597, tokens/sec=1376002.90, grad_norm=0.6089, duration=0.38s
Step 906: loss=4.6668, lr=0.000597, tokens/sec=1376746.35, grad_norm=0.4642, duration=0.38s
Step 907: loss=4.7172, lr=0.000597, tokens/sec=1375841.91, grad_norm=0.5457, duration=0.38s
Step 908: loss=4.7770, lr=0.000597, tokens/sec=1378042.21, grad_norm=0.6261, duration=0.38s
Step 909: loss=4.7221, lr=0.000597, tokens/sec=1378956.47, grad_norm=0.6603, duration=0.38s
Step 910: loss=4.7020, lr=0.000597, tokens/sec=1375853.96, grad_norm=0.7036, duration=0.38s
Step 911: loss=4.6079, lr=0.000597, tokens/sec=1378627.95, grad_norm=0.6468, duration=0.38s
Step 912: loss=4.6471, lr=0.000598, tokens/sec=1374960.14, grad_norm=0.5908, duration=0.38s
Step 913: loss=4.6658, lr=0.000598, tokens/sec=1379102.62, grad_norm=0.5235, duration=0.38s
Step 914: loss=4.6160, lr=0.000598, tokens/sec=1377914.41, grad_norm=0.6078, duration=0.38s
Step 915: loss=4.6767, lr=0.000598, tokens/sec=1379019.59, grad_norm=0.7805, duration=0.38s
Step 916: loss=4.6650, lr=0.000598, tokens/sec=1375468.42, grad_norm=0.8437, duration=0.38s
Step 917: loss=4.7365, lr=0.000598, tokens/sec=1377966.22, grad_norm=0.7490, duration=0.38s
Step 918: loss=4.7516, lr=0.000598, tokens/sec=1377181.77, grad_norm=0.6745, duration=0.38s
Step 919: loss=4.6667, lr=0.000598, tokens/sec=1375942.63, grad_norm=0.6905, duration=0.38s
Step 920: loss=4.5922, lr=0.000598, tokens/sec=1377243.87, grad_norm=0.6303, duration=0.38s
Step 921: loss=4.6796, lr=0.000599, tokens/sec=1378415.37, grad_norm=0.7531, duration=0.38s
Step 922: loss=4.6411, lr=0.000599, tokens/sec=1376304.32, grad_norm=0.8646, duration=0.38s
Step 923: loss=4.6114, lr=0.000599, tokens/sec=1374093.25, grad_norm=0.8091, duration=0.38s
Step 924: loss=4.5573, lr=0.000599, tokens/sec=1376942.04, grad_norm=0.7289, duration=0.38s
Step 925: loss=4.6188, lr=0.000599, tokens/sec=1376371.51, grad_norm=0.7496, duration=0.38s
Step 926: loss=4.5705, lr=0.000599, tokens/sec=1376540.38, grad_norm=0.6731, duration=0.38s
Step 927: loss=4.6243, lr=0.000599, tokens/sec=1376045.95, grad_norm=0.5902, duration=0.38s
Step 928: loss=4.6425, lr=0.000599, tokens/sec=1376458.52, grad_norm=0.5203, duration=0.38s
Step 929: loss=4.5443, lr=0.000599, tokens/sec=1380488.71, grad_norm=0.5240, duration=0.38s
Step 930: loss=4.5423, lr=0.000599, tokens/sec=1375919.39, grad_norm=0.5224, duration=0.38s
Step 931: loss=4.5468, lr=0.000599, tokens/sec=1379079.27, grad_norm=0.4212, duration=0.38s
Step 932: loss=4.4735, lr=0.000599, tokens/sec=1380741.81, grad_norm=0.4386, duration=0.38s
Step 933: loss=4.5342, lr=0.000599, tokens/sec=1373755.03, grad_norm=0.4297, duration=0.38s
Step 934: loss=4.4445, lr=0.000600, tokens/sec=1379913.50, grad_norm=0.3875, duration=0.38s
Step 935: loss=4.4356, lr=0.000600, tokens/sec=1379578.47, grad_norm=0.3814, duration=0.38s
Step 936: loss=4.4568, lr=0.000600, tokens/sec=1379539.53, grad_norm=0.3969, duration=0.38s
Step 937: loss=4.4973, lr=0.000600, tokens/sec=1378665.12, grad_norm=0.4315, duration=0.38s
Step 938: loss=4.4476, lr=0.000600, tokens/sec=1381896.69, grad_norm=0.4680, duration=0.38s
Step 939: loss=4.4300, lr=0.000600, tokens/sec=1376858.41, grad_norm=0.4551, duration=0.38s
Step 940: loss=4.3731, lr=0.000600, tokens/sec=1378680.68, grad_norm=0.5331, duration=0.38s
Step 941: loss=4.4772, lr=0.000600, tokens/sec=1377171.42, grad_norm=0.5854, duration=0.38s
Step 942: loss=4.5087, lr=0.000600, tokens/sec=1377301.67, grad_norm=0.5713, duration=0.38s
Step 943: loss=4.5014, lr=0.000600, tokens/sec=1378193.35, grad_norm=0.6067, duration=0.38s
Step 944: loss=4.5421, lr=0.000600, tokens/sec=1374466.85, grad_norm=0.6502, duration=0.38s
Step 945: loss=4.5637, lr=0.000600, tokens/sec=1375842.77, grad_norm=0.6262, duration=0.38s
Step 946: loss=4.6092, lr=0.000600, tokens/sec=1378170.03, grad_norm=0.6181, duration=0.38s
Step 947: loss=4.6243, lr=0.000600, tokens/sec=1374296.77, grad_norm=0.5552, duration=0.38s
Step 948: loss=4.5820, lr=0.000600, tokens/sec=1376779.97, grad_norm=0.5789, duration=0.38s
Step 949: loss=4.5633, lr=0.000600, tokens/sec=1376630.00, grad_norm=0.5913, duration=0.38s
Step 950: loss=4.6253, lr=0.000600, tokens/sec=1380317.13, grad_norm=0.6405, duration=0.38s
Step 951: loss=4.6376, lr=0.000600, tokens/sec=1374814.87, grad_norm=0.5513, duration=0.38s
Step 952: loss=4.6332, lr=0.000600, tokens/sec=1378920.15, grad_norm=0.5531, duration=0.38s
Step 953: loss=4.6730, lr=0.000600, tokens/sec=1377937.72, grad_norm=0.5959, duration=0.38s
Step 954: loss=4.6032, lr=0.000600, tokens/sec=1380158.60, grad_norm=0.5801, duration=0.38s
Step 955: loss=4.6205, lr=0.000600, tokens/sec=1377449.19, grad_norm=0.5896, duration=0.38s
Step 956: loss=4.7049, lr=0.000600, tokens/sec=1378008.53, grad_norm=0.6068, duration=0.38s
Step 957: loss=4.6069, lr=0.000600, tokens/sec=1371006.58, grad_norm=0.6492, duration=0.38s
Step 958: loss=4.6724, lr=0.000600, tokens/sec=1376390.46, grad_norm=0.6543, duration=0.38s
Step 959: loss=4.5776, lr=0.000600, tokens/sec=1380505.17, grad_norm=0.6681, duration=0.38s
Step 960: loss=4.6299, lr=0.000600, tokens/sec=1377096.39, grad_norm=0.5518, duration=0.38s
Step 961: loss=4.5532, lr=0.000600, tokens/sec=1381335.93, grad_norm=0.5359, duration=0.38s
Step 962: loss=4.5795, lr=0.000600, tokens/sec=1377860.88, grad_norm=0.5787, duration=0.38s
Step 963: loss=4.6601, lr=0.000600, tokens/sec=1380585.78, grad_norm=0.7222, duration=0.38s
Step 964: loss=4.5784, lr=0.000600, tokens/sec=1379828.65, grad_norm=0.8691, duration=0.38s
Step 965: loss=4.6328, lr=0.000600, tokens/sec=1377697.73, grad_norm=0.7845, duration=0.38s
Step 966: loss=4.5575, lr=0.000600, tokens/sec=1377624.37, grad_norm=0.5640, duration=0.38s
Step 967: loss=4.6001, lr=0.000600, tokens/sec=1382312.78, grad_norm=0.5224, duration=0.38s
Step 968: loss=4.5585, lr=0.000600, tokens/sec=1381719.56, grad_norm=0.5877, duration=0.38s
Step 969: loss=4.4188, lr=0.000600, tokens/sec=1381925.35, grad_norm=0.8423, duration=0.38s
Step 970: loss=4.5275, lr=0.000600, tokens/sec=1376825.66, grad_norm=0.5784, duration=0.38s
Step 971: loss=4.5116, lr=0.000600, tokens/sec=1381261.31, grad_norm=0.5682, duration=0.38s
Step 972: loss=4.6023, lr=0.000600, tokens/sec=1374757.28, grad_norm=0.5563, duration=0.38s
Step 973: loss=4.4679, lr=0.000600, tokens/sec=1378379.08, grad_norm=0.5005, duration=0.38s
Step 974: loss=4.5030, lr=0.000600, tokens/sec=1378519.06, grad_norm=0.4453, duration=0.38s
Step 975: loss=4.4685, lr=0.000600, tokens/sec=1378075.89, grad_norm=0.4987, duration=0.38s
Step 976: loss=4.4584, lr=0.000600, tokens/sec=1381297.75, grad_norm=0.4782, duration=0.38s
Step 977: loss=4.4751, lr=0.000600, tokens/sec=1382175.50, grad_norm=0.5083, duration=0.38s
Step 978: loss=4.4654, lr=0.000600, tokens/sec=1377419.00, grad_norm=0.5255, duration=0.38s
Step 979: loss=4.4468, lr=0.000600, tokens/sec=1380170.72, grad_norm=0.5739, duration=0.38s
Step 980: loss=4.4867, lr=0.000600, tokens/sec=1381593.68, grad_norm=0.7597, duration=0.38s
Step 981: loss=4.4992, lr=0.000600, tokens/sec=1381539.00, grad_norm=0.7191, duration=0.38s
Step 982: loss=4.4605, lr=0.000600, tokens/sec=1380279.88, grad_norm=0.8604, duration=0.38s
Step 983: loss=4.5556, lr=0.000600, tokens/sec=1375419.38, grad_norm=0.5904, duration=0.38s
Step 984: loss=4.4269, lr=0.000600, tokens/sec=1378729.95, grad_norm=0.5417, duration=0.38s
Step 985: loss=4.3491, lr=0.000600, tokens/sec=1372887.94, grad_norm=0.5314, duration=0.38s
Step 986: loss=4.4636, lr=0.000600, tokens/sec=1375049.56, grad_norm=0.6014, duration=0.38s
Step 987: loss=4.3794, lr=0.000600, tokens/sec=1372259.10, grad_norm=0.7288, duration=0.38s
Step 988: loss=4.3392, lr=0.000600, tokens/sec=1380324.07, grad_norm=0.9301, duration=0.38s
Step 989: loss=4.4046, lr=0.000600, tokens/sec=1376455.08, grad_norm=0.6687, duration=0.38s
Step 990: loss=4.4147, lr=0.000600, tokens/sec=1376319.82, grad_norm=0.5936, duration=0.38s
Step 991: loss=4.4631, lr=0.000600, tokens/sec=1378324.65, grad_norm=0.5714, duration=0.38s
Step 992: loss=4.4246, lr=0.000600, tokens/sec=1376269.86, grad_norm=0.5762, duration=0.38s
Step 993: loss=4.4651, lr=0.000600, tokens/sec=1377659.75, grad_norm=0.5396, duration=0.38s
Step 994: loss=4.4286, lr=0.000600, tokens/sec=1378098.34, grad_norm=0.4839, duration=0.38s
Step 995: loss=4.4136, lr=0.000600, tokens/sec=1373469.31, grad_norm=0.6542, duration=0.38s
Step 996: loss=4.4377, lr=0.000600, tokens/sec=1370373.48, grad_norm=0.5213, duration=0.38s
Step 997: loss=4.5410, lr=0.000600, tokens/sec=1376936.01, grad_norm=0.4839, duration=0.38s
Step 998: loss=4.5590, lr=0.000600, tokens/sec=1374593.15, grad_norm=0.5479, duration=0.38s
Step 999: loss=4.5463, lr=0.000600, tokens/sec=1377867.79, grad_norm=0.5845, duration=0.38s
Step 1000/19073 (5.2%), Elapsed time: 450.25s, Steps per hour: 7995.48, Estimated hours remaining: 2.26
Validation loss at step 1000: 4.485050678253174
Step 1000: loss=4.5478, lr=0.000600, tokens/sec=155076.61, grad_norm=0.6680, duration=3.38s
Step 1001: loss=4.5476, lr=0.000600, tokens/sec=1377715.86, grad_norm=0.6114, duration=0.38s
Step 1002: loss=4.5552, lr=0.000600, tokens/sec=1379221.12, grad_norm=0.5508, duration=0.38s
Step 1003: loss=4.5034, lr=0.000600, tokens/sec=1378168.30, grad_norm=0.5891, duration=0.38s
Step 1004: loss=4.4505, lr=0.000600, tokens/sec=1377383.62, grad_norm=0.5479, duration=0.38s
Step 1005: loss=4.4727, lr=0.000600, tokens/sec=1378803.43, grad_norm=0.5599, duration=0.38s
Step 1006: loss=4.4986, lr=0.000600, tokens/sec=1380780.82, grad_norm=0.6420, duration=0.38s
Step 1007: loss=4.5156, lr=0.000600, tokens/sec=1377601.93, grad_norm=0.6680, duration=0.38s
Step 1008: loss=4.4493, lr=0.000600, tokens/sec=1379065.43, grad_norm=0.6537, duration=0.38s
Step 1009: loss=4.4606, lr=0.000600, tokens/sec=1376928.25, grad_norm=0.5324, duration=0.38s
Step 1010: loss=4.5088, lr=0.000600, tokens/sec=1379740.34, grad_norm=0.5024, duration=0.38s
Step 1011: loss=4.4247, lr=0.000600, tokens/sec=1379403.67, grad_norm=0.5056, duration=0.38s
Step 1012: loss=4.4404, lr=0.000600, tokens/sec=1377685.65, grad_norm=0.4871, duration=0.38s
Step 1013: loss=4.4611, lr=0.000600, tokens/sec=1378911.50, grad_norm=0.5447, duration=0.38s
Step 1014: loss=4.4306, lr=0.000600, tokens/sec=1378983.27, grad_norm=0.6334, duration=0.38s
Step 1015: loss=4.5047, lr=0.000600, tokens/sec=1375191.44, grad_norm=0.6193, duration=0.38s
Step 1016: loss=4.4235, lr=0.000600, tokens/sec=1373503.62, grad_norm=0.5094, duration=0.38s
Step 1017: loss=4.5081, lr=0.000600, tokens/sec=1371548.71, grad_norm=0.5424, duration=0.38s
Step 1018: loss=4.4642, lr=0.000600, tokens/sec=1372116.97, grad_norm=0.5536, duration=0.38s
Step 1019: loss=4.3803, lr=0.000600, tokens/sec=1368675.32, grad_norm=0.4546, duration=0.38s
Step 1020: loss=4.3934, lr=0.000600, tokens/sec=1376170.82, grad_norm=0.4463, duration=0.38s
Step 1021: loss=4.4154, lr=0.000600, tokens/sec=1378016.30, grad_norm=0.5230, duration=0.38s
Step 1022: loss=4.4106, lr=0.000600, tokens/sec=1379581.94, grad_norm=0.5083, duration=0.38s
Step 1023: loss=4.3751, lr=0.000600, tokens/sec=1377599.34, grad_norm=0.4851, duration=0.38s
Step 1024: loss=4.3331, lr=0.000600, tokens/sec=1369563.54, grad_norm=0.4148, duration=0.38s
Step 1025: loss=4.3762, lr=0.000600, tokens/sec=1376704.12, grad_norm=0.4107, duration=0.38s
Step 1026: loss=4.3673, lr=0.000600, tokens/sec=1374852.69, grad_norm=0.5348, duration=0.38s
Step 1027: loss=4.3495, lr=0.000600, tokens/sec=1373426.42, grad_norm=0.6111, duration=0.38s
Step 1028: loss=4.3447, lr=0.000600, tokens/sec=1375388.41, grad_norm=0.6218, duration=0.38s
Step 1029: loss=4.3643, lr=0.000600, tokens/sec=1371864.45, grad_norm=0.5409, duration=0.38s
Step 1030: loss=4.3521, lr=0.000600, tokens/sec=1375138.13, grad_norm=0.5146, duration=0.38s
Step 1031: loss=4.2810, lr=0.000600, tokens/sec=1378527.70, grad_norm=0.5925, duration=0.38s
Step 1032: loss=4.3608, lr=0.000600, tokens/sec=1379100.89, grad_norm=0.6482, duration=0.38s
Step 1033: loss=4.2853, lr=0.000600, tokens/sec=1373234.30, grad_norm=0.5317, duration=0.38s
Step 1034: loss=4.2710, lr=0.000600, tokens/sec=1376880.83, grad_norm=0.5372, duration=0.38s
Step 1035: loss=4.2425, lr=0.000600, tokens/sec=1373958.46, grad_norm=0.5984, duration=0.38s
Step 1036: loss=4.2779, lr=0.000600, tokens/sec=1374597.44, grad_norm=0.6138, duration=0.38s
Step 1037: loss=4.3598, lr=0.000600, tokens/sec=1375980.51, grad_norm=0.5717, duration=0.38s
Step 1038: loss=4.3338, lr=0.000600, tokens/sec=1377301.67, grad_norm=0.5657, duration=0.38s
Step 1039: loss=4.3448, lr=0.000600, tokens/sec=1378027.53, grad_norm=0.5387, duration=0.38s
Step 1040: loss=4.3128, lr=0.000600, tokens/sec=1377363.78, grad_norm=0.5295, duration=0.38s
Step 1041: loss=4.3591, lr=0.000600, tokens/sec=1376689.47, grad_norm=0.5520, duration=0.38s
Step 1042: loss=4.3267, lr=0.000600, tokens/sec=1375515.74, grad_norm=0.5882, duration=0.38s
Step 1043: loss=4.3847, lr=0.000600, tokens/sec=1377147.27, grad_norm=0.5806, duration=0.38s
Step 1044: loss=4.4632, lr=0.000600, tokens/sec=1377077.42, grad_norm=0.4953, duration=0.38s
Step 1045: loss=4.4630, lr=0.000600, tokens/sec=1378557.95, grad_norm=0.4656, duration=0.38s
Step 1046: loss=4.4268, lr=0.000600, tokens/sec=1380030.41, grad_norm=0.4392, duration=0.38s
Step 1047: loss=4.4414, lr=0.000600, tokens/sec=1377728.80, grad_norm=0.4546, duration=0.38s
Step 1048: loss=4.3849, lr=0.000600, tokens/sec=1379210.74, grad_norm=0.5191, duration=0.38s
Step 1049: loss=4.4275, lr=0.000600, tokens/sec=1380724.47, grad_norm=0.4999, duration=0.38s
Step 1050: loss=4.3836, lr=0.000600, tokens/sec=1377132.61, grad_norm=0.4531, duration=0.38s
Step 1051: loss=4.4187, lr=0.000600, tokens/sec=1375342.82, grad_norm=0.4846, duration=0.38s
Step 1052: loss=4.4070, lr=0.000600, tokens/sec=1379440.01, grad_norm=0.5166, duration=0.38s
Step 1053: loss=4.3404, lr=0.000600, tokens/sec=1380188.05, grad_norm=0.5229, duration=0.38s
Step 1054: loss=4.3950, lr=0.000600, tokens/sec=1379525.68, grad_norm=0.5170, duration=0.38s
Step 1055: loss=4.3626, lr=0.000600, tokens/sec=1380323.20, grad_norm=0.4781, duration=0.38s
Step 1056: loss=4.3845, lr=0.000600, tokens/sec=1380019.15, grad_norm=0.5059, duration=0.38s
Step 1057: loss=4.3424, lr=0.000600, tokens/sec=1378876.05, grad_norm=0.5992, duration=0.38s
Step 1058: loss=4.3880, lr=0.000600, tokens/sec=1380964.65, grad_norm=0.6445, duration=0.38s
Step 1059: loss=4.3411, lr=0.000600, tokens/sec=1374216.04, grad_norm=0.6115, duration=0.38s
Step 1060: loss=4.3618, lr=0.000600, tokens/sec=1375024.62, grad_norm=0.5402, duration=0.38s
Step 1061: loss=4.3427, lr=0.000600, tokens/sec=1375846.21, grad_norm=0.5963, duration=0.38s
Step 1062: loss=4.3645, lr=0.000600, tokens/sec=1374807.99, grad_norm=0.7200, duration=0.38s
Step 1063: loss=4.3345, lr=0.000600, tokens/sec=1374717.75, grad_norm=0.5849, duration=0.38s
Step 1064: loss=4.3941, lr=0.000600, tokens/sec=1377186.08, grad_norm=0.4701, duration=0.38s
Step 1065: loss=4.3732, lr=0.000600, tokens/sec=1376467.14, grad_norm=0.4062, duration=0.38s
Step 1066: loss=4.3345, lr=0.000600, tokens/sec=1377853.98, grad_norm=0.4078, duration=0.38s
Step 1067: loss=4.3226, lr=0.000600, tokens/sec=1379586.26, grad_norm=0.5349, duration=0.38s
Step 1068: loss=4.3219, lr=0.000600, tokens/sec=1377913.55, grad_norm=0.6592, duration=0.38s
Step 1069: loss=4.3223, lr=0.000600, tokens/sec=1380541.57, grad_norm=0.5800, duration=0.38s
Step 1070: loss=4.2843, lr=0.000600, tokens/sec=1375697.31, grad_norm=0.5299, duration=0.38s
Step 1071: loss=4.2851, lr=0.000600, tokens/sec=1379964.59, grad_norm=0.5749, duration=0.38s
Step 1072: loss=4.3263, lr=0.000600, tokens/sec=1380398.58, grad_norm=0.5631, duration=0.38s
Step 1073: loss=4.2630, lr=0.000600, tokens/sec=1378354.03, grad_norm=0.5379, duration=0.38s
Step 1074: loss=4.2797, lr=0.000600, tokens/sec=1373882.06, grad_norm=0.5671, duration=0.38s
Step 1075: loss=4.3104, lr=0.000600, tokens/sec=1372639.42, grad_norm=0.5968, duration=0.38s
Step 1076: loss=4.2830, lr=0.000600, tokens/sec=1379233.23, grad_norm=0.5292, duration=0.38s
Step 1077: loss=4.3305, lr=0.000600, tokens/sec=1377330.13, grad_norm=0.4688, duration=0.38s
Step 1078: loss=4.2807, lr=0.000600, tokens/sec=1379364.73, grad_norm=0.5012, duration=0.38s
Step 1079: loss=4.1900, lr=0.000600, tokens/sec=1377974.85, grad_norm=0.4505, duration=0.38s
Step 1080: loss=4.2538, lr=0.000600, tokens/sec=1380059.86, grad_norm=0.4666, duration=0.38s
Step 1081: loss=4.1653, lr=0.000600, tokens/sec=1376693.78, grad_norm=0.4345, duration=0.38s
Step 1082: loss=4.2263, lr=0.000600, tokens/sec=1380212.31, grad_norm=0.4711, duration=0.38s
Step 1083: loss=4.2016, lr=0.000600, tokens/sec=1375819.53, grad_norm=0.5520, duration=0.38s
Step 1084: loss=4.3017, lr=0.000600, tokens/sec=1378944.36, grad_norm=0.6154, duration=0.38s
Step 1085: loss=4.3006, lr=0.000600, tokens/sec=1378176.07, grad_norm=0.5876, duration=0.38s
Step 1086: loss=4.2479, lr=0.000600, tokens/sec=1379311.09, grad_norm=0.4703, duration=0.38s
Step 1087: loss=4.2023, lr=0.000600, tokens/sec=1377343.07, grad_norm=0.4413, duration=0.38s
Step 1088: loss=4.2523, lr=0.000600, tokens/sec=1374742.67, grad_norm=0.4396, duration=0.38s
Step 1089: loss=4.2736, lr=0.000600, tokens/sec=1377278.38, grad_norm=0.4889, duration=0.38s
Step 1090: loss=4.3548, lr=0.000600, tokens/sec=1377147.27, grad_norm=0.6118, duration=0.38s
Step 1091: loss=4.3697, lr=0.000600, tokens/sec=1376338.77, grad_norm=0.6016, duration=0.38s
Step 1092: loss=4.3795, lr=0.000600, tokens/sec=1379794.02, grad_norm=0.4406, duration=0.38s
Step 1093: loss=4.3327, lr=0.000600, tokens/sec=1375942.63, grad_norm=0.5397, duration=0.38s
Step 1094: loss=4.3670, lr=0.000600, tokens/sec=1379773.24, grad_norm=0.6737, duration=0.38s
Step 1095: loss=4.3294, lr=0.000600, tokens/sec=1372048.48, grad_norm=0.6854, duration=0.38s
Step 1096: loss=4.3387, lr=0.000600, tokens/sec=1377895.42, grad_norm=0.6081, duration=0.38s
Step 1097: loss=4.3935, lr=0.000600, tokens/sec=1379653.78, grad_norm=0.5177, duration=0.38s
Step 1098: loss=4.4666, lr=0.000600, tokens/sec=1376306.04, grad_norm=0.5133, duration=0.38s
Step 1099: loss=4.3714, lr=0.000600, tokens/sec=1377031.71, grad_norm=0.5255, duration=0.38s
Step 1100/19073 (5.8%), Elapsed time: 491.40s, Steps per hour: 8058.63, Estimated hours remaining: 2.23
Step 1100: loss=4.3361, lr=0.000600, tokens/sec=1378419.69, grad_norm=0.4419, duration=0.38s
Step 1101: loss=4.2623, lr=0.000600, tokens/sec=1380012.22, grad_norm=0.3927, duration=0.38s
Step 1102: loss=4.3237, lr=0.000600, tokens/sec=1374070.06, grad_norm=0.4494, duration=0.38s
Step 1103: loss=4.3119, lr=0.000600, tokens/sec=1377932.54, grad_norm=0.5688, duration=0.38s
Step 1104: loss=4.2982, lr=0.000600, tokens/sec=1378105.25, grad_norm=0.5728, duration=0.38s
Step 1105: loss=4.3564, lr=0.000600, tokens/sec=1380522.51, grad_norm=0.4647, duration=0.38s
Step 1106: loss=4.2803, lr=0.000600, tokens/sec=1380199.31, grad_norm=0.4507, duration=0.38s
Step 1107: loss=4.3444, lr=0.000600, tokens/sec=1374096.68, grad_norm=0.5179, duration=0.38s
Step 1108: loss=4.3996, lr=0.000600, tokens/sec=1379247.94, grad_norm=0.5182, duration=0.38s
Step 1109: loss=4.3122, lr=0.000600, tokens/sec=1377474.22, grad_norm=0.4131, duration=0.38s
Step 1110: loss=4.2129, lr=0.000600, tokens/sec=1379573.28, grad_norm=0.4008, duration=0.38s
Step 1111: loss=4.2924, lr=0.000600, tokens/sec=1378023.21, grad_norm=0.4194, duration=0.38s
Step 1112: loss=4.2743, lr=0.000600, tokens/sec=1377428.49, grad_norm=0.3991, duration=0.38s
Step 1113: loss=4.2308, lr=0.000600, tokens/sec=1377582.08, grad_norm=0.4305, duration=0.38s
Step 1114: loss=4.1687, lr=0.000600, tokens/sec=1381368.03, grad_norm=0.5319, duration=0.38s
Step 1115: loss=4.2363, lr=0.000600, tokens/sec=1378966.84, grad_norm=0.5140, duration=0.38s
Step 1116: loss=4.2344, lr=0.000600, tokens/sec=1376562.78, grad_norm=0.5057, duration=0.38s
Step 1117: loss=4.2758, lr=0.000600, tokens/sec=1380773.89, grad_norm=0.5839, duration=0.38s
Step 1118: loss=4.2777, lr=0.000600, tokens/sec=1377350.84, grad_norm=0.5424, duration=0.38s
Step 1119: loss=4.1893, lr=0.000600, tokens/sec=1378193.35, grad_norm=0.4417, duration=0.38s
Step 1120: loss=4.1994, lr=0.000600, tokens/sec=1375424.54, grad_norm=0.3796, duration=0.38s
Step 1121: loss=4.1919, lr=0.000600, tokens/sec=1376623.97, grad_norm=0.4223, duration=0.38s
Step 1122: loss=4.1537, lr=0.000600, tokens/sec=1383947.42, grad_norm=0.5116, duration=0.38s
Step 1123: loss=4.1982, lr=0.000600, tokens/sec=1375606.09, grad_norm=0.6954, duration=0.38s
Step 1124: loss=4.1566, lr=0.000600, tokens/sec=1368079.27, grad_norm=0.7479, duration=0.38s
Step 1125: loss=4.1556, lr=0.000600, tokens/sec=1376689.47, grad_norm=0.5941, duration=0.38s
Step 1126: loss=4.1509, lr=0.000600, tokens/sec=1376101.92, grad_norm=0.6350, duration=0.38s
Step 1127: loss=4.2263, lr=0.000600, tokens/sec=1378245.18, grad_norm=0.5727, duration=0.38s
Step 1128: loss=4.1534, lr=0.000600, tokens/sec=1377295.63, grad_norm=0.4599, duration=0.38s
Step 1129: loss=4.1133, lr=0.000600, tokens/sec=1376331.02, grad_norm=0.4758, duration=0.38s
Step 1130: loss=4.1038, lr=0.000600, tokens/sec=1380164.66, grad_norm=0.4623, duration=0.38s
Step 1131: loss=4.1920, lr=0.000600, tokens/sec=1377116.22, grad_norm=0.4929, duration=0.38s
Step 1132: loss=4.1990, lr=0.000600, tokens/sec=1375113.19, grad_norm=0.4798, duration=0.38s
Step 1133: loss=4.2002, lr=0.000600, tokens/sec=1378005.94, grad_norm=0.4712, duration=0.38s
Step 1134: loss=4.1882, lr=0.000600, tokens/sec=1373549.09, grad_norm=0.4677, duration=0.38s
Step 1135: loss=4.2731, lr=0.000600, tokens/sec=1375705.05, grad_norm=0.5219, duration=0.38s
Step 1136: loss=4.2736, lr=0.000600, tokens/sec=1380582.31, grad_norm=0.5168, duration=0.38s
Step 1137: loss=4.3124, lr=0.000600, tokens/sec=1377453.51, grad_norm=0.5375, duration=0.38s
Step 1138: loss=4.2052, lr=0.000600, tokens/sec=1376861.00, grad_norm=0.4676, duration=0.38s
Step 1139: loss=4.2904, lr=0.000600, tokens/sec=1379173.54, grad_norm=0.4393, duration=0.38s
Step 1140: loss=4.2615, lr=0.000600, tokens/sec=1373522.50, grad_norm=0.4207, duration=0.38s
Step 1141: loss=4.3139, lr=0.000600, tokens/sec=1376333.61, grad_norm=0.4683, duration=0.38s
Step 1142: loss=4.3271, lr=0.000600, tokens/sec=1375667.19, grad_norm=0.5174, duration=0.38s
Step 1143: loss=4.3379, lr=0.000600, tokens/sec=1378535.48, grad_norm=0.4503, duration=0.38s
Step 1144: loss=4.2812, lr=0.000600, tokens/sec=1373705.26, grad_norm=0.4264, duration=0.38s
Step 1145: loss=4.3236, lr=0.000600, tokens/sec=1382093.84, grad_norm=0.4472, duration=0.38s
Step 1146: loss=4.3509, lr=0.000600, tokens/sec=1378029.25, grad_norm=0.5174, duration=0.38s
Step 1147: loss=4.3264, lr=0.000600, tokens/sec=1374058.04, grad_norm=0.5498, duration=0.38s
Step 1148: loss=4.3194, lr=0.000600, tokens/sec=1375828.14, grad_norm=0.6570, duration=0.38s
Step 1149: loss=4.3034, lr=0.000600, tokens/sec=1377027.40, grad_norm=0.5987, duration=0.38s
Step 1150: loss=4.2996, lr=0.000600, tokens/sec=1377723.63, grad_norm=0.4793, duration=0.38s
Step 1151: loss=4.2347, lr=0.000600, tokens/sec=1372665.12, grad_norm=0.4983, duration=0.38s
Step 1152: loss=4.3389, lr=0.000600, tokens/sec=1376581.74, grad_norm=0.4656, duration=0.38s
Step 1153: loss=4.2066, lr=0.000600, tokens/sec=1376621.38, grad_norm=0.4622, duration=0.38s
Step 1154: loss=4.2596, lr=0.000600, tokens/sec=1378578.69, grad_norm=0.4820, duration=0.38s
Step 1155: loss=4.2484, lr=0.000600, tokens/sec=1378778.36, grad_norm=0.4294, duration=0.38s
Step 1156: loss=4.2342, lr=0.000600, tokens/sec=1373320.92, grad_norm=0.4363, duration=0.38s
Step 1157: loss=4.2516, lr=0.000600, tokens/sec=1377181.77, grad_norm=0.5325, duration=0.38s
Step 1158: loss=4.2247, lr=0.000600, tokens/sec=1378030.98, grad_norm=0.5988, duration=0.38s
Step 1159: loss=4.1212, lr=0.000600, tokens/sec=1371537.59, grad_norm=1.0319, duration=0.38s
Step 1160: loss=4.2248, lr=0.000600, tokens/sec=1371538.45, grad_norm=0.5959, duration=0.38s
Step 1161: loss=4.2468, lr=0.000600, tokens/sec=1374414.45, grad_norm=0.7315, duration=0.38s
Step 1162: loss=4.2853, lr=0.000600, tokens/sec=1379712.64, grad_norm=0.5597, duration=0.38s
Step 1163: loss=4.1463, lr=0.000600, tokens/sec=1374141.33, grad_norm=0.4878, duration=0.38s
Step 1164: loss=4.2273, lr=0.000600, tokens/sec=1375400.45, grad_norm=0.4794, duration=0.38s
Step 1165: loss=4.1983, lr=0.000600, tokens/sec=1376883.41, grad_norm=0.4949, duration=0.38s
Step 1166: loss=4.1932, lr=0.000600, tokens/sec=1372741.39, grad_norm=0.4660, duration=0.38s
Step 1167: loss=4.1538, lr=0.000600, tokens/sec=1378513.88, grad_norm=0.4261, duration=0.38s
Step 1168: loss=4.1972, lr=0.000600, tokens/sec=1375114.91, grad_norm=0.4074, duration=0.38s
Step 1169: loss=4.1260, lr=0.000600, tokens/sec=1375961.57, grad_norm=0.4036, duration=0.38s
Step 1170: loss=4.1742, lr=0.000600, tokens/sec=1375505.41, grad_norm=0.3994, duration=0.38s
Step 1171: loss=4.1298, lr=0.000600, tokens/sec=1376767.90, grad_norm=0.3686, duration=0.38s
Step 1172: loss=4.2013, lr=0.000600, tokens/sec=1378881.24, grad_norm=0.6473, duration=0.38s
Step 1173: loss=4.2344, lr=0.000600, tokens/sec=1379638.20, grad_norm=0.5441, duration=0.38s
Step 1174: loss=4.1100, lr=0.000600, tokens/sec=1380512.97, grad_norm=0.5019, duration=0.38s
Step 1175: loss=4.1058, lr=0.000600, tokens/sec=1377487.16, grad_norm=0.5568, duration=0.38s
Step 1176: loss=4.1444, lr=0.000600, tokens/sec=1377806.49, grad_norm=0.4908, duration=0.38s
Step 1177: loss=4.0274, lr=0.000600, tokens/sec=1379029.11, grad_norm=0.6924, duration=0.38s
Step 1178: loss=4.0766, lr=0.000600, tokens/sec=1380912.62, grad_norm=0.5916, duration=0.38s
Step 1179: loss=4.1309, lr=0.000600, tokens/sec=1372126.38, grad_norm=0.5976, duration=0.38s
Step 1180: loss=4.1754, lr=0.000600, tokens/sec=1375203.48, grad_norm=0.5415, duration=0.38s
Step 1181: loss=4.1741, lr=0.000600, tokens/sec=1376611.90, grad_norm=0.5669, duration=0.38s
Step 1182: loss=4.1551, lr=0.000600, tokens/sec=1376318.10, grad_norm=0.5230, duration=0.38s
Step 1183: loss=4.1647, lr=0.000600, tokens/sec=1377593.30, grad_norm=0.5516, duration=0.38s
Step 1184: loss=4.1447, lr=0.000600, tokens/sec=1376524.87, grad_norm=0.6097, duration=0.38s
Step 1185: loss=4.1679, lr=0.000600, tokens/sec=1379904.84, grad_norm=0.6503, duration=0.38s
Step 1186: loss=4.1558, lr=0.000600, tokens/sec=1379443.47, grad_norm=0.4678, duration=0.38s
Step 1187: loss=4.2552, lr=0.000600, tokens/sec=1374860.42, grad_norm=0.4617, duration=0.38s
Step 1188: loss=4.3158, lr=0.000600, tokens/sec=1374560.50, grad_norm=0.4355, duration=0.38s
Step 1189: loss=4.2431, lr=0.000600, tokens/sec=1378162.25, grad_norm=0.4532, duration=0.38s
Step 1190: loss=4.2498, lr=0.000600, tokens/sec=1376290.54, grad_norm=0.4142, duration=0.38s
Step 1191: loss=4.2535, lr=0.000600, tokens/sec=1377266.30, grad_norm=0.3747, duration=0.38s
Step 1192: loss=4.2423, lr=0.000600, tokens/sec=1377175.73, grad_norm=0.3876, duration=0.38s
Step 1193: loss=4.2228, lr=0.000600, tokens/sec=1378049.98, grad_norm=0.4011, duration=0.38s
Step 1194: loss=4.1688, lr=0.000600, tokens/sec=1380072.85, grad_norm=0.4266, duration=0.38s
Step 1195: loss=4.1976, lr=0.000600, tokens/sec=1375166.50, grad_norm=0.4031, duration=0.38s
Step 1196: loss=4.2220, lr=0.000600, tokens/sec=1378546.71, grad_norm=0.4154, duration=0.38s
Step 1197: loss=4.1894, lr=0.000600, tokens/sec=1377664.93, grad_norm=0.4292, duration=0.38s
Step 1198: loss=4.1783, lr=0.000600, tokens/sec=1373700.11, grad_norm=0.4207, duration=0.38s
Step 1199: loss=4.2015, lr=0.000600, tokens/sec=1377191.26, grad_norm=0.3832, duration=0.38s
Step 1200/19073 (6.3%), Elapsed time: 529.55s, Steps per hour: 8157.81, Estimated hours remaining: 2.19
Step 1200: loss=4.1845, lr=0.000600, tokens/sec=1373445.29, grad_norm=0.4398, duration=0.38s
Step 1201: loss=4.1488, lr=0.000600, tokens/sec=1376146.70, grad_norm=0.4777, duration=0.38s
Step 1202: loss=4.1523, lr=0.000600, tokens/sec=1377859.16, grad_norm=0.4513, duration=0.38s
Step 1203: loss=4.1637, lr=0.000600, tokens/sec=1376918.76, grad_norm=0.4554, duration=0.38s
Step 1204: loss=4.1599, lr=0.000600, tokens/sec=1377267.16, grad_norm=0.4243, duration=0.38s
Step 1205: loss=4.2292, lr=0.000600, tokens/sec=1373175.13, grad_norm=0.4277, duration=0.38s
Step 1206: loss=4.1446, lr=0.000600, tokens/sec=1378499.19, grad_norm=0.4368, duration=0.38s
Step 1207: loss=4.2496, lr=0.000600, tokens/sec=1373776.49, grad_norm=0.3948, duration=0.38s
Step 1208: loss=4.1684, lr=0.000600, tokens/sec=1370516.11, grad_norm=0.3836, duration=0.38s
Step 1209: loss=4.1256, lr=0.000600, tokens/sec=1373571.40, grad_norm=0.4318, duration=0.38s
Step 1210: loss=4.1515, lr=0.000600, tokens/sec=1376680.85, grad_norm=0.4462, duration=0.38s
Step 1211: loss=4.1763, lr=0.000600, tokens/sec=1376306.90, grad_norm=0.4251, duration=0.38s
Step 1212: loss=4.1024, lr=0.000600, tokens/sec=1376142.40, grad_norm=0.3870, duration=0.38s
Step 1213: loss=4.1139, lr=0.000600, tokens/sec=1376491.26, grad_norm=0.4013, duration=0.38s
Step 1214: loss=4.1042, lr=0.000600, tokens/sec=1374650.72, grad_norm=0.3831, duration=0.38s
Step 1215: loss=4.1379, lr=0.000600, tokens/sec=1375688.70, grad_norm=0.4052, duration=0.38s
Step 1216: loss=4.1128, lr=0.000600, tokens/sec=1377319.78, grad_norm=0.4439, duration=0.38s
Step 1217: loss=4.0630, lr=0.000600, tokens/sec=1379021.32, grad_norm=0.4831, duration=0.38s
Step 1218: loss=4.1297, lr=0.000600, tokens/sec=1374498.64, grad_norm=0.5326, duration=0.38s
Step 1219: loss=4.1236, lr=0.000600, tokens/sec=1371993.69, grad_norm=0.5904, duration=0.38s
Step 1220: loss=4.1243, lr=0.000600, tokens/sec=1374706.58, grad_norm=0.5342, duration=0.38s
Step 1221: loss=4.0588, lr=0.000600, tokens/sec=1376597.25, grad_norm=0.4837, duration=0.38s
Step 1222: loss=4.1057, lr=0.000600, tokens/sec=1377463.00, grad_norm=0.4683, duration=0.38s
Step 1223: loss=4.0060, lr=0.000600, tokens/sec=1379231.50, grad_norm=0.4124, duration=0.38s
Step 1224: loss=4.0557, lr=0.000600, tokens/sec=1379285.13, grad_norm=0.4628, duration=0.38s
Step 1225: loss=3.9821, lr=0.000600, tokens/sec=1376157.90, grad_norm=0.4485, duration=0.38s
Step 1226: loss=4.0842, lr=0.000600, tokens/sec=1375425.40, grad_norm=0.4945, duration=0.38s
Step 1227: loss=4.1208, lr=0.000600, tokens/sec=1375407.34, grad_norm=0.4684, duration=0.38s
Step 1228: loss=4.0727, lr=0.000600, tokens/sec=1374242.66, grad_norm=0.4240, duration=0.38s
Step 1229: loss=4.0736, lr=0.000600, tokens/sec=1376536.93, grad_norm=0.3558, duration=0.38s
Step 1230: loss=4.0534, lr=0.000600, tokens/sec=1376836.86, grad_norm=0.3998, duration=0.38s
Step 1231: loss=4.0861, lr=0.000600, tokens/sec=1381616.25, grad_norm=0.3796, duration=0.38s
Step 1232: loss=4.0545, lr=0.000600, tokens/sec=1373026.81, grad_norm=0.3672, duration=0.38s
Step 1233: loss=4.1537, lr=0.000600, tokens/sec=1379119.05, grad_norm=0.4073, duration=0.38s
Step 1234: loss=4.2080, lr=0.000600, tokens/sec=1375450.35, grad_norm=0.4628, duration=0.38s
Step 1235: loss=4.2090, lr=0.000600, tokens/sec=1372573.45, grad_norm=0.6298, duration=0.38s
Step 1236: loss=4.2581, lr=0.000600, tokens/sec=1351615.66, grad_norm=0.7564, duration=0.39s
Step 1237: loss=4.2030, lr=0.000600, tokens/sec=1379206.41, grad_norm=0.7126, duration=0.38s
Step 1238: loss=4.1852, lr=0.000600, tokens/sec=1372886.22, grad_norm=0.6208, duration=0.38s
Step 1239: loss=4.2035, lr=0.000600, tokens/sec=1381423.57, grad_norm=0.5606, duration=0.38s
Step 1240: loss=4.1760, lr=0.000600, tokens/sec=1378310.83, grad_norm=0.5271, duration=0.38s
Step 1241: loss=4.2178, lr=0.000600, tokens/sec=1372602.58, grad_norm=0.5257, duration=0.38s
Step 1242: loss=4.1503, lr=0.000600, tokens/sec=1376130.34, grad_norm=0.5000, duration=0.38s
Step 1243: loss=4.1463, lr=0.000600, tokens/sec=1371478.57, grad_norm=0.4941, duration=0.38s
Step 1244: loss=4.1728, lr=0.000600, tokens/sec=1378628.82, grad_norm=0.4485, duration=0.38s
Step 1245: loss=4.1516, lr=0.000600, tokens/sec=1373993.65, grad_norm=0.4399, duration=0.38s
Step 1246: loss=4.1534, lr=0.000600, tokens/sec=1369630.07, grad_norm=0.4643, duration=0.38s
Step 1247: loss=4.1040, lr=0.000600, tokens/sec=1377060.17, grad_norm=0.4257, duration=0.38s
Step 1248: loss=4.1515, lr=0.000600, tokens/sec=1380537.24, grad_norm=0.3567, duration=0.38s
Step 1249: loss=4.0882, lr=0.000600, tokens/sec=1381703.93, grad_norm=0.3426, duration=0.38s
Validation loss at step 1250: 4.140784740447998
Step 1250: loss=4.1123, lr=0.000600, tokens/sec=154340.30, grad_norm=0.3412, duration=3.40s
Step 1251: loss=4.1117, lr=0.000600, tokens/sec=1379528.28, grad_norm=0.3571, duration=0.38s
Step 1252: loss=4.1069, lr=0.000600, tokens/sec=1378607.21, grad_norm=0.3364, duration=0.38s
Step 1253: loss=4.0684, lr=0.000600, tokens/sec=1382461.38, grad_norm=0.3221, duration=0.38s
Step 1254: loss=4.1702, lr=0.000600, tokens/sec=1379061.97, grad_norm=0.3522, duration=0.38s
Step 1255: loss=4.0871, lr=0.000600, tokens/sec=1374883.63, grad_norm=0.4074, duration=0.38s
Step 1256: loss=4.1189, lr=0.000600, tokens/sec=1374717.75, grad_norm=0.3852, duration=0.38s
Step 1257: loss=4.0794, lr=0.000600, tokens/sec=1374008.25, grad_norm=0.4770, duration=0.38s
Step 1258: loss=4.1083, lr=0.000600, tokens/sec=1376711.01, grad_norm=0.5326, duration=0.38s
Step 1259: loss=4.0676, lr=0.000600, tokens/sec=1377548.43, grad_norm=0.4608, duration=0.38s
Step 1260: loss=4.0771, lr=0.000600, tokens/sec=1377483.71, grad_norm=0.4372, duration=0.38s
Step 1261: loss=4.0785, lr=0.000600, tokens/sec=1377466.45, grad_norm=0.4573, duration=0.38s
Step 1262: loss=4.0590, lr=0.000600, tokens/sec=1376805.83, grad_norm=0.4335, duration=0.38s
Step 1263: loss=4.0342, lr=0.000600, tokens/sec=1377142.96, grad_norm=0.3580, duration=0.38s
Step 1264: loss=4.0685, lr=0.000600, tokens/sec=1373061.96, grad_norm=0.4195, duration=0.38s
Step 1265: loss=4.0750, lr=0.000600, tokens/sec=1373226.58, grad_norm=0.4445, duration=0.38s
Step 1266: loss=4.1322, lr=0.000600, tokens/sec=1374362.05, grad_norm=0.3988, duration=0.38s
Step 1267: loss=4.0762, lr=0.000600, tokens/sec=1372555.46, grad_norm=0.4191, duration=0.38s
Step 1268: loss=4.0297, lr=0.000600, tokens/sec=1374181.69, grad_norm=0.4462, duration=0.38s
Step 1269: loss=3.9888, lr=0.000600, tokens/sec=1375259.39, grad_norm=0.4634, duration=0.38s
Step 1270: loss=4.0166, lr=0.000600, tokens/sec=1376102.78, grad_norm=0.4198, duration=0.38s
Step 1271: loss=3.9670, lr=0.000600, tokens/sec=1378295.28, grad_norm=0.4365, duration=0.38s
Step 1272: loss=4.0199, lr=0.000600, tokens/sec=1375049.56, grad_norm=0.4992, duration=0.38s
Step 1273: loss=4.0274, lr=0.000600, tokens/sec=1375248.21, grad_norm=0.5522, duration=0.38s
Step 1274: loss=4.1092, lr=0.000600, tokens/sec=1370931.36, grad_norm=0.5235, duration=0.38s
Step 1275: loss=4.0722, lr=0.000600, tokens/sec=1369705.14, grad_norm=0.4647, duration=0.38s
Step 1276: loss=4.0030, lr=0.000600, tokens/sec=1374850.11, grad_norm=0.4500, duration=0.38s
Step 1277: loss=4.0233, lr=0.000600, tokens/sec=1375919.39, grad_norm=0.4376, duration=0.38s
Step 1278: loss=4.0244, lr=0.000600, tokens/sec=1376029.59, grad_norm=0.4828, duration=0.38s
Step 1279: loss=4.0644, lr=0.000600, tokens/sec=1375140.71, grad_norm=0.5377, duration=0.38s
Step 1280: loss=4.1648, lr=0.000600, tokens/sec=1376097.62, grad_norm=0.4692, duration=0.38s
Step 1281: loss=4.1350, lr=0.000600, tokens/sec=1376203.54, grad_norm=0.4145, duration=0.38s
Step 1282: loss=4.1550, lr=0.000600, tokens/sec=1377759.02, grad_norm=0.4918, duration=0.38s
Step 1283: loss=4.1274, lr=0.000600, tokens/sec=1378598.57, grad_norm=0.5519, duration=0.38s
Step 1284: loss=4.1419, lr=0.000600, tokens/sec=1377992.12, grad_norm=0.5308, duration=0.38s
Step 1285: loss=4.0971, lr=0.000600, tokens/sec=1377813.40, grad_norm=0.4841, duration=0.38s
Step 1286: loss=4.1254, lr=0.000600, tokens/sec=1375789.40, grad_norm=0.4978, duration=0.38s
Step 1287: loss=4.1883, lr=0.000600, tokens/sec=1377952.40, grad_norm=0.4017, duration=0.38s
Step 1288: loss=4.2246, lr=0.000600, tokens/sec=1380813.77, grad_norm=0.4335, duration=0.38s
Step 1289: loss=4.1322, lr=0.000600, tokens/sec=1376597.25, grad_norm=0.4029, duration=0.38s
Step 1290: loss=4.1194, lr=0.000600, tokens/sec=1376765.32, grad_norm=0.3363, duration=0.38s
Step 1291: loss=4.0682, lr=0.000600, tokens/sec=1380026.08, grad_norm=0.2957, duration=0.38s
Step 1292: loss=4.1015, lr=0.000600, tokens/sec=1374939.51, grad_norm=0.3040, duration=0.38s
Step 1293: loss=4.0830, lr=0.000600, tokens/sec=1376762.73, grad_norm=0.3024, duration=0.38s
Step 1294: loss=4.0762, lr=0.000600, tokens/sec=1378995.38, grad_norm=0.2875, duration=0.38s
Step 1295: loss=4.1239, lr=0.000600, tokens/sec=1380711.47, grad_norm=0.3011, duration=0.38s
Step 1296: loss=4.0404, lr=0.000600, tokens/sec=1380049.46, grad_norm=0.3075, duration=0.38s
Step 1297: loss=4.1192, lr=0.000600, tokens/sec=1378236.54, grad_norm=0.3570, duration=0.38s
Step 1298: loss=4.1747, lr=0.000600, tokens/sec=1378136.34, grad_norm=0.3697, duration=0.38s
Step 1299: loss=4.0874, lr=0.000600, tokens/sec=1379162.30, grad_norm=0.4475, duration=0.38s
Step 1300/19073 (6.8%), Elapsed time: 570.75s, Steps per hour: 8199.78, Estimated hours remaining: 2.17
Step 1300: loss=4.0073, lr=0.000600, tokens/sec=1380507.77, grad_norm=0.5542, duration=0.38s
Step 1301: loss=4.1110, lr=0.000600, tokens/sec=1379201.22, grad_norm=0.4497, duration=0.38s
Step 1302: loss=4.0801, lr=0.000600, tokens/sec=1373629.75, grad_norm=0.3908, duration=0.38s
Step 1303: loss=4.0273, lr=0.000600, tokens/sec=1378073.30, grad_norm=0.4166, duration=0.38s
Step 1304: loss=3.9592, lr=0.000600, tokens/sec=1378315.15, grad_norm=0.4330, duration=0.38s
Step 1305: loss=4.0719, lr=0.000600, tokens/sec=1375918.52, grad_norm=0.5151, duration=0.38s
Step 1306: loss=4.0532, lr=0.000600, tokens/sec=1377499.24, grad_norm=0.5295, duration=0.38s
Step 1307: loss=4.0515, lr=0.000600, tokens/sec=1376393.05, grad_norm=0.4922, duration=0.38s
Step 1308: loss=4.0798, lr=0.000600, tokens/sec=1378498.32, grad_norm=0.4821, duration=0.38s
Step 1309: loss=4.0135, lr=0.000600, tokens/sec=1377203.33, grad_norm=0.5501, duration=0.38s
Step 1310: loss=4.0121, lr=0.000600, tokens/sec=1382434.44, grad_norm=0.4784, duration=0.38s
Step 1311: loss=4.0251, lr=0.000600, tokens/sec=1378906.31, grad_norm=0.4014, duration=0.38s
Step 1312: loss=3.9514, lr=0.000600, tokens/sec=1377651.12, grad_norm=0.4362, duration=0.38s
Step 1313: loss=3.9950, lr=0.000600, tokens/sec=1378920.15, grad_norm=0.4651, duration=0.38s
Step 1314: loss=3.9565, lr=0.000600, tokens/sec=1379049.00, grad_norm=0.4360, duration=0.38s
Step 1315: loss=3.9412, lr=0.000600, tokens/sec=1378682.41, grad_norm=0.4137, duration=0.38s
Step 1316: loss=3.9478, lr=0.000600, tokens/sec=1378763.66, grad_norm=0.4213, duration=0.38s
Step 1317: loss=4.0244, lr=0.000600, tokens/sec=1377815.99, grad_norm=0.4514, duration=0.38s
Step 1318: loss=3.9323, lr=0.000600, tokens/sec=1380368.25, grad_norm=0.4392, duration=0.38s
Step 1319: loss=3.9466, lr=0.000600, tokens/sec=1373872.62, grad_norm=0.4102, duration=0.38s
Step 1320: loss=3.9337, lr=0.000600, tokens/sec=1373149.41, grad_norm=0.3866, duration=0.38s
Step 1321: loss=3.9881, lr=0.000600, tokens/sec=1380327.53, grad_norm=0.4224, duration=0.38s
Step 1322: loss=4.0207, lr=0.000600, tokens/sec=1380075.45, grad_norm=0.5098, duration=0.38s
Step 1323: loss=3.9804, lr=0.000600, tokens/sec=1376504.19, grad_norm=0.4714, duration=0.38s
Step 1324: loss=4.0181, lr=0.000600, tokens/sec=1377087.77, grad_norm=0.3694, duration=0.38s
Step 1325: loss=4.0597, lr=0.000600, tokens/sec=1376183.73, grad_norm=0.3599, duration=0.38s
Step 1326: loss=4.0880, lr=0.000600, tokens/sec=1381446.13, grad_norm=0.3312, duration=0.38s
Step 1327: loss=4.0526, lr=0.000600, tokens/sec=1375249.93, grad_norm=0.3203, duration=0.38s
Step 1328: loss=4.0599, lr=0.000600, tokens/sec=1379379.44, grad_norm=0.3908, duration=0.38s
Step 1329: loss=4.0831, lr=0.000600, tokens/sec=1379127.70, grad_norm=0.4471, duration=0.38s
Step 1330: loss=4.0993, lr=0.000600, tokens/sec=1373083.39, grad_norm=0.5217, duration=0.38s
Step 1331: loss=4.1467, lr=0.000600, tokens/sec=1380454.04, grad_norm=0.5545, duration=0.38s
Step 1332: loss=4.1297, lr=0.000600, tokens/sec=1371110.01, grad_norm=0.5407, duration=0.38s
Step 1333: loss=4.1768, lr=0.000600, tokens/sec=1379176.14, grad_norm=0.5963, duration=0.38s
Step 1334: loss=4.1444, lr=0.000600, tokens/sec=1380626.52, grad_norm=0.5934, duration=0.38s
Step 1335: loss=4.1164, lr=0.000600, tokens/sec=1377462.14, grad_norm=0.5271, duration=0.38s
Step 1336: loss=4.2260, lr=0.000600, tokens/sec=1377708.95, grad_norm=0.4450, duration=0.38s
Step 1337: loss=4.1148, lr=0.000600, tokens/sec=1380382.99, grad_norm=0.4861, duration=0.38s
Step 1338: loss=4.1738, lr=0.000600, tokens/sec=1377786.64, grad_norm=0.5434, duration=0.38s
Step 1339: loss=4.1063, lr=0.000600, tokens/sec=1380578.84, grad_norm=0.4756, duration=0.38s
Step 1340: loss=4.1158, lr=0.000600, tokens/sec=1377048.10, grad_norm=0.4545, duration=0.38s
Step 1341: loss=4.1196, lr=0.000600, tokens/sec=1376472.31, grad_norm=0.4592, duration=0.38s
Step 1342: loss=4.0956, lr=0.000600, tokens/sec=1377221.45, grad_norm=0.4975, duration=0.38s
Step 1343: loss=4.0499, lr=0.000600, tokens/sec=1374693.69, grad_norm=0.4868, duration=0.38s
Step 1344: loss=4.0598, lr=0.000600, tokens/sec=1377444.02, grad_norm=0.3756, duration=0.38s
Step 1345: loss=4.0829, lr=0.000600, tokens/sec=1376049.39, grad_norm=0.3729, duration=0.38s
Step 1346: loss=4.0280, lr=0.000600, tokens/sec=1379244.48, grad_norm=0.3607, duration=0.38s
Step 1347: loss=4.0675, lr=0.000600, tokens/sec=1376696.36, grad_norm=0.3684, duration=0.38s
Step 1348: loss=4.0132, lr=0.000600, tokens/sec=1376120.01, grad_norm=0.3692, duration=0.38s
Step 1349: loss=3.9334, lr=0.000600, tokens/sec=1376310.35, grad_norm=0.4859, duration=0.38s
Step 1350: loss=4.0577, lr=0.000600, tokens/sec=1374701.42, grad_norm=0.3529, duration=0.38s
Step 1351: loss=4.0042, lr=0.000600, tokens/sec=1377892.83, grad_norm=0.3451, duration=0.38s
Step 1352: loss=4.0859, lr=0.000600, tokens/sec=1374553.62, grad_norm=0.4130, duration=0.38s
Step 1353: loss=3.9611, lr=0.000600, tokens/sec=1375470.14, grad_norm=0.4565, duration=0.38s
Step 1354: loss=4.0590, lr=0.000600, tokens/sec=1377891.10, grad_norm=0.4152, duration=0.38s
Step 1355: loss=4.0363, lr=0.000600, tokens/sec=1381931.43, grad_norm=0.4141, duration=0.38s
Step 1356: loss=3.9712, lr=0.000600, tokens/sec=1376164.79, grad_norm=0.4214, duration=0.38s
Step 1357: loss=3.9917, lr=0.000600, tokens/sec=1377533.76, grad_norm=0.3972, duration=0.38s
Step 1358: loss=3.9952, lr=0.000600, tokens/sec=1378608.94, grad_norm=0.4505, duration=0.38s
Step 1359: loss=3.9804, lr=0.000600, tokens/sec=1375528.64, grad_norm=0.4785, duration=0.38s
Step 1360: loss=3.9684, lr=0.000600, tokens/sec=1375717.10, grad_norm=0.4950, duration=0.38s
Step 1361: loss=4.0117, lr=0.000600, tokens/sec=1379235.82, grad_norm=0.4484, duration=0.38s
Step 1362: loss=4.0044, lr=0.000600, tokens/sec=1377153.31, grad_norm=0.4790, duration=0.38s
Step 1363: loss=4.0428, lr=0.000600, tokens/sec=1377538.93, grad_norm=0.4872, duration=0.38s
Step 1364: loss=3.9873, lr=0.000600, tokens/sec=1376092.45, grad_norm=0.4454, duration=0.38s
Step 1365: loss=3.8970, lr=0.000600, tokens/sec=1375968.46, grad_norm=0.3790, duration=0.38s
Step 1366: loss=3.9423, lr=0.000600, tokens/sec=1376390.46, grad_norm=0.5761, duration=0.38s
Step 1367: loss=3.8996, lr=0.000600, tokens/sec=1373809.10, grad_norm=0.4849, duration=0.38s
Step 1368: loss=3.9146, lr=0.000600, tokens/sec=1377243.01, grad_norm=0.4919, duration=0.38s
Step 1369: loss=3.9981, lr=0.000600, tokens/sec=1374915.44, grad_norm=0.4889, duration=0.38s
Step 1370: loss=3.9830, lr=0.000600, tokens/sec=1373769.62, grad_norm=0.5137, duration=0.38s
Step 1371: loss=3.9982, lr=0.000600, tokens/sec=1374139.61, grad_norm=0.4698, duration=0.38s
Step 1372: loss=3.9575, lr=0.000600, tokens/sec=1378227.90, grad_norm=0.4634, duration=0.38s
Step 1373: loss=3.9853, lr=0.000600, tokens/sec=1373877.77, grad_norm=0.4160, duration=0.38s
Step 1374: loss=3.9906, lr=0.000600, tokens/sec=1376117.42, grad_norm=0.4028, duration=0.38s
Step 1375: loss=3.9650, lr=0.000600, tokens/sec=1378049.98, grad_norm=0.3986, duration=0.38s
Step 1376: loss=3.9513, lr=0.000600, tokens/sec=1377034.30, grad_norm=0.3376, duration=0.38s
Step 1377: loss=4.0930, lr=0.000600, tokens/sec=1381148.53, grad_norm=0.3293, duration=0.38s
Step 1378: loss=4.1095, lr=0.000600, tokens/sec=1375010.01, grad_norm=0.3424, duration=0.38s
Step 1379: loss=4.0687, lr=0.000600, tokens/sec=1378191.62, grad_norm=0.3749, duration=0.38s
Step 1380: loss=4.0728, lr=0.000600, tokens/sec=1378879.51, grad_norm=0.3861, duration=0.38s
Step 1381: loss=4.0649, lr=0.000600, tokens/sec=1377167.11, grad_norm=0.4177, duration=0.38s
Step 1382: loss=4.0808, lr=0.000599, tokens/sec=1379778.43, grad_norm=0.4682, duration=0.38s
Step 1383: loss=4.0480, lr=0.000599, tokens/sec=1377948.95, grad_norm=0.3846, duration=0.38s
Step 1384: loss=4.0085, lr=0.000599, tokens/sec=1379006.62, grad_norm=0.4017, duration=0.38s
Step 1385: loss=4.0513, lr=0.000599, tokens/sec=1376912.73, grad_norm=0.3849, duration=0.38s
Step 1386: loss=4.0264, lr=0.000599, tokens/sec=1381223.14, grad_norm=0.3543, duration=0.38s
Step 1387: loss=4.0305, lr=0.000599, tokens/sec=1373300.34, grad_norm=0.3023, duration=0.38s
Step 1388: loss=4.0421, lr=0.000599, tokens/sec=1380747.88, grad_norm=0.3189, duration=0.38s
Step 1389: loss=3.9900, lr=0.000599, tokens/sec=1377928.23, grad_norm=0.3063, duration=0.38s
Step 1390: loss=4.0270, lr=0.000599, tokens/sec=1380892.68, grad_norm=0.3180, duration=0.38s
Step 1391: loss=3.9578, lr=0.000599, tokens/sec=1375773.91, grad_norm=0.3431, duration=0.38s
Step 1392: loss=3.9543, lr=0.000599, tokens/sec=1376679.12, grad_norm=0.3063, duration=0.38s
Step 1393: loss=4.0070, lr=0.000599, tokens/sec=1376606.73, grad_norm=0.3640, duration=0.38s
Step 1394: loss=3.9987, lr=0.000599, tokens/sec=1374909.42, grad_norm=0.3849, duration=0.38s
Step 1395: loss=4.0616, lr=0.000599, tokens/sec=1376605.87, grad_norm=0.3927, duration=0.38s
Step 1396: loss=3.9963, lr=0.000599, tokens/sec=1377507.01, grad_norm=0.4103, duration=0.38s
Step 1397: loss=4.0657, lr=0.000599, tokens/sec=1375436.59, grad_norm=0.3475, duration=0.38s
Step 1398: loss=4.0032, lr=0.000599, tokens/sec=1379163.16, grad_norm=0.3917, duration=0.38s
Step 1399: loss=3.9857, lr=0.000599, tokens/sec=1378187.30, grad_norm=0.4860, duration=0.38s
Step 1400/19073 (7.3%), Elapsed time: 608.88s, Steps per hour: 8277.46, Estimated hours remaining: 2.14
Step 1400: loss=4.0249, lr=0.000599, tokens/sec=1374589.71, grad_norm=0.4582, duration=0.38s
Step 1401: loss=3.9728, lr=0.000599, tokens/sec=1376027.87, grad_norm=0.3952, duration=0.38s
Step 1402: loss=3.9423, lr=0.000599, tokens/sec=1375122.65, grad_norm=0.3503, duration=0.38s
Step 1403: loss=3.9727, lr=0.000599, tokens/sec=1375434.00, grad_norm=0.4030, duration=0.38s
Step 1404: loss=3.9694, lr=0.000599, tokens/sec=1375261.97, grad_norm=0.4426, duration=0.38s
Step 1405: loss=3.9811, lr=0.000599, tokens/sec=1376714.46, grad_norm=0.4461, duration=0.38s
Step 1406: loss=3.9304, lr=0.000599, tokens/sec=1379025.65, grad_norm=0.4050, duration=0.38s
Step 1407: loss=3.9357, lr=0.000599, tokens/sec=1375928.86, grad_norm=0.4236, duration=0.38s
Step 1408: loss=3.9632, lr=0.000599, tokens/sec=1374557.06, grad_norm=0.4286, duration=0.38s
Step 1409: loss=3.9464, lr=0.000599, tokens/sec=1373112.54, grad_norm=0.4071, duration=0.38s
Step 1410: loss=3.9714, lr=0.000599, tokens/sec=1378550.17, grad_norm=0.4140, duration=0.38s
Step 1411: loss=3.8885, lr=0.000599, tokens/sec=1375514.88, grad_norm=0.4039, duration=0.38s
Step 1412: loss=3.9072, lr=0.000599, tokens/sec=1375622.44, grad_norm=0.4285, duration=0.38s
Step 1413: loss=3.8928, lr=0.000599, tokens/sec=1375392.71, grad_norm=0.4851, duration=0.38s
Step 1414: loss=3.8937, lr=0.000599, tokens/sec=1376272.45, grad_norm=0.5073, duration=0.38s
Step 1415: loss=3.8824, lr=0.000599, tokens/sec=1377117.09, grad_norm=0.4442, duration=0.38s
Step 1416: loss=3.9304, lr=0.000599, tokens/sec=1376455.94, grad_norm=0.3739, duration=0.38s
Step 1417: loss=3.9584, lr=0.000599, tokens/sec=1376623.97, grad_norm=0.3539, duration=0.38s
Step 1418: loss=3.8969, lr=0.000599, tokens/sec=1376967.91, grad_norm=0.3212, duration=0.38s
Step 1419: loss=3.9157, lr=0.000599, tokens/sec=1379910.90, grad_norm=0.3448, duration=0.38s
Step 1420: loss=3.8814, lr=0.000599, tokens/sec=1376012.37, grad_norm=0.3770, duration=0.38s
Step 1421: loss=3.9169, lr=0.000599, tokens/sec=1377645.08, grad_norm=0.3772, duration=0.38s
Step 1422: loss=3.9395, lr=0.000599, tokens/sec=1377703.77, grad_norm=0.3629, duration=0.38s
Step 1423: loss=3.9950, lr=0.000599, tokens/sec=1377552.74, grad_norm=0.3708, duration=0.38s
Step 1424: loss=4.0406, lr=0.000599, tokens/sec=1373205.15, grad_norm=0.4688, duration=0.38s
Step 1425: loss=4.0893, lr=0.000599, tokens/sec=1378665.98, grad_norm=0.5312, duration=0.38s
Step 1426: loss=4.0486, lr=0.000599, tokens/sec=1374087.24, grad_norm=0.5531, duration=0.38s
Step 1427: loss=4.0565, lr=0.000599, tokens/sec=1374397.27, grad_norm=0.5766, duration=0.38s
Step 1428: loss=4.0178, lr=0.000599, tokens/sec=1373144.26, grad_norm=0.5989, duration=0.38s
Step 1429: loss=4.0583, lr=0.000599, tokens/sec=1377149.00, grad_norm=0.5954, duration=0.38s
Step 1430: loss=4.0328, lr=0.000599, tokens/sec=1372566.59, grad_norm=0.4881, duration=0.38s
Step 1431: loss=4.0087, lr=0.000599, tokens/sec=1375526.06, grad_norm=0.3604, duration=0.38s
Step 1432: loss=4.0050, lr=0.000599, tokens/sec=1377002.40, grad_norm=0.3422, duration=0.38s
Step 1433: loss=3.9826, lr=0.000599, tokens/sec=1378505.23, grad_norm=0.3269, duration=0.38s
Step 1434: loss=4.0245, lr=0.000599, tokens/sec=1376866.17, grad_norm=0.3202, duration=0.38s
Step 1435: loss=3.9824, lr=0.000599, tokens/sec=1375229.28, grad_norm=0.3659, duration=0.38s
Step 1436: loss=3.9883, lr=0.000599, tokens/sec=1366543.86, grad_norm=0.3503, duration=0.38s
Step 1437: loss=3.9515, lr=0.000599, tokens/sec=1372451.80, grad_norm=0.3061, duration=0.38s
Step 1438: loss=3.9956, lr=0.000599, tokens/sec=1373527.65, grad_norm=0.3051, duration=0.38s
Step 1439: loss=3.9264, lr=0.000599, tokens/sec=1377473.35, grad_norm=0.3197, duration=0.38s
Step 1440: loss=3.9785, lr=0.000599, tokens/sec=1375621.58, grad_norm=0.3689, duration=0.38s
Step 1441: loss=3.9669, lr=0.000599, tokens/sec=1378382.54, grad_norm=0.3501, duration=0.38s
Step 1442: loss=3.9603, lr=0.000599, tokens/sec=1376710.15, grad_norm=0.3639, duration=0.38s
Step 1443: loss=3.9465, lr=0.000599, tokens/sec=1371463.18, grad_norm=0.3466, duration=0.38s
Step 1444: loss=3.9735, lr=0.000599, tokens/sec=1374962.72, grad_norm=0.3680, duration=0.38s
Step 1445: loss=3.9511, lr=0.000599, tokens/sec=1374120.72, grad_norm=0.3473, duration=0.38s
Step 1446: loss=3.9653, lr=0.000599, tokens/sec=1371792.56, grad_norm=0.3395, duration=0.38s
Step 1447: loss=3.9355, lr=0.000599, tokens/sec=1377862.61, grad_norm=0.3273, duration=0.38s
Step 1448: loss=3.9246, lr=0.000599, tokens/sec=1374916.30, grad_norm=0.3105, duration=0.38s
Step 1449: loss=3.9207, lr=0.000599, tokens/sec=1376206.99, grad_norm=0.3046, duration=0.38s
Step 1450: loss=3.9311, lr=0.000599, tokens/sec=1374615.49, grad_norm=0.3032, duration=0.38s
Step 1451: loss=3.8836, lr=0.000599, tokens/sec=1375976.21, grad_norm=0.3355, duration=0.38s
Step 1452: loss=3.9032, lr=0.000599, tokens/sec=1376580.88, grad_norm=0.3583, duration=0.38s
Step 1453: loss=3.9005, lr=0.000599, tokens/sec=1376505.05, grad_norm=0.3847, duration=0.38s
Step 1454: loss=3.9121, lr=0.000599, tokens/sec=1371858.46, grad_norm=0.4267, duration=0.38s
Step 1455: loss=4.0105, lr=0.000599, tokens/sec=1375646.53, grad_norm=0.4540, duration=0.38s
Step 1456: loss=3.9539, lr=0.000599, tokens/sec=1374508.09, grad_norm=0.4537, duration=0.38s
Step 1457: loss=3.8992, lr=0.000599, tokens/sec=1376625.69, grad_norm=0.4126, duration=0.38s
Step 1458: loss=3.8997, lr=0.000599, tokens/sec=1378889.02, grad_norm=0.4595, duration=0.38s
Step 1459: loss=3.8339, lr=0.000599, tokens/sec=1375670.63, grad_norm=0.4845, duration=0.38s
Step 1460: loss=3.9129, lr=0.000599, tokens/sec=1376888.59, grad_norm=0.4559, duration=0.38s
Step 1461: loss=3.8352, lr=0.000599, tokens/sec=1374663.61, grad_norm=0.4174, duration=0.38s
Step 1462: loss=3.9021, lr=0.000599, tokens/sec=1370951.87, grad_norm=0.4051, duration=0.38s
Step 1463: loss=3.8951, lr=0.000599, tokens/sec=1375115.77, grad_norm=0.4042, duration=0.38s
Step 1464: loss=3.9484, lr=0.000599, tokens/sec=1376064.03, grad_norm=0.4421, duration=0.38s
Step 1465: loss=3.9020, lr=0.000599, tokens/sec=1377397.43, grad_norm=0.4899, duration=0.38s
Step 1466: loss=3.8975, lr=0.000599, tokens/sec=1374076.93, grad_norm=0.3690, duration=0.38s
Step 1467: loss=3.8487, lr=0.000599, tokens/sec=1379886.66, grad_norm=0.2969, duration=0.38s
Step 1468: loss=3.8709, lr=0.000599, tokens/sec=1371025.38, grad_norm=0.3144, duration=0.38s
Step 1469: loss=3.9428, lr=0.000599, tokens/sec=1378230.49, grad_norm=0.3385, duration=0.38s
Step 1470: loss=3.9979, lr=0.000599, tokens/sec=1372247.97, grad_norm=0.3933, duration=0.38s
Step 1471: loss=3.9891, lr=0.000599, tokens/sec=1379920.43, grad_norm=0.4798, duration=0.38s
Step 1472: loss=4.0146, lr=0.000599, tokens/sec=1376530.90, grad_norm=0.5270, duration=0.38s
Step 1473: loss=3.9744, lr=0.000599, tokens/sec=1376492.13, grad_norm=0.5400, duration=0.38s
Step 1474: loss=4.0123, lr=0.000599, tokens/sec=1373303.77, grad_norm=0.5240, duration=0.38s
Step 1475: loss=3.9633, lr=0.000599, tokens/sec=1373603.15, grad_norm=0.4432, duration=0.38s
Step 1476: loss=4.0000, lr=0.000599, tokens/sec=1380439.31, grad_norm=0.3860, duration=0.38s
Step 1477: loss=4.0221, lr=0.000599, tokens/sec=1374151.63, grad_norm=0.3591, duration=0.38s
Step 1478: loss=4.0704, lr=0.000599, tokens/sec=1380699.33, grad_norm=0.3918, duration=0.38s
Step 1479: loss=4.0098, lr=0.000599, tokens/sec=1378126.84, grad_norm=0.4266, duration=0.38s
Step 1480: loss=4.0063, lr=0.000599, tokens/sec=1369014.44, grad_norm=0.3674, duration=0.38s
Step 1481: loss=3.9326, lr=0.000599, tokens/sec=1371363.96, grad_norm=0.3242, duration=0.38s
Step 1482: loss=3.9716, lr=0.000599, tokens/sec=1376634.31, grad_norm=0.3528, duration=0.38s
Step 1483: loss=3.9707, lr=0.000599, tokens/sec=1375931.44, grad_norm=0.3541, duration=0.38s
Step 1484: loss=3.9366, lr=0.000599, tokens/sec=1376471.45, grad_norm=0.3741, duration=0.38s
Step 1485: loss=3.9859, lr=0.000599, tokens/sec=1374956.70, grad_norm=0.3966, duration=0.38s
Step 1486: loss=3.9118, lr=0.000599, tokens/sec=1378504.37, grad_norm=0.3740, duration=0.38s
Step 1487: loss=3.9909, lr=0.000599, tokens/sec=1379995.77, grad_norm=0.3243, duration=0.38s
Step 1488: loss=4.0210, lr=0.000599, tokens/sec=1378961.65, grad_norm=0.3288, duration=0.38s
Step 1489: loss=3.9431, lr=0.000599, tokens/sec=1379242.75, grad_norm=0.3311, duration=0.38s
Step 1490: loss=3.8733, lr=0.000599, tokens/sec=1375552.74, grad_norm=0.2955, duration=0.38s
Step 1491: loss=3.9641, lr=0.000599, tokens/sec=1381855.01, grad_norm=0.3733, duration=0.38s
Step 1492: loss=3.9377, lr=0.000599, tokens/sec=1374030.57, grad_norm=0.4244, duration=0.38s
Step 1493: loss=3.8897, lr=0.000599, tokens/sec=1376834.28, grad_norm=0.3944, duration=0.38s
Step 1494: loss=3.8502, lr=0.000599, tokens/sec=1372465.51, grad_norm=0.3304, duration=0.38s
Step 1495: loss=3.9245, lr=0.000599, tokens/sec=1375742.92, grad_norm=0.3024, duration=0.38s
Step 1496: loss=3.8739, lr=0.000599, tokens/sec=1372963.37, grad_norm=0.3457, duration=0.38s
Step 1497: loss=3.8969, lr=0.000599, tokens/sec=1378788.73, grad_norm=0.3619, duration=0.38s
Step 1498: loss=3.9454, lr=0.000599, tokens/sec=1375778.21, grad_norm=0.3776, duration=0.38s
Step 1499: loss=3.8576, lr=0.000599, tokens/sec=1374471.14, grad_norm=0.3828, duration=0.38s
Step 1500/19073 (7.9%), Elapsed time: 647.07s, Steps per hour: 8345.33, Estimated hours remaining: 2.11
Validation loss at step 1500: 3.986644744873047
Step 1500: loss=3.8805, lr=0.000599, tokens/sec=153161.76, grad_norm=0.3695, duration=3.42s
Step 1501: loss=3.8623, lr=0.000599, tokens/sec=1375936.60, grad_norm=0.3611, duration=0.38s
Step 1502: loss=3.7913, lr=0.000599, tokens/sec=1375458.95, grad_norm=0.3261, duration=0.38s
Step 1503: loss=3.8627, lr=0.000599, tokens/sec=1374991.09, grad_norm=0.3453, duration=0.38s
Step 1504: loss=3.8052, lr=0.000599, tokens/sec=1374638.69, grad_norm=0.3723, duration=0.38s
Step 1505: loss=3.8134, lr=0.000599, tokens/sec=1372401.27, grad_norm=0.4078, duration=0.38s
Step 1506: loss=3.8128, lr=0.000599, tokens/sec=1377974.85, grad_norm=0.4353, duration=0.38s
Step 1507: loss=3.8599, lr=0.000599, tokens/sec=1376154.45, grad_norm=0.3604, duration=0.38s
Step 1508: loss=3.8228, lr=0.000599, tokens/sec=1366621.15, grad_norm=0.3439, duration=0.38s
Step 1509: loss=3.8435, lr=0.000599, tokens/sec=1381298.62, grad_norm=0.3625, duration=0.38s
Step 1510: loss=3.8045, lr=0.000599, tokens/sec=1374438.50, grad_norm=0.3707, duration=0.38s
Step 1511: loss=3.8561, lr=0.000599, tokens/sec=1372053.61, grad_norm=0.3579, duration=0.38s
Step 1512: loss=3.8498, lr=0.000599, tokens/sec=1375588.88, grad_norm=0.3584, duration=0.38s
Step 1513: loss=3.8578, lr=0.000599, tokens/sec=1374863.86, grad_norm=0.3347, duration=0.38s
Step 1514: loss=3.8665, lr=0.000599, tokens/sec=1369086.04, grad_norm=0.3400, duration=0.38s
Step 1515: loss=3.9367, lr=0.000599, tokens/sec=1375434.87, grad_norm=0.3364, duration=0.38s
Step 1516: loss=3.9060, lr=0.000599, tokens/sec=1374186.84, grad_norm=0.3996, duration=0.38s
Step 1517: loss=3.9782, lr=0.000599, tokens/sec=1370705.76, grad_norm=0.4712, duration=0.38s
Step 1518: loss=3.9201, lr=0.000599, tokens/sec=1372947.94, grad_norm=0.4822, duration=0.38s
Step 1519: loss=3.9725, lr=0.000599, tokens/sec=1377314.61, grad_norm=0.4148, duration=0.38s
Step 1520: loss=3.9661, lr=0.000599, tokens/sec=1376602.42, grad_norm=0.3972, duration=0.38s
Step 1521: loss=3.9755, lr=0.000599, tokens/sec=1373138.26, grad_norm=0.3664, duration=0.38s
Step 1522: loss=3.9830, lr=0.000599, tokens/sec=1372495.49, grad_norm=0.3725, duration=0.38s
Step 1523: loss=4.0516, lr=0.000599, tokens/sec=1377620.05, grad_norm=0.3894, duration=0.38s
Step 1524: loss=3.9458, lr=0.000599, tokens/sec=1376224.21, grad_norm=0.4263, duration=0.38s
Step 1525: loss=4.0187, lr=0.000599, tokens/sec=1378525.11, grad_norm=0.3894, duration=0.38s
Step 1526: loss=4.0516, lr=0.000599, tokens/sec=1377406.92, grad_norm=0.4304, duration=0.38s
Step 1527: loss=4.0063, lr=0.000599, tokens/sec=1372818.52, grad_norm=0.4386, duration=0.38s
Step 1528: loss=4.0086, lr=0.000599, tokens/sec=1377388.80, grad_norm=0.4281, duration=0.38s
Step 1529: loss=3.9621, lr=0.000599, tokens/sec=1360967.84, grad_norm=0.4203, duration=0.39s
Step 1530: loss=4.0325, lr=0.000599, tokens/sec=1375251.65, grad_norm=0.4384, duration=0.38s
Step 1531: loss=3.9357, lr=0.000599, tokens/sec=1375323.04, grad_norm=0.4538, duration=0.38s
Step 1532: loss=3.9713, lr=0.000599, tokens/sec=1375279.17, grad_norm=0.3866, duration=0.38s
Step 1533: loss=3.8853, lr=0.000599, tokens/sec=1379048.13, grad_norm=0.3535, duration=0.38s
Step 1534: loss=3.9406, lr=0.000599, tokens/sec=1379538.66, grad_norm=0.3208, duration=0.38s
Step 1535: loss=3.9211, lr=0.000599, tokens/sec=1373047.38, grad_norm=0.3014, duration=0.38s
Step 1536: loss=3.8961, lr=0.000599, tokens/sec=1376647.24, grad_norm=0.2776, duration=0.38s
Step 1537: loss=3.9184, lr=0.000599, tokens/sec=1373561.96, grad_norm=0.2740, duration=0.38s
Step 1538: loss=3.8881, lr=0.000599, tokens/sec=1376677.40, grad_norm=0.3250, duration=0.38s
Step 1539: loss=3.8313, lr=0.000599, tokens/sec=1382716.08, grad_norm=0.6391, duration=0.38s
Step 1540: loss=3.8902, lr=0.000599, tokens/sec=1370758.74, grad_norm=0.3601, duration=0.38s
Step 1541: loss=3.8749, lr=0.000599, tokens/sec=1372259.10, grad_norm=0.3596, duration=0.38s
Step 1542: loss=3.9796, lr=0.000599, tokens/sec=1376614.49, grad_norm=0.4591, duration=0.38s
Step 1543: loss=3.8596, lr=0.000599, tokens/sec=1376812.73, grad_norm=0.5061, duration=0.38s
Step 1544: loss=3.9632, lr=0.000599, tokens/sec=1375035.80, grad_norm=0.4453, duration=0.38s
Step 1545: loss=3.8908, lr=0.000599, tokens/sec=1373504.48, grad_norm=0.4640, duration=0.38s
Step 1546: loss=3.8686, lr=0.000599, tokens/sec=1369699.17, grad_norm=0.4494, duration=0.38s
Step 1547: loss=3.8380, lr=0.000599, tokens/sec=1376878.24, grad_norm=0.3491, duration=0.38s
Step 1548: loss=3.8899, lr=0.000599, tokens/sec=1376979.98, grad_norm=0.3392, duration=0.38s
Step 1549: loss=3.8167, lr=0.000599, tokens/sec=1378906.31, grad_norm=0.3516, duration=0.38s
Step 1550: loss=3.8826, lr=0.000599, tokens/sec=1370804.88, grad_norm=0.4058, duration=0.38s
Step 1551: loss=3.8742, lr=0.000599, tokens/sec=1374496.06, grad_norm=0.4140, duration=0.38s
Step 1552: loss=3.8709, lr=0.000599, tokens/sec=1376700.67, grad_norm=0.4290, duration=0.38s
Step 1553: loss=3.9564, lr=0.000599, tokens/sec=1373271.18, grad_norm=0.3509, duration=0.38s
Step 1554: loss=3.8348, lr=0.000599, tokens/sec=1374869.02, grad_norm=0.3767, duration=0.38s
Step 1555: loss=3.7317, lr=0.000599, tokens/sec=1375392.71, grad_norm=0.4656, duration=0.38s
Step 1556: loss=3.8366, lr=0.000599, tokens/sec=1374960.14, grad_norm=0.3624, duration=0.38s
Step 1557: loss=3.7910, lr=0.000599, tokens/sec=1377679.61, grad_norm=0.4345, duration=0.38s
Step 1558: loss=3.8321, lr=0.000599, tokens/sec=1379258.32, grad_norm=0.4572, duration=0.38s
Step 1559: loss=3.8523, lr=0.000599, tokens/sec=1376349.97, grad_norm=0.3905, duration=0.38s
Step 1560: loss=3.8511, lr=0.000599, tokens/sec=1376648.10, grad_norm=0.3725, duration=0.38s
Step 1561: loss=3.8429, lr=0.000599, tokens/sec=1378910.64, grad_norm=0.3748, duration=0.38s
Step 1562: loss=3.8261, lr=0.000599, tokens/sec=1376105.37, grad_norm=0.3112, duration=0.38s
Step 1563: loss=3.8792, lr=0.000599, tokens/sec=1375332.50, grad_norm=0.3154, duration=0.38s
Step 1564: loss=3.8429, lr=0.000599, tokens/sec=1375272.29, grad_norm=0.3187, duration=0.38s
Step 1565: loss=3.8128, lr=0.000599, tokens/sec=1375345.40, grad_norm=0.3133, duration=0.38s
Step 1566: loss=3.8455, lr=0.000599, tokens/sec=1378975.49, grad_norm=0.2816, duration=0.38s
Step 1567: loss=3.9380, lr=0.000599, tokens/sec=1371337.45, grad_norm=0.2772, duration=0.38s
Step 1568: loss=3.9897, lr=0.000599, tokens/sec=1374355.18, grad_norm=0.3454, duration=0.38s
Step 1569: loss=3.9476, lr=0.000599, tokens/sec=1374973.04, grad_norm=0.3847, duration=0.38s
Step 1570: loss=3.9282, lr=0.000599, tokens/sec=1375339.38, grad_norm=0.4021, duration=0.38s
Step 1571: loss=3.9438, lr=0.000599, tokens/sec=1376111.39, grad_norm=0.3778, duration=0.38s
Step 1572: loss=3.9385, lr=0.000599, tokens/sec=1373355.23, grad_norm=0.3295, duration=0.38s
Step 1573: loss=3.9246, lr=0.000599, tokens/sec=1377742.62, grad_norm=0.2995, duration=0.38s
Step 1574: loss=3.8982, lr=0.000599, tokens/sec=1374957.56, grad_norm=0.3390, duration=0.38s
Step 1575: loss=3.9006, lr=0.000599, tokens/sec=1371889.27, grad_norm=0.4233, duration=0.38s
Step 1576: loss=3.9242, lr=0.000599, tokens/sec=1373674.37, grad_norm=0.4159, duration=0.38s
Step 1577: loss=3.9510, lr=0.000599, tokens/sec=1378975.49, grad_norm=0.4087, duration=0.38s
Step 1578: loss=3.8925, lr=0.000599, tokens/sec=1373875.19, grad_norm=0.4656, duration=0.38s
Step 1579: loss=3.9032, lr=0.000599, tokens/sec=1375915.94, grad_norm=0.4174, duration=0.38s
Step 1580: loss=3.9038, lr=0.000599, tokens/sec=1372500.63, grad_norm=0.3593, duration=0.38s
Step 1581: loss=3.8196, lr=0.000599, tokens/sec=1379452.12, grad_norm=0.3382, duration=0.38s
Step 1582: loss=3.8561, lr=0.000599, tokens/sec=1374146.48, grad_norm=0.3437, duration=0.38s
Step 1583: loss=3.8963, lr=0.000599, tokens/sec=1377295.63, grad_norm=0.3896, duration=0.38s
Step 1584: loss=3.8884, lr=0.000599, tokens/sec=1378503.51, grad_norm=0.4073, duration=0.38s
Step 1585: loss=3.9610, lr=0.000599, tokens/sec=1372977.94, grad_norm=0.3846, duration=0.38s
Step 1586: loss=3.8661, lr=0.000599, tokens/sec=1378457.71, grad_norm=0.3903, duration=0.38s
Step 1587: loss=3.9510, lr=0.000599, tokens/sec=1377405.19, grad_norm=0.3978, duration=0.38s
Step 1588: loss=3.9036, lr=0.000599, tokens/sec=1379870.21, grad_norm=0.3920, duration=0.38s
Step 1589: loss=3.8927, lr=0.000599, tokens/sec=1378972.90, grad_norm=0.3224, duration=0.38s
Step 1590: loss=3.8519, lr=0.000599, tokens/sec=1372917.94, grad_norm=0.2983, duration=0.38s
Step 1591: loss=3.8473, lr=0.000599, tokens/sec=1377752.97, grad_norm=0.2864, duration=0.38s
Step 1592: loss=3.8320, lr=0.000599, tokens/sec=1375887.53, grad_norm=0.2779, duration=0.38s
Step 1593: loss=3.8609, lr=0.000599, tokens/sec=1376159.62, grad_norm=0.3006, duration=0.38s
Step 1594: loss=3.8386, lr=0.000599, tokens/sec=1375317.87, grad_norm=0.3275, duration=0.38s
Step 1595: loss=3.8218, lr=0.000599, tokens/sec=1377773.69, grad_norm=0.3149, duration=0.38s
Step 1596: loss=3.8310, lr=0.000599, tokens/sec=1378371.30, grad_norm=0.2949, duration=0.38s
Step 1597: loss=3.7987, lr=0.000599, tokens/sec=1377853.11, grad_norm=0.3211, duration=0.38s
Step 1598: loss=3.8299, lr=0.000599, tokens/sec=1374974.76, grad_norm=0.3585, duration=0.38s
Step 1599: loss=3.8417, lr=0.000599, tokens/sec=1374804.55, grad_norm=0.3872, duration=0.38s
Step 1600/19073 (8.4%), Elapsed time: 688.31s, Steps per hour: 8368.37, Estimated hours remaining: 2.09
Step 1600: loss=3.8510, lr=0.000599, tokens/sec=1375047.84, grad_norm=0.4066, duration=0.38s
Step 1601: loss=3.7311, lr=0.000599, tokens/sec=1373232.59, grad_norm=0.3842, duration=0.38s
Step 1602: loss=3.8316, lr=0.000599, tokens/sec=1376483.51, grad_norm=0.3739, duration=0.38s
Step 1603: loss=3.7623, lr=0.000599, tokens/sec=1376045.95, grad_norm=0.3970, duration=0.38s
Step 1604: loss=3.8226, lr=0.000599, tokens/sec=1374740.95, grad_norm=0.4236, duration=0.38s
Step 1605: loss=3.7769, lr=0.000599, tokens/sec=1373942.14, grad_norm=0.4640, duration=0.38s
Step 1606: loss=3.8233, lr=0.000599, tokens/sec=1374746.11, grad_norm=0.3975, duration=0.38s
Step 1607: loss=3.8310, lr=0.000599, tokens/sec=1378472.40, grad_norm=0.3468, duration=0.38s
Step 1608: loss=3.7809, lr=0.000599, tokens/sec=1373591.99, grad_norm=0.3188, duration=0.38s
Step 1609: loss=3.7911, lr=0.000599, tokens/sec=1374773.61, grad_norm=0.3458, duration=0.38s
Step 1610: loss=3.7501, lr=0.000599, tokens/sec=1380376.05, grad_norm=0.3968, duration=0.38s
Step 1611: loss=3.8441, lr=0.000599, tokens/sec=1376424.06, grad_norm=0.3472, duration=0.38s
Step 1612: loss=3.8247, lr=0.000599, tokens/sec=1378987.60, grad_norm=0.3478, duration=0.38s
Step 1613: loss=3.8639, lr=0.000599, tokens/sec=1380186.32, grad_norm=0.3453, duration=0.38s
Step 1614: loss=3.9532, lr=0.000599, tokens/sec=1378164.85, grad_norm=0.3513, duration=0.38s
Step 1615: loss=3.9221, lr=0.000599, tokens/sec=1373324.35, grad_norm=0.3979, duration=0.38s
Step 1616: loss=3.9302, lr=0.000599, tokens/sec=1369767.43, grad_norm=0.3822, duration=0.38s
Step 1617: loss=3.9115, lr=0.000599, tokens/sec=1381032.30, grad_norm=0.3877, duration=0.38s
Step 1618: loss=3.8768, lr=0.000599, tokens/sec=1377373.27, grad_norm=0.4278, duration=0.38s
Step 1619: loss=3.9330, lr=0.000599, tokens/sec=1380045.13, grad_norm=0.4879, duration=0.38s
Step 1620: loss=3.8514, lr=0.000599, tokens/sec=1380428.04, grad_norm=0.5008, duration=0.38s
Step 1621: loss=3.9170, lr=0.000599, tokens/sec=1375565.64, grad_norm=0.4546, duration=0.38s
Step 1622: loss=3.8848, lr=0.000599, tokens/sec=1380535.51, grad_norm=0.3526, duration=0.38s
Step 1623: loss=3.8843, lr=0.000599, tokens/sec=1380464.44, grad_norm=0.3366, duration=0.38s
Step 1624: loss=3.9029, lr=0.000599, tokens/sec=1377898.87, grad_norm=0.3475, duration=0.38s
Step 1625: loss=3.8594, lr=0.000599, tokens/sec=1377601.07, grad_norm=0.3585, duration=0.38s
Step 1626: loss=3.8772, lr=0.000599, tokens/sec=1377641.63, grad_norm=0.3290, duration=0.38s
Step 1627: loss=3.8421, lr=0.000599, tokens/sec=1375876.34, grad_norm=0.3388, duration=0.38s
Step 1628: loss=3.8804, lr=0.000599, tokens/sec=1374273.58, grad_norm=0.3609, duration=0.38s
Step 1629: loss=3.8288, lr=0.000599, tokens/sec=1379436.55, grad_norm=0.3696, duration=0.38s
Step 1630: loss=3.8679, lr=0.000599, tokens/sec=1374692.83, grad_norm=0.3381, duration=0.38s
Step 1631: loss=3.8508, lr=0.000599, tokens/sec=1380237.43, grad_norm=0.3287, duration=0.38s
Step 1632: loss=3.8733, lr=0.000599, tokens/sec=1378403.27, grad_norm=0.3421, duration=0.38s
Step 1633: loss=3.7889, lr=0.000599, tokens/sec=1378973.76, grad_norm=0.3710, duration=0.38s
Step 1634: loss=3.8824, lr=0.000599, tokens/sec=1375937.46, grad_norm=0.3894, duration=0.38s
Step 1635: loss=3.8336, lr=0.000599, tokens/sec=1371171.57, grad_norm=0.3523, duration=0.38s
Step 1636: loss=3.8666, lr=0.000599, tokens/sec=1373590.28, grad_norm=0.3167, duration=0.38s
Step 1637: loss=3.8106, lr=0.000599, tokens/sec=1374887.07, grad_norm=0.3220, duration=0.38s
Step 1638: loss=3.8345, lr=0.000599, tokens/sec=1377780.60, grad_norm=0.3074, duration=0.38s
Step 1639: loss=3.8264, lr=0.000599, tokens/sec=1375916.80, grad_norm=0.2677, duration=0.38s
Step 1640: loss=3.7850, lr=0.000599, tokens/sec=1371636.83, grad_norm=0.2813, duration=0.38s
Step 1641: loss=3.7756, lr=0.000599, tokens/sec=1373606.58, grad_norm=0.2940, duration=0.38s
Step 1642: loss=3.8156, lr=0.000599, tokens/sec=1378888.16, grad_norm=0.2939, duration=0.38s
Step 1643: loss=3.7866, lr=0.000599, tokens/sec=1376431.82, grad_norm=0.3488, duration=0.38s
Step 1644: loss=3.8855, lr=0.000599, tokens/sec=1377648.53, grad_norm=0.3982, duration=0.38s
Step 1645: loss=3.8632, lr=0.000599, tokens/sec=1380755.68, grad_norm=0.3880, duration=0.38s
Step 1646: loss=3.8036, lr=0.000599, tokens/sec=1377933.41, grad_norm=0.3387, duration=0.38s
Step 1647: loss=3.7957, lr=0.000599, tokens/sec=1372437.24, grad_norm=0.3553, duration=0.38s
Step 1648: loss=3.7652, lr=0.000599, tokens/sec=1374950.69, grad_norm=0.3577, duration=0.38s
Step 1649: loss=3.7475, lr=0.000599, tokens/sec=1377473.35, grad_norm=0.3732, duration=0.38s
Step 1650: loss=3.8010, lr=0.000599, tokens/sec=1381255.24, grad_norm=0.3273, duration=0.38s
Step 1651: loss=3.7382, lr=0.000599, tokens/sec=1378258.13, grad_norm=0.3265, duration=0.38s
Step 1652: loss=3.7994, lr=0.000599, tokens/sec=1375112.33, grad_norm=0.3025, duration=0.38s
Step 1653: loss=3.7635, lr=0.000599, tokens/sec=1380337.93, grad_norm=0.3165, duration=0.38s
Step 1654: loss=3.7944, lr=0.000599, tokens/sec=1372709.68, grad_norm=0.3204, duration=0.38s
Step 1655: loss=3.8094, lr=0.000599, tokens/sec=1376839.45, grad_norm=0.3354, duration=0.38s
Step 1656: loss=3.7478, lr=0.000599, tokens/sec=1379153.65, grad_norm=0.3123, duration=0.38s
Step 1657: loss=3.7253, lr=0.000599, tokens/sec=1372383.28, grad_norm=0.2886, duration=0.38s
Step 1658: loss=3.7875, lr=0.000599, tokens/sec=1373210.29, grad_norm=0.3214, duration=0.38s
Step 1659: loss=3.8194, lr=0.000599, tokens/sec=1379749.00, grad_norm=0.3449, duration=0.38s
Step 1660: loss=3.8726, lr=0.000599, tokens/sec=1372549.46, grad_norm=0.3592, duration=0.38s
Step 1661: loss=3.8679, lr=0.000599, tokens/sec=1377086.04, grad_norm=0.3883, duration=0.38s
Step 1662: loss=3.8896, lr=0.000599, tokens/sec=1376917.90, grad_norm=0.4500, duration=0.38s
Step 1663: loss=3.8748, lr=0.000599, tokens/sec=1375188.00, grad_norm=0.4731, duration=0.38s
Step 1664: loss=3.9025, lr=0.000599, tokens/sec=1378044.80, grad_norm=0.5164, duration=0.38s
Step 1665: loss=3.8781, lr=0.000599, tokens/sec=1376002.90, grad_norm=0.4940, duration=0.38s
Step 1666: loss=3.8671, lr=0.000599, tokens/sec=1377723.63, grad_norm=0.4094, duration=0.38s
Step 1667: loss=3.9011, lr=0.000599, tokens/sec=1374228.92, grad_norm=0.3587, duration=0.38s
Step 1668: loss=3.9694, lr=0.000599, tokens/sec=1379250.53, grad_norm=0.3259, duration=0.38s
Step 1669: loss=3.9071, lr=0.000599, tokens/sec=1376091.59, grad_norm=0.3334, duration=0.38s
Step 1670: loss=3.8934, lr=0.000599, tokens/sec=1374969.60, grad_norm=0.3278, duration=0.38s
Step 1671: loss=3.8304, lr=0.000599, tokens/sec=1381069.60, grad_norm=0.3497, duration=0.38s
Step 1672: loss=3.8787, lr=0.000599, tokens/sec=1377637.31, grad_norm=0.3286, duration=0.38s
Step 1673: loss=3.8483, lr=0.000599, tokens/sec=1379422.70, grad_norm=0.3218, duration=0.38s
Step 1674: loss=3.8148, lr=0.000599, tokens/sec=1375481.32, grad_norm=0.3346, duration=0.38s
Step 1675: loss=3.8825, lr=0.000599, tokens/sec=1375424.54, grad_norm=0.3676, duration=0.38s
Step 1676: loss=3.8114, lr=0.000599, tokens/sec=1379850.29, grad_norm=0.3442, duration=0.38s
Step 1677: loss=3.8734, lr=0.000599, tokens/sec=1374109.56, grad_norm=0.3380, duration=0.38s
Step 1678: loss=3.9145, lr=0.000599, tokens/sec=1372172.62, grad_norm=0.3658, duration=0.38s
Step 1679: loss=3.8553, lr=0.000599, tokens/sec=1377400.01, grad_norm=0.3629, duration=0.38s
Step 1680: loss=3.7727, lr=0.000599, tokens/sec=1371649.66, grad_norm=0.3429, duration=0.38s
Step 1681: loss=3.8502, lr=0.000599, tokens/sec=1372341.32, grad_norm=0.3553, duration=0.38s
Step 1682: loss=3.8281, lr=0.000599, tokens/sec=1376859.28, grad_norm=0.3328, duration=0.38s
Step 1683: loss=3.8205, lr=0.000599, tokens/sec=1376761.87, grad_norm=0.3615, duration=0.38s
Step 1684: loss=3.7496, lr=0.000599, tokens/sec=1376357.73, grad_norm=0.3351, duration=0.38s
Step 1685: loss=3.7942, lr=0.000599, tokens/sec=1378754.15, grad_norm=0.3042, duration=0.38s
Step 1686: loss=3.7700, lr=0.000599, tokens/sec=1380697.60, grad_norm=0.3294, duration=0.38s
Step 1687: loss=3.8099, lr=0.000599, tokens/sec=1374979.92, grad_norm=0.3984, duration=0.38s
Step 1688: loss=3.8377, lr=0.000599, tokens/sec=1373471.03, grad_norm=0.3725, duration=0.38s
Step 1689: loss=3.7715, lr=0.000599, tokens/sec=1376703.26, grad_norm=0.3376, duration=0.38s
Step 1690: loss=3.7612, lr=0.000599, tokens/sec=1379229.77, grad_norm=0.3290, duration=0.38s
Step 1691: loss=3.7500, lr=0.000599, tokens/sec=1379623.48, grad_norm=0.3365, duration=0.38s
Step 1692: loss=3.7115, lr=0.000599, tokens/sec=1379465.10, grad_norm=0.3385, duration=0.38s
Step 1693: loss=3.7546, lr=0.000599, tokens/sec=1377450.06, grad_norm=0.3415, duration=0.38s
Step 1694: loss=3.7139, lr=0.000599, tokens/sec=1375854.82, grad_norm=0.3418, duration=0.38s
Step 1695: loss=3.7055, lr=0.000599, tokens/sec=1374671.34, grad_norm=0.3288, duration=0.38s
Step 1696: loss=3.6784, lr=0.000599, tokens/sec=1382144.23, grad_norm=0.3012, duration=0.38s
Step 1697: loss=3.7834, lr=0.000598, tokens/sec=1375149.30, grad_norm=0.3185, duration=0.38s
Step 1698: loss=3.7555, lr=0.000598, tokens/sec=1377246.46, grad_norm=0.3227, duration=0.38s
Step 1699: loss=3.7489, lr=0.000598, tokens/sec=1379429.63, grad_norm=0.3940, duration=0.38s
Step 1700/19073 (8.9%), Elapsed time: 726.46s, Steps per hour: 8424.37, Estimated hours remaining: 2.06
Step 1700: loss=3.7096, lr=0.000598, tokens/sec=1376708.43, grad_norm=0.4222, duration=0.38s
Step 1701: loss=3.7235, lr=0.000598, tokens/sec=1377655.44, grad_norm=0.3553, duration=0.38s
Step 1702: loss=3.7716, lr=0.000598, tokens/sec=1377597.62, grad_norm=0.3394, duration=0.38s
Step 1703: loss=3.7450, lr=0.000598, tokens/sec=1378080.21, grad_norm=0.3405, duration=0.38s
Step 1704: loss=3.7835, lr=0.000598, tokens/sec=1379547.32, grad_norm=0.3131, duration=0.38s
Step 1705: loss=3.7874, lr=0.000598, tokens/sec=1373345.79, grad_norm=0.3191, duration=0.38s
Step 1706: loss=3.8517, lr=0.000598, tokens/sec=1376234.55, grad_norm=0.3633, duration=0.38s
Step 1707: loss=3.8551, lr=0.000598, tokens/sec=1379109.54, grad_norm=0.3685, duration=0.38s
Step 1708: loss=3.8296, lr=0.000598, tokens/sec=1373602.29, grad_norm=0.3365, duration=0.38s
Step 1709: loss=3.8771, lr=0.000598, tokens/sec=1372179.47, grad_norm=0.4131, duration=0.38s
Step 1710: loss=3.8521, lr=0.000598, tokens/sec=1374563.93, grad_norm=0.5141, duration=0.38s
Step 1711: loss=3.8876, lr=0.000598, tokens/sec=1376097.62, grad_norm=0.4656, duration=0.38s
Step 1712: loss=3.9167, lr=0.000598, tokens/sec=1372196.59, grad_norm=0.4364, duration=0.38s
Step 1713: loss=3.9131, lr=0.000598, tokens/sec=1377458.68, grad_norm=0.3864, duration=0.38s
Step 1714: loss=3.8963, lr=0.000598, tokens/sec=1379438.28, grad_norm=0.3263, duration=0.38s
Step 1715: loss=3.8914, lr=0.000598, tokens/sec=1377215.41, grad_norm=0.3376, duration=0.38s
Step 1716: loss=3.9795, lr=0.000598, tokens/sec=1376638.62, grad_norm=0.3566, duration=0.38s
Step 1717: loss=3.8731, lr=0.000598, tokens/sec=1377436.25, grad_norm=0.3359, duration=0.38s
Step 1718: loss=3.8967, lr=0.000598, tokens/sec=1373694.96, grad_norm=0.3403, duration=0.38s
Step 1719: loss=3.8967, lr=0.000598, tokens/sec=1370381.17, grad_norm=0.3830, duration=0.38s
Step 1720: loss=3.8787, lr=0.000598, tokens/sec=1373870.04, grad_norm=0.3950, duration=0.38s
Step 1721: loss=3.8425, lr=0.000598, tokens/sec=1375664.60, grad_norm=0.3996, duration=0.38s
Step 1722: loss=3.8468, lr=0.000598, tokens/sec=1374445.37, grad_norm=0.3484, duration=0.38s
Step 1723: loss=3.7929, lr=0.000598, tokens/sec=1374253.83, grad_norm=0.3069, duration=0.38s
Step 1724: loss=3.8153, lr=0.000598, tokens/sec=1378697.97, grad_norm=0.3260, duration=0.38s
Step 1725: loss=3.8301, lr=0.000598, tokens/sec=1377056.72, grad_norm=0.3882, duration=0.38s
Step 1726: loss=3.7861, lr=0.000598, tokens/sec=1374948.97, grad_norm=0.3544, duration=0.38s
Step 1727: loss=3.8418, lr=0.000598, tokens/sec=1372640.28, grad_norm=0.3499, duration=0.38s
Step 1728: loss=3.8274, lr=0.000598, tokens/sec=1376824.79, grad_norm=0.3865, duration=0.38s
Step 1729: loss=3.7065, lr=0.000598, tokens/sec=1373888.07, grad_norm=0.4975, duration=0.38s
Step 1730: loss=3.7845, lr=0.000598, tokens/sec=1382318.86, grad_norm=0.2890, duration=0.38s
Step 1731: loss=3.7862, lr=0.000598, tokens/sec=1376545.55, grad_norm=0.2808, duration=0.38s
Step 1732: loss=3.8868, lr=0.000598, tokens/sec=1376703.26, grad_norm=0.2715, duration=0.38s
Step 1733: loss=3.7625, lr=0.000598, tokens/sec=1378262.45, grad_norm=0.2717, duration=0.38s
Step 1734: loss=3.8109, lr=0.000598, tokens/sec=1377823.76, grad_norm=0.2476, duration=0.38s
Step 1735: loss=3.7914, lr=0.000598, tokens/sec=1376416.31, grad_norm=0.2776, duration=0.38s
Step 1736: loss=3.7210, lr=0.000598, tokens/sec=1377426.76, grad_norm=0.2647, duration=0.38s
Step 1737: loss=3.7549, lr=0.000598, tokens/sec=1375902.17, grad_norm=0.2623, duration=0.38s
Step 1738: loss=3.7534, lr=0.000598, tokens/sec=1374268.43, grad_norm=0.2838, duration=0.38s
Step 1739: loss=3.7671, lr=0.000598, tokens/sec=1375676.65, grad_norm=0.3359, duration=0.38s
Step 1740: loss=3.7692, lr=0.000598, tokens/sec=1375083.95, grad_norm=0.3880, duration=0.38s
Step 1741: loss=3.7614, lr=0.000598, tokens/sec=1374603.46, grad_norm=0.4200, duration=0.38s
Step 1742: loss=3.8162, lr=0.000598, tokens/sec=1377456.96, grad_norm=0.4303, duration=0.38s
Step 1743: loss=3.8377, lr=0.000598, tokens/sec=1379984.51, grad_norm=0.4002, duration=0.38s
Step 1744: loss=3.6907, lr=0.000598, tokens/sec=1377150.72, grad_norm=0.4210, duration=0.38s
Step 1745: loss=3.6692, lr=0.000598, tokens/sec=1378287.50, grad_norm=0.3052, duration=0.38s
Step 1746: loss=3.7563, lr=0.000598, tokens/sec=1377636.45, grad_norm=0.3555, duration=0.38s
Step 1747: loss=3.7363, lr=0.000598, tokens/sec=1378311.69, grad_norm=0.4061, duration=0.38s
Step 1748: loss=3.7208, lr=0.000598, tokens/sec=1377122.26, grad_norm=0.3932, duration=0.38s
Step 1749: loss=3.7537, lr=0.000598, tokens/sec=1380042.53, grad_norm=0.3277, duration=0.38s
Validation loss at step 1750: 3.901888847351074
Step 1750: loss=3.7282, lr=0.000598, tokens/sec=156856.93, grad_norm=0.2981, duration=3.34s
Step 1751: loss=3.7502, lr=0.000598, tokens/sec=1376680.85, grad_norm=0.3092, duration=0.38s
Step 1752: loss=3.7602, lr=0.000598, tokens/sec=1379440.01, grad_norm=0.3100, duration=0.38s
Step 1753: loss=3.7714, lr=0.000598, tokens/sec=1378367.85, grad_norm=0.3116, duration=0.38s
Step 1754: loss=3.7297, lr=0.000598, tokens/sec=1380013.09, grad_norm=0.3176, duration=0.38s
Step 1755: loss=3.7418, lr=0.000598, tokens/sec=1381301.22, grad_norm=0.3513, duration=0.38s
Step 1756: loss=3.7368, lr=0.000598, tokens/sec=1377193.85, grad_norm=0.3457, duration=0.38s
Step 1757: loss=3.8553, lr=0.000598, tokens/sec=1375686.98, grad_norm=0.3395, duration=0.38s
Step 1758: loss=3.8982, lr=0.000598, tokens/sec=1378500.05, grad_norm=0.3383, duration=0.38s
Step 1759: loss=3.8272, lr=0.000598, tokens/sec=1376114.84, grad_norm=0.3293, duration=0.38s
Step 1760: loss=3.8385, lr=0.000598, tokens/sec=1374433.34, grad_norm=0.3940, duration=0.38s
Step 1761: loss=3.8466, lr=0.000598, tokens/sec=1376033.90, grad_norm=0.3634, duration=0.38s
Step 1762: loss=3.8532, lr=0.000598, tokens/sec=1376685.16, grad_norm=0.3452, duration=0.38s
Step 1763: loss=3.8604, lr=0.000598, tokens/sec=1375000.55, grad_norm=0.4067, duration=0.38s
Step 1764: loss=3.7836, lr=0.000598, tokens/sec=1371873.01, grad_norm=0.3780, duration=0.38s
Step 1765: loss=3.8174, lr=0.000598, tokens/sec=1375676.65, grad_norm=0.3620, duration=0.38s
Step 1766: loss=3.8587, lr=0.000598, tokens/sec=1376178.57, grad_norm=0.3196, duration=0.38s
Step 1767: loss=3.8064, lr=0.000598, tokens/sec=1376734.29, grad_norm=0.3106, duration=0.38s
Step 1768: loss=3.8076, lr=0.000598, tokens/sec=1371113.43, grad_norm=0.3128, duration=0.38s
Step 1769: loss=3.7861, lr=0.000598, tokens/sec=1369896.27, grad_norm=0.3058, duration=0.38s
Step 1770: loss=3.7807, lr=0.000598, tokens/sec=1373356.08, grad_norm=0.3105, duration=0.38s
Step 1771: loss=3.7267, lr=0.000598, tokens/sec=1375803.17, grad_norm=0.2887, duration=0.38s
Step 1772: loss=3.7547, lr=0.000598, tokens/sec=1375785.96, grad_norm=0.2667, duration=0.38s
Step 1773: loss=3.7974, lr=0.000598, tokens/sec=1376307.76, grad_norm=0.2729, duration=0.38s
Step 1774: loss=3.8007, lr=0.000598, tokens/sec=1377412.96, grad_norm=0.2882, duration=0.38s
Step 1775: loss=3.8412, lr=0.000598, tokens/sec=1376398.22, grad_norm=0.2960, duration=0.38s
Step 1776: loss=3.7572, lr=0.000598, tokens/sec=1378300.46, grad_norm=0.3104, duration=0.38s
Step 1777: loss=3.8583, lr=0.000598, tokens/sec=1370919.40, grad_norm=0.3373, duration=0.38s
Step 1778: loss=3.8347, lr=0.000598, tokens/sec=1373314.06, grad_norm=0.3666, duration=0.38s
Step 1779: loss=3.7534, lr=0.000598, tokens/sec=1374386.96, grad_norm=0.3478, duration=0.38s
Step 1780: loss=3.7617, lr=0.000598, tokens/sec=1376831.69, grad_norm=0.3567, duration=0.38s
Step 1781: loss=3.7665, lr=0.000598, tokens/sec=1381309.03, grad_norm=0.3255, duration=0.38s
Step 1782: loss=3.7557, lr=0.000598, tokens/sec=1378856.17, grad_norm=0.3169, duration=0.38s
Step 1783: loss=3.7646, lr=0.000598, tokens/sec=1373967.90, grad_norm=0.3275, duration=0.38s
Step 1784: loss=3.7194, lr=0.000598, tokens/sec=1377290.45, grad_norm=0.3439, duration=0.38s
Step 1785: loss=3.7659, lr=0.000598, tokens/sec=1378252.09, grad_norm=0.4120, duration=0.38s
Step 1786: loss=3.7501, lr=0.000598, tokens/sec=1379961.13, grad_norm=0.4244, duration=0.38s
Step 1787: loss=3.7092, lr=0.000598, tokens/sec=1379292.06, grad_norm=0.3691, duration=0.38s
Step 1788: loss=3.7652, lr=0.000598, tokens/sec=1376092.45, grad_norm=0.3587, duration=0.38s
Step 1789: loss=3.7541, lr=0.000598, tokens/sec=1377772.83, grad_norm=0.3721, duration=0.38s
Step 1790: loss=3.7175, lr=0.000598, tokens/sec=1377473.35, grad_norm=0.3901, duration=0.38s
Step 1791: loss=3.6789, lr=0.000598, tokens/sec=1379364.73, grad_norm=0.3422, duration=0.38s
Step 1792: loss=3.7274, lr=0.000598, tokens/sec=1377219.72, grad_norm=0.2629, duration=0.38s
Step 1793: loss=3.7154, lr=0.000598, tokens/sec=1379671.95, grad_norm=0.2973, duration=0.38s
Step 1794: loss=3.7293, lr=0.000598, tokens/sec=1378825.04, grad_norm=0.3057, duration=0.38s
Step 1795: loss=3.6701, lr=0.000598, tokens/sec=1379077.54, grad_norm=0.3053, duration=0.38s
Step 1796: loss=3.7058, lr=0.000598, tokens/sec=1378638.33, grad_norm=0.2950, duration=0.38s
Step 1797: loss=3.7321, lr=0.000598, tokens/sec=1374003.10, grad_norm=0.2935, duration=0.38s
Step 1798: loss=3.6739, lr=0.000598, tokens/sec=1377676.15, grad_norm=0.3100, duration=0.38s
Step 1799: loss=3.6715, lr=0.000598, tokens/sec=1377571.73, grad_norm=0.3023, duration=0.38s
Step 1800/19073 (9.4%), Elapsed time: 767.59s, Steps per hour: 8442.06, Estimated hours remaining: 2.05
Step 1800: loss=3.6950, lr=0.000598, tokens/sec=1381003.68, grad_norm=0.3050, duration=0.38s
Step 1801: loss=3.7561, lr=0.000598, tokens/sec=1378057.75, grad_norm=0.3684, duration=0.38s
Step 1802: loss=3.7222, lr=0.000598, tokens/sec=1378882.10, grad_norm=0.3532, duration=0.38s
Step 1803: loss=3.8079, lr=0.000598, tokens/sec=1374839.79, grad_norm=0.3129, duration=0.38s
Step 1804: loss=3.8172, lr=0.000598, tokens/sec=1377759.88, grad_norm=0.3119, duration=0.38s
Step 1805: loss=3.8404, lr=0.000598, tokens/sec=1376621.38, grad_norm=0.3151, duration=0.38s
Step 1806: loss=3.8221, lr=0.000598, tokens/sec=1378077.61, grad_norm=0.3050, duration=0.38s
Step 1807: loss=3.8169, lr=0.000598, tokens/sec=1378142.39, grad_norm=0.3502, duration=0.38s
Step 1808: loss=3.7908, lr=0.000598, tokens/sec=1374315.67, grad_norm=0.3737, duration=0.38s
Step 1809: loss=3.7772, lr=0.000598, tokens/sec=1376052.84, grad_norm=0.3347, duration=0.38s
Step 1810: loss=3.7660, lr=0.000598, tokens/sec=1376290.54, grad_norm=0.3174, duration=0.38s
Step 1811: loss=3.8060, lr=0.000598, tokens/sec=1375627.60, grad_norm=0.3455, duration=0.38s
Step 1812: loss=3.8100, lr=0.000598, tokens/sec=1375437.45, grad_norm=0.4289, duration=0.38s
Step 1813: loss=3.7947, lr=0.000598, tokens/sec=1373506.20, grad_norm=0.4374, duration=0.38s
Step 1814: loss=3.8067, lr=0.000598, tokens/sec=1378235.67, grad_norm=0.3992, duration=0.38s
Step 1815: loss=3.7783, lr=0.000598, tokens/sec=1379305.90, grad_norm=0.4065, duration=0.38s
Step 1816: loss=3.7923, lr=0.000598, tokens/sec=1377641.63, grad_norm=0.3994, duration=0.38s
Step 1817: loss=3.7549, lr=0.000598, tokens/sec=1375157.04, grad_norm=0.3350, duration=0.38s
Step 1818: loss=3.7988, lr=0.000598, tokens/sec=1378586.47, grad_norm=0.3312, duration=0.38s
Step 1819: loss=3.7383, lr=0.000598, tokens/sec=1371795.13, grad_norm=0.2920, duration=0.38s
Step 1820: loss=3.7743, lr=0.000598, tokens/sec=1374265.85, grad_norm=0.2983, duration=0.38s
Step 1821: loss=3.7888, lr=0.000598, tokens/sec=1372681.40, grad_norm=0.2817, duration=0.38s
Step 1822: loss=3.7377, lr=0.000598, tokens/sec=1371016.83, grad_norm=0.2909, duration=0.38s
Step 1823: loss=3.7088, lr=0.000598, tokens/sec=1378767.98, grad_norm=0.3003, duration=0.38s
Step 1824: loss=3.7839, lr=0.000598, tokens/sec=1372416.69, grad_norm=0.3319, duration=0.38s
Step 1825: loss=3.7508, lr=0.000598, tokens/sec=1372524.62, grad_norm=0.3263, duration=0.38s
Step 1826: loss=3.7649, lr=0.000598, tokens/sec=1375946.93, grad_norm=0.3625, duration=0.38s
Step 1827: loss=3.7408, lr=0.000598, tokens/sec=1372881.94, grad_norm=0.3465, duration=0.38s
Step 1828: loss=3.7674, lr=0.000598, tokens/sec=1377529.44, grad_norm=0.3308, duration=0.38s
Step 1829: loss=3.7055, lr=0.000598, tokens/sec=1379248.80, grad_norm=0.2953, duration=0.38s
Step 1830: loss=3.7058, lr=0.000598, tokens/sec=1376517.98, grad_norm=0.2937, duration=0.38s
Step 1831: loss=3.7161, lr=0.000598, tokens/sec=1377197.30, grad_norm=0.3025, duration=0.38s
Step 1832: loss=3.7257, lr=0.000598, tokens/sec=1376652.41, grad_norm=0.2976, duration=0.38s
Step 1833: loss=3.7791, lr=0.000598, tokens/sec=1378013.71, grad_norm=0.3057, duration=0.38s
Step 1834: loss=3.7653, lr=0.000598, tokens/sec=1374712.59, grad_norm=0.3380, duration=0.38s
Step 1835: loss=3.7401, lr=0.000598, tokens/sec=1376751.52, grad_norm=0.3673, duration=0.38s
Step 1836: loss=3.7337, lr=0.000598, tokens/sec=1376885.14, grad_norm=0.3620, duration=0.38s
Step 1837: loss=3.6920, lr=0.000598, tokens/sec=1374340.57, grad_norm=0.3604, duration=0.38s
Step 1838: loss=3.7197, lr=0.000598, tokens/sec=1377939.45, grad_norm=0.3679, duration=0.38s
Step 1839: loss=3.6738, lr=0.000598, tokens/sec=1378965.11, grad_norm=0.3791, duration=0.38s
Step 1840: loss=3.7321, lr=0.000598, tokens/sec=1378759.34, grad_norm=0.3151, duration=0.38s
Step 1841: loss=3.6746, lr=0.000598, tokens/sec=1375350.56, grad_norm=0.3092, duration=0.38s
Step 1842: loss=3.7042, lr=0.000598, tokens/sec=1376865.31, grad_norm=0.3347, duration=0.38s
Step 1843: loss=3.6460, lr=0.000598, tokens/sec=1378201.99, grad_norm=0.3317, duration=0.38s
Step 1844: loss=3.7467, lr=0.000598, tokens/sec=1376913.59, grad_norm=0.2858, duration=0.38s
Step 1845: loss=3.6920, lr=0.000598, tokens/sec=1377799.59, grad_norm=0.2827, duration=0.38s
Step 1846: loss=3.6534, lr=0.000598, tokens/sec=1377691.69, grad_norm=0.2800, duration=0.38s
Step 1847: loss=3.6721, lr=0.000598, tokens/sec=1378201.12, grad_norm=0.3031, duration=0.38s
Step 1848: loss=3.6877, lr=0.000598, tokens/sec=1374610.33, grad_norm=0.3247, duration=0.38s
Step 1849: loss=3.7251, lr=0.000598, tokens/sec=1380702.80, grad_norm=0.3420, duration=0.38s
Step 1850: loss=3.7860, lr=0.000598, tokens/sec=1378787.87, grad_norm=0.3886, duration=0.38s
Step 1851: loss=3.7819, lr=0.000598, tokens/sec=1374709.15, grad_norm=0.3749, duration=0.38s
Step 1852: loss=3.8111, lr=0.000598, tokens/sec=1376569.68, grad_norm=0.3663, duration=0.38s
Step 1853: loss=3.7805, lr=0.000598, tokens/sec=1378167.44, grad_norm=0.3486, duration=0.38s
Step 1854: loss=3.8250, lr=0.000598, tokens/sec=1376924.80, grad_norm=0.3345, duration=0.38s
Step 1855: loss=3.7458, lr=0.000598, tokens/sec=1376766.18, grad_norm=0.3452, duration=0.38s
Step 1856: loss=3.7600, lr=0.000598, tokens/sec=1378276.27, grad_norm=0.3474, duration=0.38s
Step 1857: loss=3.8185, lr=0.000598, tokens/sec=1375837.60, grad_norm=0.3708, duration=0.38s
Step 1858: loss=3.8953, lr=0.000598, tokens/sec=1377705.50, grad_norm=0.3874, duration=0.38s
Step 1859: loss=3.8223, lr=0.000598, tokens/sec=1377361.19, grad_norm=0.3745, duration=0.38s
Step 1860: loss=3.8168, lr=0.000598, tokens/sec=1377142.96, grad_norm=0.4245, duration=0.38s
Step 1861: loss=3.7650, lr=0.000598, tokens/sec=1376706.70, grad_norm=0.3629, duration=0.38s
Step 1862: loss=3.7836, lr=0.000598, tokens/sec=1375639.65, grad_norm=0.3686, duration=0.38s
Step 1863: loss=3.7508, lr=0.000598, tokens/sec=1374050.32, grad_norm=0.3158, duration=0.38s
Step 1864: loss=3.7323, lr=0.000598, tokens/sec=1377525.99, grad_norm=0.3387, duration=0.38s
Step 1865: loss=3.8044, lr=0.000598, tokens/sec=1378649.56, grad_norm=0.2998, duration=0.38s
Step 1866: loss=3.7065, lr=0.000598, tokens/sec=1373102.25, grad_norm=0.2917, duration=0.38s
Step 1867: loss=3.7838, lr=0.000598, tokens/sec=1376341.36, grad_norm=0.2675, duration=0.38s
Step 1868: loss=3.8346, lr=0.000598, tokens/sec=1375715.38, grad_norm=0.2638, duration=0.38s
Step 1869: loss=3.7617, lr=0.000598, tokens/sec=1380805.10, grad_norm=0.2696, duration=0.38s
Step 1870: loss=3.6736, lr=0.000598, tokens/sec=1373527.65, grad_norm=0.2722, duration=0.38s
Step 1871: loss=3.7576, lr=0.000598, tokens/sec=1377188.67, grad_norm=0.2944, duration=0.38s
Step 1872: loss=3.7766, lr=0.000598, tokens/sec=1379720.43, grad_norm=0.2883, duration=0.38s
Step 1873: loss=3.7326, lr=0.000598, tokens/sec=1382153.78, grad_norm=0.2873, duration=0.38s
Step 1874: loss=3.6331, lr=0.000598, tokens/sec=1378113.02, grad_norm=0.2686, duration=0.38s
Step 1875: loss=3.7029, lr=0.000598, tokens/sec=1377880.74, grad_norm=0.2718, duration=0.38s
Step 1876: loss=3.6948, lr=0.000598, tokens/sec=1375112.33, grad_norm=0.3014, duration=0.38s
Step 1877: loss=3.7140, lr=0.000598, tokens/sec=1380550.24, grad_norm=0.3018, duration=0.38s
Step 1878: loss=3.7687, lr=0.000598, tokens/sec=1380400.32, grad_norm=0.2907, duration=0.38s
Step 1879: loss=3.6682, lr=0.000598, tokens/sec=1377544.11, grad_norm=0.3027, duration=0.38s
Step 1880: loss=3.6636, lr=0.000598, tokens/sec=1371010.85, grad_norm=0.3215, duration=0.38s
Step 1881: loss=3.6836, lr=0.000598, tokens/sec=1375634.48, grad_norm=0.3301, duration=0.38s
Step 1882: loss=3.6249, lr=0.000598, tokens/sec=1376473.17, grad_norm=0.3661, duration=0.38s
Step 1883: loss=3.6895, lr=0.000598, tokens/sec=1378120.80, grad_norm=0.3678, duration=0.38s
Step 1884: loss=3.6405, lr=0.000598, tokens/sec=1376736.01, grad_norm=0.3850, duration=0.38s
Step 1885: loss=3.6100, lr=0.000598, tokens/sec=1376524.87, grad_norm=0.3806, duration=0.38s
Step 1886: loss=3.6319, lr=0.000598, tokens/sec=1376622.24, grad_norm=0.3262, duration=0.38s
Step 1887: loss=3.7432, lr=0.000598, tokens/sec=1376082.12, grad_norm=0.3255, duration=0.38s
Step 1888: loss=3.6820, lr=0.000598, tokens/sec=1376961.01, grad_norm=0.3375, duration=0.38s
Step 1889: loss=3.6662, lr=0.000598, tokens/sec=1379654.64, grad_norm=0.3150, duration=0.38s
Step 1890: loss=3.5955, lr=0.000598, tokens/sec=1376087.28, grad_norm=0.3197, duration=0.38s
Step 1891: loss=3.6688, lr=0.000598, tokens/sec=1375624.16, grad_norm=0.3577, duration=0.38s
Step 1892: loss=3.6817, lr=0.000598, tokens/sec=1379775.83, grad_norm=0.3774, duration=0.38s
Step 1893: loss=3.6862, lr=0.000598, tokens/sec=1377143.82, grad_norm=0.3714, duration=0.38s
Step 1894: loss=3.6542, lr=0.000598, tokens/sec=1376417.17, grad_norm=0.3111, duration=0.38s
Step 1895: loss=3.7534, lr=0.000598, tokens/sec=1376794.62, grad_norm=0.2866, duration=0.38s
Step 1896: loss=3.7500, lr=0.000598, tokens/sec=1378272.82, grad_norm=0.2902, duration=0.38s
Step 1897: loss=3.7893, lr=0.000598, tokens/sec=1376599.84, grad_norm=0.2962, duration=0.38s
Step 1898: loss=3.7534, lr=0.000598, tokens/sec=1377482.84, grad_norm=0.3046, duration=0.38s
Step 1899: loss=3.7700, lr=0.000598, tokens/sec=1376944.63, grad_norm=0.3636, duration=0.38s
Step 1900/19073 (10.0%), Elapsed time: 805.74s, Steps per hour: 8489.08, Estimated hours remaining: 2.02
Step 1900: loss=3.7558, lr=0.000598, tokens/sec=1377337.90, grad_norm=0.4239, duration=0.38s
Step 1901: loss=3.8188, lr=0.000598, tokens/sec=1374014.26, grad_norm=0.4260, duration=0.38s
Step 1902: loss=3.7739, lr=0.000598, tokens/sec=1377495.79, grad_norm=0.4002, duration=0.38s
Step 1903: loss=3.8845, lr=0.000598, tokens/sec=1378461.16, grad_norm=0.4103, duration=0.38s
Step 1904: loss=3.7881, lr=0.000598, tokens/sec=1377942.90, grad_norm=0.3682, duration=0.38s
Step 1905: loss=3.8445, lr=0.000598, tokens/sec=1375864.29, grad_norm=0.4052, duration=0.38s
Step 1906: loss=3.8737, lr=0.000598, tokens/sec=1377980.90, grad_norm=0.3545, duration=0.38s
Step 1907: loss=3.7867, lr=0.000598, tokens/sec=1379055.05, grad_norm=0.3076, duration=0.38s
Step 1908: loss=3.8467, lr=0.000598, tokens/sec=1378895.94, grad_norm=0.3731, duration=0.38s
Step 1909: loss=3.7840, lr=0.000598, tokens/sec=1371390.48, grad_norm=0.3767, duration=0.38s
Step 1910: loss=3.8133, lr=0.000598, tokens/sec=1379421.84, grad_norm=0.3877, duration=0.38s
Step 1911: loss=3.7366, lr=0.000598, tokens/sec=1374912.86, grad_norm=0.3292, duration=0.38s
Step 1912: loss=3.7762, lr=0.000598, tokens/sec=1372023.65, grad_norm=0.2900, duration=0.38s
Step 1913: loss=3.6861, lr=0.000598, tokens/sec=1375853.96, grad_norm=0.2940, duration=0.38s
Step 1914: loss=3.7410, lr=0.000598, tokens/sec=1371861.02, grad_norm=0.3140, duration=0.38s
Step 1915: loss=3.7321, lr=0.000597, tokens/sec=1371523.05, grad_norm=0.3125, duration=0.38s
Step 1916: loss=3.7132, lr=0.000597, tokens/sec=1371901.25, grad_norm=0.2851, duration=0.38s
Step 1917: loss=3.7911, lr=0.000597, tokens/sec=1372371.29, grad_norm=0.2912, duration=0.38s
Step 1918: loss=3.7034, lr=0.000597, tokens/sec=1370021.73, grad_norm=0.3157, duration=0.38s
Step 1919: loss=3.6223, lr=0.000597, tokens/sec=1376366.34, grad_norm=0.5370, duration=0.38s
Step 1920: loss=3.7101, lr=0.000597, tokens/sec=1374142.19, grad_norm=0.2808, duration=0.38s
Step 1921: loss=3.7250, lr=0.000597, tokens/sec=1376676.54, grad_norm=0.2918, duration=0.38s
Step 1922: loss=3.8348, lr=0.000597, tokens/sec=1375158.76, grad_norm=0.3358, duration=0.38s
Step 1923: loss=3.6541, lr=0.000597, tokens/sec=1375715.38, grad_norm=0.3316, duration=0.38s
Step 1924: loss=3.7573, lr=0.000597, tokens/sec=1376462.83, grad_norm=0.3473, duration=0.38s
Step 1925: loss=3.6915, lr=0.000597, tokens/sec=1374161.94, grad_norm=0.3711, duration=0.38s
Step 1926: loss=3.6754, lr=0.000597, tokens/sec=1378585.60, grad_norm=0.3641, duration=0.38s
Step 1927: loss=3.6516, lr=0.000597, tokens/sec=1378534.62, grad_norm=0.3142, duration=0.38s
Step 1928: loss=3.7275, lr=0.000597, tokens/sec=1379806.14, grad_norm=0.3384, duration=0.38s
Step 1929: loss=3.6821, lr=0.000597, tokens/sec=1375013.45, grad_norm=0.3403, duration=0.38s
Step 1930: loss=3.6696, lr=0.000597, tokens/sec=1376421.48, grad_norm=0.3323, duration=0.38s
Step 1931: loss=3.7204, lr=0.000597, tokens/sec=1374565.65, grad_norm=0.3114, duration=0.38s
Step 1932: loss=3.7407, lr=0.000597, tokens/sec=1376180.29, grad_norm=0.5351, duration=0.38s
Step 1933: loss=3.7079, lr=0.000597, tokens/sec=1378140.66, grad_norm=0.4089, duration=0.38s
Step 1934: loss=3.6593, lr=0.000597, tokens/sec=1377895.42, grad_norm=0.3391, duration=0.38s
Step 1935: loss=3.6112, lr=0.000597, tokens/sec=1376007.20, grad_norm=0.3582, duration=0.38s
Step 1936: loss=3.7293, lr=0.000597, tokens/sec=1374079.51, grad_norm=0.3609, duration=0.38s
Step 1937: loss=3.6544, lr=0.000597, tokens/sec=1377202.47, grad_norm=0.3682, duration=0.38s
Step 1938: loss=3.6483, lr=0.000597, tokens/sec=1374282.17, grad_norm=0.3534, duration=0.38s
Step 1939: loss=3.6564, lr=0.000597, tokens/sec=1377083.45, grad_norm=0.3135, duration=0.38s
Step 1940: loss=3.6661, lr=0.000597, tokens/sec=1375181.98, grad_norm=0.3346, duration=0.38s
Step 1941: loss=3.7083, lr=0.000597, tokens/sec=1375318.73, grad_norm=0.3212, duration=0.38s
Step 1942: loss=3.6747, lr=0.000597, tokens/sec=1377866.93, grad_norm=0.3055, duration=0.38s
Step 1943: loss=3.6818, lr=0.000597, tokens/sec=1371865.30, grad_norm=0.3075, duration=0.38s
Step 1944: loss=3.6720, lr=0.000597, tokens/sec=1374600.02, grad_norm=0.2748, duration=0.38s
Step 1945: loss=3.6448, lr=0.000597, tokens/sec=1372922.22, grad_norm=0.2871, duration=0.38s
Step 1946: loss=3.6643, lr=0.000597, tokens/sec=1378382.54, grad_norm=0.2810, duration=0.38s
Step 1947: loss=3.7759, lr=0.000597, tokens/sec=1379251.40, grad_norm=0.2821, duration=0.38s
Step 1948: loss=3.7948, lr=0.000597, tokens/sec=1374275.30, grad_norm=0.3070, duration=0.38s
Step 1949: loss=3.7567, lr=0.000597, tokens/sec=1380449.71, grad_norm=0.3602, duration=0.38s
Step 1950: loss=3.7522, lr=0.000597, tokens/sec=1366929.52, grad_norm=0.3588, duration=0.38s
Step 1951: loss=3.7729, lr=0.000597, tokens/sec=1376577.43, grad_norm=0.3292, duration=0.38s
Step 1952: loss=3.7925, lr=0.000597, tokens/sec=1376777.38, grad_norm=0.3099, duration=0.38s
Step 1953: loss=3.7511, lr=0.000597, tokens/sec=1379255.72, grad_norm=0.3142, duration=0.38s
Step 1954: loss=3.7194, lr=0.000597, tokens/sec=1378582.15, grad_norm=0.3397, duration=0.38s
Step 1955: loss=3.7721, lr=0.000597, tokens/sec=1377762.47, grad_norm=0.3625, duration=0.38s
Step 1956: loss=3.7371, lr=0.000597, tokens/sec=1375668.05, grad_norm=0.3418, duration=0.38s
Step 1957: loss=3.7510, lr=0.000597, tokens/sec=1376650.68, grad_norm=0.3330, duration=0.38s
Step 1958: loss=3.7166, lr=0.000597, tokens/sec=1378837.15, grad_norm=0.3226, duration=0.38s
Step 1959: loss=3.6919, lr=0.000597, tokens/sec=1375523.48, grad_norm=0.3373, duration=0.38s
Step 1960: loss=3.7162, lr=0.000597, tokens/sec=1378942.63, grad_norm=0.3294, duration=0.38s
Step 1961: loss=3.6492, lr=0.000597, tokens/sec=1377131.75, grad_norm=0.2846, duration=0.38s
Step 1962: loss=3.6841, lr=0.000597, tokens/sec=1376965.32, grad_norm=0.2886, duration=0.38s
Step 1963: loss=3.7380, lr=0.000597, tokens/sec=1375146.72, grad_norm=0.3044, duration=0.38s
Step 1964: loss=3.7079, lr=0.000597, tokens/sec=1378932.25, grad_norm=0.2922, duration=0.38s
Step 1965: loss=3.7600, lr=0.000597, tokens/sec=1378162.25, grad_norm=0.2986, duration=0.38s
Step 1966: loss=3.6917, lr=0.000597, tokens/sec=1379867.61, grad_norm=0.3121, duration=0.38s
Step 1967: loss=3.8095, lr=0.000597, tokens/sec=1377560.51, grad_norm=0.2975, duration=0.38s
Step 1968: loss=3.7134, lr=0.000597, tokens/sec=1378824.18, grad_norm=0.3577, duration=0.38s
Step 1969: loss=3.6831, lr=0.000597, tokens/sec=1372259.10, grad_norm=0.4207, duration=0.38s
Step 1970: loss=3.7087, lr=0.000597, tokens/sec=1372510.05, grad_norm=0.4448, duration=0.38s
Step 1971: loss=3.7160, lr=0.000597, tokens/sec=1377413.82, grad_norm=0.4213, duration=0.38s
Step 1972: loss=3.6853, lr=0.000597, tokens/sec=1375360.02, grad_norm=0.3821, duration=0.38s
Step 1973: loss=3.6680, lr=0.000597, tokens/sec=1375999.45, grad_norm=0.3331, duration=0.38s
Step 1974: loss=3.6733, lr=0.000597, tokens/sec=1374633.53, grad_norm=0.3288, duration=0.38s
Step 1975: loss=3.6802, lr=0.000597, tokens/sec=1379721.30, grad_norm=0.3029, duration=0.38s
Step 1976: loss=3.6632, lr=0.000597, tokens/sec=1374458.26, grad_norm=0.2818, duration=0.38s
Step 1977: loss=3.6476, lr=0.000597, tokens/sec=1374252.97, grad_norm=0.3028, duration=0.38s
Step 1978: loss=3.6843, lr=0.000597, tokens/sec=1377828.08, grad_norm=0.3126, duration=0.38s
Step 1979: loss=3.6254, lr=0.000597, tokens/sec=1380784.29, grad_norm=0.3066, duration=0.38s
Step 1980: loss=3.6824, lr=0.000597, tokens/sec=1375185.42, grad_norm=0.2970, duration=0.38s
Step 1981: loss=3.5963, lr=0.000597, tokens/sec=1376595.53, grad_norm=0.3186, duration=0.38s
Step 1982: loss=3.7090, lr=0.000597, tokens/sec=1377558.78, grad_norm=0.3499, duration=0.38s
Step 1983: loss=3.6474, lr=0.000597, tokens/sec=1377570.86, grad_norm=0.3202, duration=0.38s
Step 1984: loss=3.6558, lr=0.000597, tokens/sec=1374997.97, grad_norm=0.3014, duration=0.38s
Step 1985: loss=3.5906, lr=0.000597, tokens/sec=1376873.93, grad_norm=0.3163, duration=0.38s
Step 1986: loss=3.6368, lr=0.000597, tokens/sec=1378042.21, grad_norm=0.3062, duration=0.38s
Step 1987: loss=3.6484, lr=0.000597, tokens/sec=1375515.74, grad_norm=0.2881, duration=0.38s
Step 1988: loss=3.5757, lr=0.000597, tokens/sec=1374673.06, grad_norm=0.2735, duration=0.38s
Step 1989: loss=3.6456, lr=0.000597, tokens/sec=1373931.84, grad_norm=0.2827, duration=0.38s
Step 1990: loss=3.6230, lr=0.000597, tokens/sec=1374687.67, grad_norm=0.2761, duration=0.38s
Step 1991: loss=3.6573, lr=0.000597, tokens/sec=1376061.45, grad_norm=0.2512, duration=0.38s
Step 1992: loss=3.6768, lr=0.000597, tokens/sec=1379910.90, grad_norm=0.2527, duration=0.38s
Step 1993: loss=3.6904, lr=0.000597, tokens/sec=1375132.11, grad_norm=0.2942, duration=0.38s
Step 1994: loss=3.7613, lr=0.000597, tokens/sec=1379047.27, grad_norm=0.3751, duration=0.38s
Step 1995: loss=3.7601, lr=0.000597, tokens/sec=1377255.09, grad_norm=0.4140, duration=0.38s
Step 1996: loss=3.7544, lr=0.000597, tokens/sec=1379837.30, grad_norm=0.3664, duration=0.38s
Step 1997: loss=3.7602, lr=0.000597, tokens/sec=1376842.03, grad_norm=0.3799, duration=0.38s
Step 1998: loss=3.6715, lr=0.000597, tokens/sec=1372943.65, grad_norm=0.4107, duration=0.38s
Step 1999: loss=3.7303, lr=0.000597, tokens/sec=1375736.90, grad_norm=0.4201, duration=0.38s
Step 2000/19073 (10.5%), Elapsed time: 843.92s, Steps per hour: 8531.65, Estimated hours remaining: 2.00
Validation loss at step 2000: 3.8342227935791016
Step 2000: loss=3.6886, lr=0.000597, tokens/sec=154829.74, grad_norm=0.3411, duration=3.39s
Step 2001: loss=3.7507, lr=0.000597, tokens/sec=1379389.82, grad_norm=0.3269, duration=0.38s
Step 2002: loss=3.7250, lr=0.000597, tokens/sec=1379408.86, grad_norm=0.3197, duration=0.38s
Step 2003: loss=3.7003, lr=0.000597, tokens/sec=1378563.13, grad_norm=0.2823, duration=0.38s
Step 2004: loss=3.7344, lr=0.000597, tokens/sec=1370480.24, grad_norm=0.3152, duration=0.38s
Step 2005: loss=3.6959, lr=0.000597, tokens/sec=1377075.69, grad_norm=0.3362, duration=0.38s
Step 2006: loss=3.7125, lr=0.000597, tokens/sec=1375658.58, grad_norm=0.3418, duration=0.38s
Step 2007: loss=3.6906, lr=0.000597, tokens/sec=1378805.16, grad_norm=0.3444, duration=0.38s
Step 2008: loss=3.7273, lr=0.000597, tokens/sec=1378243.45, grad_norm=0.3344, duration=0.38s
Step 2009: loss=3.6588, lr=0.000597, tokens/sec=1375705.05, grad_norm=0.3200, duration=0.38s
Step 2010: loss=3.7263, lr=0.000597, tokens/sec=1375717.96, grad_norm=0.2672, duration=0.38s
Step 2011: loss=3.6687, lr=0.000597, tokens/sec=1377927.36, grad_norm=0.2895, duration=0.38s
Step 2012: loss=3.6810, lr=0.000597, tokens/sec=1375651.70, grad_norm=0.2957, duration=0.38s
Step 2013: loss=3.6292, lr=0.000597, tokens/sec=1377522.54, grad_norm=0.2741, duration=0.38s
Step 2014: loss=3.7175, lr=0.000597, tokens/sec=1377859.16, grad_norm=0.2757, duration=0.38s
Step 2015: loss=3.6560, lr=0.000597, tokens/sec=1377412.96, grad_norm=0.2824, duration=0.38s
Step 2016: loss=3.7024, lr=0.000597, tokens/sec=1379093.97, grad_norm=0.2872, duration=0.38s
Step 2017: loss=3.6787, lr=0.000597, tokens/sec=1378885.56, grad_norm=0.3084, duration=0.38s
Step 2018: loss=3.6563, lr=0.000597, tokens/sec=1378136.34, grad_norm=0.3030, duration=0.38s
Step 2019: loss=3.6357, lr=0.000597, tokens/sec=1378821.58, grad_norm=0.3320, duration=0.38s
Step 2020: loss=3.6549, lr=0.000597, tokens/sec=1373815.11, grad_norm=0.3317, duration=0.38s
Step 2021: loss=3.6396, lr=0.000597, tokens/sec=1370010.64, grad_norm=0.3255, duration=0.38s
Step 2022: loss=3.7437, lr=0.000597, tokens/sec=1373848.58, grad_norm=0.3848, duration=0.38s
Step 2023: loss=3.6946, lr=0.000597, tokens/sec=1378077.61, grad_norm=0.4418, duration=0.38s
Step 2024: loss=3.6667, lr=0.000597, tokens/sec=1371205.77, grad_norm=0.3569, duration=0.38s
Step 2025: loss=3.6818, lr=0.000597, tokens/sec=1376151.87, grad_norm=0.3120, duration=0.38s
Step 2026: loss=3.6384, lr=0.000597, tokens/sec=1373272.89, grad_norm=0.3225, duration=0.38s
Step 2027: loss=3.6557, lr=0.000597, tokens/sec=1375182.84, grad_norm=0.3240, duration=0.38s
Step 2028: loss=3.6552, lr=0.000597, tokens/sec=1371384.49, grad_norm=0.3016, duration=0.38s
Step 2029: loss=3.6165, lr=0.000597, tokens/sec=1376754.11, grad_norm=0.3115, duration=0.38s
Step 2030: loss=3.6874, lr=0.000597, tokens/sec=1373113.40, grad_norm=0.3144, duration=0.38s
Step 2031: loss=3.5896, lr=0.000597, tokens/sec=1375330.78, grad_norm=0.3391, duration=0.38s
Step 2032: loss=3.5945, lr=0.000597, tokens/sec=1374321.68, grad_norm=0.3047, duration=0.38s
Step 2033: loss=3.6124, lr=0.000597, tokens/sec=1370640.83, grad_norm=0.2829, duration=0.38s
Step 2034: loss=3.6474, lr=0.000597, tokens/sec=1368001.82, grad_norm=0.2961, duration=0.38s
Step 2035: loss=3.6129, lr=0.000597, tokens/sec=1373318.35, grad_norm=0.2685, duration=0.38s
Step 2036: loss=3.6135, lr=0.000597, tokens/sec=1370985.21, grad_norm=0.2774, duration=0.38s
Step 2037: loss=3.5876, lr=0.000597, tokens/sec=1373241.16, grad_norm=0.2852, duration=0.38s
Step 2038: loss=3.6092, lr=0.000597, tokens/sec=1375735.18, grad_norm=0.3110, duration=0.38s
Step 2039: loss=3.6507, lr=0.000597, tokens/sec=1372326.76, grad_norm=0.3499, duration=0.38s
Step 2040: loss=3.7037, lr=0.000597, tokens/sec=1374429.91, grad_norm=0.3597, duration=0.38s
Step 2041: loss=3.7174, lr=0.000597, tokens/sec=1379927.36, grad_norm=0.3450, duration=0.38s
Step 2042: loss=3.7374, lr=0.000597, tokens/sec=1373943.00, grad_norm=0.3639, duration=0.38s
Step 2043: loss=3.7332, lr=0.000597, tokens/sec=1374012.54, grad_norm=0.3771, duration=0.38s
Step 2044: loss=3.7253, lr=0.000597, tokens/sec=1373942.14, grad_norm=0.3551, duration=0.38s
Step 2045: loss=3.6671, lr=0.000597, tokens/sec=1372600.01, grad_norm=0.3613, duration=0.38s
Step 2046: loss=3.7013, lr=0.000597, tokens/sec=1375712.80, grad_norm=0.3518, duration=0.38s
Step 2047: loss=3.7531, lr=0.000597, tokens/sec=1375865.15, grad_norm=0.3055, duration=0.38s
Step 2048: loss=3.8164, lr=0.000597, tokens/sec=1373180.28, grad_norm=0.2831, duration=0.38s
Step 2049: loss=3.7495, lr=0.000597, tokens/sec=1377353.43, grad_norm=0.3135, duration=0.38s
Step 2050: loss=3.7567, lr=0.000597, tokens/sec=1378718.71, grad_norm=0.3715, duration=0.38s
Step 2051: loss=3.6781, lr=0.000597, tokens/sec=1373252.31, grad_norm=0.3424, duration=0.38s
Step 2052: loss=3.6991, lr=0.000597, tokens/sec=1375988.26, grad_norm=0.3180, duration=0.38s
Step 2053: loss=3.6800, lr=0.000597, tokens/sec=1381587.61, grad_norm=0.3090, duration=0.38s
Step 2054: loss=3.6621, lr=0.000597, tokens/sec=1375875.48, grad_norm=0.2980, duration=0.38s
Step 2055: loss=3.7152, lr=0.000597, tokens/sec=1377538.93, grad_norm=0.2879, duration=0.38s
Step 2056: loss=3.6329, lr=0.000597, tokens/sec=1375564.78, grad_norm=0.2978, duration=0.38s
Step 2057: loss=3.7256, lr=0.000597, tokens/sec=1379099.16, grad_norm=0.2859, duration=0.38s
Step 2058: loss=3.7623, lr=0.000597, tokens/sec=1375019.47, grad_norm=0.2905, duration=0.38s
Step 2059: loss=3.6822, lr=0.000597, tokens/sec=1374849.25, grad_norm=0.2661, duration=0.38s
Step 2060: loss=3.5997, lr=0.000597, tokens/sec=1376243.16, grad_norm=0.2592, duration=0.38s
Step 2061: loss=3.7161, lr=0.000597, tokens/sec=1379639.06, grad_norm=0.2642, duration=0.38s
Step 2062: loss=3.7065, lr=0.000597, tokens/sec=1377012.74, grad_norm=0.2750, duration=0.38s
Step 2063: loss=3.6357, lr=0.000597, tokens/sec=1372924.80, grad_norm=0.2810, duration=0.38s
Step 2064: loss=3.5597, lr=0.000597, tokens/sec=1376077.81, grad_norm=0.2728, duration=0.38s
Step 2065: loss=3.6486, lr=0.000597, tokens/sec=1375493.37, grad_norm=0.3248, duration=0.38s
Step 2066: loss=3.6249, lr=0.000597, tokens/sec=1378877.78, grad_norm=0.3496, duration=0.38s
Step 2067: loss=3.6735, lr=0.000597, tokens/sec=1375265.41, grad_norm=0.3699, duration=0.38s
Step 2068: loss=3.6891, lr=0.000597, tokens/sec=1377007.57, grad_norm=0.3787, duration=0.38s
Step 2069: loss=3.5920, lr=0.000597, tokens/sec=1374234.93, grad_norm=0.3761, duration=0.38s
Step 2070: loss=3.6194, lr=0.000597, tokens/sec=1374690.25, grad_norm=0.3308, duration=0.38s
Step 2071: loss=3.6123, lr=0.000597, tokens/sec=1379393.28, grad_norm=0.3259, duration=0.38s
Step 2072: loss=3.5654, lr=0.000597, tokens/sec=1377325.82, grad_norm=0.2924, duration=0.38s
Step 2073: loss=3.6175, lr=0.000597, tokens/sec=1376186.32, grad_norm=0.2955, duration=0.38s
Step 2074: loss=3.5426, lr=0.000597, tokens/sec=1377513.05, grad_norm=0.2870, duration=0.38s
Step 2075: loss=3.5609, lr=0.000597, tokens/sec=1376903.24, grad_norm=0.2773, duration=0.38s
Step 2076: loss=3.6026, lr=0.000597, tokens/sec=1377891.10, grad_norm=0.2868, duration=0.38s
Step 2077: loss=3.6761, lr=0.000597, tokens/sec=1374771.03, grad_norm=0.2828, duration=0.38s
Step 2078: loss=3.6134, lr=0.000597, tokens/sec=1372617.14, grad_norm=0.2940, duration=0.38s
Step 2079: loss=3.5657, lr=0.000597, tokens/sec=1376992.05, grad_norm=0.2843, duration=0.38s
Step 2080: loss=3.5461, lr=0.000597, tokens/sec=1372946.23, grad_norm=0.2587, duration=0.38s
Step 2081: loss=3.5811, lr=0.000597, tokens/sec=1373212.01, grad_norm=0.2899, duration=0.38s
Step 2082: loss=3.6283, lr=0.000597, tokens/sec=1375071.91, grad_norm=0.3233, duration=0.38s
Step 2083: loss=3.5694, lr=0.000597, tokens/sec=1373672.65, grad_norm=0.3595, duration=0.38s
Step 2084: loss=3.6364, lr=0.000597, tokens/sec=1376752.39, grad_norm=0.3484, duration=0.38s
Step 2085: loss=3.6751, lr=0.000597, tokens/sec=1376176.84, grad_norm=0.3218, duration=0.38s
Step 2086: loss=3.7066, lr=0.000597, tokens/sec=1375382.39, grad_norm=0.2814, duration=0.38s
Step 2087: loss=3.7285, lr=0.000597, tokens/sec=1376200.96, grad_norm=0.3021, duration=0.38s
Step 2088: loss=3.6618, lr=0.000597, tokens/sec=1370376.90, grad_norm=0.2984, duration=0.38s
Step 2089: loss=3.6941, lr=0.000597, tokens/sec=1367928.64, grad_norm=0.3038, duration=0.38s
Step 2090: loss=3.7055, lr=0.000597, tokens/sec=1367462.48, grad_norm=0.3240, duration=0.38s
Step 2091: loss=3.6858, lr=0.000596, tokens/sec=1374445.37, grad_norm=0.3140, duration=0.38s
Step 2092: loss=3.7465, lr=0.000596, tokens/sec=1372863.08, grad_norm=0.3095, duration=0.38s
Step 2093: loss=3.7794, lr=0.000596, tokens/sec=1376290.54, grad_norm=0.3412, duration=0.38s
Step 2094: loss=3.7473, lr=0.000596, tokens/sec=1372552.03, grad_norm=0.4606, duration=0.38s
Step 2095: loss=3.7522, lr=0.000596, tokens/sec=1380671.59, grad_norm=0.4599, duration=0.38s
Step 2096: loss=3.8134, lr=0.000596, tokens/sec=1376511.94, grad_norm=0.5083, duration=0.38s
Step 2097: loss=3.7772, lr=0.000596, tokens/sec=1376802.38, grad_norm=0.6384, duration=0.38s
Step 2098: loss=3.7828, lr=0.000596, tokens/sec=1371750.63, grad_norm=0.5840, duration=0.38s
Step 2099: loss=3.7434, lr=0.000596, tokens/sec=1375623.30, grad_norm=0.4595, duration=0.38s
Step 2100/19073 (11.0%), Elapsed time: 885.12s, Steps per hour: 8541.25, Estimated hours remaining: 1.99
Step 2100: loss=3.7330, lr=0.000596, tokens/sec=1376678.26, grad_norm=0.3209, duration=0.38s
Step 2101: loss=3.6983, lr=0.000596, tokens/sec=1377290.45, grad_norm=0.3309, duration=0.38s
Step 2102: loss=3.6932, lr=0.000596, tokens/sec=1378126.84, grad_norm=0.3035, duration=0.38s
Step 2103: loss=3.6311, lr=0.000596, tokens/sec=1374034.00, grad_norm=0.2601, duration=0.38s
Step 2104: loss=3.6656, lr=0.000596, tokens/sec=1380034.74, grad_norm=0.2612, duration=0.38s
Step 2105: loss=3.6842, lr=0.000596, tokens/sec=1372576.88, grad_norm=0.3013, duration=0.38s
Step 2106: loss=3.6867, lr=0.000596, tokens/sec=1373876.91, grad_norm=0.3095, duration=0.38s
Step 2107: loss=3.6947, lr=0.000596, tokens/sec=1376020.12, grad_norm=0.2971, duration=0.38s
Step 2108: loss=3.6321, lr=0.000596, tokens/sec=1373731.86, grad_norm=0.3138, duration=0.38s
Step 2109: loss=3.5784, lr=0.000596, tokens/sec=1375983.96, grad_norm=0.5124, duration=0.38s
Step 2110: loss=3.6650, lr=0.000596, tokens/sec=1377218.86, grad_norm=0.3025, duration=0.38s
Step 2111: loss=3.6891, lr=0.000596, tokens/sec=1379617.42, grad_norm=0.2891, duration=0.38s
Step 2112: loss=3.7402, lr=0.000596, tokens/sec=1377653.71, grad_norm=0.3067, duration=0.38s
Step 2113: loss=3.6033, lr=0.000596, tokens/sec=1375327.34, grad_norm=0.3475, duration=0.38s
Step 2114: loss=3.6602, lr=0.000596, tokens/sec=1375973.62, grad_norm=0.3119, duration=0.38s
Step 2115: loss=3.6486, lr=0.000596, tokens/sec=1374087.24, grad_norm=0.2859, duration=0.38s
Step 2116: loss=3.5698, lr=0.000596, tokens/sec=1375206.06, grad_norm=0.2559, duration=0.38s
Step 2117: loss=3.6293, lr=0.000596, tokens/sec=1379210.74, grad_norm=0.2494, duration=0.38s
Step 2118: loss=3.6515, lr=0.000596, tokens/sec=1376387.02, grad_norm=0.2717, duration=0.38s
Step 2119: loss=3.5960, lr=0.000596, tokens/sec=1375902.17, grad_norm=0.2946, duration=0.38s
Step 2120: loss=3.6484, lr=0.000596, tokens/sec=1377736.57, grad_norm=0.3385, duration=0.38s
Step 2121: loss=3.6343, lr=0.000596, tokens/sec=1378930.53, grad_norm=0.3299, duration=0.38s
Step 2122: loss=3.5955, lr=0.000596, tokens/sec=1378548.44, grad_norm=0.4557, duration=0.38s
Step 2123: loss=3.6829, lr=0.000596, tokens/sec=1375868.59, grad_norm=0.3335, duration=0.38s
Step 2124: loss=3.6085, lr=0.000596, tokens/sec=1374630.95, grad_norm=0.3444, duration=0.38s
Step 2125: loss=3.5899, lr=0.000596, tokens/sec=1376510.22, grad_norm=0.3182, duration=0.38s
Step 2126: loss=3.6437, lr=0.000596, tokens/sec=1378530.30, grad_norm=0.3034, duration=0.38s
Step 2127: loss=3.5871, lr=0.000596, tokens/sec=1376241.44, grad_norm=0.2938, duration=0.38s
Step 2128: loss=3.5559, lr=0.000596, tokens/sec=1376349.97, grad_norm=0.2646, duration=0.38s
Step 2129: loss=3.5986, lr=0.000596, tokens/sec=1375937.46, grad_norm=0.3087, duration=0.38s
Step 2130: loss=3.6309, lr=0.000596, tokens/sec=1376051.98, grad_norm=0.3177, duration=0.38s
Step 2131: loss=3.6276, lr=0.000596, tokens/sec=1376005.48, grad_norm=0.2995, duration=0.38s
Step 2132: loss=3.5937, lr=0.000596, tokens/sec=1375436.59, grad_norm=0.2983, duration=0.38s
Step 2133: loss=3.6342, lr=0.000596, tokens/sec=1373993.65, grad_norm=0.3001, duration=0.38s
Step 2134: loss=3.5904, lr=0.000596, tokens/sec=1377786.64, grad_norm=0.3075, duration=0.38s
Step 2135: loss=3.5906, lr=0.000596, tokens/sec=1376380.12, grad_norm=0.3305, duration=0.38s
Step 2136: loss=3.6062, lr=0.000596, tokens/sec=1377502.69, grad_norm=0.3035, duration=0.38s
Step 2137: loss=3.6899, lr=0.000596, tokens/sec=1380274.68, grad_norm=0.2980, duration=0.38s
Step 2138: loss=3.7357, lr=0.000596, tokens/sec=1376826.52, grad_norm=0.2717, duration=0.38s
Step 2139: loss=3.6812, lr=0.000596, tokens/sec=1379561.16, grad_norm=0.2636, duration=0.38s
Step 2140: loss=3.6907, lr=0.000596, tokens/sec=1378716.98, grad_norm=0.3043, duration=0.38s
Step 2141: loss=3.7268, lr=0.000596, tokens/sec=1377236.97, grad_norm=0.3256, duration=0.38s
Step 2142: loss=3.7012, lr=0.000596, tokens/sec=1377853.98, grad_norm=0.3144, duration=0.38s
Step 2143: loss=3.6968, lr=0.000596, tokens/sec=1376210.43, grad_norm=0.3128, duration=0.38s
Step 2144: loss=3.6857, lr=0.000596, tokens/sec=1375286.91, grad_norm=0.3157, duration=0.38s
Step 2145: loss=3.6549, lr=0.000596, tokens/sec=1376900.66, grad_norm=0.2948, duration=0.38s
Step 2146: loss=3.6919, lr=0.000596, tokens/sec=1375880.65, grad_norm=0.2823, duration=0.38s
Step 2147: loss=3.6715, lr=0.000596, tokens/sec=1375177.68, grad_norm=0.3186, duration=0.38s
Step 2148: loss=3.6343, lr=0.000596, tokens/sec=1380475.71, grad_norm=0.3534, duration=0.38s
Step 2149: loss=3.6397, lr=0.000596, tokens/sec=1372811.66, grad_norm=0.3287, duration=0.38s
Step 2150: loss=3.6530, lr=0.000596, tokens/sec=1376250.91, grad_norm=0.3171, duration=0.38s
Step 2151: loss=3.5890, lr=0.000596, tokens/sec=1375663.74, grad_norm=0.2987, duration=0.38s
Step 2152: loss=3.6371, lr=0.000596, tokens/sec=1370175.39, grad_norm=0.3506, duration=0.38s
Step 2153: loss=3.6627, lr=0.000596, tokens/sec=1376898.07, grad_norm=0.3508, duration=0.38s
Step 2154: loss=3.6379, lr=0.000596, tokens/sec=1377111.05, grad_norm=0.2869, duration=0.38s
Step 2155: loss=3.7055, lr=0.000596, tokens/sec=1377554.47, grad_norm=0.2999, duration=0.38s
Step 2156: loss=3.6588, lr=0.000596, tokens/sec=1379461.64, grad_norm=0.3001, duration=0.38s
Step 2157: loss=3.7005, lr=0.000596, tokens/sec=1375605.23, grad_norm=0.2591, duration=0.38s
Step 2158: loss=3.6459, lr=0.000596, tokens/sec=1376737.73, grad_norm=0.2734, duration=0.38s
Step 2159: loss=3.6233, lr=0.000596, tokens/sec=1378546.71, grad_norm=0.2926, duration=0.38s
Step 2160: loss=3.6434, lr=0.000596, tokens/sec=1373898.37, grad_norm=0.2751, duration=0.38s
Step 2161: loss=3.6350, lr=0.000596, tokens/sec=1372576.02, grad_norm=0.2526, duration=0.38s
Step 2162: loss=3.5809, lr=0.000596, tokens/sec=1375737.76, grad_norm=0.2640, duration=0.38s
Step 2163: loss=3.6213, lr=0.000596, tokens/sec=1375175.96, grad_norm=0.2777, duration=0.38s
Step 2164: loss=3.6031, lr=0.000596, tokens/sec=1375689.56, grad_norm=0.2979, duration=0.38s
Step 2165: loss=3.6092, lr=0.000596, tokens/sec=1373866.61, grad_norm=0.2836, duration=0.38s
Step 2166: loss=3.6182, lr=0.000596, tokens/sec=1376061.45, grad_norm=0.2825, duration=0.38s
Step 2167: loss=3.5847, lr=0.000596, tokens/sec=1373404.98, grad_norm=0.3282, duration=0.38s
Step 2168: loss=3.5709, lr=0.000596, tokens/sec=1380533.77, grad_norm=0.3294, duration=0.38s
Step 2169: loss=3.6124, lr=0.000596, tokens/sec=1372624.00, grad_norm=0.2975, duration=0.38s
Step 2170: loss=3.6113, lr=0.000596, tokens/sec=1375157.04, grad_norm=0.2640, duration=0.38s
Step 2171: loss=3.5818, lr=0.000596, tokens/sec=1374825.18, grad_norm=0.2909, duration=0.38s
Step 2172: loss=3.6476, lr=0.000596, tokens/sec=1375228.42, grad_norm=0.3154, duration=0.38s
Step 2173: loss=3.5860, lr=0.000596, tokens/sec=1377699.46, grad_norm=0.3247, duration=0.38s
Step 2174: loss=3.5826, lr=0.000596, tokens/sec=1381686.57, grad_norm=0.3459, duration=0.38s
Step 2175: loss=3.5294, lr=0.000596, tokens/sec=1376886.86, grad_norm=0.3306, duration=0.38s
Step 2176: loss=3.5617, lr=0.000596, tokens/sec=1374441.08, grad_norm=0.2981, duration=0.38s
Step 2177: loss=3.5622, lr=0.000596, tokens/sec=1374891.37, grad_norm=0.3042, duration=0.38s
Step 2178: loss=3.5623, lr=0.000596, tokens/sec=1378671.17, grad_norm=0.3196, duration=0.38s
Step 2179: loss=3.5837, lr=0.000596, tokens/sec=1378766.26, grad_norm=0.3200, duration=0.38s
Step 2180: loss=3.5497, lr=0.000596, tokens/sec=1373457.30, grad_norm=0.3206, duration=0.38s
Step 2181: loss=3.6394, lr=0.000596, tokens/sec=1375194.88, grad_norm=0.3182, duration=0.38s
Step 2182: loss=3.5812, lr=0.000596, tokens/sec=1375221.54, grad_norm=0.3036, duration=0.38s
Step 2183: loss=3.6454, lr=0.000596, tokens/sec=1375724.85, grad_norm=0.2787, duration=0.38s
Step 2184: loss=3.6836, lr=0.000596, tokens/sec=1373824.55, grad_norm=0.2809, duration=0.38s
Step 2185: loss=3.6889, lr=0.000596, tokens/sec=1374224.63, grad_norm=0.3130, duration=0.38s
Step 2186: loss=3.7039, lr=0.000596, tokens/sec=1375541.55, grad_norm=0.3558, duration=0.38s
Step 2187: loss=3.6487, lr=0.000596, tokens/sec=1375108.89, grad_norm=0.3541, duration=0.38s
Step 2188: loss=3.6218, lr=0.000596, tokens/sec=1376902.38, grad_norm=0.3339, duration=0.38s
Step 2189: loss=3.6604, lr=0.000596, tokens/sec=1371049.32, grad_norm=0.3693, duration=0.38s
Step 2190: loss=3.6497, lr=0.000596, tokens/sec=1373411.84, grad_norm=0.4260, duration=0.38s
Step 2191: loss=3.6900, lr=0.000596, tokens/sec=1375658.58, grad_norm=0.4075, duration=0.38s
Step 2192: loss=3.6621, lr=0.000596, tokens/sec=1374714.31, grad_norm=0.3741, duration=0.38s
Step 2193: loss=3.6572, lr=0.000596, tokens/sec=1371689.88, grad_norm=0.3388, duration=0.38s
Step 2194: loss=3.6764, lr=0.000596, tokens/sec=1375319.59, grad_norm=0.3602, duration=0.38s
Step 2195: loss=3.6392, lr=0.000596, tokens/sec=1376038.20, grad_norm=0.3234, duration=0.38s
Step 2196: loss=3.6608, lr=0.000596, tokens/sec=1375413.36, grad_norm=0.3341, duration=0.38s
Step 2197: loss=3.6349, lr=0.000596, tokens/sec=1376113.12, grad_norm=0.3194, duration=0.38s
Step 2198: loss=3.6584, lr=0.000596, tokens/sec=1371386.20, grad_norm=0.2866, duration=0.38s
Step 2199: loss=3.6246, lr=0.000596, tokens/sec=1373480.46, grad_norm=0.2966, duration=0.38s
Step 2200/19073 (11.5%), Elapsed time: 923.29s, Steps per hour: 8577.99, Estimated hours remaining: 1.97
Step 2200: loss=3.6221, lr=0.000596, tokens/sec=1376193.21, grad_norm=0.2922, duration=0.38s
Step 2201: loss=3.6221, lr=0.000596, tokens/sec=1373260.89, grad_norm=0.2706, duration=0.38s
Step 2202: loss=3.6162, lr=0.000596, tokens/sec=1372379.00, grad_norm=0.2546, duration=0.38s
Step 2203: loss=3.5755, lr=0.000596, tokens/sec=1370433.27, grad_norm=0.2700, duration=0.38s
Step 2204: loss=3.6354, lr=0.000596, tokens/sec=1372140.94, grad_norm=0.2655, duration=0.38s
Step 2205: loss=3.6093, lr=0.000596, tokens/sec=1370493.91, grad_norm=0.2748, duration=0.38s
Step 2206: loss=3.6542, lr=0.000596, tokens/sec=1376428.37, grad_norm=0.2587, duration=0.38s
Step 2207: loss=3.5780, lr=0.000596, tokens/sec=1374949.83, grad_norm=0.2454, duration=0.38s
Step 2208: loss=3.5951, lr=0.000596, tokens/sec=1374076.07, grad_norm=0.3208, duration=0.38s
Step 2209: loss=3.5973, lr=0.000596, tokens/sec=1373820.26, grad_norm=0.3649, duration=0.38s
Step 2210: loss=3.5908, lr=0.000596, tokens/sec=1375622.44, grad_norm=0.3262, duration=0.38s
Step 2211: loss=3.6621, lr=0.000596, tokens/sec=1378503.51, grad_norm=0.3358, duration=0.38s
Step 2212: loss=3.6474, lr=0.000596, tokens/sec=1374775.33, grad_norm=0.3110, duration=0.38s
Step 2213: loss=3.5863, lr=0.000596, tokens/sec=1376829.97, grad_norm=0.3019, duration=0.38s
Step 2214: loss=3.6118, lr=0.000596, tokens/sec=1378034.44, grad_norm=0.2974, duration=0.38s
Step 2215: loss=3.5954, lr=0.000596, tokens/sec=1378617.58, grad_norm=0.2970, duration=0.38s
Step 2216: loss=3.6111, lr=0.000596, tokens/sec=1378189.03, grad_norm=0.3091, duration=0.38s
Step 2217: loss=3.6017, lr=0.000596, tokens/sec=1377328.41, grad_norm=0.2905, duration=0.38s
Step 2218: loss=3.6060, lr=0.000596, tokens/sec=1377399.15, grad_norm=0.2667, duration=0.38s
Step 2219: loss=3.5773, lr=0.000596, tokens/sec=1377051.55, grad_norm=0.2960, duration=0.38s
Step 2220: loss=3.6077, lr=0.000596, tokens/sec=1374696.26, grad_norm=0.2986, duration=0.38s
Step 2221: loss=3.4919, lr=0.000596, tokens/sec=1376135.51, grad_norm=0.2852, duration=0.38s
Step 2222: loss=3.5675, lr=0.000596, tokens/sec=1378120.80, grad_norm=0.2591, duration=0.38s
Step 2223: loss=3.5206, lr=0.000596, tokens/sec=1377077.42, grad_norm=0.2835, duration=0.38s
Step 2224: loss=3.5801, lr=0.000596, tokens/sec=1376457.66, grad_norm=0.2933, duration=0.38s
Step 2225: loss=3.5822, lr=0.000596, tokens/sec=1374273.58, grad_norm=0.2748, duration=0.38s
Step 2226: loss=3.5394, lr=0.000596, tokens/sec=1376424.92, grad_norm=0.2674, duration=0.38s
Step 2227: loss=3.5184, lr=0.000596, tokens/sec=1377998.17, grad_norm=0.2580, duration=0.38s
Step 2228: loss=3.5396, lr=0.000596, tokens/sec=1375317.87, grad_norm=0.2671, duration=0.38s
Step 2229: loss=3.5779, lr=0.000596, tokens/sec=1375156.18, grad_norm=0.2301, duration=0.38s
Step 2230: loss=3.6450, lr=0.000596, tokens/sec=1377142.10, grad_norm=0.2491, duration=0.38s
Step 2231: loss=3.6542, lr=0.000596, tokens/sec=1378819.86, grad_norm=0.2909, duration=0.38s
Step 2232: loss=3.6956, lr=0.000596, tokens/sec=1377807.36, grad_norm=0.3204, duration=0.38s
Step 2233: loss=3.6357, lr=0.000596, tokens/sec=1378557.95, grad_norm=0.2914, duration=0.38s
Step 2234: loss=3.6528, lr=0.000596, tokens/sec=1378645.24, grad_norm=0.3303, duration=0.38s
Step 2235: loss=3.6132, lr=0.000596, tokens/sec=1375504.55, grad_norm=0.3494, duration=0.38s
Step 2236: loss=3.6458, lr=0.000596, tokens/sec=1377593.30, grad_norm=0.3660, duration=0.38s
Step 2237: loss=3.6957, lr=0.000596, tokens/sec=1374255.54, grad_norm=0.3481, duration=0.38s
Step 2238: loss=3.7594, lr=0.000596, tokens/sec=1378507.83, grad_norm=0.3431, duration=0.38s
Step 2239: loss=3.7125, lr=0.000596, tokens/sec=1376101.06, grad_norm=0.4009, duration=0.38s
Step 2240: loss=3.6864, lr=0.000596, tokens/sec=1374845.81, grad_norm=0.3652, duration=0.38s
Step 2241: loss=3.6078, lr=0.000596, tokens/sec=1377992.12, grad_norm=0.3747, duration=0.38s
Step 2242: loss=3.6452, lr=0.000596, tokens/sec=1372719.11, grad_norm=0.3640, duration=0.38s
Step 2243: loss=3.6258, lr=0.000596, tokens/sec=1377532.03, grad_norm=0.3093, duration=0.38s
Step 2244: loss=3.5885, lr=0.000596, tokens/sec=1374237.51, grad_norm=0.3549, duration=0.38s
Step 2245: loss=3.6561, lr=0.000595, tokens/sec=1373888.07, grad_norm=0.3086, duration=0.38s
Step 2246: loss=3.5871, lr=0.000595, tokens/sec=1377190.40, grad_norm=0.2851, duration=0.38s
Step 2247: loss=3.6662, lr=0.000595, tokens/sec=1375504.55, grad_norm=0.2788, duration=0.38s
Step 2248: loss=3.6928, lr=0.000595, tokens/sec=1372558.88, grad_norm=0.2973, duration=0.38s
Step 2249: loss=3.6201, lr=0.000595, tokens/sec=1372373.86, grad_norm=0.2734, duration=0.38s
Validation loss at step 2250: 3.7844440937042236
Step 2250: loss=3.5756, lr=0.000595, tokens/sec=152704.96, grad_norm=0.2538, duration=3.43s
Step 2251: loss=3.6565, lr=0.000595, tokens/sec=1376688.60, grad_norm=0.2585, duration=0.38s
Step 2252: loss=3.6214, lr=0.000595, tokens/sec=1375675.79, grad_norm=0.2615, duration=0.38s
Step 2253: loss=3.5729, lr=0.000595, tokens/sec=1375406.48, grad_norm=0.2468, duration=0.38s
Step 2254: loss=3.5107, lr=0.000595, tokens/sec=1378036.16, grad_norm=0.2455, duration=0.38s
Step 2255: loss=3.5800, lr=0.000595, tokens/sec=1380141.27, grad_norm=0.2724, duration=0.38s
Step 2256: loss=3.5840, lr=0.000595, tokens/sec=1378422.28, grad_norm=0.2944, duration=0.38s
Step 2257: loss=3.5896, lr=0.000595, tokens/sec=1376426.65, grad_norm=0.2933, duration=0.38s
Step 2258: loss=3.6113, lr=0.000595, tokens/sec=1373236.02, grad_norm=0.2884, duration=0.38s
Step 2259: loss=3.5469, lr=0.000595, tokens/sec=1378303.92, grad_norm=0.2763, duration=0.38s
Step 2260: loss=3.5518, lr=0.000595, tokens/sec=1378576.10, grad_norm=0.3099, duration=0.38s
Step 2261: loss=3.5612, lr=0.000595, tokens/sec=1375990.84, grad_norm=0.3110, duration=0.38s
Step 2262: loss=3.5060, lr=0.000595, tokens/sec=1378612.40, grad_norm=0.2981, duration=0.38s
Step 2263: loss=3.5372, lr=0.000595, tokens/sec=1374854.41, grad_norm=0.3275, duration=0.38s
Step 2264: loss=3.5153, lr=0.000595, tokens/sec=1379057.64, grad_norm=0.3351, duration=0.38s
Step 2265: loss=3.5445, lr=0.000595, tokens/sec=1376288.81, grad_norm=0.3107, duration=0.38s
Step 2266: loss=3.5496, lr=0.000595, tokens/sec=1375709.36, grad_norm=0.3174, duration=0.38s
Step 2267: loss=3.6198, lr=0.000595, tokens/sec=1376926.52, grad_norm=0.3074, duration=0.38s
Step 2268: loss=3.5279, lr=0.000595, tokens/sec=1376186.32, grad_norm=0.3136, duration=0.38s
Step 2269: loss=3.5334, lr=0.000595, tokens/sec=1379510.97, grad_norm=0.3085, duration=0.38s
Step 2270: loss=3.4802, lr=0.000595, tokens/sec=1378344.52, grad_norm=0.2996, duration=0.38s
Step 2271: loss=3.5416, lr=0.000595, tokens/sec=1374213.46, grad_norm=0.2876, duration=0.38s
Step 2272: loss=3.5183, lr=0.000595, tokens/sec=1372136.66, grad_norm=0.2573, duration=0.38s
Step 2273: loss=3.5514, lr=0.000595, tokens/sec=1374595.72, grad_norm=0.2683, duration=0.38s
Step 2274: loss=3.5610, lr=0.000595, tokens/sec=1372244.54, grad_norm=0.2924, duration=0.38s
Step 2275: loss=3.6374, lr=0.000595, tokens/sec=1374508.95, grad_norm=0.2838, duration=0.38s
Step 2276: loss=3.6554, lr=0.000595, tokens/sec=1377791.82, grad_norm=0.2962, duration=0.38s
Step 2277: loss=3.6485, lr=0.000595, tokens/sec=1377999.03, grad_norm=0.3134, duration=0.38s
Step 2278: loss=3.6026, lr=0.000595, tokens/sec=1376136.37, grad_norm=0.3304, duration=0.38s
Step 2279: loss=3.6657, lr=0.000595, tokens/sec=1377831.53, grad_norm=0.3507, duration=0.38s
Step 2280: loss=3.5942, lr=0.000595, tokens/sec=1376614.49, grad_norm=0.3062, duration=0.38s
Step 2281: loss=3.6792, lr=0.000595, tokens/sec=1371110.87, grad_norm=0.2900, duration=0.38s
Step 2282: loss=3.6692, lr=0.000595, tokens/sec=1377503.55, grad_norm=0.3277, duration=0.38s
Step 2283: loss=3.7576, lr=0.000595, tokens/sec=1376374.96, grad_norm=0.3807, duration=0.38s
Step 2284: loss=3.6571, lr=0.000595, tokens/sec=1375690.42, grad_norm=0.3502, duration=0.38s
Step 2285: loss=3.6807, lr=0.000595, tokens/sec=1378001.62, grad_norm=0.2898, duration=0.38s
Step 2286: loss=3.7671, lr=0.000595, tokens/sec=1376166.51, grad_norm=0.3534, duration=0.38s
Step 2287: loss=3.6765, lr=0.000595, tokens/sec=1371549.57, grad_norm=0.4450, duration=0.38s
Step 2288: loss=3.7128, lr=0.000595, tokens/sec=1373985.07, grad_norm=0.4218, duration=0.38s
Step 2289: loss=3.6470, lr=0.000595, tokens/sec=1374876.76, grad_norm=0.3449, duration=0.38s
Step 2290: loss=3.6777, lr=0.000595, tokens/sec=1373870.04, grad_norm=0.3308, duration=0.38s
Step 2291: loss=3.6059, lr=0.000595, tokens/sec=1371849.04, grad_norm=0.3151, duration=0.38s
Step 2292: loss=3.6327, lr=0.000595, tokens/sec=1372251.40, grad_norm=0.2856, duration=0.38s
Step 2293: loss=3.5538, lr=0.000595, tokens/sec=1374571.67, grad_norm=0.2610, duration=0.38s
Step 2294: loss=3.6203, lr=0.000595, tokens/sec=1379832.98, grad_norm=0.2769, duration=0.38s
Step 2295: loss=3.6531, lr=0.000595, tokens/sec=1372256.53, grad_norm=0.2852, duration=0.38s
Step 2296: loss=3.5864, lr=0.000595, tokens/sec=1377623.51, grad_norm=0.3028, duration=0.38s
Step 2297: loss=3.6329, lr=0.000595, tokens/sec=1375884.95, grad_norm=0.3067, duration=0.38s
Step 2298: loss=3.5787, lr=0.000595, tokens/sec=1375150.16, grad_norm=0.2972, duration=0.38s
Step 2299: loss=3.5331, lr=0.000595, tokens/sec=1377765.06, grad_norm=0.5500, duration=0.38s
Step 2300/19073 (12.1%), Elapsed time: 964.52s, Steps per hour: 8584.54, Estimated hours remaining: 1.95
Step 2300: loss=3.6227, lr=0.000595, tokens/sec=1374956.70, grad_norm=0.2799, duration=0.38s
Step 2301: loss=3.5943, lr=0.000595, tokens/sec=1371470.02, grad_norm=0.2731, duration=0.38s
Step 2302: loss=3.6894, lr=0.000595, tokens/sec=1374626.66, grad_norm=0.2685, duration=0.38s
Step 2303: loss=3.5090, lr=0.000595, tokens/sec=1378496.59, grad_norm=0.2791, duration=0.38s
Step 2304: loss=3.6229, lr=0.000595, tokens/sec=1377843.62, grad_norm=0.3037, duration=0.38s
Step 2305: loss=3.5603, lr=0.000595, tokens/sec=1377268.02, grad_norm=0.3356, duration=0.38s
Step 2306: loss=3.5636, lr=0.000595, tokens/sec=1377153.31, grad_norm=0.3624, duration=0.38s
Step 2307: loss=3.5682, lr=0.000595, tokens/sec=1374772.75, grad_norm=0.3197, duration=0.38s
Step 2308: loss=3.5774, lr=0.000595, tokens/sec=1377245.60, grad_norm=0.3177, duration=0.38s
Step 2309: loss=3.5887, lr=0.000595, tokens/sec=1377975.72, grad_norm=0.3177, duration=0.38s
Step 2310: loss=3.5678, lr=0.000595, tokens/sec=1374266.71, grad_norm=0.3093, duration=0.38s
Step 2311: loss=3.5204, lr=0.000595, tokens/sec=1378144.98, grad_norm=0.2991, duration=0.38s
Step 2312: loss=3.5785, lr=0.000595, tokens/sec=1376888.59, grad_norm=0.3240, duration=0.38s
Step 2313: loss=3.6323, lr=0.000595, tokens/sec=1376306.04, grad_norm=0.3184, duration=0.38s
Step 2314: loss=3.5859, lr=0.000595, tokens/sec=1371504.23, grad_norm=0.2849, duration=0.38s
Step 2315: loss=3.5163, lr=0.000595, tokens/sec=1373475.32, grad_norm=0.2904, duration=0.38s
Step 2316: loss=3.5870, lr=0.000595, tokens/sec=1376879.97, grad_norm=0.3043, duration=0.38s
Step 2317: loss=3.5043, lr=0.000595, tokens/sec=1372566.59, grad_norm=0.2934, duration=0.38s
Step 2318: loss=3.5047, lr=0.000595, tokens/sec=1375991.71, grad_norm=0.2857, duration=0.38s
Step 2319: loss=3.5678, lr=0.000595, tokens/sec=1373633.18, grad_norm=0.3011, duration=0.38s
Step 2320: loss=3.5576, lr=0.000595, tokens/sec=1371579.51, grad_norm=0.2946, duration=0.38s
Step 2321: loss=3.5545, lr=0.000595, tokens/sec=1377192.98, grad_norm=0.3503, duration=0.38s
Step 2322: loss=3.5527, lr=0.000595, tokens/sec=1371605.18, grad_norm=0.3071, duration=0.38s
Step 2323: loss=3.5574, lr=0.000595, tokens/sec=1373836.57, grad_norm=0.2966, duration=0.38s
Step 2324: loss=3.5416, lr=0.000595, tokens/sec=1370130.14, grad_norm=0.3012, duration=0.38s
Step 2325: loss=3.5359, lr=0.000595, tokens/sec=1379455.59, grad_norm=0.2801, duration=0.38s
Step 2326: loss=3.5249, lr=0.000595, tokens/sec=1375861.71, grad_norm=0.2750, duration=0.38s
Step 2327: loss=3.6385, lr=0.000595, tokens/sec=1379481.55, grad_norm=0.2883, duration=0.38s
Step 2328: loss=3.6707, lr=0.000595, tokens/sec=1376841.17, grad_norm=0.2686, duration=0.38s
Step 2329: loss=3.6275, lr=0.000595, tokens/sec=1378640.92, grad_norm=0.2724, duration=0.38s
Step 2330: loss=3.6534, lr=0.000595, tokens/sec=1378517.33, grad_norm=0.2803, duration=0.38s
Step 2331: loss=3.6419, lr=0.000595, tokens/sec=1375502.83, grad_norm=0.2930, duration=0.38s
Step 2332: loss=3.6544, lr=0.000595, tokens/sec=1376777.38, grad_norm=0.2793, duration=0.38s
Step 2333: loss=3.6701, lr=0.000595, tokens/sec=1377544.97, grad_norm=0.2841, duration=0.38s
Step 2334: loss=3.5844, lr=0.000595, tokens/sec=1374900.83, grad_norm=0.3162, duration=0.38s
Step 2335: loss=3.6204, lr=0.000595, tokens/sec=1376213.02, grad_norm=0.3038, duration=0.38s
Step 2336: loss=3.6235, lr=0.000595, tokens/sec=1376941.18, grad_norm=0.3261, duration=0.38s
Step 2337: loss=3.5975, lr=0.000595, tokens/sec=1379930.82, grad_norm=0.3855, duration=0.38s
Step 2338: loss=3.5896, lr=0.000595, tokens/sec=1379163.16, grad_norm=0.3250, duration=0.38s
Step 2339: loss=3.5818, lr=0.000595, tokens/sec=1375048.70, grad_norm=0.2886, duration=0.38s
Step 2340: loss=3.5989, lr=0.000595, tokens/sec=1378506.10, grad_norm=0.2812, duration=0.38s
Step 2341: loss=3.5418, lr=0.000595, tokens/sec=1371511.08, grad_norm=0.2834, duration=0.38s
Step 2342: loss=3.5612, lr=0.000595, tokens/sec=1380192.38, grad_norm=0.2932, duration=0.38s
Step 2343: loss=3.5982, lr=0.000595, tokens/sec=1378244.31, grad_norm=0.3272, duration=0.38s
Step 2344: loss=3.5946, lr=0.000595, tokens/sec=1376337.05, grad_norm=0.2998, duration=0.38s
Step 2345: loss=3.6787, lr=0.000595, tokens/sec=1376811.86, grad_norm=0.2853, duration=0.38s
Step 2346: loss=3.5555, lr=0.000595, tokens/sec=1376124.31, grad_norm=0.2661, duration=0.38s
Step 2347: loss=3.6451, lr=0.000595, tokens/sec=1374844.95, grad_norm=0.2748, duration=0.38s
Step 2348: loss=3.6008, lr=0.000595, tokens/sec=1378170.89, grad_norm=0.3017, duration=0.38s
Step 2349: loss=3.5763, lr=0.000595, tokens/sec=1380179.39, grad_norm=0.2593, duration=0.38s
Step 2350: loss=3.5849, lr=0.000595, tokens/sec=1374034.00, grad_norm=0.2911, duration=0.38s
Step 2351: loss=3.5547, lr=0.000595, tokens/sec=1378621.04, grad_norm=0.3208, duration=0.38s
Step 2352: loss=3.5495, lr=0.000595, tokens/sec=1374626.66, grad_norm=0.2870, duration=0.38s
Step 2353: loss=3.5595, lr=0.000595, tokens/sec=1372358.44, grad_norm=0.2864, duration=0.38s
Step 2354: loss=3.5438, lr=0.000595, tokens/sec=1378970.30, grad_norm=0.2827, duration=0.38s
Step 2355: loss=3.5734, lr=0.000595, tokens/sec=1374846.67, grad_norm=0.2976, duration=0.38s
Step 2356: loss=3.5642, lr=0.000595, tokens/sec=1377186.08, grad_norm=0.2890, duration=0.38s
Step 2357: loss=3.4786, lr=0.000595, tokens/sec=1374980.78, grad_norm=0.3120, duration=0.38s
Step 2358: loss=3.5700, lr=0.000595, tokens/sec=1376307.76, grad_norm=0.3516, duration=0.38s
Step 2359: loss=3.5537, lr=0.000595, tokens/sec=1372442.38, grad_norm=0.3136, duration=0.38s
Step 2360: loss=3.6117, lr=0.000595, tokens/sec=1375785.10, grad_norm=0.3320, duration=0.38s
Step 2361: loss=3.5358, lr=0.000595, tokens/sec=1372671.98, grad_norm=0.3528, duration=0.38s
Step 2362: loss=3.5982, lr=0.000595, tokens/sec=1377520.81, grad_norm=0.3221, duration=0.38s
Step 2363: loss=3.5200, lr=0.000595, tokens/sec=1378011.12, grad_norm=0.3110, duration=0.38s
Step 2364: loss=3.5380, lr=0.000595, tokens/sec=1374078.65, grad_norm=0.3581, duration=0.38s
Step 2365: loss=3.4667, lr=0.000595, tokens/sec=1376586.91, grad_norm=0.3371, duration=0.38s
Step 2366: loss=3.4825, lr=0.000595, tokens/sec=1378305.65, grad_norm=0.2802, duration=0.38s
Step 2367: loss=3.5549, lr=0.000595, tokens/sec=1377953.27, grad_norm=0.2701, duration=0.38s
Step 2368: loss=3.5071, lr=0.000595, tokens/sec=1369444.13, grad_norm=0.2575, duration=0.38s
Step 2369: loss=3.5141, lr=0.000595, tokens/sec=1374973.90, grad_norm=0.2802, duration=0.38s
Step 2370: loss=3.5291, lr=0.000595, tokens/sec=1375207.78, grad_norm=0.2809, duration=0.38s
Step 2371: loss=3.5447, lr=0.000595, tokens/sec=1375428.84, grad_norm=0.3006, duration=0.38s
Step 2372: loss=3.5437, lr=0.000595, tokens/sec=1377556.19, grad_norm=0.3165, duration=0.38s
Step 2373: loss=3.5800, lr=0.000595, tokens/sec=1373238.59, grad_norm=0.2874, duration=0.38s
Step 2374: loss=3.6254, lr=0.000595, tokens/sec=1377623.51, grad_norm=0.2687, duration=0.38s
Step 2375: loss=3.6471, lr=0.000595, tokens/sec=1378037.03, grad_norm=0.3053, duration=0.38s
Step 2376: loss=3.5973, lr=0.000595, tokens/sec=1378240.86, grad_norm=0.3240, duration=0.38s
Step 2377: loss=3.6136, lr=0.000595, tokens/sec=1379145.00, grad_norm=0.3334, duration=0.38s
Step 2378: loss=3.5601, lr=0.000595, tokens/sec=1376190.62, grad_norm=0.3656, duration=0.38s
Step 2379: loss=3.6188, lr=0.000595, tokens/sec=1380188.92, grad_norm=0.3327, duration=0.38s
Step 2380: loss=3.5782, lr=0.000595, tokens/sec=1376870.48, grad_norm=0.3205, duration=0.38s
Step 2381: loss=3.6157, lr=0.000595, tokens/sec=1377157.62, grad_norm=0.3076, duration=0.38s
Step 2382: loss=3.6105, lr=0.000594, tokens/sec=1375504.55, grad_norm=0.3317, duration=0.38s
Step 2383: loss=3.5972, lr=0.000594, tokens/sec=1375754.97, grad_norm=0.3178, duration=0.38s
Step 2384: loss=3.6179, lr=0.000594, tokens/sec=1378021.48, grad_norm=0.3059, duration=0.38s
Step 2385: loss=3.5858, lr=0.000594, tokens/sec=1377919.59, grad_norm=0.2336, duration=0.38s
Step 2386: loss=3.6035, lr=0.000594, tokens/sec=1375662.88, grad_norm=0.2626, duration=0.38s
Step 2387: loss=3.5685, lr=0.000594, tokens/sec=1374918.02, grad_norm=0.2536, duration=0.38s
Step 2388: loss=3.6275, lr=0.000594, tokens/sec=1377954.13, grad_norm=0.2505, duration=0.38s
Step 2389: loss=3.5186, lr=0.000594, tokens/sec=1376768.76, grad_norm=0.2454, duration=0.38s
Step 2390: loss=3.5789, lr=0.000594, tokens/sec=1376588.64, grad_norm=0.2573, duration=0.38s
Step 2391: loss=3.5652, lr=0.000594, tokens/sec=1376671.37, grad_norm=0.2742, duration=0.38s
Step 2392: loss=3.5697, lr=0.000594, tokens/sec=1378911.50, grad_norm=0.2717, duration=0.38s
Step 2393: loss=3.5036, lr=0.000594, tokens/sec=1377879.01, grad_norm=0.2682, duration=0.38s
Step 2394: loss=3.5957, lr=0.000594, tokens/sec=1377576.90, grad_norm=0.2600, duration=0.38s
Step 2395: loss=3.5685, lr=0.000594, tokens/sec=1376935.14, grad_norm=0.2575, duration=0.38s
Step 2396: loss=3.5630, lr=0.000594, tokens/sec=1375273.15, grad_norm=0.2462, duration=0.38s
Step 2397: loss=3.5236, lr=0.000594, tokens/sec=1377924.77, grad_norm=0.2556, duration=0.38s
Step 2398: loss=3.5611, lr=0.000594, tokens/sec=1373751.60, grad_norm=0.2967, duration=0.38s
Step 2399: loss=3.5321, lr=0.000594, tokens/sec=1377425.90, grad_norm=0.2719, duration=0.38s
Step 2400/19073 (12.6%), Elapsed time: 1002.69s, Steps per hour: 8616.81, Estimated hours remaining: 1.93
Step 2400: loss=3.6125, lr=0.000594, tokens/sec=1379339.64, grad_norm=0.2611, duration=0.38s
Step 2401: loss=3.5758, lr=0.000594, tokens/sec=1378793.92, grad_norm=0.2948, duration=0.38s
Step 2402: loss=3.5601, lr=0.000594, tokens/sec=1372654.84, grad_norm=0.3195, duration=0.38s
Step 2403: loss=3.5399, lr=0.000594, tokens/sec=1371378.50, grad_norm=0.3307, duration=0.38s
Step 2404: loss=3.5325, lr=0.000594, tokens/sec=1376428.37, grad_norm=0.3097, duration=0.38s
Step 2405: loss=3.5771, lr=0.000594, tokens/sec=1375542.41, grad_norm=0.2924, duration=0.38s
Step 2406: loss=3.5675, lr=0.000594, tokens/sec=1373537.94, grad_norm=0.2935, duration=0.38s
Step 2407: loss=3.5610, lr=0.000594, tokens/sec=1373840.00, grad_norm=0.2964, duration=0.38s
Step 2408: loss=3.5811, lr=0.000594, tokens/sec=1378957.33, grad_norm=0.3182, duration=0.38s
Step 2409: loss=3.5131, lr=0.000594, tokens/sec=1376938.59, grad_norm=0.3618, duration=0.38s
Step 2410: loss=3.5184, lr=0.000594, tokens/sec=1377303.39, grad_norm=0.3083, duration=0.38s
Step 2411: loss=3.4795, lr=0.000594, tokens/sec=1377394.84, grad_norm=0.3067, duration=0.38s
Step 2412: loss=3.4830, lr=0.000594, tokens/sec=1378933.98, grad_norm=0.2674, duration=0.38s
Step 2413: loss=3.4626, lr=0.000594, tokens/sec=1376847.21, grad_norm=0.2696, duration=0.38s
Step 2414: loss=3.5579, lr=0.000594, tokens/sec=1373590.28, grad_norm=0.2731, duration=0.38s
Step 2415: loss=3.5183, lr=0.000594, tokens/sec=1377455.23, grad_norm=0.2872, duration=0.38s
Step 2416: loss=3.4834, lr=0.000594, tokens/sec=1379400.21, grad_norm=0.2836, duration=0.38s
Step 2417: loss=3.4641, lr=0.000594, tokens/sec=1377256.81, grad_norm=0.3210, duration=0.38s
Step 2418: loss=3.4927, lr=0.000594, tokens/sec=1376973.94, grad_norm=0.3323, duration=0.38s
Step 2419: loss=3.5468, lr=0.000594, tokens/sec=1382336.24, grad_norm=0.3278, duration=0.38s
Step 2420: loss=3.6009, lr=0.000594, tokens/sec=1377925.64, grad_norm=0.3332, duration=0.38s
Step 2421: loss=3.6299, lr=0.000594, tokens/sec=1375025.48, grad_norm=0.3678, duration=0.38s
Step 2422: loss=3.6186, lr=0.000594, tokens/sec=1377185.22, grad_norm=0.3720, duration=0.38s
Step 2423: loss=3.5774, lr=0.000594, tokens/sec=1378481.04, grad_norm=0.2986, duration=0.38s
Step 2424: loss=3.6153, lr=0.000594, tokens/sec=1376209.57, grad_norm=0.3050, duration=0.38s
Step 2425: loss=3.5661, lr=0.000594, tokens/sec=1376903.24, grad_norm=0.2856, duration=0.38s
Step 2426: loss=3.5939, lr=0.000594, tokens/sec=1376665.33, grad_norm=0.2656, duration=0.38s
Step 2427: loss=3.6436, lr=0.000594, tokens/sec=1375487.35, grad_norm=0.3084, duration=0.38s
Step 2428: loss=3.7315, lr=0.000594, tokens/sec=1374461.69, grad_norm=0.3587, duration=0.38s
Step 2429: loss=3.6457, lr=0.000594, tokens/sec=1375312.71, grad_norm=0.3701, duration=0.38s
Step 2430: loss=3.6223, lr=0.000594, tokens/sec=1381342.00, grad_norm=0.3153, duration=0.38s
Step 2431: loss=3.5562, lr=0.000594, tokens/sec=1377241.28, grad_norm=0.3206, duration=0.38s
Step 2432: loss=3.5950, lr=0.000594, tokens/sec=1377474.22, grad_norm=0.2797, duration=0.38s
Step 2433: loss=3.5584, lr=0.000594, tokens/sec=1378887.29, grad_norm=0.3123, duration=0.38s
Step 2434: loss=3.5312, lr=0.000594, tokens/sec=1375104.59, grad_norm=0.3010, duration=0.38s
Step 2435: loss=3.6169, lr=0.000594, tokens/sec=1379942.08, grad_norm=0.2937, duration=0.38s
Step 2436: loss=3.5268, lr=0.000594, tokens/sec=1373719.85, grad_norm=0.2526, duration=0.38s
Step 2437: loss=3.6022, lr=0.000594, tokens/sec=1378424.87, grad_norm=0.2415, duration=0.38s
Step 2438: loss=3.6335, lr=0.000594, tokens/sec=1377917.87, grad_norm=0.2755, duration=0.38s
Step 2439: loss=3.5994, lr=0.000594, tokens/sec=1376696.36, grad_norm=0.2756, duration=0.38s
Step 2440: loss=3.5258, lr=0.000594, tokens/sec=1365137.32, grad_norm=0.2902, duration=0.38s
Step 2441: loss=3.5757, lr=0.000594, tokens/sec=1376673.09, grad_norm=0.2536, duration=0.38s
Step 2442: loss=3.5653, lr=0.000594, tokens/sec=1377037.75, grad_norm=0.2393, duration=0.38s
Step 2443: loss=3.5334, lr=0.000594, tokens/sec=1374069.20, grad_norm=0.2691, duration=0.38s
Step 2444: loss=3.4539, lr=0.000594, tokens/sec=1376178.57, grad_norm=0.2618, duration=0.38s
Step 2445: loss=3.5485, lr=0.000594, tokens/sec=1373771.34, grad_norm=0.2630, duration=0.38s
Step 2446: loss=3.5111, lr=0.000594, tokens/sec=1373031.09, grad_norm=0.2386, duration=0.38s
Step 2447: loss=3.5244, lr=0.000594, tokens/sec=1374152.49, grad_norm=0.2867, duration=0.38s
Step 2448: loss=3.5829, lr=0.000594, tokens/sec=1374862.14, grad_norm=0.3233, duration=0.38s
Step 2449: loss=3.4924, lr=0.000594, tokens/sec=1377312.02, grad_norm=0.3213, duration=0.38s
Step 2450: loss=3.5135, lr=0.000594, tokens/sec=1373966.18, grad_norm=0.3256, duration=0.38s
Step 2451: loss=3.5122, lr=0.000594, tokens/sec=1379226.31, grad_norm=0.3154, duration=0.38s
Step 2452: loss=3.4375, lr=0.000594, tokens/sec=1376881.69, grad_norm=0.3723, duration=0.38s
Step 2453: loss=3.5154, lr=0.000594, tokens/sec=1378481.04, grad_norm=0.3501, duration=0.38s
Step 2454: loss=3.5018, lr=0.000594, tokens/sec=1367925.23, grad_norm=0.2742, duration=0.38s
Step 2455: loss=3.4989, lr=0.000594, tokens/sec=1376829.97, grad_norm=0.2793, duration=0.38s
Step 2456: loss=3.4959, lr=0.000594, tokens/sec=1378742.92, grad_norm=0.2931, duration=0.38s
Step 2457: loss=3.5408, lr=0.000594, tokens/sec=1373923.26, grad_norm=0.2650, duration=0.38s
Step 2458: loss=3.5008, lr=0.000594, tokens/sec=1376194.93, grad_norm=0.2790, duration=0.38s
Step 2459: loss=3.4684, lr=0.000594, tokens/sec=1375840.19, grad_norm=0.2753, duration=0.38s
Step 2460: loss=3.4504, lr=0.000594, tokens/sec=1379841.63, grad_norm=0.2828, duration=0.38s
Step 2461: loss=3.4457, lr=0.000594, tokens/sec=1371600.04, grad_norm=0.3174, duration=0.38s
Step 2462: loss=3.5184, lr=0.000594, tokens/sec=1371707.85, grad_norm=0.3172, duration=0.38s
Step 2463: loss=3.4952, lr=0.000594, tokens/sec=1370111.36, grad_norm=0.3309, duration=0.38s
Step 2464: loss=3.5360, lr=0.000594, tokens/sec=1373233.44, grad_norm=0.2945, duration=0.38s
Step 2465: loss=3.5968, lr=0.000594, tokens/sec=1373106.54, grad_norm=0.3067, duration=0.38s
Step 2466: loss=3.5831, lr=0.000594, tokens/sec=1375417.66, grad_norm=0.2935, duration=0.38s
Step 2467: loss=3.5935, lr=0.000594, tokens/sec=1372027.93, grad_norm=0.2897, duration=0.38s
Step 2468: loss=3.5813, lr=0.000594, tokens/sec=1374364.62, grad_norm=0.3286, duration=0.38s
Step 2469: loss=3.5642, lr=0.000594, tokens/sec=1374118.15, grad_norm=0.3880, duration=0.38s
Step 2470: loss=3.6043, lr=0.000594, tokens/sec=1377473.35, grad_norm=0.4304, duration=0.38s
Step 2471: loss=3.6154, lr=0.000594, tokens/sec=1376312.93, grad_norm=0.3708, duration=0.38s
Step 2472: loss=3.6497, lr=0.000594, tokens/sec=1376473.17, grad_norm=0.3169, duration=0.38s
Step 2473: loss=3.6762, lr=0.000594, tokens/sec=1372789.38, grad_norm=0.3283, duration=0.38s
Step 2474: loss=3.6099, lr=0.000594, tokens/sec=1372767.95, grad_norm=0.3273, duration=0.38s
Step 2475: loss=3.6534, lr=0.000594, tokens/sec=1376283.65, grad_norm=0.3235, duration=0.38s
Step 2476: loss=3.7015, lr=0.000594, tokens/sec=1374726.34, grad_norm=0.3895, duration=0.38s
Step 2477: loss=3.6308, lr=0.000594, tokens/sec=1376876.52, grad_norm=0.3659, duration=0.38s
Step 2478: loss=3.6318, lr=0.000594, tokens/sec=1373634.04, grad_norm=0.3145, duration=0.38s
Step 2479: loss=3.6103, lr=0.000594, tokens/sec=1375274.01, grad_norm=0.3088, duration=0.38s
Step 2480: loss=3.5993, lr=0.000594, tokens/sec=1375554.46, grad_norm=0.2616, duration=0.38s
Step 2481: loss=3.5564, lr=0.000594, tokens/sec=1374180.83, grad_norm=0.2485, duration=0.38s
Step 2482: loss=3.5699, lr=0.000594, tokens/sec=1370852.73, grad_norm=0.2709, duration=0.38s
Step 2483: loss=3.5145, lr=0.000594, tokens/sec=1376372.37, grad_norm=0.2704, duration=0.38s
Step 2484: loss=3.6002, lr=0.000594, tokens/sec=1373897.51, grad_norm=0.2700, duration=0.38s
Step 2485: loss=3.5655, lr=0.000594, tokens/sec=1375544.99, grad_norm=0.2853, duration=0.38s
Step 2486: loss=3.5271, lr=0.000594, tokens/sec=1371250.23, grad_norm=0.2582, duration=0.38s
Step 2487: loss=3.5889, lr=0.000594, tokens/sec=1374338.00, grad_norm=0.2593, duration=0.38s
Step 2488: loss=3.5420, lr=0.000594, tokens/sec=1379503.18, grad_norm=0.2777, duration=0.38s
Step 2489: loss=3.5113, lr=0.000594, tokens/sec=1378732.54, grad_norm=0.7275, duration=0.38s
Step 2490: loss=3.5431, lr=0.000594, tokens/sec=1379483.28, grad_norm=0.3133, duration=0.38s
Step 2491: loss=3.5636, lr=0.000594, tokens/sec=1376569.68, grad_norm=0.2922, duration=0.38s
Step 2492: loss=3.6161, lr=0.000594, tokens/sec=1376568.82, grad_norm=0.3133, duration=0.38s
Step 2493: loss=3.4839, lr=0.000594, tokens/sec=1377576.90, grad_norm=0.2684, duration=0.38s
Step 2494: loss=3.5458, lr=0.000594, tokens/sec=1377058.45, grad_norm=0.2961, duration=0.38s
Step 2495: loss=3.5559, lr=0.000594, tokens/sec=1378347.98, grad_norm=0.2945, duration=0.38s
Step 2496: loss=3.5023, lr=0.000594, tokens/sec=1374825.18, grad_norm=0.3947, duration=0.38s
Step 2497: loss=3.4998, lr=0.000594, tokens/sec=1378852.71, grad_norm=0.2623, duration=0.38s
Step 2498: loss=3.5714, lr=0.000594, tokens/sec=1372688.26, grad_norm=0.2563, duration=0.38s
Step 2499: loss=3.5237, lr=0.000594, tokens/sec=1375565.64, grad_norm=0.3173, duration=0.38s
Step 2500/19073 (13.1%), Elapsed time: 1040.87s, Steps per hour: 8646.60, Estimated hours remaining: 1.92
Validation loss at step 2500: 3.767566680908203
Step 2500: loss=3.4679, lr=0.000594, tokens/sec=151215.82, grad_norm=0.4253, duration=3.47s
Step 2501: loss=3.5310, lr=0.000594, tokens/sec=1380592.71, grad_norm=0.4090, duration=0.38s
Step 2502: loss=3.5763, lr=0.000594, tokens/sec=1376594.67, grad_norm=0.4886, duration=0.38s
Step 2503: loss=3.6378, lr=0.000594, tokens/sec=1374027.14, grad_norm=0.3870, duration=0.38s
Step 2504: loss=3.5473, lr=0.000594, tokens/sec=1378990.19, grad_norm=0.4607, duration=0.38s
Step 2505: loss=3.4942, lr=0.000594, tokens/sec=1378933.98, grad_norm=0.4615, duration=0.38s
Step 2506: loss=3.5338, lr=0.000594, tokens/sec=1374028.85, grad_norm=0.3692, duration=0.38s
Step 2507: loss=3.4781, lr=0.000593, tokens/sec=1373046.52, grad_norm=0.3436, duration=0.38s
Step 2508: loss=3.5022, lr=0.000593, tokens/sec=1374752.99, grad_norm=0.3432, duration=0.38s
Step 2509: loss=3.5182, lr=0.000593, tokens/sec=1375327.34, grad_norm=0.2975, duration=0.38s
Step 2510: loss=3.4992, lr=0.000593, tokens/sec=1376872.21, grad_norm=0.2667, duration=0.38s
Step 2511: loss=3.5255, lr=0.000593, tokens/sec=1376794.62, grad_norm=0.2709, duration=0.38s
Step 2512: loss=3.4905, lr=0.000593, tokens/sec=1372568.31, grad_norm=0.2631, duration=0.38s
Step 2513: loss=3.5229, lr=0.000593, tokens/sec=1374935.21, grad_norm=0.2992, duration=0.38s
Step 2514: loss=3.5037, lr=0.000593, tokens/sec=1373248.88, grad_norm=0.3103, duration=0.38s
Step 2515: loss=3.4686, lr=0.000593, tokens/sec=1373690.67, grad_norm=0.2699, duration=0.38s
Step 2516: loss=3.4893, lr=0.000593, tokens/sec=1370989.48, grad_norm=0.2595, duration=0.38s
Step 2517: loss=3.5857, lr=0.000593, tokens/sec=1374869.88, grad_norm=0.2561, duration=0.38s
Step 2518: loss=3.6323, lr=0.000593, tokens/sec=1371476.86, grad_norm=0.2860, duration=0.38s
Step 2519: loss=3.6065, lr=0.000593, tokens/sec=1374675.64, grad_norm=0.2823, duration=0.38s
Step 2520: loss=3.5782, lr=0.000593, tokens/sec=1377461.27, grad_norm=0.2619, duration=0.38s
Step 2521: loss=3.6051, lr=0.000593, tokens/sec=1377029.13, grad_norm=0.2639, duration=0.38s
Step 2522: loss=3.6367, lr=0.000593, tokens/sec=1375498.53, grad_norm=0.2798, duration=0.38s
Step 2523: loss=3.5786, lr=0.000593, tokens/sec=1375627.60, grad_norm=0.2866, duration=0.38s
Step 2524: loss=3.5607, lr=0.000593, tokens/sec=1374115.57, grad_norm=0.2871, duration=0.38s
Step 2525: loss=3.5566, lr=0.000593, tokens/sec=1375884.95, grad_norm=0.2705, duration=0.38s
Step 2526: loss=3.5553, lr=0.000593, tokens/sec=1378125.12, grad_norm=0.2891, duration=0.38s
Step 2527: loss=3.5562, lr=0.000593, tokens/sec=1380040.80, grad_norm=0.2879, duration=0.38s
Step 2528: loss=3.5401, lr=0.000593, tokens/sec=1375528.64, grad_norm=0.3018, duration=0.38s
Step 2529: loss=3.5413, lr=0.000593, tokens/sec=1376919.62, grad_norm=0.3153, duration=0.38s
Step 2530: loss=3.5622, lr=0.000593, tokens/sec=1375069.33, grad_norm=0.2868, duration=0.38s
Step 2531: loss=3.4774, lr=0.000593, tokens/sec=1375420.24, grad_norm=0.2793, duration=0.38s
Step 2532: loss=3.5049, lr=0.000593, tokens/sec=1371198.07, grad_norm=0.2388, duration=0.38s
Step 2533: loss=3.5556, lr=0.000593, tokens/sec=1376034.76, grad_norm=0.2530, duration=0.38s
Step 2534: loss=3.5738, lr=0.000593, tokens/sec=1376141.53, grad_norm=0.2909, duration=0.38s
Step 2535: loss=3.5879, lr=0.000593, tokens/sec=1373864.03, grad_norm=0.2879, duration=0.38s
Step 2536: loss=3.5068, lr=0.000593, tokens/sec=1377149.00, grad_norm=0.2560, duration=0.38s
Step 2537: loss=3.6045, lr=0.000593, tokens/sec=1373130.55, grad_norm=0.2895, duration=0.38s
Step 2538: loss=3.5608, lr=0.000593, tokens/sec=1374143.91, grad_norm=0.2585, duration=0.38s
Step 2539: loss=3.5274, lr=0.000593, tokens/sec=1372269.38, grad_norm=0.2631, duration=0.38s
Step 2540: loss=3.5056, lr=0.000593, tokens/sec=1375089.97, grad_norm=0.2723, duration=0.38s
Step 2541: loss=3.5294, lr=0.000593, tokens/sec=1372289.93, grad_norm=0.2826, duration=0.38s
Step 2542: loss=3.4956, lr=0.000593, tokens/sec=1371872.15, grad_norm=0.2808, duration=0.38s
Step 2543: loss=3.5088, lr=0.000593, tokens/sec=1375048.70, grad_norm=0.2773, duration=0.38s
Step 2544: loss=3.5131, lr=0.000593, tokens/sec=1374321.68, grad_norm=0.2842, duration=0.38s
Step 2545: loss=3.5255, lr=0.000593, tokens/sec=1375548.43, grad_norm=0.2785, duration=0.38s
Step 2546: loss=3.4637, lr=0.000593, tokens/sec=1376852.38, grad_norm=0.2542, duration=0.38s
Step 2547: loss=3.4809, lr=0.000593, tokens/sec=1373966.18, grad_norm=0.2811, duration=0.38s
Step 2548: loss=3.5126, lr=0.000593, tokens/sec=1369207.09, grad_norm=0.2830, duration=0.38s
Step 2549: loss=3.5485, lr=0.000593, tokens/sec=1375994.29, grad_norm=0.2444, duration=0.38s
Step 2550: loss=3.5592, lr=0.000593, tokens/sec=1375212.94, grad_norm=0.2582, duration=0.38s
Step 2551: loss=3.4818, lr=0.000593, tokens/sec=1369277.85, grad_norm=0.2912, duration=0.38s
Step 2552: loss=3.5385, lr=0.000593, tokens/sec=1372920.51, grad_norm=0.3704, duration=0.38s
Step 2553: loss=3.4791, lr=0.000593, tokens/sec=1373604.86, grad_norm=0.3822, duration=0.38s
Step 2554: loss=3.4764, lr=0.000593, tokens/sec=1376455.94, grad_norm=0.3231, duration=0.38s
Step 2555: loss=3.3958, lr=0.000593, tokens/sec=1375711.94, grad_norm=0.3165, duration=0.38s
Step 2556: loss=3.4828, lr=0.000593, tokens/sec=1371027.95, grad_norm=0.2869, duration=0.38s
Step 2557: loss=3.5074, lr=0.000593, tokens/sec=1368650.61, grad_norm=0.2398, duration=0.38s
Step 2558: loss=3.4452, lr=0.000593, tokens/sec=1378383.40, grad_norm=0.2580, duration=0.38s
Step 2559: loss=3.5060, lr=0.000593, tokens/sec=1375600.06, grad_norm=0.2724, duration=0.38s
Step 2560: loss=3.4449, lr=0.000593, tokens/sec=1375592.32, grad_norm=0.2985, duration=0.38s
Step 2561: loss=3.5162, lr=0.000593, tokens/sec=1373048.24, grad_norm=0.2922, duration=0.38s
Step 2562: loss=3.4863, lr=0.000593, tokens/sec=1375530.37, grad_norm=0.2933, duration=0.38s
Step 2563: loss=3.5331, lr=0.000593, tokens/sec=1377166.25, grad_norm=0.3517, duration=0.38s
Step 2564: loss=3.6027, lr=0.000593, tokens/sec=1378997.97, grad_norm=0.3908, duration=0.38s
Step 2565: loss=3.5533, lr=0.000593, tokens/sec=1374804.55, grad_norm=0.3613, duration=0.38s
Step 2566: loss=3.5752, lr=0.000593, tokens/sec=1370311.15, grad_norm=0.3221, duration=0.38s
Step 2567: loss=3.5552, lr=0.000593, tokens/sec=1373326.07, grad_norm=0.3135, duration=0.38s
Step 2568: loss=3.5333, lr=0.000593, tokens/sec=1372415.83, grad_norm=0.3551, duration=0.38s
Step 2569: loss=3.5662, lr=0.000593, tokens/sec=1375517.46, grad_norm=0.3158, duration=0.38s
Step 2570: loss=3.5199, lr=0.000593, tokens/sec=1372613.72, grad_norm=0.3230, duration=0.38s
Step 2571: loss=3.5811, lr=0.000593, tokens/sec=1370683.55, grad_norm=0.3345, duration=0.38s
Step 2572: loss=3.5643, lr=0.000593, tokens/sec=1369037.45, grad_norm=0.3375, duration=0.38s
Step 2573: loss=3.5542, lr=0.000593, tokens/sec=1374021.13, grad_norm=0.3160, duration=0.38s
Step 2574: loss=3.5814, lr=0.000593, tokens/sec=1370933.07, grad_norm=0.2878, duration=0.38s
Step 2575: loss=3.5440, lr=0.000593, tokens/sec=1372648.84, grad_norm=0.2851, duration=0.38s
Step 2576: loss=3.5512, lr=0.000593, tokens/sec=1373979.92, grad_norm=0.2989, duration=0.38s
Step 2577: loss=3.5532, lr=0.000593, tokens/sec=1374727.20, grad_norm=0.2880, duration=0.38s
Step 2578: loss=3.5381, lr=0.000593, tokens/sec=1373021.66, grad_norm=0.2915, duration=0.38s
Step 2579: loss=3.4867, lr=0.000593, tokens/sec=1374100.11, grad_norm=0.2691, duration=0.38s
Step 2580: loss=3.5317, lr=0.000593, tokens/sec=1378358.35, grad_norm=0.2604, duration=0.38s
Step 2581: loss=3.5229, lr=0.000593, tokens/sec=1371831.93, grad_norm=0.2693, duration=0.38s
Step 2582: loss=3.5066, lr=0.000593, tokens/sec=1374575.10, grad_norm=0.2691, duration=0.38s
Step 2583: loss=3.4759, lr=0.000593, tokens/sec=1375687.84, grad_norm=0.2789, duration=0.38s
Step 2584: loss=3.5697, lr=0.000593, tokens/sec=1374783.93, grad_norm=0.2929, duration=0.38s
Step 2585: loss=3.4855, lr=0.000593, tokens/sec=1379203.82, grad_norm=0.2925, duration=0.38s
Step 2586: loss=3.5179, lr=0.000593, tokens/sec=1376132.06, grad_norm=0.2723, duration=0.38s
Step 2587: loss=3.4994, lr=0.000593, tokens/sec=1377210.23, grad_norm=0.2719, duration=0.38s
Step 2588: loss=3.5086, lr=0.000593, tokens/sec=1379397.61, grad_norm=0.2683, duration=0.38s
Step 2589: loss=3.5615, lr=0.000593, tokens/sec=1375602.64, grad_norm=0.2302, duration=0.38s
Step 2590: loss=3.5416, lr=0.000593, tokens/sec=1375817.81, grad_norm=0.2809, duration=0.38s
Step 2591: loss=3.4938, lr=0.000593, tokens/sec=1376429.23, grad_norm=0.2622, duration=0.38s
Step 2592: loss=3.5223, lr=0.000593, tokens/sec=1376430.09, grad_norm=0.2670, duration=0.38s
Step 2593: loss=3.4703, lr=0.000593, tokens/sec=1378447.34, grad_norm=0.2922, duration=0.38s
Step 2594: loss=3.5247, lr=0.000593, tokens/sec=1378344.52, grad_norm=0.2848, duration=0.38s
Step 2595: loss=3.5435, lr=0.000593, tokens/sec=1376500.74, grad_norm=0.3372, duration=0.38s
Step 2596: loss=3.5362, lr=0.000593, tokens/sec=1380093.63, grad_norm=0.3353, duration=0.38s
Step 2597: loss=3.5463, lr=0.000593, tokens/sec=1376941.18, grad_norm=0.3394, duration=0.38s
Step 2598: loss=3.5234, lr=0.000593, tokens/sec=1374975.62, grad_norm=0.3377, duration=0.38s
Step 2599: loss=3.4338, lr=0.000593, tokens/sec=1376736.87, grad_norm=0.3562, duration=0.38s
Step 2600/19073 (13.6%), Elapsed time: 1082.16s, Steps per hour: 8649.35, Estimated hours remaining: 1.90
Step 2600: loss=3.5111, lr=0.000593, tokens/sec=1378743.78, grad_norm=0.2815, duration=0.38s
Step 2601: loss=3.4017, lr=0.000593, tokens/sec=1377737.44, grad_norm=0.3059, duration=0.38s
Step 2602: loss=3.4343, lr=0.000593, tokens/sec=1379691.00, grad_norm=0.2972, duration=0.38s
Step 2603: loss=3.4525, lr=0.000593, tokens/sec=1379494.53, grad_norm=0.3160, duration=0.38s
Step 2604: loss=3.5036, lr=0.000593, tokens/sec=1374930.05, grad_norm=0.3110, duration=0.38s
Step 2605: loss=3.4700, lr=0.000593, tokens/sec=1375738.62, grad_norm=0.2927, duration=0.38s
Step 2606: loss=3.4315, lr=0.000593, tokens/sec=1369089.45, grad_norm=0.2689, duration=0.38s
Step 2607: loss=3.4149, lr=0.000593, tokens/sec=1372354.16, grad_norm=0.2538, duration=0.38s
Step 2608: loss=3.4585, lr=0.000593, tokens/sec=1375410.78, grad_norm=0.2763, duration=0.38s
Step 2609: loss=3.4996, lr=0.000593, tokens/sec=1374291.62, grad_norm=0.2634, duration=0.38s
Step 2610: loss=3.5735, lr=0.000593, tokens/sec=1371961.16, grad_norm=0.2805, duration=0.38s
Step 2611: loss=3.5525, lr=0.000593, tokens/sec=1379442.61, grad_norm=0.3050, duration=0.38s
Step 2612: loss=3.5570, lr=0.000593, tokens/sec=1378214.08, grad_norm=0.2991, duration=0.38s
Step 2613: loss=3.5473, lr=0.000593, tokens/sec=1374767.60, grad_norm=0.3714, duration=0.38s
Step 2614: loss=3.5749, lr=0.000593, tokens/sec=1373682.95, grad_norm=0.3557, duration=0.38s
Step 2615: loss=3.5294, lr=0.000593, tokens/sec=1374908.56, grad_norm=0.3308, duration=0.38s
Step 2616: loss=3.5528, lr=0.000593, tokens/sec=1373828.84, grad_norm=0.3128, duration=0.38s
Step 2617: loss=3.6198, lr=0.000593, tokens/sec=1375124.37, grad_norm=0.2923, duration=0.38s
Step 2618: loss=3.6649, lr=0.000593, tokens/sec=1370734.81, grad_norm=0.2755, duration=0.38s
Step 2619: loss=3.5825, lr=0.000593, tokens/sec=1374771.03, grad_norm=0.2680, duration=0.38s
Step 2620: loss=3.5739, lr=0.000593, tokens/sec=1374392.97, grad_norm=0.2794, duration=0.38s
Step 2621: loss=3.5134, lr=0.000593, tokens/sec=1375723.99, grad_norm=0.2828, duration=0.38s
Step 2622: loss=3.5353, lr=0.000593, tokens/sec=1375656.00, grad_norm=0.3197, duration=0.38s
Step 2623: loss=3.5113, lr=0.000592, tokens/sec=1372761.95, grad_norm=0.2881, duration=0.38s
Step 2624: loss=3.4926, lr=0.000592, tokens/sec=1376303.46, grad_norm=0.2936, duration=0.38s
Step 2625: loss=3.5697, lr=0.000592, tokens/sec=1377596.75, grad_norm=0.3052, duration=0.38s
Step 2626: loss=3.4764, lr=0.000592, tokens/sec=1377679.61, grad_norm=0.3019, duration=0.38s
Step 2627: loss=3.5558, lr=0.000592, tokens/sec=1371351.99, grad_norm=0.2931, duration=0.38s
Step 2628: loss=3.6207, lr=0.000592, tokens/sec=1377221.45, grad_norm=0.2726, duration=0.38s
Step 2629: loss=3.5534, lr=0.000592, tokens/sec=1375575.11, grad_norm=0.2676, duration=0.38s
Step 2630: loss=3.4556, lr=0.000592, tokens/sec=1378247.77, grad_norm=0.2761, duration=0.38s
Step 2631: loss=3.5277, lr=0.000592, tokens/sec=1379396.74, grad_norm=0.2807, duration=0.38s
Step 2632: loss=3.5338, lr=0.000592, tokens/sec=1376148.42, grad_norm=0.2660, duration=0.38s
Step 2633: loss=3.4852, lr=0.000592, tokens/sec=1374373.21, grad_norm=0.2687, duration=0.38s
Step 2634: loss=3.4319, lr=0.000592, tokens/sec=1378061.21, grad_norm=0.2666, duration=0.38s
Step 2635: loss=3.4867, lr=0.000592, tokens/sec=1376389.60, grad_norm=0.2768, duration=0.38s
Step 2636: loss=3.4621, lr=0.000592, tokens/sec=1374873.32, grad_norm=0.3415, duration=0.38s
Step 2637: loss=3.5091, lr=0.000592, tokens/sec=1378985.00, grad_norm=0.3726, duration=0.38s
Step 2638: loss=3.5343, lr=0.000592, tokens/sec=1373739.58, grad_norm=0.3354, duration=0.38s
Step 2639: loss=3.4574, lr=0.000592, tokens/sec=1374822.60, grad_norm=0.2872, duration=0.38s
Step 2640: loss=3.4671, lr=0.000592, tokens/sec=1371300.68, grad_norm=0.2820, duration=0.38s
Step 2641: loss=3.4437, lr=0.000592, tokens/sec=1375603.50, grad_norm=0.2581, duration=0.38s
Step 2642: loss=3.4146, lr=0.000592, tokens/sec=1378685.00, grad_norm=0.2960, duration=0.38s
Step 2643: loss=3.5070, lr=0.000592, tokens/sec=1374346.59, grad_norm=0.2710, duration=0.38s
Step 2644: loss=3.4669, lr=0.000592, tokens/sec=1371074.96, grad_norm=0.2834, duration=0.38s
Step 2645: loss=3.4494, lr=0.000592, tokens/sec=1375088.25, grad_norm=0.2766, duration=0.38s
Step 2646: loss=3.4265, lr=0.000592, tokens/sec=1375274.01, grad_norm=0.2777, duration=0.38s
Step 2647: loss=3.5196, lr=0.000592, tokens/sec=1374537.30, grad_norm=0.2922, duration=0.38s
Step 2648: loss=3.4414, lr=0.000592, tokens/sec=1374980.78, grad_norm=0.2658, duration=0.38s
Step 2649: loss=3.4437, lr=0.000592, tokens/sec=1376439.57, grad_norm=0.2611, duration=0.38s
Step 2650: loss=3.3571, lr=0.000592, tokens/sec=1376621.38, grad_norm=0.2570, duration=0.38s
Step 2651: loss=3.4439, lr=0.000592, tokens/sec=1377645.08, grad_norm=0.2679, duration=0.38s
Step 2652: loss=3.4604, lr=0.000592, tokens/sec=1380207.11, grad_norm=0.2566, duration=0.38s
Step 2653: loss=3.4684, lr=0.000592, tokens/sec=1374761.58, grad_norm=0.2581, duration=0.38s
Step 2654: loss=3.4998, lr=0.000592, tokens/sec=1373554.24, grad_norm=0.2564, duration=0.38s
Step 2655: loss=3.5296, lr=0.000592, tokens/sec=1375855.68, grad_norm=0.2866, duration=0.38s
Step 2656: loss=3.5328, lr=0.000592, tokens/sec=1378018.03, grad_norm=0.2704, duration=0.38s
Step 2657: loss=3.5734, lr=0.000592, tokens/sec=1377549.29, grad_norm=0.2837, duration=0.38s
Step 2658: loss=3.4800, lr=0.000592, tokens/sec=1377500.10, grad_norm=0.3166, duration=0.38s
Step 2659: loss=3.5667, lr=0.000592, tokens/sec=1377866.06, grad_norm=0.3304, duration=0.38s
Step 2660: loss=3.5334, lr=0.000592, tokens/sec=1373809.10, grad_norm=0.3527, duration=0.38s
Step 2661: loss=3.6064, lr=0.000592, tokens/sec=1378357.48, grad_norm=0.3668, duration=0.38s
Step 2662: loss=3.5726, lr=0.000592, tokens/sec=1374785.64, grad_norm=0.3479, duration=0.38s
Step 2663: loss=3.6348, lr=0.000592, tokens/sec=1373344.08, grad_norm=0.3455, duration=0.38s
Step 2664: loss=3.5844, lr=0.000592, tokens/sec=1375417.66, grad_norm=0.3657, duration=0.38s
Step 2665: loss=3.5960, lr=0.000592, tokens/sec=1376701.53, grad_norm=0.3495, duration=0.38s
Step 2666: loss=3.6649, lr=0.000592, tokens/sec=1374552.76, grad_norm=0.3858, duration=0.38s
Step 2667: loss=3.5644, lr=0.000592, tokens/sec=1377778.87, grad_norm=0.3330, duration=0.38s
Step 2668: loss=3.5989, lr=0.000592, tokens/sec=1377937.72, grad_norm=0.2816, duration=0.38s
Step 2669: loss=3.5389, lr=0.000592, tokens/sec=1374851.83, grad_norm=0.2750, duration=0.38s
Step 2670: loss=3.5619, lr=0.000592, tokens/sec=1373402.40, grad_norm=0.2931, duration=0.38s
Step 2671: loss=3.5051, lr=0.000592, tokens/sec=1378675.49, grad_norm=0.2945, duration=0.38s
Step 2672: loss=3.5377, lr=0.000592, tokens/sec=1375053.00, grad_norm=0.2835, duration=0.38s
Step 2673: loss=3.5026, lr=0.000592, tokens/sec=1379620.02, grad_norm=0.2563, duration=0.38s
Step 2674: loss=3.5208, lr=0.000592, tokens/sec=1374339.71, grad_norm=0.2583, duration=0.38s
Step 2675: loss=3.5162, lr=0.000592, tokens/sec=1375176.82, grad_norm=0.2836, duration=0.38s
Step 2676: loss=3.4977, lr=0.000592, tokens/sec=1374661.03, grad_norm=0.3318, duration=0.38s
Step 2677: loss=3.5665, lr=0.000592, tokens/sec=1373362.95, grad_norm=0.3241, duration=0.38s
Step 2678: loss=3.5256, lr=0.000592, tokens/sec=1379151.92, grad_norm=0.3357, duration=0.38s
Step 2679: loss=3.4262, lr=0.000592, tokens/sec=1374418.74, grad_norm=0.4217, duration=0.38s
Step 2680: loss=3.5051, lr=0.000592, tokens/sec=1377433.66, grad_norm=0.2663, duration=0.38s
Step 2681: loss=3.4870, lr=0.000592, tokens/sec=1375298.09, grad_norm=0.3356, duration=0.38s
Step 2682: loss=3.5957, lr=0.000592, tokens/sec=1379288.60, grad_norm=0.3192, duration=0.38s
Step 2683: loss=3.4050, lr=0.000592, tokens/sec=1379993.17, grad_norm=0.2598, duration=0.38s
Step 2684: loss=3.5419, lr=0.000592, tokens/sec=1376101.92, grad_norm=0.2802, duration=0.38s
Step 2685: loss=3.5051, lr=0.000592, tokens/sec=1378767.12, grad_norm=0.2888, duration=0.38s
Step 2686: loss=3.4355, lr=0.000592, tokens/sec=1375329.06, grad_norm=0.2691, duration=0.38s
Step 2687: loss=3.4989, lr=0.000592, tokens/sec=1376803.24, grad_norm=0.2701, duration=0.38s
Step 2688: loss=3.5074, lr=0.000592, tokens/sec=1377570.86, grad_norm=0.2846, duration=0.38s
Step 2689: loss=3.4159, lr=0.000592, tokens/sec=1377798.73, grad_norm=0.2799, duration=0.38s
Step 2690: loss=3.4620, lr=0.000592, tokens/sec=1377778.87, grad_norm=0.2575, duration=0.38s
Step 2691: loss=3.4858, lr=0.000592, tokens/sec=1375494.23, grad_norm=0.2881, duration=0.38s
Step 2692: loss=3.5404, lr=0.000592, tokens/sec=1377728.80, grad_norm=0.3523, duration=0.38s
Step 2693: loss=3.5690, lr=0.000592, tokens/sec=1375400.45, grad_norm=0.2761, duration=0.38s
Step 2694: loss=3.4913, lr=0.000592, tokens/sec=1379138.94, grad_norm=0.2664, duration=0.38s
Step 2695: loss=3.4098, lr=0.000592, tokens/sec=1375043.54, grad_norm=0.2645, duration=0.38s
Step 2696: loss=3.4826, lr=0.000592, tokens/sec=1374535.58, grad_norm=0.2505, duration=0.38s
Step 2697: loss=3.4560, lr=0.000592, tokens/sec=1370685.26, grad_norm=0.2680, duration=0.38s
Step 2698: loss=3.4371, lr=0.000592, tokens/sec=1376815.31, grad_norm=0.2886, duration=0.38s
Step 2699: loss=3.4493, lr=0.000592, tokens/sec=1372876.80, grad_norm=0.2939, duration=0.38s
Step 2700/19073 (14.2%), Elapsed time: 1120.34s, Steps per hour: 8675.90, Estimated hours remaining: 1.89
Step 2700: loss=3.4666, lr=0.000592, tokens/sec=1375509.72, grad_norm=0.2711, duration=0.38s
Step 2701: loss=3.4566, lr=0.000592, tokens/sec=1377612.29, grad_norm=0.2566, duration=0.38s
Step 2702: loss=3.4546, lr=0.000592, tokens/sec=1375761.86, grad_norm=0.2844, duration=0.38s
Step 2703: loss=3.4800, lr=0.000592, tokens/sec=1377637.31, grad_norm=0.2801, duration=0.38s
Step 2704: loss=3.4326, lr=0.000592, tokens/sec=1376894.62, grad_norm=0.2949, duration=0.38s
Step 2705: loss=3.4306, lr=0.000592, tokens/sec=1375601.78, grad_norm=0.2770, duration=0.38s
Step 2706: loss=3.4397, lr=0.000592, tokens/sec=1375496.81, grad_norm=0.2587, duration=0.38s
Step 2707: loss=3.5466, lr=0.000592, tokens/sec=1376814.45, grad_norm=0.2559, duration=0.38s
Step 2708: loss=3.6077, lr=0.000592, tokens/sec=1376678.26, grad_norm=0.2538, duration=0.38s
Step 2709: loss=3.5322, lr=0.000592, tokens/sec=1374507.23, grad_norm=0.2761, duration=0.38s
Step 2710: loss=3.5466, lr=0.000592, tokens/sec=1362700.12, grad_norm=0.3134, duration=0.38s
Step 2711: loss=3.5947, lr=0.000592, tokens/sec=1371942.33, grad_norm=0.3468, duration=0.38s
Step 2712: loss=3.5511, lr=0.000592, tokens/sec=1368349.13, grad_norm=0.3496, duration=0.38s
Step 2713: loss=3.5633, lr=0.000592, tokens/sec=1374954.13, grad_norm=0.3318, duration=0.38s
Step 2714: loss=3.5080, lr=0.000592, tokens/sec=1374672.20, grad_norm=0.3282, duration=0.38s
Step 2715: loss=3.4931, lr=0.000592, tokens/sec=1375857.40, grad_norm=0.2756, duration=0.38s
Step 2716: loss=3.5243, lr=0.000592, tokens/sec=1376690.33, grad_norm=0.2813, duration=0.38s
Step 2717: loss=3.5112, lr=0.000592, tokens/sec=1373873.47, grad_norm=0.2705, duration=0.38s
Step 2718: loss=3.5009, lr=0.000592, tokens/sec=1372629.99, grad_norm=0.2743, duration=0.38s
Step 2719: loss=3.5107, lr=0.000592, tokens/sec=1373582.55, grad_norm=0.3126, duration=0.38s
Step 2720: loss=3.5024, lr=0.000592, tokens/sec=1376405.97, grad_norm=0.2898, duration=0.38s
Step 2721: loss=3.4271, lr=0.000592, tokens/sec=1376039.06, grad_norm=0.2637, duration=0.38s
Step 2722: loss=3.4685, lr=0.000592, tokens/sec=1374694.55, grad_norm=0.2550, duration=0.38s
Step 2723: loss=3.5390, lr=0.000592, tokens/sec=1376649.82, grad_norm=0.2518, duration=0.38s
Step 2724: loss=3.4859, lr=0.000592, tokens/sec=1371238.26, grad_norm=0.2706, duration=0.38s
Step 2725: loss=3.5416, lr=0.000592, tokens/sec=1376611.90, grad_norm=0.2787, duration=0.38s
Step 2726: loss=3.4693, lr=0.000592, tokens/sec=1374430.77, grad_norm=0.2540, duration=0.38s
Step 2727: loss=3.5716, lr=0.000592, tokens/sec=1375427.12, grad_norm=0.2538, duration=0.38s
Step 2728: loss=3.5182, lr=0.000592, tokens/sec=1374078.65, grad_norm=0.2841, duration=0.38s
Step 2729: loss=3.4543, lr=0.000592, tokens/sec=1372138.37, grad_norm=0.2645, duration=0.38s
Step 2730: loss=3.4846, lr=0.000592, tokens/sec=1375871.18, grad_norm=0.2761, duration=0.38s
Step 2731: loss=3.4797, lr=0.000592, tokens/sec=1372559.74, grad_norm=0.2640, duration=0.38s
Step 2732: loss=3.4487, lr=0.000591, tokens/sec=1377142.96, grad_norm=0.2385, duration=0.38s
Step 2733: loss=3.4802, lr=0.000591, tokens/sec=1378120.80, grad_norm=0.2618, duration=0.38s
Step 2734: loss=3.4727, lr=0.000591, tokens/sec=1376384.43, grad_norm=0.2843, duration=0.38s
Step 2735: loss=3.4322, lr=0.000591, tokens/sec=1376213.88, grad_norm=0.3025, duration=0.38s
Step 2736: loss=3.4764, lr=0.000591, tokens/sec=1376289.67, grad_norm=0.2836, duration=0.38s
Step 2737: loss=3.4369, lr=0.000591, tokens/sec=1375008.29, grad_norm=0.3630, duration=0.38s
Step 2738: loss=3.5205, lr=0.000591, tokens/sec=1376338.77, grad_norm=0.3308, duration=0.38s
Step 2739: loss=3.5120, lr=0.000591, tokens/sec=1374686.81, grad_norm=0.3097, duration=0.38s
Step 2740: loss=3.5203, lr=0.000591, tokens/sec=1379304.17, grad_norm=0.2604, duration=0.38s
Step 2741: loss=3.4292, lr=0.000591, tokens/sec=1379364.73, grad_norm=0.2514, duration=0.38s
Step 2742: loss=3.4934, lr=0.000591, tokens/sec=1375113.19, grad_norm=0.2688, duration=0.38s
Step 2743: loss=3.4178, lr=0.000591, tokens/sec=1377094.67, grad_norm=0.2555, duration=0.38s
Step 2744: loss=3.4060, lr=0.000591, tokens/sec=1372547.75, grad_norm=0.2648, duration=0.38s
Step 2745: loss=3.3966, lr=0.000591, tokens/sec=1377089.49, grad_norm=0.2755, duration=0.38s
Step 2746: loss=3.4441, lr=0.000591, tokens/sec=1372477.50, grad_norm=0.3083, duration=0.38s
Step 2747: loss=3.4560, lr=0.000591, tokens/sec=1375709.36, grad_norm=0.3223, duration=0.38s
Step 2748: loss=3.4445, lr=0.000591, tokens/sec=1378319.47, grad_norm=0.3360, duration=0.38s
Step 2749: loss=3.4238, lr=0.000591, tokens/sec=1376440.43, grad_norm=0.2868, duration=0.38s
Validation loss at step 2750: 3.734853506088257
Step 2750: loss=3.4201, lr=0.000591, tokens/sec=152821.34, grad_norm=0.2802, duration=3.43s
Step 2751: loss=3.4612, lr=0.000591, tokens/sec=1380234.83, grad_norm=0.2531, duration=0.38s
Step 2752: loss=3.4443, lr=0.000591, tokens/sec=1374710.01, grad_norm=0.2702, duration=0.38s
Step 2753: loss=3.5046, lr=0.000591, tokens/sec=1378575.23, grad_norm=0.3206, duration=0.38s
Step 2754: loss=3.5022, lr=0.000591, tokens/sec=1380331.00, grad_norm=0.3137, duration=0.38s
Step 2755: loss=3.5310, lr=0.000591, tokens/sec=1375455.51, grad_norm=0.3131, duration=0.38s
Step 2756: loss=3.5194, lr=0.000591, tokens/sec=1377207.65, grad_norm=0.3379, duration=0.38s
Step 2757: loss=3.5412, lr=0.000591, tokens/sec=1379224.58, grad_norm=0.3818, duration=0.38s
Step 2758: loss=3.4811, lr=0.000591, tokens/sec=1374832.92, grad_norm=0.3682, duration=0.38s
Step 2759: loss=3.5084, lr=0.000591, tokens/sec=1374174.82, grad_norm=0.2974, duration=0.38s
Step 2760: loss=3.4869, lr=0.000591, tokens/sec=1369317.92, grad_norm=0.3018, duration=0.38s
Step 2761: loss=3.5338, lr=0.000591, tokens/sec=1368702.58, grad_norm=0.2908, duration=0.38s
Step 2762: loss=3.5246, lr=0.000591, tokens/sec=1372140.08, grad_norm=0.3000, duration=0.38s
Step 2763: loss=3.5217, lr=0.000591, tokens/sec=1374718.61, grad_norm=0.2913, duration=0.38s
Step 2764: loss=3.5383, lr=0.000591, tokens/sec=1374592.29, grad_norm=0.2764, duration=0.38s
Step 2765: loss=3.4940, lr=0.000591, tokens/sec=1373686.38, grad_norm=0.2950, duration=0.38s
Step 2766: loss=3.5351, lr=0.000591, tokens/sec=1373989.36, grad_norm=0.2876, duration=0.38s
Step 2767: loss=3.4648, lr=0.000591, tokens/sec=1377073.11, grad_norm=0.2763, duration=0.38s
Step 2768: loss=3.5043, lr=0.000591, tokens/sec=1375568.22, grad_norm=0.2513, duration=0.38s
Step 2769: loss=3.4444, lr=0.000591, tokens/sec=1377856.57, grad_norm=0.2661, duration=0.38s
Step 2770: loss=3.4932, lr=0.000591, tokens/sec=1371580.37, grad_norm=0.2722, duration=0.38s
Step 2771: loss=3.4654, lr=0.000591, tokens/sec=1374404.14, grad_norm=0.2859, duration=0.38s
Step 2772: loss=3.4808, lr=0.000591, tokens/sec=1373284.04, grad_norm=0.2624, duration=0.38s
Step 2773: loss=3.4493, lr=0.000591, tokens/sec=1372629.99, grad_norm=0.2717, duration=0.38s
Step 2774: loss=3.4902, lr=0.000591, tokens/sec=1373241.16, grad_norm=0.2831, duration=0.38s
Step 2775: loss=3.4382, lr=0.000591, tokens/sec=1376633.45, grad_norm=0.2738, duration=0.38s
Step 2776: loss=3.4919, lr=0.000591, tokens/sec=1372376.43, grad_norm=0.2563, duration=0.38s
Step 2777: loss=3.4517, lr=0.000591, tokens/sec=1374495.20, grad_norm=0.2700, duration=0.38s
Step 2778: loss=3.5439, lr=0.000591, tokens/sec=1378027.53, grad_norm=0.2690, duration=0.38s
Step 2779: loss=3.4960, lr=0.000591, tokens/sec=1373386.96, grad_norm=0.2663, duration=0.38s
Step 2780: loss=3.4669, lr=0.000591, tokens/sec=1372443.24, grad_norm=0.2996, duration=0.38s
Step 2781: loss=3.4653, lr=0.000591, tokens/sec=1374022.84, grad_norm=0.2999, duration=0.38s
Step 2782: loss=3.4590, lr=0.000591, tokens/sec=1375314.43, grad_norm=0.3476, duration=0.38s
Step 2783: loss=3.4690, lr=0.000591, tokens/sec=1378309.97, grad_norm=0.3303, duration=0.38s
Step 2784: loss=3.4974, lr=0.000591, tokens/sec=1375053.86, grad_norm=0.2965, duration=0.38s
Step 2785: loss=3.5125, lr=0.000591, tokens/sec=1377368.09, grad_norm=0.3059, duration=0.38s
Step 2786: loss=3.5218, lr=0.000591, tokens/sec=1378079.34, grad_norm=0.3092, duration=0.38s
Step 2787: loss=3.4906, lr=0.000591, tokens/sec=1374301.06, grad_norm=0.3116, duration=0.38s
Step 2788: loss=3.4433, lr=0.000591, tokens/sec=1375861.71, grad_norm=0.2891, duration=0.38s
Step 2789: loss=3.4294, lr=0.000591, tokens/sec=1378244.31, grad_norm=0.2792, duration=0.38s
Step 2790: loss=3.4422, lr=0.000591, tokens/sec=1377456.10, grad_norm=0.3351, duration=0.38s
Step 2791: loss=3.3534, lr=0.000591, tokens/sec=1377855.70, grad_norm=0.2934, duration=0.38s
Step 2792: loss=3.4265, lr=0.000591, tokens/sec=1376259.53, grad_norm=0.3202, duration=0.38s
Step 2793: loss=3.3962, lr=0.000591, tokens/sec=1381748.21, grad_norm=0.3028, duration=0.38s
Step 2794: loss=3.4597, lr=0.000591, tokens/sec=1373400.69, grad_norm=0.3135, duration=0.38s
Step 2795: loss=3.4224, lr=0.000591, tokens/sec=1374637.83, grad_norm=0.3184, duration=0.38s
Step 2796: loss=3.3914, lr=0.000591, tokens/sec=1372498.06, grad_norm=0.2902, duration=0.38s
Step 2797: loss=3.3894, lr=0.000591, tokens/sec=1375114.91, grad_norm=0.2535, duration=0.38s
Step 2798: loss=3.4218, lr=0.000591, tokens/sec=1380584.04, grad_norm=0.2731, duration=0.38s
Step 2799: loss=3.4837, lr=0.000591, tokens/sec=1376511.94, grad_norm=0.2584, duration=0.38s
Step 2800/19073 (14.7%), Elapsed time: 1161.59s, Steps per hour: 8677.75, Estimated hours remaining: 1.88
Step 2800: loss=3.5022, lr=0.000591, tokens/sec=1377995.58, grad_norm=0.2701, duration=0.38s
Step 2801: loss=3.5030, lr=0.000591, tokens/sec=1377069.66, grad_norm=0.2750, duration=0.38s
Step 2802: loss=3.5318, lr=0.000591, tokens/sec=1375372.93, grad_norm=0.2988, duration=0.38s
Step 2803: loss=3.5070, lr=0.000591, tokens/sec=1377249.05, grad_norm=0.3555, duration=0.38s
Step 2804: loss=3.5403, lr=0.000591, tokens/sec=1376048.53, grad_norm=0.3840, duration=0.38s
Step 2805: loss=3.4866, lr=0.000591, tokens/sec=1374268.43, grad_norm=0.3444, duration=0.38s
Step 2806: loss=3.5334, lr=0.000591, tokens/sec=1376564.51, grad_norm=0.3120, duration=0.38s
Step 2807: loss=3.5617, lr=0.000591, tokens/sec=1377122.26, grad_norm=0.3004, duration=0.38s
Step 2808: loss=3.6140, lr=0.000591, tokens/sec=1375208.64, grad_norm=0.2710, duration=0.38s
Step 2809: loss=3.5428, lr=0.000591, tokens/sec=1374986.79, grad_norm=0.2851, duration=0.38s
Step 2810: loss=3.5437, lr=0.000591, tokens/sec=1379382.04, grad_norm=0.3308, duration=0.38s
Step 2811: loss=3.4587, lr=0.000591, tokens/sec=1373935.28, grad_norm=0.3014, duration=0.38s
Step 2812: loss=3.4949, lr=0.000591, tokens/sec=1372853.65, grad_norm=0.2795, duration=0.38s
Step 2813: loss=3.4809, lr=0.000591, tokens/sec=1374456.54, grad_norm=0.3006, duration=0.38s
Step 2814: loss=3.4551, lr=0.000591, tokens/sec=1377111.91, grad_norm=0.3204, duration=0.38s
Step 2815: loss=3.5191, lr=0.000591, tokens/sec=1371582.08, grad_norm=0.2922, duration=0.38s
Step 2816: loss=3.4283, lr=0.000591, tokens/sec=1372492.06, grad_norm=0.2983, duration=0.38s
Step 2817: loss=3.5471, lr=0.000591, tokens/sec=1377340.49, grad_norm=0.2874, duration=0.38s
Step 2818: loss=3.5742, lr=0.000591, tokens/sec=1375992.57, grad_norm=0.2488, duration=0.38s
Step 2819: loss=3.4856, lr=0.000591, tokens/sec=1378445.61, grad_norm=0.2781, duration=0.38s
Step 2820: loss=3.4076, lr=0.000591, tokens/sec=1374034.86, grad_norm=0.2556, duration=0.38s
Step 2821: loss=3.4932, lr=0.000591, tokens/sec=1377175.73, grad_norm=0.2526, duration=0.38s
Step 2822: loss=3.4869, lr=0.000591, tokens/sec=1380083.24, grad_norm=0.2550, duration=0.38s
Step 2823: loss=3.4618, lr=0.000591, tokens/sec=1377319.78, grad_norm=0.2507, duration=0.38s
Step 2824: loss=3.3715, lr=0.000591, tokens/sec=1378317.74, grad_norm=0.2571, duration=0.38s
Step 2825: loss=3.4325, lr=0.000591, tokens/sec=1376499.02, grad_norm=0.3074, duration=0.38s
Step 2826: loss=3.4413, lr=0.000591, tokens/sec=1376505.91, grad_norm=0.3207, duration=0.38s
Step 2827: loss=3.4575, lr=0.000591, tokens/sec=1376067.48, grad_norm=0.2899, duration=0.38s
Step 2828: loss=3.4995, lr=0.000591, tokens/sec=1378618.45, grad_norm=0.2625, duration=0.38s
Step 2829: loss=3.4167, lr=0.000591, tokens/sec=1376656.72, grad_norm=0.2724, duration=0.38s
Step 2830: loss=3.4034, lr=0.000591, tokens/sec=1378081.93, grad_norm=0.2926, duration=0.38s
Step 2831: loss=3.4279, lr=0.000591, tokens/sec=1372672.84, grad_norm=0.3043, duration=0.38s
Step 2832: loss=3.4137, lr=0.000591, tokens/sec=1377685.65, grad_norm=0.2825, duration=0.38s
Step 2833: loss=3.4685, lr=0.000591, tokens/sec=1374510.66, grad_norm=0.2583, duration=0.38s
Step 2834: loss=3.4196, lr=0.000590, tokens/sec=1380434.11, grad_norm=0.2613, duration=0.38s
Step 2835: loss=3.3831, lr=0.000590, tokens/sec=1376272.45, grad_norm=0.2805, duration=0.38s
Step 2836: loss=3.4061, lr=0.000590, tokens/sec=1376375.82, grad_norm=0.2621, duration=0.38s
Step 2837: loss=3.4644, lr=0.000590, tokens/sec=1378374.76, grad_norm=0.2645, duration=0.38s
Step 2838: loss=3.4210, lr=0.000590, tokens/sec=1376381.85, grad_norm=0.2685, duration=0.38s
Step 2839: loss=3.3563, lr=0.000590, tokens/sec=1374945.53, grad_norm=0.2643, duration=0.38s
Step 2840: loss=3.3608, lr=0.000590, tokens/sec=1375528.64, grad_norm=0.2464, duration=0.38s
Step 2841: loss=3.3911, lr=0.000590, tokens/sec=1378997.97, grad_norm=0.2709, duration=0.38s
Step 2842: loss=3.4446, lr=0.000590, tokens/sec=1377139.51, grad_norm=0.2807, duration=0.38s
Step 2843: loss=3.4402, lr=0.000590, tokens/sec=1377213.68, grad_norm=0.2911, duration=0.38s
Step 2844: loss=3.4426, lr=0.000590, tokens/sec=1375662.88, grad_norm=0.3236, duration=0.38s
Step 2845: loss=3.4886, lr=0.000590, tokens/sec=1374111.28, grad_norm=0.3015, duration=0.38s
Step 2846: loss=3.5221, lr=0.000590, tokens/sec=1379307.63, grad_norm=0.3068, duration=0.38s
Step 2847: loss=3.4776, lr=0.000590, tokens/sec=1377488.88, grad_norm=0.2940, duration=0.38s
Step 2848: loss=3.4896, lr=0.000590, tokens/sec=1373618.59, grad_norm=0.3119, duration=0.38s
Step 2849: loss=3.5092, lr=0.000590, tokens/sec=1374527.85, grad_norm=0.3564, duration=0.38s
Step 2850: loss=3.5159, lr=0.000590, tokens/sec=1380584.91, grad_norm=0.3209, duration=0.38s
Step 2851: loss=3.5233, lr=0.000590, tokens/sec=1373896.65, grad_norm=0.3387, duration=0.38s
Step 2852: loss=3.5318, lr=0.000590, tokens/sec=1376673.09, grad_norm=0.3134, duration=0.38s
Step 2853: loss=3.6013, lr=0.000590, tokens/sec=1373822.83, grad_norm=0.3538, duration=0.38s
Step 2854: loss=3.5321, lr=0.000590, tokens/sec=1376600.70, grad_norm=0.3700, duration=0.38s
Step 2855: loss=3.5657, lr=0.000590, tokens/sec=1375040.96, grad_norm=0.3745, duration=0.38s
Step 2856: loss=3.6015, lr=0.000590, tokens/sec=1379838.17, grad_norm=0.3587, duration=0.38s
Step 2857: loss=3.5366, lr=0.000590, tokens/sec=1377260.26, grad_norm=0.3077, duration=0.38s
Step 2858: loss=3.5359, lr=0.000590, tokens/sec=1378665.12, grad_norm=0.3004, duration=0.38s
Step 2859: loss=3.5017, lr=0.000590, tokens/sec=1376219.91, grad_norm=0.2600, duration=0.38s
Step 2860: loss=3.5083, lr=0.000590, tokens/sec=1375822.97, grad_norm=0.2541, duration=0.38s
Step 2861: loss=3.4725, lr=0.000590, tokens/sec=1378714.39, grad_norm=0.2390, duration=0.38s
Step 2862: loss=3.5274, lr=0.000590, tokens/sec=1376082.12, grad_norm=0.2492, duration=0.38s
Step 2863: loss=3.4228, lr=0.000590, tokens/sec=1378506.10, grad_norm=0.2225, duration=0.38s
Step 2864: loss=3.4742, lr=0.000590, tokens/sec=1377800.45, grad_norm=0.2490, duration=0.38s
Step 2865: loss=3.4875, lr=0.000590, tokens/sec=1378640.05, grad_norm=0.2614, duration=0.38s
Step 2866: loss=3.4650, lr=0.000590, tokens/sec=1377492.34, grad_norm=0.2435, duration=0.38s
Step 2867: loss=3.5432, lr=0.000590, tokens/sec=1374537.30, grad_norm=0.2779, duration=0.38s
Step 2868: loss=3.4452, lr=0.000590, tokens/sec=1378712.66, grad_norm=0.2629, duration=0.38s
Step 2869: loss=3.4093, lr=0.000590, tokens/sec=1379202.09, grad_norm=0.6911, duration=0.38s
Step 2870: loss=3.4321, lr=0.000590, tokens/sec=1380382.12, grad_norm=0.2873, duration=0.38s
Step 2871: loss=3.4686, lr=0.000590, tokens/sec=1379415.78, grad_norm=0.2652, duration=0.38s
Step 2872: loss=3.5216, lr=0.000590, tokens/sec=1377924.77, grad_norm=0.2850, duration=0.38s
Step 2873: loss=3.4085, lr=0.000590, tokens/sec=1375627.60, grad_norm=0.2564, duration=0.38s
Step 2874: loss=3.4961, lr=0.000590, tokens/sec=1377238.70, grad_norm=0.2676, duration=0.38s
Step 2875: loss=3.4425, lr=0.000590, tokens/sec=1375997.73, grad_norm=0.2531, duration=0.38s
Step 2876: loss=3.4409, lr=0.000590, tokens/sec=1377285.28, grad_norm=0.2457, duration=0.38s
Step 2877: loss=3.4410, lr=0.000590, tokens/sec=1375053.86, grad_norm=0.2382, duration=0.38s
Step 2878: loss=3.4095, lr=0.000590, tokens/sec=1377917.00, grad_norm=0.2708, duration=0.38s
Step 2879: loss=3.4253, lr=0.000590, tokens/sec=1375063.32, grad_norm=0.2648, duration=0.38s
Step 2880: loss=3.4331, lr=0.000590, tokens/sec=1376957.56, grad_norm=0.2682, duration=0.38s
Step 2881: loss=3.4901, lr=0.000590, tokens/sec=1377852.25, grad_norm=0.3067, duration=0.38s
Step 2882: loss=3.4948, lr=0.000590, tokens/sec=1379445.20, grad_norm=0.3598, duration=0.38s
Step 2883: loss=3.5383, lr=0.000590, tokens/sec=1379113.86, grad_norm=0.3285, duration=0.38s
Step 2884: loss=3.4321, lr=0.000590, tokens/sec=1375571.67, grad_norm=0.2757, duration=0.38s
Step 2885: loss=3.3815, lr=0.000590, tokens/sec=1378010.26, grad_norm=0.2507, duration=0.38s
Step 2886: loss=3.4800, lr=0.000590, tokens/sec=1376424.06, grad_norm=0.2543, duration=0.38s
Step 2887: loss=3.4036, lr=0.000590, tokens/sec=1371962.02, grad_norm=0.2458, duration=0.38s
Step 2888: loss=3.3816, lr=0.000590, tokens/sec=1376077.81, grad_norm=0.2662, duration=0.38s
Step 2889: loss=3.4237, lr=0.000590, tokens/sec=1378227.90, grad_norm=0.2986, duration=0.38s
Step 2890: loss=3.4126, lr=0.000590, tokens/sec=1376151.01, grad_norm=0.2899, duration=0.38s
Step 2891: loss=3.4291, lr=0.000590, tokens/sec=1379258.32, grad_norm=0.3020, duration=0.38s
Step 2892: loss=3.4218, lr=0.000590, tokens/sec=1376576.57, grad_norm=0.2889, duration=0.38s
Step 2893: loss=3.4188, lr=0.000590, tokens/sec=1377339.62, grad_norm=0.2692, duration=0.38s
Step 2894: loss=3.4039, lr=0.000590, tokens/sec=1375534.67, grad_norm=0.2489, duration=0.38s
Step 2895: loss=3.3942, lr=0.000590, tokens/sec=1377825.49, grad_norm=0.3035, duration=0.38s
Step 2896: loss=3.4120, lr=0.000590, tokens/sec=1374423.04, grad_norm=0.3249, duration=0.38s
Step 2897: loss=3.5377, lr=0.000590, tokens/sec=1379303.30, grad_norm=0.3611, duration=0.38s
Step 2898: loss=3.5516, lr=0.000590, tokens/sec=1380959.45, grad_norm=0.3785, duration=0.38s
Step 2899: loss=3.5130, lr=0.000590, tokens/sec=1377774.55, grad_norm=0.3330, duration=0.38s
Step 2900/19073 (15.2%), Elapsed time: 1199.75s, Steps per hour: 8701.82, Estimated hours remaining: 1.86
Step 2900: loss=3.5416, lr=0.000590, tokens/sec=1379967.19, grad_norm=0.3315, duration=0.38s
Step 2901: loss=3.5143, lr=0.000590, tokens/sec=1376967.04, grad_norm=0.2998, duration=0.38s
Step 2902: loss=3.5382, lr=0.000590, tokens/sec=1376393.05, grad_norm=0.3287, duration=0.38s
Step 2903: loss=3.5156, lr=0.000590, tokens/sec=1375551.02, grad_norm=0.3532, duration=0.38s
Step 2904: loss=3.4506, lr=0.000590, tokens/sec=1375431.42, grad_norm=0.2947, duration=0.38s
Step 2905: loss=3.4739, lr=0.000590, tokens/sec=1376192.35, grad_norm=0.3132, duration=0.38s
Step 2906: loss=3.4888, lr=0.000590, tokens/sec=1378278.87, grad_norm=0.3385, duration=0.38s
Step 2907: loss=3.4843, lr=0.000590, tokens/sec=1375257.67, grad_norm=0.3102, duration=0.38s
Step 2908: loss=3.4720, lr=0.000590, tokens/sec=1376233.69, grad_norm=0.2705, duration=0.38s
Step 2909: loss=3.4576, lr=0.000590, tokens/sec=1375264.55, grad_norm=0.2741, duration=0.38s
Step 2910: loss=3.4578, lr=0.000590, tokens/sec=1376961.01, grad_norm=0.2545, duration=0.38s
Step 2911: loss=3.4007, lr=0.000590, tokens/sec=1375730.01, grad_norm=0.3082, duration=0.38s
Step 2912: loss=3.4633, lr=0.000590, tokens/sec=1375242.18, grad_norm=0.2710, duration=0.38s
Step 2913: loss=3.4639, lr=0.000590, tokens/sec=1377274.06, grad_norm=0.2769, duration=0.38s
Step 2914: loss=3.4503, lr=0.000590, tokens/sec=1374021.13, grad_norm=0.2561, duration=0.38s
Step 2915: loss=3.5078, lr=0.000590, tokens/sec=1377951.54, grad_norm=0.2606, duration=0.38s
Step 2916: loss=3.4437, lr=0.000590, tokens/sec=1376485.23, grad_norm=0.2750, duration=0.38s
Step 2917: loss=3.5306, lr=0.000590, tokens/sec=1375265.41, grad_norm=0.2745, duration=0.38s
Step 2918: loss=3.4485, lr=0.000590, tokens/sec=1377130.02, grad_norm=0.2836, duration=0.38s
Step 2919: loss=3.4377, lr=0.000590, tokens/sec=1374805.41, grad_norm=0.2496, duration=0.38s
Step 2920: loss=3.4418, lr=0.000590, tokens/sec=1377253.36, grad_norm=0.2722, duration=0.38s
Step 2921: loss=3.4389, lr=0.000590, tokens/sec=1378485.36, grad_norm=0.2580, duration=0.38s
Step 2922: loss=3.4292, lr=0.000590, tokens/sec=1377325.82, grad_norm=0.2816, duration=0.38s
Step 2923: loss=3.4458, lr=0.000590, tokens/sec=1379323.20, grad_norm=0.2804, duration=0.38s
Step 2924: loss=3.3857, lr=0.000590, tokens/sec=1377511.32, grad_norm=0.2950, duration=0.38s
Step 2925: loss=3.4466, lr=0.000590, tokens/sec=1378084.52, grad_norm=0.2702, duration=0.38s
Step 2926: loss=3.4354, lr=0.000590, tokens/sec=1374752.99, grad_norm=0.2912, duration=0.38s
Step 2927: loss=3.4460, lr=0.000590, tokens/sec=1377142.96, grad_norm=0.3195, duration=0.38s
Step 2928: loss=3.4794, lr=0.000590, tokens/sec=1378492.27, grad_norm=0.2673, duration=0.38s
Step 2929: loss=3.4730, lr=0.000590, tokens/sec=1376216.46, grad_norm=0.2926, duration=0.38s
Step 2930: loss=3.4689, lr=0.000590, tokens/sec=1379723.03, grad_norm=0.2455, duration=0.38s
Step 2931: loss=3.3919, lr=0.000590, tokens/sec=1377652.85, grad_norm=0.2756, duration=0.38s
Step 2932: loss=3.4439, lr=0.000589, tokens/sec=1376427.51, grad_norm=0.2646, duration=0.38s
Step 2933: loss=3.3570, lr=0.000589, tokens/sec=1374407.57, grad_norm=0.2566, duration=0.38s
Step 2934: loss=3.4154, lr=0.000589, tokens/sec=1376369.79, grad_norm=0.2554, duration=0.38s
Step 2935: loss=3.3647, lr=0.000589, tokens/sec=1375270.57, grad_norm=0.2726, duration=0.38s
Step 2936: loss=3.3932, lr=0.000589, tokens/sec=1375592.32, grad_norm=0.2860, duration=0.38s
Step 2937: loss=3.4519, lr=0.000589, tokens/sec=1376944.63, grad_norm=0.2881, duration=0.38s
Step 2938: loss=3.3627, lr=0.000589, tokens/sec=1380293.74, grad_norm=0.2539, duration=0.38s
Step 2939: loss=3.4073, lr=0.000589, tokens/sec=1381080.00, grad_norm=0.2625, duration=0.38s
Step 2940: loss=3.3719, lr=0.000589, tokens/sec=1378826.77, grad_norm=0.2546, duration=0.38s
Step 2941: loss=3.4228, lr=0.000589, tokens/sec=1375625.88, grad_norm=0.2539, duration=0.38s
Step 2942: loss=3.4241, lr=0.000589, tokens/sec=1378633.14, grad_norm=0.2991, duration=0.38s
Step 2943: loss=3.4171, lr=0.000589, tokens/sec=1379570.68, grad_norm=0.3284, duration=0.38s
Step 2944: loss=3.4877, lr=0.000589, tokens/sec=1380531.17, grad_norm=0.3317, duration=0.38s
Step 2945: loss=3.4782, lr=0.000589, tokens/sec=1379409.72, grad_norm=0.2926, duration=0.38s
Step 2946: loss=3.5026, lr=0.000589, tokens/sec=1378312.56, grad_norm=0.3147, duration=0.38s
Step 2947: loss=3.4933, lr=0.000589, tokens/sec=1373091.11, grad_norm=0.3133, duration=0.38s
Step 2948: loss=3.4235, lr=0.000589, tokens/sec=1375880.65, grad_norm=0.2470, duration=0.38s
Step 2949: loss=3.4837, lr=0.000589, tokens/sec=1381269.99, grad_norm=0.3412, duration=0.38s
Step 2950: loss=3.4485, lr=0.000589, tokens/sec=1375233.58, grad_norm=0.3874, duration=0.38s
Step 2951: loss=3.5064, lr=0.000589, tokens/sec=1376153.59, grad_norm=0.3656, duration=0.38s
Step 2952: loss=3.5031, lr=0.000589, tokens/sec=1377104.15, grad_norm=0.3549, duration=0.38s
Step 2953: loss=3.4887, lr=0.000589, tokens/sec=1373068.81, grad_norm=0.3234, duration=0.38s
Step 2954: loss=3.4957, lr=0.000589, tokens/sec=1374082.94, grad_norm=0.3185, duration=0.38s
Step 2955: loss=3.4891, lr=0.000589, tokens/sec=1374387.82, grad_norm=0.3594, duration=0.38s
Step 2956: loss=3.4547, lr=0.000589, tokens/sec=1377223.17, grad_norm=0.3325, duration=0.38s
Step 2957: loss=3.4406, lr=0.000589, tokens/sec=1372152.93, grad_norm=0.2828, duration=0.38s
Step 2958: loss=3.4713, lr=0.000589, tokens/sec=1378003.35, grad_norm=0.2789, duration=0.38s
Step 2959: loss=3.4099, lr=0.000589, tokens/sec=1374508.95, grad_norm=0.2618, duration=0.38s
Step 2960: loss=3.4413, lr=0.000589, tokens/sec=1374483.17, grad_norm=0.2808, duration=0.38s
Step 2961: loss=3.4467, lr=0.000589, tokens/sec=1377797.00, grad_norm=0.2557, duration=0.38s
Step 2962: loss=3.4637, lr=0.000589, tokens/sec=1372054.47, grad_norm=0.2679, duration=0.38s
Step 2963: loss=3.3735, lr=0.000589, tokens/sec=1374891.37, grad_norm=0.2532, duration=0.38s
Step 2964: loss=3.4497, lr=0.000589, tokens/sec=1376191.49, grad_norm=0.2685, duration=0.38s
Step 2965: loss=3.4197, lr=0.000589, tokens/sec=1377309.43, grad_norm=0.2611, duration=0.38s
Step 2966: loss=3.4525, lr=0.000589, tokens/sec=1374044.31, grad_norm=0.2884, duration=0.38s
Step 2967: loss=3.4959, lr=0.000589, tokens/sec=1375699.03, grad_norm=0.3182, duration=0.38s
Step 2968: loss=3.4833, lr=0.000589, tokens/sec=1378840.60, grad_norm=0.3006, duration=0.38s
Step 2969: loss=3.4268, lr=0.000589, tokens/sec=1371823.37, grad_norm=0.2883, duration=0.38s
Step 2970: loss=3.4372, lr=0.000589, tokens/sec=1378295.28, grad_norm=0.2574, duration=0.38s
Step 2971: loss=3.4015, lr=0.000589, tokens/sec=1374236.65, grad_norm=0.2760, duration=0.38s
Step 2972: loss=3.4581, lr=0.000589, tokens/sec=1376101.06, grad_norm=0.2822, duration=0.38s
Step 2973: loss=3.4440, lr=0.000589, tokens/sec=1378467.21, grad_norm=0.2954, duration=0.38s
Step 2974: loss=3.4690, lr=0.000589, tokens/sec=1379457.32, grad_norm=0.2937, duration=0.38s
Step 2975: loss=3.4998, lr=0.000589, tokens/sec=1380062.45, grad_norm=0.2780, duration=0.38s
Step 2976: loss=3.4664, lr=0.000589, tokens/sec=1377492.34, grad_norm=0.2782, duration=0.38s
Step 2977: loss=3.4205, lr=0.000589, tokens/sec=1376854.10, grad_norm=0.3104, duration=0.38s
Step 2978: loss=3.4486, lr=0.000589, tokens/sec=1377091.22, grad_norm=0.2917, duration=0.38s
Step 2979: loss=3.3630, lr=0.000589, tokens/sec=1373662.35, grad_norm=0.2997, duration=0.38s
Step 2980: loss=3.4007, lr=0.000589, tokens/sec=1373699.25, grad_norm=0.3132, duration=0.38s
Step 2981: loss=3.3459, lr=0.000589, tokens/sec=1378237.40, grad_norm=0.2753, duration=0.38s
Step 2982: loss=3.3742, lr=0.000589, tokens/sec=1374548.47, grad_norm=0.2985, duration=0.38s
Step 2983: loss=3.3535, lr=0.000589, tokens/sec=1380753.95, grad_norm=0.2619, duration=0.38s
Step 2984: loss=3.4146, lr=0.000589, tokens/sec=1378066.39, grad_norm=0.2736, duration=0.38s
Step 2985: loss=3.3824, lr=0.000589, tokens/sec=1374940.37, grad_norm=0.2660, duration=0.38s
Step 2986: loss=3.3675, lr=0.000589, tokens/sec=1372435.53, grad_norm=0.2426, duration=0.38s
Step 2987: loss=3.3580, lr=0.000589, tokens/sec=1374865.58, grad_norm=0.2534, duration=0.38s
Step 2988: loss=3.4096, lr=0.000589, tokens/sec=1375961.57, grad_norm=0.2555, duration=0.38s
Step 2989: loss=3.4205, lr=0.000589, tokens/sec=1375147.58, grad_norm=0.2501, duration=0.38s
Step 2990: loss=3.4567, lr=0.000589, tokens/sec=1378838.88, grad_norm=0.2661, duration=0.38s
Step 2991: loss=3.4781, lr=0.000589, tokens/sec=1375200.90, grad_norm=0.2608, duration=0.38s
Step 2992: loss=3.4948, lr=0.000589, tokens/sec=1376758.42, grad_norm=0.2995, duration=0.38s
Step 2993: loss=3.4772, lr=0.000589, tokens/sec=1376388.74, grad_norm=0.3532, duration=0.38s
Step 2994: loss=3.4999, lr=0.000589, tokens/sec=1376332.74, grad_norm=0.3316, duration=0.38s
Step 2995: loss=3.4686, lr=0.000589, tokens/sec=1377395.70, grad_norm=0.3065, duration=0.38s
Step 2996: loss=3.4789, lr=0.000589, tokens/sec=1378352.30, grad_norm=0.3234, duration=0.38s
Step 2997: loss=3.5124, lr=0.000589, tokens/sec=1377130.02, grad_norm=0.2793, duration=0.38s
Step 2998: loss=3.5778, lr=0.000589, tokens/sec=1376770.49, grad_norm=0.2817, duration=0.38s
Step 2999: loss=3.5170, lr=0.000589, tokens/sec=1378285.78, grad_norm=0.3150, duration=0.38s
Step 3000/19073 (15.7%), Elapsed time: 1237.91s, Steps per hour: 8724.40, Estimated hours remaining: 1.84
Validation loss at step 3000: 3.720472812652588
Step 3000: loss=3.4886, lr=0.000589, tokens/sec=152378.39, grad_norm=0.3057, duration=3.44s
Step 3001: loss=3.4242, lr=0.000589, tokens/sec=1376023.56, grad_norm=0.2691, duration=0.38s
Step 3002: loss=3.4680, lr=0.000589, tokens/sec=1377197.30, grad_norm=0.2705, duration=0.38s
Step 3003: loss=3.4420, lr=0.000589, tokens/sec=1382388.38, grad_norm=0.2802, duration=0.38s
Step 3004: loss=3.4029, lr=0.000589, tokens/sec=1379142.40, grad_norm=0.2713, duration=0.38s
Step 3005: loss=3.4764, lr=0.000589, tokens/sec=1373896.65, grad_norm=0.2800, duration=0.38s
Step 3006: loss=3.4220, lr=0.000589, tokens/sec=1378178.66, grad_norm=0.2676, duration=0.38s
Step 3007: loss=3.5064, lr=0.000589, tokens/sec=1378791.33, grad_norm=0.2629, duration=0.38s
Step 3008: loss=3.5134, lr=0.000589, tokens/sec=1376374.96, grad_norm=0.2724, duration=0.38s
Step 3009: loss=3.4433, lr=0.000589, tokens/sec=1377096.39, grad_norm=0.3080, duration=0.38s
Step 3010: loss=3.3843, lr=0.000589, tokens/sec=1379439.14, grad_norm=0.2688, duration=0.38s
Step 3011: loss=3.4503, lr=0.000589, tokens/sec=1373176.85, grad_norm=0.2703, duration=0.38s
Step 3012: loss=3.4708, lr=0.000589, tokens/sec=1378583.01, grad_norm=0.2512, duration=0.38s
Step 3013: loss=3.4095, lr=0.000589, tokens/sec=1378471.53, grad_norm=0.2524, duration=0.38s
Step 3014: loss=3.3255, lr=0.000589, tokens/sec=1378610.67, grad_norm=0.2823, duration=0.38s
Step 3015: loss=3.4159, lr=0.000589, tokens/sec=1376398.22, grad_norm=0.2904, duration=0.38s
Step 3016: loss=3.3971, lr=0.000589, tokens/sec=1372391.85, grad_norm=0.2719, duration=0.38s
Step 3017: loss=3.4326, lr=0.000589, tokens/sec=1377033.44, grad_norm=0.2836, duration=0.38s
Step 3018: loss=3.4648, lr=0.000589, tokens/sec=1377651.12, grad_norm=0.2664, duration=0.38s
Step 3019: loss=3.3567, lr=0.000589, tokens/sec=1378018.89, grad_norm=0.2695, duration=0.38s
Step 3020: loss=3.3903, lr=0.000589, tokens/sec=1377318.92, grad_norm=0.2742, duration=0.38s
Step 3021: loss=3.4328, lr=0.000589, tokens/sec=1376171.68, grad_norm=0.2889, duration=0.38s
Step 3022: loss=3.3855, lr=0.000589, tokens/sec=1379493.66, grad_norm=0.3018, duration=0.38s
Step 3023: loss=3.4316, lr=0.000589, tokens/sec=1375829.00, grad_norm=0.3194, duration=0.38s
Step 3024: loss=3.3668, lr=0.000589, tokens/sec=1374739.23, grad_norm=0.3542, duration=0.38s
Step 3025: loss=3.3751, lr=0.000588, tokens/sec=1368103.10, grad_norm=0.3320, duration=0.38s
Step 3026: loss=3.3614, lr=0.000588, tokens/sec=1375749.81, grad_norm=0.3068, duration=0.38s
Step 3027: loss=3.4510, lr=0.000588, tokens/sec=1375752.39, grad_norm=0.2791, duration=0.38s
Step 3028: loss=3.3457, lr=0.000588, tokens/sec=1370576.76, grad_norm=0.3012, duration=0.38s
Step 3029: loss=3.3660, lr=0.000588, tokens/sec=1377885.92, grad_norm=0.2675, duration=0.38s
Step 3030: loss=3.3188, lr=0.000588, tokens/sec=1378062.07, grad_norm=0.2842, duration=0.38s
Step 3031: loss=3.3845, lr=0.000588, tokens/sec=1377785.78, grad_norm=0.2767, duration=0.38s
Step 3032: loss=3.4206, lr=0.000588, tokens/sec=1376917.04, grad_norm=0.2988, duration=0.38s
Step 3033: loss=3.3851, lr=0.000588, tokens/sec=1371582.93, grad_norm=0.2785, duration=0.38s
Step 3034: loss=3.4043, lr=0.000588, tokens/sec=1376749.80, grad_norm=0.3081, duration=0.38s
Step 3035: loss=3.4813, lr=0.000588, tokens/sec=1374551.90, grad_norm=0.2882, duration=0.38s
Step 3036: loss=3.4293, lr=0.000588, tokens/sec=1375312.71, grad_norm=0.3041, duration=0.38s
Step 3037: loss=3.4950, lr=0.000588, tokens/sec=1375238.74, grad_norm=0.3132, duration=0.38s
Step 3038: loss=3.4391, lr=0.000588, tokens/sec=1372801.38, grad_norm=0.3149, duration=0.38s
Step 3039: loss=3.4980, lr=0.000588, tokens/sec=1374027.99, grad_norm=0.3015, duration=0.38s
Step 3040: loss=3.4493, lr=0.000588, tokens/sec=1375653.42, grad_norm=0.3232, duration=0.38s
Step 3041: loss=3.4891, lr=0.000588, tokens/sec=1377796.14, grad_norm=0.3020, duration=0.38s
Step 3042: loss=3.5029, lr=0.000588, tokens/sec=1374091.53, grad_norm=0.3118, duration=0.38s
Step 3043: loss=3.5623, lr=0.000588, tokens/sec=1375783.38, grad_norm=0.3491, duration=0.38s
Step 3044: loss=3.4991, lr=0.000588, tokens/sec=1373738.73, grad_norm=0.3018, duration=0.38s
Step 3045: loss=3.5093, lr=0.000588, tokens/sec=1371181.83, grad_norm=0.3440, duration=0.38s
Step 3046: loss=3.5752, lr=0.000588, tokens/sec=1375325.62, grad_norm=0.3299, duration=0.38s
Step 3047: loss=3.4756, lr=0.000588, tokens/sec=1373408.41, grad_norm=0.2953, duration=0.38s
Step 3048: loss=3.5020, lr=0.000588, tokens/sec=1378257.27, grad_norm=0.2641, duration=0.38s
Step 3049: loss=3.4556, lr=0.000588, tokens/sec=1373892.36, grad_norm=0.2687, duration=0.38s
Step 3050: loss=3.4853, lr=0.000588, tokens/sec=1374929.19, grad_norm=0.2624, duration=0.38s
Step 3051: loss=3.4700, lr=0.000588, tokens/sec=1374277.87, grad_norm=0.2435, duration=0.38s
Step 3052: loss=3.4539, lr=0.000588, tokens/sec=1376193.21, grad_norm=0.2312, duration=0.38s
Step 3053: loss=3.3836, lr=0.000588, tokens/sec=1380403.78, grad_norm=0.2423, duration=0.38s
Step 3054: loss=3.4525, lr=0.000588, tokens/sec=1375700.75, grad_norm=0.2510, duration=0.38s
Step 3055: loss=3.4646, lr=0.000588, tokens/sec=1374773.61, grad_norm=0.2514, duration=0.38s
Step 3056: loss=3.4510, lr=0.000588, tokens/sec=1378469.81, grad_norm=0.2563, duration=0.38s
Step 3057: loss=3.4747, lr=0.000588, tokens/sec=1378875.19, grad_norm=0.2954, duration=0.38s
Step 3058: loss=3.4198, lr=0.000588, tokens/sec=1374698.84, grad_norm=0.2914, duration=0.38s
Step 3059: loss=3.3294, lr=0.000588, tokens/sec=1375026.34, grad_norm=0.3481, duration=0.38s
Step 3060: loss=3.4167, lr=0.000588, tokens/sec=1376631.72, grad_norm=0.2543, duration=0.38s
Step 3061: loss=3.4001, lr=0.000588, tokens/sec=1376791.17, grad_norm=0.2484, duration=0.38s
Step 3062: loss=3.5259, lr=0.000588, tokens/sec=1379269.56, grad_norm=0.2776, duration=0.38s
Step 3063: loss=3.3618, lr=0.000588, tokens/sec=1376052.84, grad_norm=0.2716, duration=0.38s
Step 3064: loss=3.4360, lr=0.000588, tokens/sec=1375782.51, grad_norm=0.2654, duration=0.38s
Step 3065: loss=3.4520, lr=0.000588, tokens/sec=1377754.70, grad_norm=0.2819, duration=0.38s
Step 3066: loss=3.3876, lr=0.000588, tokens/sec=1379720.43, grad_norm=0.2838, duration=0.38s
Step 3067: loss=3.3489, lr=0.000588, tokens/sec=1374005.67, grad_norm=0.2862, duration=0.38s
Step 3068: loss=3.4242, lr=0.000588, tokens/sec=1371929.49, grad_norm=0.3007, duration=0.38s
Step 3069: loss=3.4013, lr=0.000588, tokens/sec=1371012.56, grad_norm=0.3072, duration=0.38s
Step 3070: loss=3.4401, lr=0.000588, tokens/sec=1375348.84, grad_norm=0.2882, duration=0.38s
Step 3071: loss=3.4441, lr=0.000588, tokens/sec=1380266.02, grad_norm=0.2609, duration=0.38s
Step 3072: loss=3.4672, lr=0.000588, tokens/sec=1375952.10, grad_norm=0.3515, duration=0.38s
Step 3073: loss=3.4803, lr=0.000588, tokens/sec=1372816.80, grad_norm=0.3049, duration=0.38s
Step 3074: loss=3.4133, lr=0.000588, tokens/sec=1377737.44, grad_norm=0.3232, duration=0.38s
Step 3075: loss=3.3838, lr=0.000588, tokens/sec=1376241.44, grad_norm=0.2818, duration=0.38s
Step 3076: loss=3.4355, lr=0.000588, tokens/sec=1372638.56, grad_norm=0.2822, duration=0.38s
Step 3077: loss=3.3564, lr=0.000588, tokens/sec=1373870.04, grad_norm=0.2800, duration=0.38s
Step 3078: loss=3.3625, lr=0.000588, tokens/sec=1374336.28, grad_norm=0.2600, duration=0.38s
Step 3079: loss=3.3737, lr=0.000588, tokens/sec=1373648.62, grad_norm=0.2670, duration=0.38s
Step 3080: loss=3.3882, lr=0.000588, tokens/sec=1376172.54, grad_norm=0.2645, duration=0.38s
Step 3081: loss=3.4004, lr=0.000588, tokens/sec=1377080.87, grad_norm=0.2871, duration=0.38s
Step 3082: loss=3.3622, lr=0.000588, tokens/sec=1372725.11, grad_norm=0.2758, duration=0.38s
Step 3083: loss=3.3963, lr=0.000588, tokens/sec=1366093.93, grad_norm=0.2703, duration=0.38s
Step 3084: loss=3.3647, lr=0.000588, tokens/sec=1379540.39, grad_norm=0.2388, duration=0.38s
Step 3085: loss=3.3625, lr=0.000588, tokens/sec=1373820.26, grad_norm=0.2606, duration=0.38s
Step 3086: loss=3.3995, lr=0.000588, tokens/sec=1376100.20, grad_norm=0.2514, duration=0.38s
Step 3087: loss=3.4698, lr=0.000588, tokens/sec=1371509.37, grad_norm=0.2478, duration=0.38s
Step 3088: loss=3.5226, lr=0.000588, tokens/sec=1373501.91, grad_norm=0.2756, duration=0.38s
Step 3089: loss=3.5035, lr=0.000588, tokens/sec=1372576.88, grad_norm=0.2865, duration=0.38s
Step 3090: loss=3.4594, lr=0.000588, tokens/sec=1377048.10, grad_norm=0.3140, duration=0.38s
Step 3091: loss=3.5071, lr=0.000588, tokens/sec=1376893.76, grad_norm=0.3223, duration=0.38s
Step 3092: loss=3.4875, lr=0.000588, tokens/sec=1377913.55, grad_norm=0.3074, duration=0.38s
Step 3093: loss=3.4577, lr=0.000588, tokens/sec=1376975.67, grad_norm=0.2973, duration=0.38s
Step 3094: loss=3.4285, lr=0.000588, tokens/sec=1376070.92, grad_norm=0.2910, duration=0.38s
Step 3095: loss=3.4340, lr=0.000588, tokens/sec=1378194.21, grad_norm=0.2807, duration=0.38s
Step 3096: loss=3.4567, lr=0.000588, tokens/sec=1376531.76, grad_norm=0.2799, duration=0.38s
Step 3097: loss=3.4547, lr=0.000588, tokens/sec=1375625.88, grad_norm=0.2714, duration=0.38s
Step 3098: loss=3.4245, lr=0.000588, tokens/sec=1375707.63, grad_norm=0.2727, duration=0.38s
Step 3099: loss=3.4156, lr=0.000588, tokens/sec=1377614.88, grad_norm=0.2705, duration=0.38s
Step 3100/19073 (16.3%), Elapsed time: 1279.15s, Steps per hour: 8724.56, Estimated hours remaining: 1.83
Step 3100: loss=3.4331, lr=0.000588, tokens/sec=1377760.74, grad_norm=0.3038, duration=0.38s
Step 3101: loss=3.3924, lr=0.000588, tokens/sec=1381790.75, grad_norm=0.3120, duration=0.38s
Step 3102: loss=3.3836, lr=0.000588, tokens/sec=1376353.42, grad_norm=0.2397, duration=0.38s
Step 3103: loss=3.4306, lr=0.000588, tokens/sec=1372163.20, grad_norm=0.3020, duration=0.38s
Step 3104: loss=3.4224, lr=0.000588, tokens/sec=1371926.93, grad_norm=0.2833, duration=0.38s
Step 3105: loss=3.4863, lr=0.000588, tokens/sec=1377326.68, grad_norm=0.2808, duration=0.38s
Step 3106: loss=3.4088, lr=0.000588, tokens/sec=1377113.64, grad_norm=0.2584, duration=0.38s
Step 3107: loss=3.4641, lr=0.000588, tokens/sec=1378781.82, grad_norm=0.2597, duration=0.38s
Step 3108: loss=3.4368, lr=0.000588, tokens/sec=1378584.74, grad_norm=0.2809, duration=0.38s
Step 3109: loss=3.4004, lr=0.000588, tokens/sec=1380322.33, grad_norm=0.2635, duration=0.38s
Step 3110: loss=3.4067, lr=0.000588, tokens/sec=1374217.76, grad_norm=0.2762, duration=0.38s
Step 3111: loss=3.4165, lr=0.000588, tokens/sec=1374582.84, grad_norm=0.2444, duration=0.38s
Step 3112: loss=3.3976, lr=0.000588, tokens/sec=1377322.37, grad_norm=0.2832, duration=0.38s
Step 3113: loss=3.3585, lr=0.000588, tokens/sec=1373163.13, grad_norm=0.3003, duration=0.38s
Step 3114: loss=3.4053, lr=0.000587, tokens/sec=1376129.48, grad_norm=0.3114, duration=0.38s
Step 3115: loss=3.4085, lr=0.000587, tokens/sec=1375771.33, grad_norm=0.2848, duration=0.38s
Step 3116: loss=3.4470, lr=0.000587, tokens/sec=1375769.60, grad_norm=0.2759, duration=0.38s
Step 3117: loss=3.4120, lr=0.000587, tokens/sec=1378157.07, grad_norm=0.2756, duration=0.38s
Step 3118: loss=3.4495, lr=0.000587, tokens/sec=1376020.12, grad_norm=0.2918, duration=0.38s
Step 3119: loss=3.4286, lr=0.000587, tokens/sec=1372263.38, grad_norm=0.2925, duration=0.38s
Step 3120: loss=3.4404, lr=0.000587, tokens/sec=1377131.75, grad_norm=0.2963, duration=0.38s
Step 3121: loss=3.3491, lr=0.000587, tokens/sec=1375845.35, grad_norm=0.3075, duration=0.38s
Step 3122: loss=3.3882, lr=0.000587, tokens/sec=1374690.25, grad_norm=0.3110, duration=0.38s
Step 3123: loss=3.3714, lr=0.000587, tokens/sec=1376020.98, grad_norm=0.2738, duration=0.38s
Step 3124: loss=3.3836, lr=0.000587, tokens/sec=1375984.82, grad_norm=0.2521, duration=0.38s
Step 3125: loss=3.3171, lr=0.000587, tokens/sec=1378049.12, grad_norm=0.2626, duration=0.38s
Step 3126: loss=3.3916, lr=0.000587, tokens/sec=1375408.20, grad_norm=0.2363, duration=0.38s
Step 3127: loss=3.3775, lr=0.000587, tokens/sec=1374927.48, grad_norm=0.2507, duration=0.38s
Step 3128: loss=3.3470, lr=0.000587, tokens/sec=1373767.05, grad_norm=0.2408, duration=0.38s
Step 3129: loss=3.3598, lr=0.000587, tokens/sec=1375181.98, grad_norm=0.2524, duration=0.38s
Step 3130: loss=3.3375, lr=0.000587, tokens/sec=1378557.09, grad_norm=0.2584, duration=0.38s
Step 3131: loss=3.4049, lr=0.000587, tokens/sec=1381302.09, grad_norm=0.2642, duration=0.38s
Step 3132: loss=3.3372, lr=0.000587, tokens/sec=1378212.35, grad_norm=0.2660, duration=0.38s
Step 3133: loss=3.4009, lr=0.000587, tokens/sec=1377512.18, grad_norm=0.2577, duration=0.38s
Step 3134: loss=3.4345, lr=0.000587, tokens/sec=1374838.94, grad_norm=0.2716, duration=0.38s
Step 3135: loss=3.4683, lr=0.000587, tokens/sec=1375601.78, grad_norm=0.3307, duration=0.38s
Step 3136: loss=3.4701, lr=0.000587, tokens/sec=1375113.19, grad_norm=0.3947, duration=0.38s
Step 3137: loss=3.4449, lr=0.000587, tokens/sec=1378669.44, grad_norm=0.3248, duration=0.38s
Step 3138: loss=3.4074, lr=0.000587, tokens/sec=1378870.87, grad_norm=0.3179, duration=0.38s
Step 3139: loss=3.4470, lr=0.000587, tokens/sec=1381374.98, grad_norm=0.3210, duration=0.38s
Step 3140: loss=3.4177, lr=0.000587, tokens/sec=1379672.82, grad_norm=0.2823, duration=0.38s
Step 3141: loss=3.4839, lr=0.000587, tokens/sec=1376605.87, grad_norm=0.2983, duration=0.38s
Step 3142: loss=3.4700, lr=0.000587, tokens/sec=1374462.55, grad_norm=0.3141, duration=0.38s
Step 3143: loss=3.4464, lr=0.000587, tokens/sec=1377271.48, grad_norm=0.2710, duration=0.38s
Step 3144: loss=3.4901, lr=0.000587, tokens/sec=1378104.39, grad_norm=0.3213, duration=0.38s
Step 3145: loss=3.4076, lr=0.000587, tokens/sec=1374300.21, grad_norm=0.3268, duration=0.38s
Step 3146: loss=3.4286, lr=0.000587, tokens/sec=1376836.00, grad_norm=0.3005, duration=0.38s
Step 3147: loss=3.4114, lr=0.000587, tokens/sec=1377860.02, grad_norm=0.2768, duration=0.38s
Step 3148: loss=3.4407, lr=0.000587, tokens/sec=1375639.65, grad_norm=0.2937, duration=0.38s
Step 3149: loss=3.3614, lr=0.000587, tokens/sec=1375378.95, grad_norm=0.2791, duration=0.38s
Step 3150: loss=3.4256, lr=0.000587, tokens/sec=1373973.91, grad_norm=0.2738, duration=0.38s
Step 3151: loss=3.4312, lr=0.000587, tokens/sec=1374075.22, grad_norm=0.2717, duration=0.38s
Step 3152: loss=3.3959, lr=0.000587, tokens/sec=1377680.47, grad_norm=0.2601, duration=0.38s
Step 3153: loss=3.3400, lr=0.000587, tokens/sec=1377318.06, grad_norm=0.2440, duration=0.38s
Step 3154: loss=3.4356, lr=0.000587, tokens/sec=1378611.53, grad_norm=0.2543, duration=0.38s
Step 3155: loss=3.3834, lr=0.000587, tokens/sec=1377909.23, grad_norm=0.2592, duration=0.38s
Step 3156: loss=3.4955, lr=0.000587, tokens/sec=1376771.35, grad_norm=0.2784, duration=0.38s
Step 3157: loss=3.4373, lr=0.000587, tokens/sec=1376717.91, grad_norm=0.3143, duration=0.38s
Step 3158: loss=3.4155, lr=0.000587, tokens/sec=1373024.23, grad_norm=0.2922, duration=0.38s
Step 3159: loss=3.4057, lr=0.000587, tokens/sec=1377882.47, grad_norm=0.2957, duration=0.38s
Step 3160: loss=3.3820, lr=0.000587, tokens/sec=1376293.98, grad_norm=0.2916, duration=0.38s
Step 3161: loss=3.4128, lr=0.000587, tokens/sec=1377716.72, grad_norm=0.3171, duration=0.38s
Step 3162: loss=3.4387, lr=0.000587, tokens/sec=1379108.67, grad_norm=0.2938, duration=0.38s
Step 3163: loss=3.4215, lr=0.000587, tokens/sec=1377050.69, grad_norm=0.3122, duration=0.38s
Step 3164: loss=3.4707, lr=0.000587, tokens/sec=1374771.03, grad_norm=0.3479, duration=0.38s
Step 3165: loss=3.4548, lr=0.000587, tokens/sec=1377960.17, grad_norm=0.2847, duration=0.38s
Step 3166: loss=3.4037, lr=0.000587, tokens/sec=1378572.64, grad_norm=0.3039, duration=0.38s
Step 3167: loss=3.4304, lr=0.000587, tokens/sec=1381725.63, grad_norm=0.3146, duration=0.38s
Step 3168: loss=3.3871, lr=0.000587, tokens/sec=1376296.57, grad_norm=0.2670, duration=0.38s
Step 3169: loss=3.3241, lr=0.000587, tokens/sec=1377021.37, grad_norm=0.2689, duration=0.38s
Step 3170: loss=3.3960, lr=0.000587, tokens/sec=1372557.17, grad_norm=0.2659, duration=0.38s
Step 3171: loss=3.2986, lr=0.000587, tokens/sec=1372538.32, grad_norm=0.2631, duration=0.38s
Step 3172: loss=3.3330, lr=0.000587, tokens/sec=1378598.57, grad_norm=0.2515, duration=0.38s
Step 3173: loss=3.3186, lr=0.000587, tokens/sec=1375993.43, grad_norm=0.2784, duration=0.38s
Step 3174: loss=3.3779, lr=0.000587, tokens/sec=1377741.75, grad_norm=0.2705, duration=0.38s
Step 3175: loss=3.3663, lr=0.000587, tokens/sec=1373754.17, grad_norm=0.2931, duration=0.38s
Step 3176: loss=3.3379, lr=0.000587, tokens/sec=1375581.99, grad_norm=0.2701, duration=0.38s
Step 3177: loss=3.3479, lr=0.000587, tokens/sec=1372957.37, grad_norm=0.2758, duration=0.38s
Step 3178: loss=3.3501, lr=0.000587, tokens/sec=1376219.05, grad_norm=0.2581, duration=0.38s
Step 3179: loss=3.3818, lr=0.000587, tokens/sec=1380110.09, grad_norm=0.2607, duration=0.38s
Step 3180: loss=3.4376, lr=0.000587, tokens/sec=1377015.33, grad_norm=0.2843, duration=0.38s
Step 3181: loss=3.4471, lr=0.000587, tokens/sec=1374650.72, grad_norm=0.2616, duration=0.38s
Step 3182: loss=3.4677, lr=0.000587, tokens/sec=1374565.65, grad_norm=0.2765, duration=0.38s
Step 3183: loss=3.4408, lr=0.000587, tokens/sec=1371950.89, grad_norm=0.2838, duration=0.38s
Step 3184: loss=3.4876, lr=0.000587, tokens/sec=1371736.94, grad_norm=0.2766, duration=0.38s
Step 3185: loss=3.4187, lr=0.000587, tokens/sec=1375600.06, grad_norm=0.3067, duration=0.38s
Step 3186: loss=3.4351, lr=0.000587, tokens/sec=1375522.62, grad_norm=0.3110, duration=0.38s
Step 3187: loss=3.4828, lr=0.000587, tokens/sec=1377113.64, grad_norm=0.3201, duration=0.38s
Step 3188: loss=3.5531, lr=0.000587, tokens/sec=1374135.32, grad_norm=0.3088, duration=0.38s
Step 3189: loss=3.4663, lr=0.000587, tokens/sec=1371257.07, grad_norm=0.2934, duration=0.38s
Step 3190: loss=3.4547, lr=0.000587, tokens/sec=1372476.65, grad_norm=0.2702, duration=0.38s
Step 3191: loss=3.3973, lr=0.000587, tokens/sec=1377281.83, grad_norm=0.2547, duration=0.38s
Step 3192: loss=3.4354, lr=0.000587, tokens/sec=1376297.43, grad_norm=0.3049, duration=0.38s
Step 3193: loss=3.3987, lr=0.000587, tokens/sec=1375053.86, grad_norm=0.3061, duration=0.38s
Step 3194: loss=3.3618, lr=0.000587, tokens/sec=1379016.13, grad_norm=0.3108, duration=0.38s
Step 3195: loss=3.4752, lr=0.000587, tokens/sec=1374183.41, grad_norm=0.2641, duration=0.38s
Step 3196: loss=3.3904, lr=0.000587, tokens/sec=1376175.12, grad_norm=0.2897, duration=0.38s
Step 3197: loss=3.4503, lr=0.000587, tokens/sec=1379071.48, grad_norm=0.2925, duration=0.38s
Step 3198: loss=3.4718, lr=0.000587, tokens/sec=1380068.52, grad_norm=0.2918, duration=0.38s
Step 3199: loss=3.4194, lr=0.000587, tokens/sec=1380138.67, grad_norm=0.2557, duration=0.38s
Step 3200/19073 (16.8%), Elapsed time: 1317.31s, Steps per hour: 8745.10, Estimated hours remaining: 1.82
Step 3200: loss=3.3476, lr=0.000586, tokens/sec=1374585.41, grad_norm=0.3004, duration=0.38s
Step 3201: loss=3.4373, lr=0.000586, tokens/sec=1374752.13, grad_norm=0.2640, duration=0.38s
Step 3202: loss=3.4237, lr=0.000586, tokens/sec=1380196.71, grad_norm=0.2788, duration=0.38s
Step 3203: loss=3.3639, lr=0.000586, tokens/sec=1378058.62, grad_norm=0.2457, duration=0.38s
Step 3204: loss=3.3143, lr=0.000586, tokens/sec=1376740.32, grad_norm=0.2632, duration=0.38s
Step 3205: loss=3.3779, lr=0.000586, tokens/sec=1378965.11, grad_norm=0.2510, duration=0.38s
Step 3206: loss=3.3773, lr=0.000586, tokens/sec=1376189.76, grad_norm=0.2905, duration=0.38s
Step 3207: loss=3.3997, lr=0.000586, tokens/sec=1376377.54, grad_norm=0.2778, duration=0.38s
Step 3208: loss=3.4115, lr=0.000586, tokens/sec=1378058.62, grad_norm=0.3095, duration=0.38s
Step 3209: loss=3.3472, lr=0.000586, tokens/sec=1371980.85, grad_norm=0.2729, duration=0.38s
Step 3210: loss=3.3993, lr=0.000586, tokens/sec=1379686.67, grad_norm=0.2807, duration=0.38s
Step 3211: loss=3.4051, lr=0.000586, tokens/sec=1379471.16, grad_norm=0.2522, duration=0.38s
Step 3212: loss=3.3481, lr=0.000586, tokens/sec=1378256.41, grad_norm=0.2674, duration=0.38s
Step 3213: loss=3.3709, lr=0.000586, tokens/sec=1378937.44, grad_norm=0.2567, duration=0.38s
Step 3214: loss=3.3535, lr=0.000586, tokens/sec=1377717.58, grad_norm=0.2838, duration=0.38s
Step 3215: loss=3.3266, lr=0.000586, tokens/sec=1378199.39, grad_norm=0.2944, duration=0.38s
Step 3216: loss=3.3502, lr=0.000586, tokens/sec=1376012.37, grad_norm=0.2863, duration=0.38s
Step 3217: loss=3.3759, lr=0.000586, tokens/sec=1378894.21, grad_norm=0.3114, duration=0.38s
Step 3218: loss=3.3548, lr=0.000586, tokens/sec=1376126.89, grad_norm=0.2999, duration=0.38s
Step 3219: loss=3.3228, lr=0.000586, tokens/sec=1378157.94, grad_norm=0.2748, duration=0.38s
Step 3220: loss=3.3111, lr=0.000586, tokens/sec=1377787.50, grad_norm=0.2496, duration=0.38s
Step 3221: loss=3.3620, lr=0.000586, tokens/sec=1378493.14, grad_norm=0.2679, duration=0.38s
Step 3222: loss=3.3665, lr=0.000586, tokens/sec=1376307.76, grad_norm=0.2526, duration=0.38s
Step 3223: loss=3.3502, lr=0.000586, tokens/sec=1379044.67, grad_norm=0.2649, duration=0.38s
Step 3224: loss=3.3975, lr=0.000586, tokens/sec=1375423.68, grad_norm=0.2609, duration=0.38s
Step 3225: loss=3.3908, lr=0.000586, tokens/sec=1377221.45, grad_norm=0.3092, duration=0.38s
Step 3226: loss=3.4537, lr=0.000586, tokens/sec=1376836.86, grad_norm=0.3484, duration=0.38s
Step 3227: loss=3.4469, lr=0.000586, tokens/sec=1378043.93, grad_norm=0.3407, duration=0.38s
Step 3228: loss=3.4380, lr=0.000586, tokens/sec=1374505.51, grad_norm=0.3105, duration=0.38s
Step 3229: loss=3.4342, lr=0.000586, tokens/sec=1373797.08, grad_norm=0.3069, duration=0.38s
Step 3230: loss=3.4221, lr=0.000586, tokens/sec=1376974.80, grad_norm=0.3218, duration=0.38s
Step 3231: loss=3.4638, lr=0.000586, tokens/sec=1372360.16, grad_norm=0.3566, duration=0.38s
Step 3232: loss=3.4694, lr=0.000586, tokens/sec=1375739.48, grad_norm=0.2973, duration=0.38s
Step 3233: loss=3.5363, lr=0.000586, tokens/sec=1373694.10, grad_norm=0.3095, duration=0.38s
Step 3234: loss=3.4482, lr=0.000586, tokens/sec=1377744.34, grad_norm=0.2818, duration=0.38s
Step 3235: loss=3.4867, lr=0.000586, tokens/sec=1373090.25, grad_norm=0.3109, duration=0.38s
Step 3236: loss=3.5175, lr=0.000586, tokens/sec=1375754.97, grad_norm=0.3170, duration=0.38s
Step 3237: loss=3.4463, lr=0.000586, tokens/sec=1376251.78, grad_norm=0.2891, duration=0.38s
Step 3238: loss=3.4591, lr=0.000586, tokens/sec=1376644.65, grad_norm=0.2923, duration=0.38s
Step 3239: loss=3.4338, lr=0.000586, tokens/sec=1377289.59, grad_norm=0.2823, duration=0.38s
Step 3240: loss=3.4829, lr=0.000586, tokens/sec=1378313.42, grad_norm=0.2558, duration=0.38s
Step 3241: loss=3.4026, lr=0.000586, tokens/sec=1377965.35, grad_norm=0.2679, duration=0.38s
Step 3242: loss=3.4181, lr=0.000586, tokens/sec=1372903.37, grad_norm=0.2527, duration=0.38s
Step 3243: loss=3.3601, lr=0.000586, tokens/sec=1373383.53, grad_norm=0.2405, duration=0.38s
Step 3244: loss=3.4344, lr=0.000586, tokens/sec=1374746.97, grad_norm=0.2638, duration=0.38s
Step 3245: loss=3.4552, lr=0.000586, tokens/sec=1379478.95, grad_norm=0.2617, duration=0.38s
Step 3246: loss=3.3887, lr=0.000586, tokens/sec=1379145.00, grad_norm=0.2986, duration=0.38s
Step 3247: loss=3.4579, lr=0.000586, tokens/sec=1377420.72, grad_norm=0.3257, duration=0.38s
Step 3248: loss=3.3576, lr=0.000586, tokens/sec=1379537.80, grad_norm=0.2922, duration=0.38s
Step 3249: loss=3.3199, lr=0.000586, tokens/sec=1375966.74, grad_norm=0.3770, duration=0.38s
Validation loss at step 3250: 3.7130088806152344
Step 3250: loss=3.3552, lr=0.000586, tokens/sec=151296.51, grad_norm=0.3159, duration=3.47s
Step 3251: loss=3.4095, lr=0.000586, tokens/sec=1380773.02, grad_norm=0.2715, duration=0.38s
Step 3252: loss=3.4827, lr=0.000586, tokens/sec=1378849.25, grad_norm=0.2754, duration=0.38s
Step 3253: loss=3.3117, lr=0.000586, tokens/sec=1380157.73, grad_norm=0.2976, duration=0.38s
Step 3254: loss=3.4487, lr=0.000586, tokens/sec=1379288.60, grad_norm=0.2835, duration=0.38s
Step 3255: loss=3.4034, lr=0.000586, tokens/sec=1376624.83, grad_norm=0.2911, duration=0.38s
Step 3256: loss=3.2980, lr=0.000586, tokens/sec=1378582.15, grad_norm=0.2878, duration=0.38s
Step 3257: loss=3.3667, lr=0.000586, tokens/sec=1381476.51, grad_norm=0.2864, duration=0.38s
Step 3258: loss=3.3959, lr=0.000586, tokens/sec=1377400.01, grad_norm=0.2547, duration=0.38s
Step 3259: loss=3.4089, lr=0.000586, tokens/sec=1375035.80, grad_norm=0.2589, duration=0.38s
Step 3260: loss=3.3921, lr=0.000586, tokens/sec=1370037.10, grad_norm=0.2619, duration=0.38s
Step 3261: loss=3.4176, lr=0.000586, tokens/sec=1373963.61, grad_norm=0.2937, duration=0.38s
Step 3262: loss=3.4198, lr=0.000586, tokens/sec=1376792.04, grad_norm=0.3429, duration=0.38s
Step 3263: loss=3.4582, lr=0.000586, tokens/sec=1378342.79, grad_norm=0.2863, duration=0.38s
Step 3264: loss=3.4118, lr=0.000586, tokens/sec=1375412.50, grad_norm=0.2919, duration=0.38s
Step 3265: loss=3.3400, lr=0.000586, tokens/sec=1378142.39, grad_norm=0.2782, duration=0.38s
Step 3266: loss=3.3855, lr=0.000586, tokens/sec=1380839.78, grad_norm=0.2712, duration=0.38s
Step 3267: loss=3.3380, lr=0.000586, tokens/sec=1375533.81, grad_norm=0.2863, duration=0.38s
Step 3268: loss=3.3146, lr=0.000586, tokens/sec=1379854.62, grad_norm=0.2514, duration=0.38s
Step 3269: loss=3.3505, lr=0.000586, tokens/sec=1376438.71, grad_norm=0.2637, duration=0.38s
Step 3270: loss=3.3603, lr=0.000586, tokens/sec=1377018.78, grad_norm=0.2612, duration=0.38s
Step 3271: loss=3.3417, lr=0.000586, tokens/sec=1376376.68, grad_norm=0.2798, duration=0.38s
Step 3272: loss=3.3456, lr=0.000586, tokens/sec=1375139.85, grad_norm=0.3038, duration=0.38s
Step 3273: loss=3.3627, lr=0.000586, tokens/sec=1375641.37, grad_norm=0.2750, duration=0.38s
Step 3274: loss=3.3413, lr=0.000586, tokens/sec=1377713.27, grad_norm=0.2639, duration=0.38s
Step 3275: loss=3.3609, lr=0.000586, tokens/sec=1377028.27, grad_norm=0.2841, duration=0.38s
Step 3276: loss=3.3466, lr=0.000586, tokens/sec=1376648.96, grad_norm=0.3189, duration=0.38s
Step 3277: loss=3.4537, lr=0.000586, tokens/sec=1376779.97, grad_norm=0.2910, duration=0.38s
Step 3278: loss=3.5212, lr=0.000586, tokens/sec=1371454.62, grad_norm=0.3160, duration=0.38s
Step 3279: loss=3.4327, lr=0.000586, tokens/sec=1375334.22, grad_norm=0.3504, duration=0.38s
Step 3280: loss=3.4546, lr=0.000586, tokens/sec=1371541.02, grad_norm=0.3230, duration=0.38s
Step 3281: loss=3.4611, lr=0.000586, tokens/sec=1375566.50, grad_norm=0.3011, duration=0.38s
Step 3282: loss=3.4425, lr=0.000586, tokens/sec=1376668.78, grad_norm=0.3181, duration=0.38s
Step 3283: loss=3.4426, lr=0.000585, tokens/sec=1378477.58, grad_norm=0.3265, duration=0.38s
Step 3284: loss=3.3951, lr=0.000585, tokens/sec=1377538.93, grad_norm=0.2872, duration=0.38s
Step 3285: loss=3.4127, lr=0.000585, tokens/sec=1372141.79, grad_norm=0.3122, duration=0.38s
Step 3286: loss=3.4379, lr=0.000585, tokens/sec=1372375.57, grad_norm=0.3256, duration=0.38s
Step 3287: loss=3.4114, lr=0.000585, tokens/sec=1371636.83, grad_norm=0.2976, duration=0.38s
Step 3288: loss=3.3911, lr=0.000585, tokens/sec=1373192.28, grad_norm=0.2828, duration=0.38s
Step 3289: loss=3.3958, lr=0.000585, tokens/sec=1376390.46, grad_norm=0.2896, duration=0.38s
Step 3290: loss=3.4303, lr=0.000585, tokens/sec=1375829.00, grad_norm=0.2740, duration=0.38s
Step 3291: loss=3.3162, lr=0.000585, tokens/sec=1377709.82, grad_norm=0.2703, duration=0.38s
Step 3292: loss=3.3586, lr=0.000585, tokens/sec=1373937.85, grad_norm=0.2599, duration=0.38s
Step 3293: loss=3.4040, lr=0.000585, tokens/sec=1376125.17, grad_norm=0.2885, duration=0.38s
Step 3294: loss=3.4030, lr=0.000585, tokens/sec=1376427.51, grad_norm=0.2669, duration=0.38s
Step 3295: loss=3.4539, lr=0.000585, tokens/sec=1377175.73, grad_norm=0.2704, duration=0.38s
Step 3296: loss=3.3439, lr=0.000585, tokens/sec=1375521.76, grad_norm=0.2666, duration=0.38s
Step 3297: loss=3.4572, lr=0.000585, tokens/sec=1377836.71, grad_norm=0.2933, duration=0.38s
Step 3298: loss=3.4002, lr=0.000585, tokens/sec=1376308.63, grad_norm=0.2677, duration=0.38s
Step 3299: loss=3.3672, lr=0.000585, tokens/sec=1377161.07, grad_norm=0.2645, duration=0.38s
Step 3300/19073 (17.3%), Elapsed time: 1358.55s, Steps per hour: 8744.60, Estimated hours remaining: 1.80
Step 3300: loss=3.3865, lr=0.000585, tokens/sec=1376847.21, grad_norm=0.2433, duration=0.38s
Step 3301: loss=3.3903, lr=0.000585, tokens/sec=1375002.27, grad_norm=0.2538, duration=0.38s
Step 3302: loss=3.3110, lr=0.000585, tokens/sec=1377263.71, grad_norm=0.2627, duration=0.38s
Step 3303: loss=3.3791, lr=0.000585, tokens/sec=1377331.86, grad_norm=0.2528, duration=0.38s
Step 3304: loss=3.3657, lr=0.000585, tokens/sec=1374938.65, grad_norm=0.2496, duration=0.38s
Step 3305: loss=3.4223, lr=0.000585, tokens/sec=1379284.27, grad_norm=0.2685, duration=0.38s
Step 3306: loss=3.4152, lr=0.000585, tokens/sec=1375285.19, grad_norm=0.2608, duration=0.38s
Step 3307: loss=3.3794, lr=0.000585, tokens/sec=1375868.59, grad_norm=0.2787, duration=0.38s
Step 3308: loss=3.4053, lr=0.000585, tokens/sec=1376484.37, grad_norm=0.2509, duration=0.38s
Step 3309: loss=3.3953, lr=0.000585, tokens/sec=1374673.92, grad_norm=0.2735, duration=0.38s
Step 3310: loss=3.3980, lr=0.000585, tokens/sec=1377891.96, grad_norm=0.2969, duration=0.38s
Step 3311: loss=3.2934, lr=0.000585, tokens/sec=1377086.04, grad_norm=0.2872, duration=0.38s
Step 3312: loss=3.4046, lr=0.000585, tokens/sec=1376630.00, grad_norm=0.2991, duration=0.38s
Step 3313: loss=3.3444, lr=0.000585, tokens/sec=1376391.32, grad_norm=0.2671, duration=0.38s
Step 3314: loss=3.3414, lr=0.000585, tokens/sec=1379360.40, grad_norm=0.2951, duration=0.38s
Step 3315: loss=3.3223, lr=0.000585, tokens/sec=1376238.86, grad_norm=0.2901, duration=0.38s
Step 3316: loss=3.3229, lr=0.000585, tokens/sec=1376455.94, grad_norm=0.2464, duration=0.38s
Step 3317: loss=3.3697, lr=0.000585, tokens/sec=1377211.96, grad_norm=0.2958, duration=0.38s
Step 3318: loss=3.3070, lr=0.000585, tokens/sec=1374853.55, grad_norm=0.2591, duration=0.38s
Step 3319: loss=3.3296, lr=0.000585, tokens/sec=1379104.35, grad_norm=0.2843, duration=0.38s
Step 3320: loss=3.3232, lr=0.000585, tokens/sec=1378385.99, grad_norm=0.2712, duration=0.38s
Step 3321: loss=3.3238, lr=0.000585, tokens/sec=1380281.61, grad_norm=0.2774, duration=0.38s
Step 3322: loss=3.3319, lr=0.000585, tokens/sec=1375754.97, grad_norm=0.2691, duration=0.38s
Step 3323: loss=3.3591, lr=0.000585, tokens/sec=1375453.79, grad_norm=0.2987, duration=0.38s
Step 3324: loss=3.4245, lr=0.000585, tokens/sec=1373367.23, grad_norm=0.2443, duration=0.38s
Step 3325: loss=3.4281, lr=0.000585, tokens/sec=1375113.19, grad_norm=0.2896, duration=0.38s
Step 3326: loss=3.4156, lr=0.000585, tokens/sec=1372822.80, grad_norm=0.2915, duration=0.38s
Step 3327: loss=3.4235, lr=0.000585, tokens/sec=1376859.28, grad_norm=0.2655, duration=0.38s
Step 3328: loss=3.3689, lr=0.000585, tokens/sec=1378006.80, grad_norm=0.3053, duration=0.38s
Step 3329: loss=3.4179, lr=0.000585, tokens/sec=1377107.60, grad_norm=0.2852, duration=0.38s
Step 3330: loss=3.3979, lr=0.000585, tokens/sec=1373694.10, grad_norm=0.2699, duration=0.38s
Step 3331: loss=3.4503, lr=0.000585, tokens/sec=1375339.38, grad_norm=0.3056, duration=0.38s
Step 3332: loss=3.4304, lr=0.000585, tokens/sec=1373960.17, grad_norm=0.3099, duration=0.38s
Step 3333: loss=3.4445, lr=0.000585, tokens/sec=1376293.12, grad_norm=0.2921, duration=0.38s
Step 3334: loss=3.4118, lr=0.000585, tokens/sec=1376792.90, grad_norm=0.3069, duration=0.38s
Step 3335: loss=3.3856, lr=0.000585, tokens/sec=1377989.53, grad_norm=0.3086, duration=0.38s
Step 3336: loss=3.3974, lr=0.000585, tokens/sec=1377387.94, grad_norm=0.2665, duration=0.38s
Step 3337: loss=3.3824, lr=0.000585, tokens/sec=1380366.52, grad_norm=0.2856, duration=0.38s
Step 3338: loss=3.3895, lr=0.000585, tokens/sec=1374600.02, grad_norm=0.2640, duration=0.38s
Step 3339: loss=3.3469, lr=0.000585, tokens/sec=1379749.86, grad_norm=0.2811, duration=0.38s
Step 3340: loss=3.4079, lr=0.000585, tokens/sec=1376840.31, grad_norm=0.2928, duration=0.38s
Step 3341: loss=3.3640, lr=0.000585, tokens/sec=1378483.63, grad_norm=0.2880, duration=0.38s
Step 3342: loss=3.3615, lr=0.000585, tokens/sec=1377905.78, grad_norm=0.2537, duration=0.38s
Step 3343: loss=3.3295, lr=0.000585, tokens/sec=1375742.92, grad_norm=0.2872, duration=0.38s
Step 3344: loss=3.3982, lr=0.000585, tokens/sec=1376401.66, grad_norm=0.2848, duration=0.38s
Step 3345: loss=3.4270, lr=0.000585, tokens/sec=1376420.62, grad_norm=0.2891, duration=0.38s
Step 3346: loss=3.4417, lr=0.000585, tokens/sec=1378602.03, grad_norm=0.3008, duration=0.38s
Step 3347: loss=3.3698, lr=0.000585, tokens/sec=1379383.77, grad_norm=0.2861, duration=0.38s
Step 3348: loss=3.3949, lr=0.000585, tokens/sec=1378317.74, grad_norm=0.2924, duration=0.38s
Step 3349: loss=3.3444, lr=0.000585, tokens/sec=1374983.36, grad_norm=0.3131, duration=0.38s
Step 3350: loss=3.3915, lr=0.000585, tokens/sec=1377764.20, grad_norm=0.3021, duration=0.38s
Step 3351: loss=3.3951, lr=0.000585, tokens/sec=1377478.53, grad_norm=0.2997, duration=0.38s
Step 3352: loss=3.4151, lr=0.000585, tokens/sec=1378464.62, grad_norm=0.2965, duration=0.38s
Step 3353: loss=3.4165, lr=0.000585, tokens/sec=1376815.31, grad_norm=0.3161, duration=0.38s
Step 3354: loss=3.4206, lr=0.000585, tokens/sec=1378985.87, grad_norm=0.3197, duration=0.38s
Step 3355: loss=3.3887, lr=0.000585, tokens/sec=1379141.54, grad_norm=0.2724, duration=0.38s
Step 3356: loss=3.4100, lr=0.000585, tokens/sec=1372568.31, grad_norm=0.3063, duration=0.38s
Step 3357: loss=3.3627, lr=0.000585, tokens/sec=1377350.84, grad_norm=0.2512, duration=0.38s
Step 3358: loss=3.3498, lr=0.000585, tokens/sec=1377056.72, grad_norm=0.2672, duration=0.38s
Step 3359: loss=3.3216, lr=0.000585, tokens/sec=1379890.12, grad_norm=0.2628, duration=0.38s
Step 3360: loss=3.3491, lr=0.000585, tokens/sec=1376940.32, grad_norm=0.2595, duration=0.38s
Step 3361: loss=3.2628, lr=0.000585, tokens/sec=1373836.57, grad_norm=0.2693, duration=0.38s
Step 3362: loss=3.2987, lr=0.000585, tokens/sec=1372206.01, grad_norm=0.2973, duration=0.38s
Step 3363: loss=3.2809, lr=0.000584, tokens/sec=1377448.33, grad_norm=0.2736, duration=0.38s
Step 3364: loss=3.3650, lr=0.000584, tokens/sec=1376981.70, grad_norm=0.2721, duration=0.38s
Step 3365: loss=3.3372, lr=0.000584, tokens/sec=1373942.14, grad_norm=0.2868, duration=0.38s
Step 3366: loss=3.3301, lr=0.000584, tokens/sec=1372420.11, grad_norm=0.2407, duration=0.38s
Step 3367: loss=3.2907, lr=0.000584, tokens/sec=1373507.91, grad_norm=0.2593, duration=0.38s
Step 3368: loss=3.3135, lr=0.000584, tokens/sec=1377380.17, grad_norm=0.2546, duration=0.38s
Step 3369: loss=3.3643, lr=0.000584, tokens/sec=1378474.13, grad_norm=0.2547, duration=0.38s
Step 3370: loss=3.4049, lr=0.000584, tokens/sec=1374708.30, grad_norm=0.2575, duration=0.38s
Step 3371: loss=3.4268, lr=0.000584, tokens/sec=1374165.37, grad_norm=0.2642, duration=0.38s
Step 3372: loss=3.4392, lr=0.000584, tokens/sec=1373824.55, grad_norm=0.2836, duration=0.38s
Step 3373: loss=3.4281, lr=0.000584, tokens/sec=1373212.01, grad_norm=0.2422, duration=0.38s
Step 3374: loss=3.4381, lr=0.000584, tokens/sec=1377729.67, grad_norm=0.2833, duration=0.38s
Step 3375: loss=3.3805, lr=0.000584, tokens/sec=1374946.39, grad_norm=0.3050, duration=0.38s
Step 3376: loss=3.4039, lr=0.000584, tokens/sec=1376882.55, grad_norm=0.2773, duration=0.38s
Step 3377: loss=3.4560, lr=0.000584, tokens/sec=1376973.94, grad_norm=0.2850, duration=0.38s
Step 3378: loss=3.5104, lr=0.000584, tokens/sec=1376836.86, grad_norm=0.3526, duration=0.38s
Step 3379: loss=3.4467, lr=0.000584, tokens/sec=1376761.01, grad_norm=0.4208, duration=0.38s
Step 3380: loss=3.4404, lr=0.000584, tokens/sec=1377690.83, grad_norm=0.3499, duration=0.38s
Step 3381: loss=3.3709, lr=0.000584, tokens/sec=1372851.94, grad_norm=0.2939, duration=0.38s
Step 3382: loss=3.3980, lr=0.000584, tokens/sec=1375954.68, grad_norm=0.2959, duration=0.38s
Step 3383: loss=3.3626, lr=0.000584, tokens/sec=1373236.02, grad_norm=0.2987, duration=0.38s
Step 3384: loss=3.3637, lr=0.000584, tokens/sec=1375770.46, grad_norm=0.2666, duration=0.38s
Step 3385: loss=3.4446, lr=0.000584, tokens/sec=1373797.08, grad_norm=0.2574, duration=0.38s
Step 3386: loss=3.3336, lr=0.000584, tokens/sec=1376086.42, grad_norm=0.2814, duration=0.38s
Step 3387: loss=3.4103, lr=0.000584, tokens/sec=1376532.62, grad_norm=0.2554, duration=0.38s
Step 3388: loss=3.4562, lr=0.000584, tokens/sec=1373112.54, grad_norm=0.2918, duration=0.38s
Step 3389: loss=3.3861, lr=0.000584, tokens/sec=1379260.91, grad_norm=0.2759, duration=0.38s
Step 3390: loss=3.3412, lr=0.000584, tokens/sec=1375563.92, grad_norm=0.3109, duration=0.38s
Step 3391: loss=3.3927, lr=0.000584, tokens/sec=1379407.13, grad_norm=0.2773, duration=0.38s
Step 3392: loss=3.3812, lr=0.000584, tokens/sec=1379895.32, grad_norm=0.2944, duration=0.38s
Step 3393: loss=3.3562, lr=0.000584, tokens/sec=1377908.37, grad_norm=0.2435, duration=0.38s
Step 3394: loss=3.2820, lr=0.000584, tokens/sec=1378655.61, grad_norm=0.2630, duration=0.38s
Step 3395: loss=3.3607, lr=0.000584, tokens/sec=1377632.14, grad_norm=0.2646, duration=0.38s
Step 3396: loss=3.3502, lr=0.000584, tokens/sec=1376342.22, grad_norm=0.2723, duration=0.38s
Step 3397: loss=3.3463, lr=0.000584, tokens/sec=1377653.71, grad_norm=0.2676, duration=0.38s
Step 3398: loss=3.4045, lr=0.000584, tokens/sec=1376886.00, grad_norm=0.3024, duration=0.38s
Step 3399: loss=3.3598, lr=0.000584, tokens/sec=1380606.58, grad_norm=0.2617, duration=0.38s
Step 3400/19073 (17.8%), Elapsed time: 1396.72s, Steps per hour: 8763.39, Estimated hours remaining: 1.79
Step 3400: loss=3.3775, lr=0.000584, tokens/sec=1375813.50, grad_norm=0.2914, duration=0.38s
Step 3401: loss=3.3723, lr=0.000584, tokens/sec=1376188.04, grad_norm=0.2945, duration=0.38s
Step 3402: loss=3.2934, lr=0.000584, tokens/sec=1372705.40, grad_norm=0.2683, duration=0.38s
Step 3403: loss=3.3655, lr=0.000584, tokens/sec=1377074.83, grad_norm=0.2633, duration=0.38s
Step 3404: loss=3.3093, lr=0.000584, tokens/sec=1374417.02, grad_norm=0.2707, duration=0.38s
Step 3405: loss=3.3178, lr=0.000584, tokens/sec=1371543.58, grad_norm=0.2522, duration=0.38s
Step 3406: loss=3.2743, lr=0.000584, tokens/sec=1375529.50, grad_norm=0.2821, duration=0.38s
Step 3407: loss=3.3854, lr=0.000584, tokens/sec=1373669.22, grad_norm=0.3017, duration=0.38s
Step 3408: loss=3.3126, lr=0.000584, tokens/sec=1374360.33, grad_norm=0.2750, duration=0.38s
Step 3409: loss=3.3201, lr=0.000584, tokens/sec=1377408.64, grad_norm=0.2708, duration=0.38s
Step 3410: loss=3.2931, lr=0.000584, tokens/sec=1376427.51, grad_norm=0.2794, duration=0.38s
Step 3411: loss=3.3137, lr=0.000584, tokens/sec=1376261.25, grad_norm=0.3191, duration=0.38s
Step 3412: loss=3.3339, lr=0.000584, tokens/sec=1375693.86, grad_norm=0.2728, duration=0.38s
Step 3413: loss=3.3500, lr=0.000584, tokens/sec=1375950.38, grad_norm=0.2892, duration=0.38s
Step 3414: loss=3.3111, lr=0.000584, tokens/sec=1374877.62, grad_norm=0.2938, duration=0.38s
Step 3415: loss=3.4105, lr=0.000584, tokens/sec=1371480.28, grad_norm=0.2896, duration=0.38s
Step 3416: loss=3.4029, lr=0.000584, tokens/sec=1373636.61, grad_norm=0.2823, duration=0.38s
Step 3417: loss=3.4470, lr=0.000584, tokens/sec=1375122.65, grad_norm=0.3137, duration=0.38s
Step 3418: loss=3.3702, lr=0.000584, tokens/sec=1374765.02, grad_norm=0.2948, duration=0.38s
Step 3419: loss=3.4120, lr=0.000584, tokens/sec=1376587.77, grad_norm=0.3231, duration=0.38s
Step 3420: loss=3.3962, lr=0.000584, tokens/sec=1376061.45, grad_norm=0.3580, duration=0.38s
Step 3421: loss=3.4392, lr=0.000584, tokens/sec=1374301.06, grad_norm=0.3874, duration=0.38s
Step 3422: loss=3.4525, lr=0.000584, tokens/sec=1371387.91, grad_norm=0.3277, duration=0.38s
Step 3423: loss=3.4909, lr=0.000584, tokens/sec=1368891.72, grad_norm=0.3236, duration=0.38s
Step 3424: loss=3.4370, lr=0.000584, tokens/sec=1377306.84, grad_norm=0.3128, duration=0.38s
Step 3425: loss=3.4330, lr=0.000584, tokens/sec=1372985.66, grad_norm=0.2793, duration=0.38s
Step 3426: loss=3.4910, lr=0.000584, tokens/sec=1376455.08, grad_norm=0.2723, duration=0.38s
Step 3427: loss=3.4060, lr=0.000584, tokens/sec=1377569.14, grad_norm=0.2598, duration=0.38s
Step 3428: loss=3.4391, lr=0.000584, tokens/sec=1372205.16, grad_norm=0.2558, duration=0.38s
Step 3429: loss=3.4379, lr=0.000584, tokens/sec=1376981.70, grad_norm=0.2804, duration=0.38s
Step 3430: loss=3.4143, lr=0.000584, tokens/sec=1375413.36, grad_norm=0.2654, duration=0.38s
Step 3431: loss=3.3694, lr=0.000584, tokens/sec=1376758.42, grad_norm=0.2722, duration=0.38s
Step 3432: loss=3.3995, lr=0.000584, tokens/sec=1375166.50, grad_norm=0.2436, duration=0.38s
Step 3433: loss=3.3477, lr=0.000584, tokens/sec=1375134.69, grad_norm=0.2446, duration=0.38s
Step 3434: loss=3.4295, lr=0.000584, tokens/sec=1375328.20, grad_norm=0.2567, duration=0.38s
Step 3435: loss=3.3922, lr=0.000584, tokens/sec=1377952.40, grad_norm=0.2654, duration=0.38s
Step 3436: loss=3.3669, lr=0.000584, tokens/sec=1376514.53, grad_norm=0.2606, duration=0.38s
Step 3437: loss=3.3938, lr=0.000584, tokens/sec=1380268.62, grad_norm=0.2474, duration=0.38s
Step 3438: loss=3.3466, lr=0.000584, tokens/sec=1376023.56, grad_norm=0.2656, duration=0.38s
Step 3439: loss=3.2594, lr=0.000584, tokens/sec=1377123.99, grad_norm=0.3588, duration=0.38s
Step 3440: loss=3.3652, lr=0.000584, tokens/sec=1375473.58, grad_norm=0.2942, duration=0.38s
Step 3441: loss=3.3733, lr=0.000583, tokens/sec=1374281.31, grad_norm=0.2966, duration=0.38s
Step 3442: loss=3.4355, lr=0.000583, tokens/sec=1377291.32, grad_norm=0.3031, duration=0.38s
Step 3443: loss=3.3259, lr=0.000583, tokens/sec=1379920.43, grad_norm=0.2844, duration=0.38s
Step 3444: loss=3.4046, lr=0.000583, tokens/sec=1380351.79, grad_norm=0.2983, duration=0.38s
Step 3445: loss=3.3158, lr=0.000583, tokens/sec=1376513.67, grad_norm=0.3011, duration=0.38s
Step 3446: loss=3.3174, lr=0.000583, tokens/sec=1376487.82, grad_norm=0.2808, duration=0.38s
Step 3447: loss=3.3455, lr=0.000583, tokens/sec=1378315.15, grad_norm=0.2606, duration=0.38s
Step 3448: loss=3.4076, lr=0.000583, tokens/sec=1376543.83, grad_norm=0.2497, duration=0.38s
Step 3449: loss=3.3727, lr=0.000583, tokens/sec=1375680.09, grad_norm=0.2582, duration=0.38s
Step 3450: loss=3.3691, lr=0.000583, tokens/sec=1378849.25, grad_norm=0.2755, duration=0.38s
Step 3451: loss=3.3648, lr=0.000583, tokens/sec=1378641.78, grad_norm=0.2640, duration=0.38s
Step 3452: loss=3.3831, lr=0.000583, tokens/sec=1377134.34, grad_norm=0.2920, duration=0.38s
Step 3453: loss=3.4645, lr=0.000583, tokens/sec=1375294.65, grad_norm=0.2911, duration=0.38s
Step 3454: loss=3.3706, lr=0.000583, tokens/sec=1379684.94, grad_norm=0.2733, duration=0.38s
Step 3455: loss=3.2953, lr=0.000583, tokens/sec=1377174.01, grad_norm=0.2826, duration=0.38s
Step 3456: loss=3.3699, lr=0.000583, tokens/sec=1375588.02, grad_norm=0.2964, duration=0.38s
Step 3457: loss=3.2926, lr=0.000583, tokens/sec=1376916.18, grad_norm=0.2886, duration=0.38s
Step 3458: loss=3.2946, lr=0.000583, tokens/sec=1375923.69, grad_norm=0.2630, duration=0.38s
Step 3459: loss=3.3287, lr=0.000583, tokens/sec=1379379.44, grad_norm=0.3198, duration=0.38s
Step 3460: loss=3.3092, lr=0.000583, tokens/sec=1378646.97, grad_norm=0.2732, duration=0.38s
Step 3461: loss=3.3253, lr=0.000583, tokens/sec=1376171.68, grad_norm=0.2667, duration=0.38s
Step 3462: loss=3.3104, lr=0.000583, tokens/sec=1376058.87, grad_norm=0.2476, duration=0.38s
Step 3463: loss=3.3358, lr=0.000583, tokens/sec=1373544.80, grad_norm=0.2635, duration=0.38s
Step 3464: loss=3.3380, lr=0.000583, tokens/sec=1378265.91, grad_norm=0.2497, duration=0.38s
Step 3465: loss=3.3037, lr=0.000583, tokens/sec=1379412.32, grad_norm=0.2723, duration=0.38s
Step 3466: loss=3.3269, lr=0.000583, tokens/sec=1377488.88, grad_norm=0.2690, duration=0.38s
Step 3467: loss=3.4503, lr=0.000583, tokens/sec=1375354.86, grad_norm=0.2626, duration=0.38s
Step 3468: loss=3.4477, lr=0.000583, tokens/sec=1374830.34, grad_norm=0.3069, duration=0.38s
Step 3469: loss=3.4277, lr=0.000583, tokens/sec=1374029.71, grad_norm=0.3232, duration=0.38s
Step 3470: loss=3.4122, lr=0.000583, tokens/sec=1375889.25, grad_norm=0.3054, duration=0.38s
Step 3471: loss=3.4137, lr=0.000583, tokens/sec=1378940.90, grad_norm=0.2856, duration=0.38s
Step 3472: loss=3.4241, lr=0.000583, tokens/sec=1372757.67, grad_norm=0.3108, duration=0.38s
Step 3473: loss=3.4102, lr=0.000583, tokens/sec=1380222.70, grad_norm=0.3109, duration=0.38s
Step 3474: loss=3.3748, lr=0.000583, tokens/sec=1378096.61, grad_norm=0.2844, duration=0.38s
Step 3475: loss=3.3917, lr=0.000583, tokens/sec=1378744.64, grad_norm=0.3280, duration=0.38s
Step 3476: loss=3.3954, lr=0.000583, tokens/sec=1371096.33, grad_norm=0.3167, duration=0.38s
Step 3477: loss=3.3739, lr=0.000583, tokens/sec=1376876.52, grad_norm=0.2829, duration=0.38s
Step 3478: loss=3.3674, lr=0.000583, tokens/sec=1377036.89, grad_norm=0.2753, duration=0.38s
Step 3479: loss=3.3926, lr=0.000583, tokens/sec=1376016.67, grad_norm=0.2565, duration=0.38s
Step 3480: loss=3.3601, lr=0.000583, tokens/sec=1376294.84, grad_norm=0.2639, duration=0.38s
Step 3481: loss=3.2872, lr=0.000583, tokens/sec=1378440.43, grad_norm=0.2704, duration=0.38s
Step 3482: loss=3.3314, lr=0.000583, tokens/sec=1373025.09, grad_norm=0.2791, duration=0.38s
Step 3483: loss=3.3841, lr=0.000583, tokens/sec=1373144.26, grad_norm=0.2769, duration=0.38s
Step 3484: loss=3.3715, lr=0.000583, tokens/sec=1376632.59, grad_norm=0.2411, duration=0.38s
Step 3485: loss=3.3936, lr=0.000583, tokens/sec=1375483.90, grad_norm=0.2875, duration=0.38s
Step 3486: loss=3.3368, lr=0.000583, tokens/sec=1375017.75, grad_norm=0.2760, duration=0.38s
Step 3487: loss=3.4211, lr=0.000583, tokens/sec=1375475.30, grad_norm=0.2635, duration=0.38s
Step 3488: loss=3.3671, lr=0.000583, tokens/sec=1373290.90, grad_norm=0.2660, duration=0.38s
Step 3489: loss=3.3517, lr=0.000583, tokens/sec=1374953.27, grad_norm=0.2652, duration=0.38s
Step 3490: loss=3.3581, lr=0.000583, tokens/sec=1377414.68, grad_norm=0.2422, duration=0.38s
Step 3491: loss=3.3068, lr=0.000583, tokens/sec=1371961.16, grad_norm=0.2725, duration=0.38s
Step 3492: loss=3.3355, lr=0.000583, tokens/sec=1374799.40, grad_norm=0.2506, duration=0.38s
Step 3493: loss=3.3467, lr=0.000583, tokens/sec=1377259.40, grad_norm=0.2582, duration=0.38s
Step 3494: loss=3.3849, lr=0.000583, tokens/sec=1374710.87, grad_norm=0.2603, duration=0.38s
Step 3495: loss=3.3944, lr=0.000583, tokens/sec=1378035.30, grad_norm=0.2833, duration=0.38s
Step 3496: loss=3.3867, lr=0.000583, tokens/sec=1372554.60, grad_norm=0.2553, duration=0.38s
Step 3497: loss=3.3397, lr=0.000583, tokens/sec=1374499.49, grad_norm=0.2620, duration=0.38s
Step 3498: loss=3.3756, lr=0.000583, tokens/sec=1374558.78, grad_norm=0.2392, duration=0.38s
Step 3499: loss=3.3551, lr=0.000583, tokens/sec=1376714.46, grad_norm=0.2696, duration=0.38s
Step 3500/19073 (18.4%), Elapsed time: 1434.90s, Steps per hour: 8781.08, Estimated hours remaining: 1.77
Validation loss at step 3500: 3.70514178276062
Step 3500: loss=3.3432, lr=0.000583, tokens/sec=152557.20, grad_norm=0.2772, duration=3.44s
Step 3501: loss=3.3119, lr=0.000583, tokens/sec=1377959.31, grad_norm=0.2699, duration=0.38s
Step 3502: loss=3.3759, lr=0.000583, tokens/sec=1377588.99, grad_norm=0.2642, duration=0.38s
Step 3503: loss=3.3071, lr=0.000583, tokens/sec=1379665.89, grad_norm=0.2993, duration=0.38s
Step 3504: loss=3.3489, lr=0.000583, tokens/sec=1379419.24, grad_norm=0.2932, duration=0.38s
Step 3505: loss=3.2564, lr=0.000583, tokens/sec=1378559.68, grad_norm=0.2573, duration=0.38s
Step 3506: loss=3.3161, lr=0.000583, tokens/sec=1380736.61, grad_norm=0.2826, duration=0.38s
Step 3507: loss=3.3276, lr=0.000583, tokens/sec=1377923.05, grad_norm=0.2696, duration=0.38s
Step 3508: loss=3.2768, lr=0.000583, tokens/sec=1378103.52, grad_norm=0.2710, duration=0.38s
Step 3509: loss=3.3170, lr=0.000583, tokens/sec=1377519.95, grad_norm=0.3047, duration=0.38s
Step 3510: loss=3.2420, lr=0.000583, tokens/sec=1370654.50, grad_norm=0.2620, duration=0.38s
Step 3511: loss=3.3237, lr=0.000583, tokens/sec=1373507.06, grad_norm=0.3261, duration=0.38s
Step 3512: loss=3.2908, lr=0.000583, tokens/sec=1368884.05, grad_norm=0.2923, duration=0.38s
Step 3513: loss=3.3536, lr=0.000583, tokens/sec=1374069.20, grad_norm=0.2976, duration=0.38s
Step 3514: loss=3.3916, lr=0.000583, tokens/sec=1374173.10, grad_norm=0.2601, duration=0.38s
Step 3515: loss=3.3829, lr=0.000583, tokens/sec=1371543.58, grad_norm=0.3031, duration=0.38s
Step 3516: loss=3.4005, lr=0.000582, tokens/sec=1371781.44, grad_norm=0.2873, duration=0.38s
Step 3517: loss=3.3921, lr=0.000582, tokens/sec=1375802.31, grad_norm=0.2774, duration=0.38s
Step 3518: loss=3.3468, lr=0.000582, tokens/sec=1368452.16, grad_norm=0.2891, duration=0.38s
Step 3519: loss=3.4007, lr=0.000582, tokens/sec=1374494.34, grad_norm=0.2701, duration=0.38s
Step 3520: loss=3.3670, lr=0.000582, tokens/sec=1372624.00, grad_norm=0.2767, duration=0.38s
Step 3521: loss=3.4153, lr=0.000582, tokens/sec=1368683.83, grad_norm=0.3090, duration=0.38s
Step 3522: loss=3.4273, lr=0.000582, tokens/sec=1372875.94, grad_norm=0.2982, duration=0.38s
Step 3523: loss=3.3690, lr=0.000582, tokens/sec=1374119.86, grad_norm=0.2936, duration=0.38s
Step 3524: loss=3.3949, lr=0.000582, tokens/sec=1374529.56, grad_norm=0.3204, duration=0.38s
Step 3525: loss=3.3600, lr=0.000582, tokens/sec=1369120.13, grad_norm=0.3001, duration=0.38s
Step 3526: loss=3.3692, lr=0.000582, tokens/sec=1371259.63, grad_norm=0.2547, duration=0.38s
Step 3527: loss=3.3383, lr=0.000582, tokens/sec=1373682.09, grad_norm=0.2902, duration=0.38s
Step 3528: loss=3.3786, lr=0.000582, tokens/sec=1368398.51, grad_norm=0.2457, duration=0.38s
Step 3529: loss=3.3341, lr=0.000582, tokens/sec=1375711.08, grad_norm=0.2981, duration=0.38s
Step 3530: loss=3.3448, lr=0.000582, tokens/sec=1374873.32, grad_norm=0.2739, duration=0.38s
Step 3531: loss=3.3310, lr=0.000582, tokens/sec=1369085.19, grad_norm=0.2754, duration=0.38s
Step 3532: loss=3.3558, lr=0.000582, tokens/sec=1377475.94, grad_norm=0.3030, duration=0.38s
Step 3533: loss=3.2952, lr=0.000582, tokens/sec=1373042.24, grad_norm=0.2811, duration=0.38s
Step 3534: loss=3.4478, lr=0.000582, tokens/sec=1376634.31, grad_norm=0.2937, duration=0.38s
Step 3535: loss=3.3736, lr=0.000582, tokens/sec=1375080.51, grad_norm=0.2885, duration=0.38s
Step 3536: loss=3.3780, lr=0.000582, tokens/sec=1375480.46, grad_norm=0.2806, duration=0.38s
Step 3537: loss=3.3510, lr=0.000582, tokens/sec=1375416.80, grad_norm=0.2878, duration=0.38s
Step 3538: loss=3.3397, lr=0.000582, tokens/sec=1375730.87, grad_norm=0.2923, duration=0.38s
Step 3539: loss=3.3537, lr=0.000582, tokens/sec=1376058.01, grad_norm=0.2570, duration=0.38s
Step 3540: loss=3.3723, lr=0.000582, tokens/sec=1372564.88, grad_norm=0.2779, duration=0.38s
Step 3541: loss=3.3716, lr=0.000582, tokens/sec=1373821.12, grad_norm=0.2810, duration=0.38s
Step 3542: loss=3.4158, lr=0.000582, tokens/sec=1373042.24, grad_norm=0.2932, duration=0.38s
Step 3543: loss=3.3733, lr=0.000582, tokens/sec=1376858.41, grad_norm=0.3000, duration=0.38s
Step 3544: loss=3.3576, lr=0.000582, tokens/sec=1374629.24, grad_norm=0.2951, duration=0.38s
Step 3545: loss=3.3997, lr=0.000582, tokens/sec=1369282.97, grad_norm=0.2853, duration=0.38s
Step 3546: loss=3.3503, lr=0.000582, tokens/sec=1368343.17, grad_norm=0.2932, duration=0.38s
Step 3547: loss=3.3328, lr=0.000582, tokens/sec=1368880.65, grad_norm=0.2835, duration=0.38s
Step 3548: loss=3.3481, lr=0.000582, tokens/sec=1374215.18, grad_norm=0.2569, duration=0.38s
Step 3549: loss=3.2792, lr=0.000582, tokens/sec=1375232.72, grad_norm=0.2699, duration=0.38s
Step 3550: loss=3.3148, lr=0.000582, tokens/sec=1374564.79, grad_norm=0.2649, duration=0.38s
Step 3551: loss=3.2224, lr=0.000582, tokens/sec=1374915.44, grad_norm=0.2489, duration=0.38s
Step 3552: loss=3.2644, lr=0.000582, tokens/sec=1376269.86, grad_norm=0.2665, duration=0.38s
Step 3553: loss=3.2693, lr=0.000582, tokens/sec=1375049.56, grad_norm=0.2809, duration=0.38s
Step 3554: loss=3.3370, lr=0.000582, tokens/sec=1376048.53, grad_norm=0.2700, duration=0.38s
Step 3555: loss=3.3322, lr=0.000582, tokens/sec=1376429.23, grad_norm=0.2832, duration=0.38s
Step 3556: loss=3.2763, lr=0.000582, tokens/sec=1380774.75, grad_norm=0.2581, duration=0.38s
Step 3557: loss=3.2557, lr=0.000582, tokens/sec=1377306.84, grad_norm=0.2687, duration=0.38s
Step 3558: loss=3.2998, lr=0.000582, tokens/sec=1375890.98, grad_norm=0.2686, duration=0.38s
Step 3559: loss=3.3379, lr=0.000582, tokens/sec=1376115.70, grad_norm=0.2754, duration=0.38s
Step 3560: loss=3.3843, lr=0.000582, tokens/sec=1374319.10, grad_norm=0.2462, duration=0.38s
Step 3561: loss=3.3962, lr=0.000582, tokens/sec=1379969.79, grad_norm=0.2559, duration=0.38s
Step 3562: loss=3.4319, lr=0.000582, tokens/sec=1376505.91, grad_norm=0.3221, duration=0.38s
Step 3563: loss=3.3871, lr=0.000582, tokens/sec=1376642.07, grad_norm=0.3471, duration=0.38s
Step 3564: loss=3.4039, lr=0.000582, tokens/sec=1378532.02, grad_norm=0.3198, duration=0.38s
Step 3565: loss=3.3526, lr=0.000582, tokens/sec=1377211.10, grad_norm=0.2792, duration=0.38s
Step 3566: loss=3.3849, lr=0.000582, tokens/sec=1374063.19, grad_norm=0.3030, duration=0.38s
Step 3567: loss=3.4122, lr=0.000582, tokens/sec=1373605.72, grad_norm=0.2882, duration=0.38s
Step 3568: loss=3.4824, lr=0.000582, tokens/sec=1377562.23, grad_norm=0.2988, duration=0.38s
Step 3569: loss=3.4195, lr=0.000582, tokens/sec=1374145.62, grad_norm=0.2815, duration=0.38s
Step 3570: loss=3.4063, lr=0.000582, tokens/sec=1375649.97, grad_norm=0.2809, duration=0.38s
Step 3571: loss=3.3334, lr=0.000582, tokens/sec=1380217.50, grad_norm=0.2859, duration=0.38s
Step 3572: loss=3.3645, lr=0.000582, tokens/sec=1376853.24, grad_norm=0.2765, duration=0.38s
Step 3573: loss=3.3673, lr=0.000582, tokens/sec=1376496.43, grad_norm=0.2745, duration=0.38s
Step 3574: loss=3.3358, lr=0.000582, tokens/sec=1375375.51, grad_norm=0.2578, duration=0.38s
Step 3575: loss=3.3923, lr=0.000582, tokens/sec=1375395.29, grad_norm=0.2727, duration=0.38s
Step 3576: loss=3.2994, lr=0.000582, tokens/sec=1375283.47, grad_norm=0.2739, duration=0.38s
Step 3577: loss=3.3967, lr=0.000582, tokens/sec=1377481.98, grad_norm=0.2589, duration=0.38s
Step 3578: loss=3.4191, lr=0.000582, tokens/sec=1377285.28, grad_norm=0.2860, duration=0.38s
Step 3579: loss=3.3767, lr=0.000582, tokens/sec=1375608.67, grad_norm=0.2749, duration=0.38s
Step 3580: loss=3.2950, lr=0.000582, tokens/sec=1376045.09, grad_norm=0.2911, duration=0.38s
Step 3581: loss=3.3480, lr=0.000582, tokens/sec=1374031.43, grad_norm=0.3083, duration=0.38s
Step 3582: loss=3.3735, lr=0.000582, tokens/sec=1374670.48, grad_norm=0.2850, duration=0.38s
Step 3583: loss=3.3278, lr=0.000582, tokens/sec=1376666.20, grad_norm=0.2757, duration=0.38s
Step 3584: loss=3.2655, lr=0.000582, tokens/sec=1376861.86, grad_norm=0.3111, duration=0.38s
Step 3585: loss=3.3343, lr=0.000582, tokens/sec=1374447.95, grad_norm=0.2892, duration=0.38s
Step 3586: loss=3.3014, lr=0.000582, tokens/sec=1378812.94, grad_norm=0.3031, duration=0.38s
Step 3587: loss=3.3481, lr=0.000582, tokens/sec=1381274.32, grad_norm=0.3428, duration=0.38s
Step 3588: loss=3.4219, lr=0.000582, tokens/sec=1374047.74, grad_norm=0.3021, duration=0.38s
Step 3589: loss=3.3421, lr=0.000582, tokens/sec=1379854.62, grad_norm=0.2911, duration=0.38s
Step 3590: loss=3.3462, lr=0.000581, tokens/sec=1372845.08, grad_norm=0.2921, duration=0.38s
Step 3591: loss=3.3217, lr=0.000581, tokens/sec=1377022.23, grad_norm=0.2840, duration=0.38s
Step 3592: loss=3.2927, lr=0.000581, tokens/sec=1374013.40, grad_norm=0.2808, duration=0.38s
Step 3593: loss=3.3296, lr=0.000581, tokens/sec=1371535.03, grad_norm=0.3119, duration=0.38s
Step 3594: loss=3.3072, lr=0.000581, tokens/sec=1373835.71, grad_norm=0.3009, duration=0.38s
Step 3595: loss=3.2479, lr=0.000581, tokens/sec=1374722.91, grad_norm=0.2619, duration=0.38s
Step 3596: loss=3.2915, lr=0.000581, tokens/sec=1375050.42, grad_norm=0.3132, duration=0.38s
Step 3597: loss=3.3492, lr=0.000581, tokens/sec=1379507.51, grad_norm=0.2941, duration=0.38s
Step 3598: loss=3.3118, lr=0.000581, tokens/sec=1374487.47, grad_norm=0.2867, duration=0.38s
Step 3599: loss=3.3054, lr=0.000581, tokens/sec=1374430.77, grad_norm=0.2964, duration=0.38s
Step 3600/19073 (18.9%), Elapsed time: 1476.16s, Steps per hour: 8779.52, Estimated hours remaining: 1.76
Step 3600: loss=3.2492, lr=0.000581, tokens/sec=1377779.73, grad_norm=0.3197, duration=0.38s
Step 3601: loss=3.2827, lr=0.000581, tokens/sec=1376461.11, grad_norm=0.2937, duration=0.38s
Step 3602: loss=3.3377, lr=0.000581, tokens/sec=1375053.86, grad_norm=0.2688, duration=0.38s
Step 3603: loss=3.2657, lr=0.000581, tokens/sec=1379843.37, grad_norm=0.2699, duration=0.38s
Step 3604: loss=3.3353, lr=0.000581, tokens/sec=1374898.25, grad_norm=0.2689, duration=0.38s
Step 3605: loss=3.3714, lr=0.000581, tokens/sec=1371430.67, grad_norm=0.2734, duration=0.38s
Step 3606: loss=3.4052, lr=0.000581, tokens/sec=1378836.28, grad_norm=0.2648, duration=0.38s
Step 3607: loss=3.3813, lr=0.000581, tokens/sec=1373361.23, grad_norm=0.3144, duration=0.38s
Step 3608: loss=3.3552, lr=0.000581, tokens/sec=1377913.55, grad_norm=0.3431, duration=0.38s
Step 3609: loss=3.3802, lr=0.000581, tokens/sec=1378428.33, grad_norm=0.3044, duration=0.38s
Step 3610: loss=3.3740, lr=0.000581, tokens/sec=1377602.79, grad_norm=0.3612, duration=0.38s
Step 3611: loss=3.4179, lr=0.000581, tokens/sec=1372674.55, grad_norm=0.3564, duration=0.38s
Step 3612: loss=3.4060, lr=0.000581, tokens/sec=1377011.02, grad_norm=0.3593, duration=0.38s
Step 3613: loss=3.4797, lr=0.000581, tokens/sec=1378375.62, grad_norm=0.3362, duration=0.38s
Step 3614: loss=3.3867, lr=0.000581, tokens/sec=1378708.34, grad_norm=0.3105, duration=0.38s
Step 3615: loss=3.4153, lr=0.000581, tokens/sec=1377573.45, grad_norm=0.2938, duration=0.38s
Step 3616: loss=3.4576, lr=0.000581, tokens/sec=1373697.53, grad_norm=0.3114, duration=0.38s
Step 3617: loss=3.3914, lr=0.000581, tokens/sec=1376321.55, grad_norm=0.2723, duration=0.38s
Step 3618: loss=3.4437, lr=0.000581, tokens/sec=1375427.12, grad_norm=0.2663, duration=0.38s
Step 3619: loss=3.3746, lr=0.000581, tokens/sec=1376317.24, grad_norm=0.2634, duration=0.38s
Step 3620: loss=3.3857, lr=0.000581, tokens/sec=1376931.69, grad_norm=0.2640, duration=0.38s
Step 3621: loss=3.3539, lr=0.000581, tokens/sec=1374019.41, grad_norm=0.2756, duration=0.38s
Step 3622: loss=3.3897, lr=0.000581, tokens/sec=1377584.67, grad_norm=0.2656, duration=0.38s
Step 3623: loss=3.3459, lr=0.000581, tokens/sec=1378645.24, grad_norm=0.2612, duration=0.38s
Step 3624: loss=3.3665, lr=0.000581, tokens/sec=1377315.47, grad_norm=0.2589, duration=0.38s
Step 3625: loss=3.3765, lr=0.000581, tokens/sec=1376225.07, grad_norm=0.2831, duration=0.38s
Step 3626: loss=3.3083, lr=0.000581, tokens/sec=1368231.64, grad_norm=0.2680, duration=0.38s
Step 3627: loss=3.3914, lr=0.000581, tokens/sec=1372416.69, grad_norm=0.2582, duration=0.38s
Step 3628: loss=3.2881, lr=0.000581, tokens/sec=1370534.05, grad_norm=0.2687, duration=0.38s
Step 3629: loss=3.2732, lr=0.000581, tokens/sec=1372146.93, grad_norm=0.4767, duration=0.38s
Step 3630: loss=3.3246, lr=0.000581, tokens/sec=1376951.53, grad_norm=0.2683, duration=0.38s
Step 3631: loss=3.3254, lr=0.000581, tokens/sec=1378865.68, grad_norm=0.2840, duration=0.38s
Step 3632: loss=3.4574, lr=0.000581, tokens/sec=1378317.74, grad_norm=0.3348, duration=0.38s
Step 3633: loss=3.2829, lr=0.000581, tokens/sec=1377404.33, grad_norm=0.3187, duration=0.38s
Step 3634: loss=3.3195, lr=0.000581, tokens/sec=1375618.99, grad_norm=0.3148, duration=0.38s
Step 3635: loss=3.3401, lr=0.000581, tokens/sec=1375044.40, grad_norm=0.2890, duration=0.38s
Step 3636: loss=3.2997, lr=0.000581, tokens/sec=1379201.22, grad_norm=0.2910, duration=0.38s
Step 3637: loss=3.3610, lr=0.000581, tokens/sec=1374894.81, grad_norm=0.2788, duration=0.38s
Step 3638: loss=3.3735, lr=0.000581, tokens/sec=1379723.89, grad_norm=0.2701, duration=0.38s
Step 3639: loss=3.3531, lr=0.000581, tokens/sec=1375915.94, grad_norm=0.2846, duration=0.38s
Step 3640: loss=3.3203, lr=0.000581, tokens/sec=1377331.86, grad_norm=0.2633, duration=0.38s
Step 3641: loss=3.3510, lr=0.000581, tokens/sec=1379271.29, grad_norm=0.2587, duration=0.38s
Step 3642: loss=3.4056, lr=0.000581, tokens/sec=1376536.07, grad_norm=0.2955, duration=0.38s
Step 3643: loss=3.4250, lr=0.000581, tokens/sec=1377936.00, grad_norm=0.2920, duration=0.38s
Step 3644: loss=3.3291, lr=0.000581, tokens/sec=1371447.78, grad_norm=0.2895, duration=0.38s
Step 3645: loss=3.2806, lr=0.000581, tokens/sec=1372756.81, grad_norm=0.2677, duration=0.38s
Step 3646: loss=3.3291, lr=0.000581, tokens/sec=1374565.65, grad_norm=0.2917, duration=0.38s
Step 3647: loss=3.2767, lr=0.000581, tokens/sec=1375106.31, grad_norm=0.2608, duration=0.38s
Step 3648: loss=3.2756, lr=0.000581, tokens/sec=1373955.88, grad_norm=0.2705, duration=0.38s
Step 3649: loss=3.2758, lr=0.000581, tokens/sec=1376101.92, grad_norm=0.2714, duration=0.38s
Step 3650: loss=3.2944, lr=0.000581, tokens/sec=1376155.31, grad_norm=0.2528, duration=0.38s
Step 3651: loss=3.2962, lr=0.000581, tokens/sec=1380408.11, grad_norm=0.2738, duration=0.38s
Step 3652: loss=3.2883, lr=0.000581, tokens/sec=1378599.43, grad_norm=0.2511, duration=0.38s
Step 3653: loss=3.3356, lr=0.000581, tokens/sec=1373823.69, grad_norm=0.2804, duration=0.38s
Step 3654: loss=3.2851, lr=0.000581, tokens/sec=1376862.72, grad_norm=0.2532, duration=0.38s
Step 3655: loss=3.2907, lr=0.000581, tokens/sec=1375246.49, grad_norm=0.2637, duration=0.38s
Step 3656: loss=3.3295, lr=0.000581, tokens/sec=1374500.35, grad_norm=0.2473, duration=0.38s
Step 3657: loss=3.3803, lr=0.000581, tokens/sec=1376791.17, grad_norm=0.2593, duration=0.38s
Step 3658: loss=3.4481, lr=0.000581, tokens/sec=1372665.98, grad_norm=0.2983, duration=0.38s
Step 3659: loss=3.3840, lr=0.000581, tokens/sec=1378007.67, grad_norm=0.2802, duration=0.38s
Step 3660: loss=3.3659, lr=0.000581, tokens/sec=1375120.93, grad_norm=0.2625, duration=0.38s
Step 3661: loss=3.4011, lr=0.000581, tokens/sec=1378080.21, grad_norm=0.2881, duration=0.38s
Step 3662: loss=3.3927, lr=0.000580, tokens/sec=1376992.05, grad_norm=0.3137, duration=0.38s
Step 3663: loss=3.3894, lr=0.000580, tokens/sec=1372115.25, grad_norm=0.2882, duration=0.38s
Step 3664: loss=3.3588, lr=0.000580, tokens/sec=1375893.56, grad_norm=0.3159, duration=0.38s
Step 3665: loss=3.3511, lr=0.000580, tokens/sec=1371005.72, grad_norm=0.2988, duration=0.38s
Step 3666: loss=3.3610, lr=0.000580, tokens/sec=1376480.93, grad_norm=0.2797, duration=0.38s
Step 3667: loss=3.3587, lr=0.000580, tokens/sec=1371213.46, grad_norm=0.3255, duration=0.38s
Step 3668: loss=3.3724, lr=0.000580, tokens/sec=1374267.57, grad_norm=0.3110, duration=0.38s
Step 3669: loss=3.3291, lr=0.000580, tokens/sec=1376713.60, grad_norm=0.2881, duration=0.38s
Step 3670: loss=3.3363, lr=0.000580, tokens/sec=1374295.05, grad_norm=0.2736, duration=0.38s
Step 3671: loss=3.2646, lr=0.000580, tokens/sec=1375341.96, grad_norm=0.2784, duration=0.38s
Step 3672: loss=3.3160, lr=0.000580, tokens/sec=1375600.92, grad_norm=0.2692, duration=0.38s
Step 3673: loss=3.3561, lr=0.000580, tokens/sec=1376758.42, grad_norm=0.2603, duration=0.38s
Step 3674: loss=3.3161, lr=0.000580, tokens/sec=1373375.81, grad_norm=0.2547, duration=0.38s
Step 3675: loss=3.3891, lr=0.000580, tokens/sec=1374046.88, grad_norm=0.2688, duration=0.38s
Step 3676: loss=3.3049, lr=0.000580, tokens/sec=1372507.48, grad_norm=0.2481, duration=0.38s
Step 3677: loss=3.3902, lr=0.000580, tokens/sec=1371192.09, grad_norm=0.2569, duration=0.38s
Step 3678: loss=3.3563, lr=0.000580, tokens/sec=1371328.90, grad_norm=0.2845, duration=0.38s
Step 3679: loss=3.3313, lr=0.000580, tokens/sec=1371505.09, grad_norm=0.2704, duration=0.38s
Step 3680: loss=3.2798, lr=0.000580, tokens/sec=1368115.87, grad_norm=0.2687, duration=0.38s
Step 3681: loss=3.3384, lr=0.000580, tokens/sec=1372891.37, grad_norm=0.2828, duration=0.38s
Step 3682: loss=3.3088, lr=0.000580, tokens/sec=1376823.93, grad_norm=0.2753, duration=0.38s
Step 3683: loss=3.3682, lr=0.000580, tokens/sec=1375679.23, grad_norm=0.2547, duration=0.38s
Step 3684: loss=3.3633, lr=0.000580, tokens/sec=1373870.90, grad_norm=0.2935, duration=0.38s
Step 3685: loss=3.3700, lr=0.000580, tokens/sec=1373962.75, grad_norm=0.2694, duration=0.38s
Step 3686: loss=3.3546, lr=0.000580, tokens/sec=1372043.34, grad_norm=0.2908, duration=0.38s
Step 3687: loss=3.3162, lr=0.000580, tokens/sec=1372030.50, grad_norm=0.2861, duration=0.38s
Step 3688: loss=3.3426, lr=0.000580, tokens/sec=1374795.96, grad_norm=0.2790, duration=0.38s
Step 3689: loss=3.3062, lr=0.000580, tokens/sec=1371400.74, grad_norm=0.3029, duration=0.38s
Step 3690: loss=3.3635, lr=0.000580, tokens/sec=1374741.81, grad_norm=0.2835, duration=0.38s
Step 3691: loss=3.2899, lr=0.000580, tokens/sec=1377556.19, grad_norm=0.2947, duration=0.38s
Step 3692: loss=3.3439, lr=0.000580, tokens/sec=1377539.80, grad_norm=0.3320, duration=0.38s
Step 3693: loss=3.3121, lr=0.000580, tokens/sec=1377466.45, grad_norm=0.2833, duration=0.38s
Step 3694: loss=3.2878, lr=0.000580, tokens/sec=1374541.59, grad_norm=0.2952, duration=0.38s
Step 3695: loss=3.2526, lr=0.000580, tokens/sec=1377199.88, grad_norm=0.2987, duration=0.38s
Step 3696: loss=3.2809, lr=0.000580, tokens/sec=1378350.57, grad_norm=0.2939, duration=0.38s
Step 3697: loss=3.2991, lr=0.000580, tokens/sec=1374443.65, grad_norm=0.2524, duration=0.38s
Step 3698: loss=3.2729, lr=0.000580, tokens/sec=1375662.02, grad_norm=0.2907, duration=0.38s
Step 3699: loss=3.2411, lr=0.000580, tokens/sec=1367294.98, grad_norm=0.2664, duration=0.38s
Step 3700/19073 (19.4%), Elapsed time: 1514.36s, Steps per hour: 8795.81, Estimated hours remaining: 1.75
Step 3700: loss=3.2452, lr=0.000580, tokens/sec=1374508.95, grad_norm=0.3034, duration=0.38s
Step 3701: loss=3.2815, lr=0.000580, tokens/sec=1375827.27, grad_norm=0.3234, duration=0.38s
Step 3702: loss=3.2927, lr=0.000580, tokens/sec=1374035.72, grad_norm=0.3342, duration=0.38s
Step 3703: loss=3.3228, lr=0.000580, tokens/sec=1375753.25, grad_norm=0.2842, duration=0.38s
Step 3704: loss=3.3525, lr=0.000580, tokens/sec=1378278.00, grad_norm=0.3233, duration=0.38s
Step 3705: loss=3.3759, lr=0.000580, tokens/sec=1374942.95, grad_norm=0.3317, duration=0.38s
Step 3706: loss=3.3689, lr=0.000580, tokens/sec=1370322.25, grad_norm=0.2720, duration=0.38s
Step 3707: loss=3.3775, lr=0.000580, tokens/sec=1376899.80, grad_norm=0.3113, duration=0.38s
Step 3708: loss=3.3339, lr=0.000580, tokens/sec=1375656.00, grad_norm=0.3180, duration=0.38s
Step 3709: loss=3.3798, lr=0.000580, tokens/sec=1377904.05, grad_norm=0.3018, duration=0.38s
Step 3710: loss=3.3383, lr=0.000580, tokens/sec=1377061.03, grad_norm=0.2930, duration=0.38s
Step 3711: loss=3.4173, lr=0.000580, tokens/sec=1371772.88, grad_norm=0.2998, duration=0.38s
Step 3712: loss=3.3553, lr=0.000580, tokens/sec=1371906.38, grad_norm=0.2839, duration=0.38s
Step 3713: loss=3.3581, lr=0.000580, tokens/sec=1370870.68, grad_norm=0.3055, duration=0.38s
Step 3714: loss=3.3719, lr=0.000580, tokens/sec=1374715.17, grad_norm=0.2791, duration=0.38s
Step 3715: loss=3.3359, lr=0.000580, tokens/sec=1376497.30, grad_norm=0.2976, duration=0.38s
Step 3716: loss=3.3297, lr=0.000580, tokens/sec=1373169.99, grad_norm=0.2995, duration=0.38s
Step 3717: loss=3.3321, lr=0.000580, tokens/sec=1372233.41, grad_norm=0.2829, duration=0.38s
Step 3718: loss=3.3703, lr=0.000580, tokens/sec=1374014.26, grad_norm=0.2794, duration=0.38s
Step 3719: loss=3.2753, lr=0.000580, tokens/sec=1372608.58, grad_norm=0.2777, duration=0.38s
Step 3720: loss=3.3164, lr=0.000580, tokens/sec=1373565.40, grad_norm=0.2698, duration=0.38s
Step 3721: loss=3.3265, lr=0.000580, tokens/sec=1375597.48, grad_norm=0.2996, duration=0.38s
Step 3722: loss=3.3241, lr=0.000580, tokens/sec=1371956.88, grad_norm=0.2614, duration=0.38s
Step 3723: loss=3.3458, lr=0.000580, tokens/sec=1373446.15, grad_norm=0.2819, duration=0.38s
Step 3724: loss=3.3981, lr=0.000580, tokens/sec=1373702.68, grad_norm=0.3074, duration=0.38s
Step 3725: loss=3.3136, lr=0.000580, tokens/sec=1378413.64, grad_norm=0.2815, duration=0.38s
Step 3726: loss=3.3639, lr=0.000580, tokens/sec=1372391.85, grad_norm=0.3113, duration=0.38s
Step 3727: loss=3.2982, lr=0.000580, tokens/sec=1375741.20, grad_norm=0.2866, duration=0.38s
Step 3728: loss=3.3534, lr=0.000580, tokens/sec=1378337.61, grad_norm=0.2708, duration=0.38s
Step 3729: loss=3.3499, lr=0.000580, tokens/sec=1370480.24, grad_norm=0.3486, duration=0.38s
Step 3730: loss=3.3528, lr=0.000580, tokens/sec=1374907.70, grad_norm=0.3218, duration=0.38s
Step 3731: loss=3.3774, lr=0.000580, tokens/sec=1377307.71, grad_norm=0.3051, duration=0.38s
Step 3732: loss=3.3758, lr=0.000579, tokens/sec=1374423.90, grad_norm=0.3055, duration=0.38s
Step 3733: loss=3.3152, lr=0.000579, tokens/sec=1373644.33, grad_norm=0.2963, duration=0.38s
Step 3734: loss=3.3713, lr=0.000579, tokens/sec=1378044.80, grad_norm=0.2872, duration=0.38s
Step 3735: loss=3.3469, lr=0.000579, tokens/sec=1377048.96, grad_norm=0.3136, duration=0.38s
Step 3736: loss=3.3217, lr=0.000579, tokens/sec=1376191.49, grad_norm=0.2824, duration=0.38s
Step 3737: loss=3.3340, lr=0.000579, tokens/sec=1378736.00, grad_norm=0.2793, duration=0.38s
Step 3738: loss=3.3113, lr=0.000579, tokens/sec=1374999.69, grad_norm=0.2748, duration=0.38s
Step 3739: loss=3.2502, lr=0.000579, tokens/sec=1376886.00, grad_norm=0.2750, duration=0.38s
Step 3740: loss=3.2803, lr=0.000579, tokens/sec=1378288.37, grad_norm=0.2880, duration=0.38s
Step 3741: loss=3.1993, lr=0.000579, tokens/sec=1378177.80, grad_norm=0.2958, duration=0.38s
Step 3742: loss=3.2582, lr=0.000579, tokens/sec=1377560.51, grad_norm=0.2691, duration=0.38s
Step 3743: loss=3.2464, lr=0.000579, tokens/sec=1375584.57, grad_norm=0.2650, duration=0.38s
Step 3744: loss=3.3363, lr=0.000579, tokens/sec=1376536.07, grad_norm=0.2763, duration=0.38s
Step 3745: loss=3.2820, lr=0.000579, tokens/sec=1376855.83, grad_norm=0.2709, duration=0.38s
Step 3746: loss=3.2459, lr=0.000579, tokens/sec=1376604.15, grad_norm=0.2802, duration=0.38s
Step 3747: loss=3.2457, lr=0.000579, tokens/sec=1377765.92, grad_norm=0.2912, duration=0.38s
Step 3748: loss=3.2744, lr=0.000579, tokens/sec=1377520.81, grad_norm=0.2939, duration=0.38s
Step 3749: loss=3.3249, lr=0.000579, tokens/sec=1378646.97, grad_norm=0.2685, duration=0.38s
Validation loss at step 3750: 3.700080156326294
Step 3750: loss=3.3624, lr=0.000579, tokens/sec=152463.21, grad_norm=0.3011, duration=3.44s
Step 3751: loss=3.3943, lr=0.000579, tokens/sec=1378871.73, grad_norm=0.2943, duration=0.38s
Step 3752: loss=3.3900, lr=0.000579, tokens/sec=1375735.18, grad_norm=0.3226, duration=0.38s
Step 3753: loss=3.3531, lr=0.000579, tokens/sec=1374009.11, grad_norm=0.3059, duration=0.38s
Step 3754: loss=3.3787, lr=0.000579, tokens/sec=1372393.56, grad_norm=0.2908, duration=0.38s
Step 3755: loss=3.3365, lr=0.000579, tokens/sec=1376470.59, grad_norm=0.3386, duration=0.38s
Step 3756: loss=3.3419, lr=0.000579, tokens/sec=1376181.15, grad_norm=0.2921, duration=0.38s
Step 3757: loss=3.3929, lr=0.000579, tokens/sec=1377671.84, grad_norm=0.3178, duration=0.38s
Step 3758: loss=3.4611, lr=0.000579, tokens/sec=1377379.31, grad_norm=0.2848, duration=0.38s
Step 3759: loss=3.3960, lr=0.000579, tokens/sec=1376254.36, grad_norm=0.2951, duration=0.38s
Step 3760: loss=3.3741, lr=0.000579, tokens/sec=1378211.49, grad_norm=0.2989, duration=0.38s
Step 3761: loss=3.3022, lr=0.000579, tokens/sec=1377704.64, grad_norm=0.2829, duration=0.38s
Step 3762: loss=3.3677, lr=0.000579, tokens/sec=1373937.85, grad_norm=0.2598, duration=0.38s
Step 3763: loss=3.3415, lr=0.000579, tokens/sec=1375625.88, grad_norm=0.2871, duration=0.38s
Step 3764: loss=3.2818, lr=0.000579, tokens/sec=1378664.26, grad_norm=0.2657, duration=0.38s
Step 3765: loss=3.3600, lr=0.000579, tokens/sec=1376693.78, grad_norm=0.3261, duration=0.38s
Step 3766: loss=3.2849, lr=0.000579, tokens/sec=1375145.01, grad_norm=0.2891, duration=0.38s
Step 3767: loss=3.3627, lr=0.000579, tokens/sec=1374466.85, grad_norm=0.2850, duration=0.38s
Step 3768: loss=3.4143, lr=0.000579, tokens/sec=1378161.39, grad_norm=0.2981, duration=0.38s
Step 3769: loss=3.3338, lr=0.000579, tokens/sec=1376442.15, grad_norm=0.3107, duration=0.38s
Step 3770: loss=3.2529, lr=0.000579, tokens/sec=1376121.73, grad_norm=0.2700, duration=0.38s
Step 3771: loss=3.3412, lr=0.000579, tokens/sec=1376576.57, grad_norm=0.3111, duration=0.38s
Step 3772: loss=3.3428, lr=0.000579, tokens/sec=1376898.93, grad_norm=0.2560, duration=0.38s
Step 3773: loss=3.3126, lr=0.000579, tokens/sec=1372503.20, grad_norm=0.2869, duration=0.38s
Step 3774: loss=3.2396, lr=0.000579, tokens/sec=1378527.70, grad_norm=0.2545, duration=0.38s
Step 3775: loss=3.2815, lr=0.000579, tokens/sec=1375939.19, grad_norm=0.2592, duration=0.38s
Step 3776: loss=3.2991, lr=0.000579, tokens/sec=1369489.33, grad_norm=0.2741, duration=0.38s
Step 3777: loss=3.3650, lr=0.000579, tokens/sec=1369149.97, grad_norm=0.3130, duration=0.38s
Step 3778: loss=3.4009, lr=0.000579, tokens/sec=1373546.52, grad_norm=0.2969, duration=0.38s
Step 3779: loss=3.3110, lr=0.000579, tokens/sec=1373030.23, grad_norm=0.3132, duration=0.38s
Step 3780: loss=3.2957, lr=0.000579, tokens/sec=1371108.30, grad_norm=0.2733, duration=0.38s
Step 3781: loss=3.3183, lr=0.000579, tokens/sec=1375962.43, grad_norm=0.2788, duration=0.38s
Step 3782: loss=3.2522, lr=0.000579, tokens/sec=1375102.01, grad_norm=0.2561, duration=0.38s
Step 3783: loss=3.3230, lr=0.000579, tokens/sec=1375204.34, grad_norm=0.2708, duration=0.38s
Step 3784: loss=3.2342, lr=0.000579, tokens/sec=1373877.77, grad_norm=0.2570, duration=0.38s
Step 3785: loss=3.2652, lr=0.000579, tokens/sec=1373616.88, grad_norm=0.2679, duration=0.38s
Step 3786: loss=3.2573, lr=0.000579, tokens/sec=1373701.83, grad_norm=0.2823, duration=0.38s
Step 3787: loss=3.3506, lr=0.000579, tokens/sec=1377557.92, grad_norm=0.2720, duration=0.38s
Step 3788: loss=3.2992, lr=0.000579, tokens/sec=1371509.37, grad_norm=0.2855, duration=0.38s
Step 3789: loss=3.2579, lr=0.000579, tokens/sec=1374044.31, grad_norm=0.2829, duration=0.38s
Step 3790: loss=3.2218, lr=0.000579, tokens/sec=1374630.95, grad_norm=0.2905, duration=0.38s
Step 3791: loss=3.2865, lr=0.000579, tokens/sec=1374614.63, grad_norm=0.2921, duration=0.38s
Step 3792: loss=3.2516, lr=0.000579, tokens/sec=1375303.25, grad_norm=0.2632, duration=0.38s
Step 3793: loss=3.2921, lr=0.000579, tokens/sec=1370622.89, grad_norm=0.3032, duration=0.38s
Step 3794: loss=3.2994, lr=0.000579, tokens/sec=1375712.80, grad_norm=0.2869, duration=0.38s
Step 3795: loss=3.3739, lr=0.000579, tokens/sec=1376251.78, grad_norm=0.2953, duration=0.38s
Step 3796: loss=3.3450, lr=0.000579, tokens/sec=1376280.20, grad_norm=0.3011, duration=0.38s
Step 3797: loss=3.3634, lr=0.000579, tokens/sec=1373420.41, grad_norm=0.2961, duration=0.38s
Step 3798: loss=3.3241, lr=0.000579, tokens/sec=1373980.78, grad_norm=0.3420, duration=0.38s
Step 3799: loss=3.3661, lr=0.000579, tokens/sec=1372255.68, grad_norm=0.3309, duration=0.38s
Step 3800/19073 (19.9%), Elapsed time: 1555.61s, Steps per hour: 8793.96, Estimated hours remaining: 1.74
Step 3800: loss=3.3557, lr=0.000578, tokens/sec=1374578.54, grad_norm=0.3433, duration=0.38s
Step 3801: loss=3.3762, lr=0.000578, tokens/sec=1374455.68, grad_norm=0.3638, duration=0.38s
Step 3802: loss=3.3920, lr=0.000578, tokens/sec=1374766.74, grad_norm=0.3165, duration=0.38s
Step 3803: loss=3.4303, lr=0.000578, tokens/sec=1374819.17, grad_norm=0.3302, duration=0.38s
Step 3804: loss=3.3618, lr=0.000578, tokens/sec=1374996.25, grad_norm=0.2799, duration=0.38s
Step 3805: loss=3.3775, lr=0.000578, tokens/sec=1376566.23, grad_norm=0.2900, duration=0.38s
Step 3806: loss=3.4388, lr=0.000578, tokens/sec=1376989.46, grad_norm=0.2947, duration=0.38s
Step 3807: loss=3.3993, lr=0.000578, tokens/sec=1373643.47, grad_norm=0.3027, duration=0.38s
Step 3808: loss=3.3814, lr=0.000578, tokens/sec=1370587.87, grad_norm=0.2527, duration=0.38s
Step 3809: loss=3.3448, lr=0.000578, tokens/sec=1374923.18, grad_norm=0.2621, duration=0.38s
Step 3810: loss=3.3739, lr=0.000578, tokens/sec=1377036.89, grad_norm=0.2578, duration=0.38s
Step 3811: loss=3.3441, lr=0.000578, tokens/sec=1375056.44, grad_norm=0.2581, duration=0.38s
Step 3812: loss=3.3823, lr=0.000578, tokens/sec=1374542.45, grad_norm=0.2573, duration=0.38s
Step 3813: loss=3.2870, lr=0.000578, tokens/sec=1375790.26, grad_norm=0.2674, duration=0.38s
Step 3814: loss=3.3552, lr=0.000578, tokens/sec=1376057.14, grad_norm=0.2848, duration=0.38s
Step 3815: loss=3.3204, lr=0.000578, tokens/sec=1379341.37, grad_norm=0.2943, duration=0.38s
Step 3816: loss=3.3074, lr=0.000578, tokens/sec=1376521.42, grad_norm=0.2931, duration=0.38s
Step 3817: loss=3.3360, lr=0.000578, tokens/sec=1373109.97, grad_norm=0.2870, duration=0.38s
Step 3818: loss=3.3030, lr=0.000578, tokens/sec=1375255.09, grad_norm=0.2869, duration=0.38s
Step 3819: loss=3.2381, lr=0.000578, tokens/sec=1379196.90, grad_norm=0.3370, duration=0.38s
Step 3820: loss=3.2789, lr=0.000578, tokens/sec=1375182.84, grad_norm=0.2848, duration=0.38s
Step 3821: loss=3.3436, lr=0.000578, tokens/sec=1377080.87, grad_norm=0.2895, duration=0.38s
Step 3822: loss=3.4095, lr=0.000578, tokens/sec=1377400.01, grad_norm=0.2705, duration=0.38s
Step 3823: loss=3.1936, lr=0.000578, tokens/sec=1376483.51, grad_norm=0.3344, duration=0.38s
Step 3824: loss=3.3453, lr=0.000578, tokens/sec=1377206.78, grad_norm=0.3476, duration=0.38s
Step 3825: loss=3.3235, lr=0.000578, tokens/sec=1374791.66, grad_norm=0.2904, duration=0.38s
Step 3826: loss=3.3135, lr=0.000578, tokens/sec=1378926.20, grad_norm=0.3012, duration=0.38s
Step 3827: loss=3.3257, lr=0.000578, tokens/sec=1377507.87, grad_norm=0.2792, duration=0.38s
Step 3828: loss=3.3560, lr=0.000578, tokens/sec=1377484.57, grad_norm=0.3033, duration=0.38s
Step 3829: loss=3.3049, lr=0.000578, tokens/sec=1376232.83, grad_norm=0.2632, duration=0.38s
Step 3830: loss=3.3088, lr=0.000578, tokens/sec=1375372.07, grad_norm=0.2652, duration=0.38s
Step 3831: loss=3.3624, lr=0.000578, tokens/sec=1377601.93, grad_norm=0.2634, duration=0.38s
Step 3832: loss=3.3685, lr=0.000578, tokens/sec=1377282.69, grad_norm=0.3344, duration=0.38s
Step 3833: loss=3.3819, lr=0.000578, tokens/sec=1375239.60, grad_norm=0.2812, duration=0.38s
Step 3834: loss=3.3174, lr=0.000578, tokens/sec=1377218.86, grad_norm=0.2567, duration=0.38s
Step 3835: loss=3.2438, lr=0.000578, tokens/sec=1381737.79, grad_norm=0.2496, duration=0.38s
Step 3836: loss=3.3155, lr=0.000578, tokens/sec=1378917.56, grad_norm=0.2939, duration=0.38s
Step 3837: loss=3.2583, lr=0.000578, tokens/sec=1379006.62, grad_norm=0.2771, duration=0.38s
Step 3838: loss=3.2276, lr=0.000578, tokens/sec=1378872.59, grad_norm=0.2546, duration=0.38s
Step 3839: loss=3.2622, lr=0.000578, tokens/sec=1375437.45, grad_norm=0.2763, duration=0.38s
Step 3840: loss=3.2664, lr=0.000578, tokens/sec=1368396.81, grad_norm=0.2418, duration=0.38s
Step 3841: loss=3.2735, lr=0.000578, tokens/sec=1374791.66, grad_norm=0.2736, duration=0.38s
Step 3842: loss=3.2943, lr=0.000578, tokens/sec=1378333.29, grad_norm=0.2858, duration=0.38s
Step 3843: loss=3.2844, lr=0.000578, tokens/sec=1376162.20, grad_norm=0.2995, duration=0.38s
Step 3844: loss=3.2740, lr=0.000578, tokens/sec=1375464.12, grad_norm=0.2837, duration=0.38s
Step 3845: loss=3.2947, lr=0.000578, tokens/sec=1378979.81, grad_norm=0.2869, duration=0.38s
Step 3846: loss=3.2636, lr=0.000578, tokens/sec=1376611.90, grad_norm=0.2577, duration=0.38s
Step 3847: loss=3.3835, lr=0.000578, tokens/sec=1372282.22, grad_norm=0.2824, duration=0.38s
Step 3848: loss=3.4118, lr=0.000578, tokens/sec=1376577.43, grad_norm=0.2953, duration=0.38s
Step 3849: loss=3.3446, lr=0.000578, tokens/sec=1375004.85, grad_norm=0.2793, duration=0.38s
Step 3850: loss=3.3556, lr=0.000578, tokens/sec=1373514.78, grad_norm=0.2686, duration=0.38s
Step 3851: loss=3.3750, lr=0.000578, tokens/sec=1371626.56, grad_norm=0.2971, duration=0.38s
Step 3852: loss=3.3752, lr=0.000578, tokens/sec=1376269.00, grad_norm=0.2937, duration=0.38s
Step 3853: loss=3.3710, lr=0.000578, tokens/sec=1377560.51, grad_norm=0.2758, duration=0.38s
Step 3854: loss=3.3206, lr=0.000578, tokens/sec=1378725.63, grad_norm=0.3167, duration=0.38s
Step 3855: loss=3.3190, lr=0.000578, tokens/sec=1372741.39, grad_norm=0.2601, duration=0.38s
Step 3856: loss=3.3440, lr=0.000578, tokens/sec=1376975.67, grad_norm=0.2772, duration=0.38s
Step 3857: loss=3.3607, lr=0.000578, tokens/sec=1377499.24, grad_norm=0.2879, duration=0.38s
Step 3858: loss=3.3066, lr=0.000578, tokens/sec=1376893.76, grad_norm=0.2821, duration=0.38s
Step 3859: loss=3.3054, lr=0.000578, tokens/sec=1370417.04, grad_norm=0.2641, duration=0.38s
Step 3860: loss=3.3128, lr=0.000578, tokens/sec=1377065.34, grad_norm=0.2643, duration=0.38s
Step 3861: loss=3.2563, lr=0.000578, tokens/sec=1374119.01, grad_norm=0.2991, duration=0.38s
Step 3862: loss=3.2943, lr=0.000578, tokens/sec=1378440.43, grad_norm=0.2973, duration=0.38s
Step 3863: loss=3.3035, lr=0.000578, tokens/sec=1368418.95, grad_norm=0.2838, duration=0.38s
Step 3864: loss=3.3163, lr=0.000578, tokens/sec=1375739.48, grad_norm=0.3180, duration=0.38s
Step 3865: loss=3.3561, lr=0.000578, tokens/sec=1377105.88, grad_norm=0.2572, duration=0.38s
Step 3866: loss=3.2799, lr=0.000578, tokens/sec=1374224.63, grad_norm=0.2892, duration=0.38s
Step 3867: loss=3.3791, lr=0.000577, tokens/sec=1378249.49, grad_norm=0.2867, duration=0.38s
Step 3868: loss=3.3369, lr=0.000577, tokens/sec=1376775.66, grad_norm=0.2877, duration=0.38s
Step 3869: loss=3.2489, lr=0.000577, tokens/sec=1370299.19, grad_norm=0.2656, duration=0.38s
Step 3870: loss=3.3108, lr=0.000577, tokens/sec=1375073.63, grad_norm=0.2667, duration=0.38s
Step 3871: loss=3.3094, lr=0.000577, tokens/sec=1372458.66, grad_norm=0.2620, duration=0.38s
Step 3872: loss=3.3284, lr=0.000577, tokens/sec=1371618.01, grad_norm=0.2577, duration=0.38s
Step 3873: loss=3.3431, lr=0.000577, tokens/sec=1373557.67, grad_norm=0.2580, duration=0.38s
Step 3874: loss=3.3382, lr=0.000577, tokens/sec=1374309.65, grad_norm=0.2680, duration=0.38s
Step 3875: loss=3.3337, lr=0.000577, tokens/sec=1378379.08, grad_norm=0.2679, duration=0.38s
Step 3876: loss=3.3324, lr=0.000577, tokens/sec=1377017.92, grad_norm=0.2644, duration=0.38s
Step 3877: loss=3.2792, lr=0.000577, tokens/sec=1374491.76, grad_norm=0.2721, duration=0.38s
Step 3878: loss=3.2961, lr=0.000577, tokens/sec=1375829.00, grad_norm=0.3275, duration=0.38s
Step 3879: loss=3.3319, lr=0.000577, tokens/sec=1378997.11, grad_norm=0.3235, duration=0.38s
Step 3880: loss=3.3450, lr=0.000577, tokens/sec=1376182.01, grad_norm=0.2836, duration=0.38s
Step 3881: loss=3.2566, lr=0.000577, tokens/sec=1375077.07, grad_norm=0.3169, duration=0.38s
Step 3882: loss=3.3538, lr=0.000577, tokens/sec=1372644.56, grad_norm=0.3033, duration=0.38s
Step 3883: loss=3.2577, lr=0.000577, tokens/sec=1376202.68, grad_norm=0.3200, duration=0.38s
Step 3884: loss=3.2852, lr=0.000577, tokens/sec=1377748.66, grad_norm=0.3087, duration=0.38s
Step 3885: loss=3.2198, lr=0.000577, tokens/sec=1379089.64, grad_norm=0.2891, duration=0.38s
Step 3886: loss=3.2567, lr=0.000577, tokens/sec=1374275.30, grad_norm=0.2952, duration=0.38s
Step 3887: loss=3.2961, lr=0.000577, tokens/sec=1380529.44, grad_norm=0.2918, duration=0.38s
Step 3888: loss=3.1959, lr=0.000577, tokens/sec=1375520.04, grad_norm=0.2865, duration=0.38s
Step 3889: loss=3.2434, lr=0.000577, tokens/sec=1376303.46, grad_norm=0.2737, duration=0.38s
Step 3890: loss=3.2061, lr=0.000577, tokens/sec=1377260.26, grad_norm=0.2915, duration=0.38s
Step 3891: loss=3.2844, lr=0.000577, tokens/sec=1373597.14, grad_norm=0.2841, duration=0.38s
Step 3892: loss=3.2625, lr=0.000577, tokens/sec=1378260.72, grad_norm=0.3012, duration=0.38s
Step 3893: loss=3.2817, lr=0.000577, tokens/sec=1379607.90, grad_norm=0.2840, duration=0.38s
Step 3894: loss=3.3450, lr=0.000577, tokens/sec=1376729.11, grad_norm=0.3324, duration=0.38s
Step 3895: loss=3.3442, lr=0.000577, tokens/sec=1377599.34, grad_norm=0.3239, duration=0.38s
Step 3896: loss=3.3566, lr=0.000577, tokens/sec=1379919.56, grad_norm=0.3080, duration=0.38s
Step 3897: loss=3.3668, lr=0.000577, tokens/sec=1380227.03, grad_norm=0.3325, duration=0.38s
Step 3898: loss=3.3131, lr=0.000577, tokens/sec=1376193.21, grad_norm=0.3335, duration=0.38s
Step 3899: loss=3.3489, lr=0.000577, tokens/sec=1382444.87, grad_norm=0.3116, duration=0.38s
Step 3900/19073 (20.4%), Elapsed time: 1593.79s, Steps per hour: 8809.19, Estimated hours remaining: 1.72
Step 3900: loss=3.3463, lr=0.000577, tokens/sec=1378445.61, grad_norm=0.3501, duration=0.38s
Step 3901: loss=3.3463, lr=0.000577, tokens/sec=1376343.94, grad_norm=0.2586, duration=0.38s
Step 3902: loss=3.3464, lr=0.000577, tokens/sec=1377109.33, grad_norm=0.3129, duration=0.38s
Step 3903: loss=3.3366, lr=0.000577, tokens/sec=1378084.52, grad_norm=0.2969, duration=0.38s
Step 3904: loss=3.3489, lr=0.000577, tokens/sec=1380841.52, grad_norm=0.2828, duration=0.38s
Step 3905: loss=3.2933, lr=0.000577, tokens/sec=1378871.73, grad_norm=0.2803, duration=0.38s
Step 3906: loss=3.3218, lr=0.000577, tokens/sec=1376614.49, grad_norm=0.2867, duration=0.38s
Step 3907: loss=3.3229, lr=0.000577, tokens/sec=1376412.86, grad_norm=0.2798, duration=0.38s
Step 3908: loss=3.3123, lr=0.000577, tokens/sec=1375565.64, grad_norm=0.2764, duration=0.38s
Step 3909: loss=3.2490, lr=0.000577, tokens/sec=1378984.14, grad_norm=0.2841, duration=0.38s
Step 3910: loss=3.3111, lr=0.000577, tokens/sec=1378908.04, grad_norm=0.2822, duration=0.38s
Step 3911: loss=3.2948, lr=0.000577, tokens/sec=1378571.78, grad_norm=0.2673, duration=0.38s
Step 3912: loss=3.3767, lr=0.000577, tokens/sec=1376744.63, grad_norm=0.2812, duration=0.38s
Step 3913: loss=3.3046, lr=0.000577, tokens/sec=1374280.45, grad_norm=0.3132, duration=0.38s
Step 3914: loss=3.3375, lr=0.000577, tokens/sec=1373249.74, grad_norm=0.2776, duration=0.38s
Step 3915: loss=3.2962, lr=0.000577, tokens/sec=1378397.22, grad_norm=0.2801, duration=0.38s
Step 3916: loss=3.3082, lr=0.000577, tokens/sec=1373110.83, grad_norm=0.2807, duration=0.38s
Step 3917: loss=3.3145, lr=0.000577, tokens/sec=1376069.20, grad_norm=0.2704, duration=0.38s
Step 3918: loss=3.3448, lr=0.000577, tokens/sec=1380207.11, grad_norm=0.3027, duration=0.38s
Step 3919: loss=3.3225, lr=0.000577, tokens/sec=1380067.65, grad_norm=0.2813, duration=0.38s
Step 3920: loss=3.3562, lr=0.000577, tokens/sec=1377810.81, grad_norm=0.2791, duration=0.38s
Step 3921: loss=3.3377, lr=0.000577, tokens/sec=1374955.84, grad_norm=0.2877, duration=0.38s
Step 3922: loss=3.3163, lr=0.000577, tokens/sec=1378215.81, grad_norm=0.2660, duration=0.38s
Step 3923: loss=3.3291, lr=0.000577, tokens/sec=1378007.67, grad_norm=0.2851, duration=0.38s
Step 3924: loss=3.3155, lr=0.000577, tokens/sec=1376497.30, grad_norm=0.2851, duration=0.38s
Step 3925: loss=3.3130, lr=0.000577, tokens/sec=1377995.58, grad_norm=0.2723, duration=0.38s
Step 3926: loss=3.3236, lr=0.000577, tokens/sec=1377815.99, grad_norm=0.2770, duration=0.38s
Step 3927: loss=3.2959, lr=0.000577, tokens/sec=1377355.15, grad_norm=0.2663, duration=0.38s
Step 3928: loss=3.2828, lr=0.000577, tokens/sec=1376644.65, grad_norm=0.2704, duration=0.38s
Step 3929: loss=3.2166, lr=0.000577, tokens/sec=1372542.61, grad_norm=0.2703, duration=0.38s
Step 3930: loss=3.2514, lr=0.000577, tokens/sec=1369479.10, grad_norm=0.2744, duration=0.38s
Step 3931: loss=3.1914, lr=0.000577, tokens/sec=1376828.24, grad_norm=0.2810, duration=0.38s
Step 3932: loss=3.2313, lr=0.000576, tokens/sec=1375818.67, grad_norm=0.2584, duration=0.38s
Step 3933: loss=3.2462, lr=0.000576, tokens/sec=1374484.03, grad_norm=0.2705, duration=0.38s
Step 3934: loss=3.2848, lr=0.000576, tokens/sec=1377830.67, grad_norm=0.2871, duration=0.38s
Step 3935: loss=3.2485, lr=0.000576, tokens/sec=1373584.27, grad_norm=0.2614, duration=0.38s
Step 3936: loss=3.2378, lr=0.000576, tokens/sec=1376579.16, grad_norm=0.2828, duration=0.38s
Step 3937: loss=3.2200, lr=0.000576, tokens/sec=1376064.89, grad_norm=0.3053, duration=0.38s
Step 3938: loss=3.2642, lr=0.000576, tokens/sec=1372815.09, grad_norm=0.2832, duration=0.38s
Step 3939: loss=3.3012, lr=0.000576, tokens/sec=1376904.11, grad_norm=0.2569, duration=0.38s
Step 3940: loss=3.3594, lr=0.000576, tokens/sec=1378767.98, grad_norm=0.3014, duration=0.38s
Step 3941: loss=3.3510, lr=0.000576, tokens/sec=1376598.98, grad_norm=0.2915, duration=0.38s
Step 3942: loss=3.3578, lr=0.000576, tokens/sec=1378152.75, grad_norm=0.3002, duration=0.38s
Step 3943: loss=3.3311, lr=0.000576, tokens/sec=1374605.18, grad_norm=0.3205, duration=0.38s
Step 3944: loss=3.3598, lr=0.000576, tokens/sec=1377157.62, grad_norm=0.2917, duration=0.38s
Step 3945: loss=3.2955, lr=0.000576, tokens/sec=1376975.67, grad_norm=0.3567, duration=0.38s
Step 3946: loss=3.3204, lr=0.000576, tokens/sec=1376943.77, grad_norm=0.2911, duration=0.38s
Step 3947: loss=3.3785, lr=0.000576, tokens/sec=1378627.09, grad_norm=0.3403, duration=0.38s
Step 3948: loss=3.4404, lr=0.000576, tokens/sec=1375721.41, grad_norm=0.3137, duration=0.38s
Step 3949: loss=3.3635, lr=0.000576, tokens/sec=1379781.89, grad_norm=0.3138, duration=0.38s
Step 3950: loss=3.3414, lr=0.000576, tokens/sec=1374795.10, grad_norm=0.2799, duration=0.38s
Step 3951: loss=3.3107, lr=0.000576, tokens/sec=1377122.26, grad_norm=0.2601, duration=0.38s
Step 3952: loss=3.3468, lr=0.000576, tokens/sec=1374823.46, grad_norm=0.2743, duration=0.38s
Step 3953: loss=3.2884, lr=0.000576, tokens/sec=1375974.49, grad_norm=0.2606, duration=0.38s
Step 3954: loss=3.2501, lr=0.000576, tokens/sec=1376693.78, grad_norm=0.2830, duration=0.38s
Step 3955: loss=3.3474, lr=0.000576, tokens/sec=1372284.79, grad_norm=0.2611, duration=0.38s
Step 3956: loss=3.2544, lr=0.000576, tokens/sec=1371492.26, grad_norm=0.2627, duration=0.38s
Step 3957: loss=3.3607, lr=0.000576, tokens/sec=1376240.58, grad_norm=0.2877, duration=0.38s
Step 3958: loss=3.3683, lr=0.000576, tokens/sec=1376504.19, grad_norm=0.2635, duration=0.38s
Step 3959: loss=3.2965, lr=0.000576, tokens/sec=1379311.09, grad_norm=0.2892, duration=0.38s
Step 3960: loss=3.2500, lr=0.000576, tokens/sec=1371800.26, grad_norm=0.2649, duration=0.38s
Step 3961: loss=3.3139, lr=0.000576, tokens/sec=1374868.16, grad_norm=0.2865, duration=0.38s
Step 3962: loss=3.3305, lr=0.000576, tokens/sec=1377013.61, grad_norm=0.2761, duration=0.38s
Step 3963: loss=3.2893, lr=0.000576, tokens/sec=1378397.22, grad_norm=0.2726, duration=0.38s
Step 3964: loss=3.1897, lr=0.000576, tokens/sec=1372900.80, grad_norm=0.2621, duration=0.38s
Step 3965: loss=3.2840, lr=0.000576, tokens/sec=1368985.46, grad_norm=0.2835, duration=0.38s
Step 3966: loss=3.3189, lr=0.000576, tokens/sec=1371010.85, grad_norm=0.2845, duration=0.38s
Step 3967: loss=3.3448, lr=0.000576, tokens/sec=1369723.91, grad_norm=0.2672, duration=0.38s
Step 3968: loss=3.3714, lr=0.000576, tokens/sec=1375569.95, grad_norm=0.2952, duration=0.38s
Step 3969: loss=3.2641, lr=0.000576, tokens/sec=1377764.20, grad_norm=0.2789, duration=0.38s
Step 3970: loss=3.2968, lr=0.000576, tokens/sec=1376164.79, grad_norm=0.2979, duration=0.38s
Step 3971: loss=3.2835, lr=0.000576, tokens/sec=1375717.10, grad_norm=0.3044, duration=0.38s
Step 3972: loss=3.2519, lr=0.000576, tokens/sec=1379027.38, grad_norm=0.2776, duration=0.38s
Step 3973: loss=3.2555, lr=0.000576, tokens/sec=1378475.85, grad_norm=0.2838, duration=0.38s
Step 3974: loss=3.2512, lr=0.000576, tokens/sec=1373633.18, grad_norm=0.2530, duration=0.38s
Step 3975: loss=3.2326, lr=0.000576, tokens/sec=1376723.94, grad_norm=0.2770, duration=0.38s
Step 3976: loss=3.2611, lr=0.000576, tokens/sec=1375933.16, grad_norm=0.2882, duration=0.38s
Step 3977: loss=3.3361, lr=0.000576, tokens/sec=1376657.58, grad_norm=0.2638, duration=0.38s
Step 3978: loss=3.2561, lr=0.000576, tokens/sec=1378700.56, grad_norm=0.2935, duration=0.38s
Step 3979: loss=3.2319, lr=0.000576, tokens/sec=1379105.21, grad_norm=0.2627, duration=0.38s
Step 3980: loss=3.2251, lr=0.000576, tokens/sec=1376837.72, grad_norm=0.2800, duration=0.38s
Step 3981: loss=3.2040, lr=0.000576, tokens/sec=1375877.20, grad_norm=0.2882, duration=0.38s
Step 3982: loss=3.2818, lr=0.000576, tokens/sec=1377437.11, grad_norm=0.2632, duration=0.38s
Step 3983: loss=3.2576, lr=0.000576, tokens/sec=1379992.30, grad_norm=0.3002, duration=0.38s
Step 3984: loss=3.3020, lr=0.000576, tokens/sec=1381039.24, grad_norm=0.2703, duration=0.38s
Step 3985: loss=3.3145, lr=0.000576, tokens/sec=1376110.53, grad_norm=0.3004, duration=0.38s
Step 3986: loss=3.3311, lr=0.000576, tokens/sec=1374771.89, grad_norm=0.2868, duration=0.38s
Step 3987: loss=3.3362, lr=0.000576, tokens/sec=1379530.01, grad_norm=0.3134, duration=0.38s
Step 3988: loss=3.3092, lr=0.000576, tokens/sec=1376213.88, grad_norm=0.3090, duration=0.38s
Step 3989: loss=3.3479, lr=0.000576, tokens/sec=1371034.78, grad_norm=0.3143, duration=0.38s
Step 3990: loss=3.3137, lr=0.000576, tokens/sec=1371269.04, grad_norm=0.3114, duration=0.38s
Step 3991: loss=3.3603, lr=0.000576, tokens/sec=1377329.27, grad_norm=0.2766, duration=0.38s
Step 3992: loss=3.3443, lr=0.000576, tokens/sec=1376895.48, grad_norm=0.2918, duration=0.38s
Step 3993: loss=3.4087, lr=0.000576, tokens/sec=1368037.57, grad_norm=0.2826, duration=0.38s
Step 3994: loss=3.3287, lr=0.000576, tokens/sec=1376225.94, grad_norm=0.2709, duration=0.38s
Step 3995: loss=3.3660, lr=0.000576, tokens/sec=1372676.26, grad_norm=0.2877, duration=0.38s
Step 3996: loss=3.4473, lr=0.000576, tokens/sec=1377487.16, grad_norm=0.2858, duration=0.38s
Step 3997: loss=3.3358, lr=0.000575, tokens/sec=1372465.51, grad_norm=0.2629, duration=0.38s
Step 3998: loss=3.3544, lr=0.000575, tokens/sec=1376635.17, grad_norm=0.2848, duration=0.38s
Step 3999: loss=3.3310, lr=0.000575, tokens/sec=1372893.08, grad_norm=0.2715, duration=0.38s
Step 4000/19073 (21.0%), Elapsed time: 1631.96s, Steps per hour: 8823.73, Estimated hours remaining: 1.71
Validation loss at step 4000: 3.6951022148132324
Step 4000: loss=3.3643, lr=0.000575, tokens/sec=152337.24, grad_norm=0.2775, duration=3.44s
Step 4001: loss=3.3412, lr=0.000575, tokens/sec=1375804.89, grad_norm=0.2432, duration=0.38s
Step 4002: loss=3.3272, lr=0.000575, tokens/sec=1372076.73, grad_norm=0.2557, duration=0.38s
Step 4003: loss=3.2730, lr=0.000575, tokens/sec=1375566.50, grad_norm=0.2596, duration=0.38s
Step 4004: loss=3.3016, lr=0.000575, tokens/sec=1377712.40, grad_norm=0.2521, duration=0.38s
Step 4005: loss=3.3223, lr=0.000575, tokens/sec=1374823.46, grad_norm=0.2797, duration=0.38s
Step 4006: loss=3.2523, lr=0.000575, tokens/sec=1378265.91, grad_norm=0.3034, duration=0.38s
Step 4007: loss=3.3534, lr=0.000575, tokens/sec=1374230.64, grad_norm=0.2933, duration=0.38s
Step 4008: loss=3.2687, lr=0.000575, tokens/sec=1377004.12, grad_norm=0.2860, duration=0.38s
Step 4009: loss=3.1956, lr=0.000575, tokens/sec=1381139.86, grad_norm=0.3361, duration=0.38s
Step 4010: loss=3.2987, lr=0.000575, tokens/sec=1376163.06, grad_norm=0.2731, duration=0.38s
Step 4011: loss=3.3013, lr=0.000575, tokens/sec=1377034.30, grad_norm=0.2569, duration=0.38s
Step 4012: loss=3.3269, lr=0.000575, tokens/sec=1377362.92, grad_norm=0.3542, duration=0.38s
Step 4013: loss=3.2218, lr=0.000575, tokens/sec=1377847.07, grad_norm=0.3028, duration=0.38s
Step 4014: loss=3.3284, lr=0.000575, tokens/sec=1380746.14, grad_norm=0.3142, duration=0.38s
Step 4015: loss=3.3432, lr=0.000575, tokens/sec=1377387.07, grad_norm=0.2838, duration=0.38s
Step 4016: loss=3.2860, lr=0.000575, tokens/sec=1375485.62, grad_norm=0.3008, duration=0.38s
Step 4017: loss=3.3129, lr=0.000575, tokens/sec=1371338.31, grad_norm=0.2936, duration=0.38s
Step 4018: loss=3.3137, lr=0.000575, tokens/sec=1376940.32, grad_norm=0.2857, duration=0.38s
Step 4019: loss=3.3002, lr=0.000575, tokens/sec=1374617.21, grad_norm=0.2883, duration=0.38s
Step 4020: loss=3.3237, lr=0.000575, tokens/sec=1375463.26, grad_norm=0.2841, duration=0.38s
Step 4021: loss=3.3298, lr=0.000575, tokens/sec=1379454.72, grad_norm=0.2699, duration=0.38s
Step 4022: loss=3.3307, lr=0.000575, tokens/sec=1376557.61, grad_norm=0.3194, duration=0.38s
Step 4023: loss=3.3729, lr=0.000575, tokens/sec=1373002.80, grad_norm=0.2617, duration=0.38s
Step 4024: loss=3.2822, lr=0.000575, tokens/sec=1376435.26, grad_norm=0.2706, duration=0.38s
Step 4025: loss=3.2326, lr=0.000575, tokens/sec=1375971.04, grad_norm=0.2575, duration=0.38s
Step 4026: loss=3.2953, lr=0.000575, tokens/sec=1377590.71, grad_norm=0.2715, duration=0.38s
Step 4027: loss=3.2129, lr=0.000575, tokens/sec=1379096.56, grad_norm=0.2496, duration=0.38s
Step 4028: loss=3.2173, lr=0.000575, tokens/sec=1381257.84, grad_norm=0.2661, duration=0.38s
Step 4029: loss=3.2362, lr=0.000575, tokens/sec=1376637.76, grad_norm=0.2528, duration=0.38s
Step 4030: loss=3.2479, lr=0.000575, tokens/sec=1372647.99, grad_norm=0.2539, duration=0.38s
Step 4031: loss=3.2802, lr=0.000575, tokens/sec=1375795.43, grad_norm=0.2712, duration=0.38s
Step 4032: loss=3.2434, lr=0.000575, tokens/sec=1378729.95, grad_norm=0.2425, duration=0.38s
Step 4033: loss=3.2758, lr=0.000575, tokens/sec=1377751.25, grad_norm=0.2843, duration=0.38s
Step 4034: loss=3.2797, lr=0.000575, tokens/sec=1373755.03, grad_norm=0.2644, duration=0.38s
Step 4035: loss=3.2331, lr=0.000575, tokens/sec=1376870.48, grad_norm=0.2874, duration=0.38s
Step 4036: loss=3.2697, lr=0.000575, tokens/sec=1372472.36, grad_norm=0.2541, duration=0.38s
Step 4037: loss=3.3485, lr=0.000575, tokens/sec=1376201.82, grad_norm=0.2630, duration=0.38s
Step 4038: loss=3.3703, lr=0.000575, tokens/sec=1376598.11, grad_norm=0.2577, duration=0.38s
Step 4039: loss=3.3336, lr=0.000575, tokens/sec=1375781.65, grad_norm=0.2788, duration=0.38s
Step 4040: loss=3.3297, lr=0.000575, tokens/sec=1370845.04, grad_norm=0.2813, duration=0.38s
Step 4041: loss=3.3610, lr=0.000575, tokens/sec=1378085.39, grad_norm=0.3000, duration=0.38s
Step 4042: loss=3.3614, lr=0.000575, tokens/sec=1374741.81, grad_norm=0.2888, duration=0.38s
Step 4043: loss=3.3380, lr=0.000575, tokens/sec=1376605.87, grad_norm=0.2734, duration=0.38s
Step 4044: loss=3.2942, lr=0.000575, tokens/sec=1379647.72, grad_norm=0.2757, duration=0.38s
Step 4045: loss=3.3033, lr=0.000575, tokens/sec=1374742.67, grad_norm=0.2725, duration=0.38s
Step 4046: loss=3.3499, lr=0.000575, tokens/sec=1378430.06, grad_norm=0.2782, duration=0.38s
Step 4047: loss=3.2972, lr=0.000575, tokens/sec=1374413.59, grad_norm=0.2743, duration=0.38s
Step 4048: loss=3.2856, lr=0.000575, tokens/sec=1377913.55, grad_norm=0.2655, duration=0.38s
Step 4049: loss=3.2841, lr=0.000575, tokens/sec=1375928.86, grad_norm=0.2586, duration=0.38s
Step 4050: loss=3.3060, lr=0.000575, tokens/sec=1375309.27, grad_norm=0.3108, duration=0.38s
Step 4051: loss=3.2301, lr=0.000575, tokens/sec=1374409.29, grad_norm=0.2701, duration=0.38s
Step 4052: loss=3.2417, lr=0.000575, tokens/sec=1377144.68, grad_norm=0.2895, duration=0.38s
Step 4053: loss=3.3017, lr=0.000575, tokens/sec=1371950.89, grad_norm=0.2904, duration=0.38s
Step 4054: loss=3.2852, lr=0.000575, tokens/sec=1377515.63, grad_norm=0.2749, duration=0.38s
Step 4055: loss=3.3338, lr=0.000575, tokens/sec=1379359.54, grad_norm=0.2940, duration=0.38s
Step 4056: loss=3.2694, lr=0.000575, tokens/sec=1375852.24, grad_norm=0.2631, duration=0.38s
Step 4057: loss=3.3576, lr=0.000575, tokens/sec=1371398.17, grad_norm=0.2617, duration=0.38s
Step 4058: loss=3.2552, lr=0.000575, tokens/sec=1372260.82, grad_norm=0.2742, duration=0.38s
Step 4059: loss=3.2837, lr=0.000575, tokens/sec=1371678.75, grad_norm=0.2640, duration=0.38s
Step 4060: loss=3.2867, lr=0.000574, tokens/sec=1378017.16, grad_norm=0.2942, duration=0.38s
Step 4061: loss=3.3318, lr=0.000574, tokens/sec=1375194.88, grad_norm=0.3145, duration=0.38s
Step 4062: loss=3.3080, lr=0.000574, tokens/sec=1376742.04, grad_norm=0.2763, duration=0.38s
Step 4063: loss=3.3255, lr=0.000574, tokens/sec=1379057.64, grad_norm=0.2841, duration=0.38s
Step 4064: loss=3.3053, lr=0.000574, tokens/sec=1370898.03, grad_norm=0.2559, duration=0.38s
Step 4065: loss=3.3146, lr=0.000574, tokens/sec=1379447.80, grad_norm=0.2492, duration=0.38s
Step 4066: loss=3.2972, lr=0.000574, tokens/sec=1372300.21, grad_norm=0.2748, duration=0.38s
Step 4067: loss=3.2343, lr=0.000574, tokens/sec=1375912.50, grad_norm=0.2722, duration=0.38s
Step 4068: loss=3.3220, lr=0.000574, tokens/sec=1374762.44, grad_norm=0.3020, duration=0.38s
Step 4069: loss=3.3111, lr=0.000574, tokens/sec=1377339.62, grad_norm=0.2766, duration=0.38s
Step 4070: loss=3.3102, lr=0.000574, tokens/sec=1376928.25, grad_norm=0.2693, duration=0.38s
Step 4071: loss=3.2691, lr=0.000574, tokens/sec=1375329.92, grad_norm=0.3051, duration=0.38s
Step 4072: loss=3.2940, lr=0.000574, tokens/sec=1373651.20, grad_norm=0.2981, duration=0.38s
Step 4073: loss=3.2522, lr=0.000574, tokens/sec=1375402.17, grad_norm=0.2676, duration=0.38s
Step 4074: loss=3.2525, lr=0.000574, tokens/sec=1374188.56, grad_norm=0.2867, duration=0.38s
Step 4075: loss=3.1978, lr=0.000574, tokens/sec=1375050.42, grad_norm=0.3025, duration=0.38s
Step 4076: loss=3.2494, lr=0.000574, tokens/sec=1374654.16, grad_norm=0.2664, duration=0.38s
Step 4077: loss=3.2231, lr=0.000574, tokens/sec=1374338.86, grad_norm=0.2904, duration=0.38s
Step 4078: loss=3.1980, lr=0.000574, tokens/sec=1377118.81, grad_norm=0.2787, duration=0.38s
Step 4079: loss=3.2058, lr=0.000574, tokens/sec=1375726.57, grad_norm=0.3101, duration=0.38s
Step 4080: loss=3.2119, lr=0.000574, tokens/sec=1373882.92, grad_norm=0.3094, duration=0.38s
Step 4081: loss=3.2592, lr=0.000574, tokens/sec=1371850.75, grad_norm=0.2791, duration=0.38s
Step 4082: loss=3.2224, lr=0.000574, tokens/sec=1378220.99, grad_norm=0.2701, duration=0.38s
Step 4083: loss=3.2785, lr=0.000574, tokens/sec=1374542.45, grad_norm=0.3053, duration=0.38s
Step 4084: loss=3.3185, lr=0.000574, tokens/sec=1373932.70, grad_norm=0.3142, duration=0.38s
Step 4085: loss=3.3285, lr=0.000574, tokens/sec=1374922.32, grad_norm=0.2857, duration=0.38s
Step 4086: loss=3.3494, lr=0.000574, tokens/sec=1371823.37, grad_norm=0.3300, duration=0.38s
Step 4087: loss=3.3478, lr=0.000574, tokens/sec=1376837.72, grad_norm=0.3537, duration=0.38s
Step 4088: loss=3.2805, lr=0.000574, tokens/sec=1376071.78, grad_norm=0.3208, duration=0.38s
Step 4089: loss=3.3527, lr=0.000574, tokens/sec=1374083.80, grad_norm=0.3065, duration=0.38s
Step 4090: loss=3.2787, lr=0.000574, tokens/sec=1368472.60, grad_norm=0.3398, duration=0.38s
Step 4091: loss=3.3356, lr=0.000574, tokens/sec=1369642.87, grad_norm=0.2823, duration=0.38s
Step 4092: loss=3.3321, lr=0.000574, tokens/sec=1374837.22, grad_norm=0.3394, duration=0.38s
Step 4093: loss=3.3175, lr=0.000574, tokens/sec=1374939.51, grad_norm=0.3057, duration=0.38s
Step 4094: loss=3.3095, lr=0.000574, tokens/sec=1369394.67, grad_norm=0.2882, duration=0.38s
Step 4095: loss=3.2894, lr=0.000574, tokens/sec=1374870.74, grad_norm=0.3039, duration=0.38s
Step 4096: loss=3.3135, lr=0.000574, tokens/sec=1374930.05, grad_norm=0.2881, duration=0.38s
Step 4097: loss=3.2694, lr=0.000574, tokens/sec=1370287.24, grad_norm=0.2835, duration=0.38s
Step 4098: loss=3.2872, lr=0.000574, tokens/sec=1375504.55, grad_norm=0.2812, duration=0.38s
Step 4099: loss=3.2438, lr=0.000574, tokens/sec=1377069.66, grad_norm=0.2701, duration=0.38s
Step 4100/19073 (21.5%), Elapsed time: 1673.22s, Steps per hour: 8821.34, Estimated hours remaining: 1.70
Step 4100: loss=3.2855, lr=0.000574, tokens/sec=1375038.38, grad_norm=0.2899, duration=0.38s
Step 4101: loss=3.3484, lr=0.000574, tokens/sec=1372903.37, grad_norm=0.2786, duration=0.38s
Step 4102: loss=3.3334, lr=0.000574, tokens/sec=1371557.27, grad_norm=0.2857, duration=0.38s
Step 4103: loss=3.2422, lr=0.000574, tokens/sec=1373799.66, grad_norm=0.2795, duration=0.38s
Step 4104: loss=3.3220, lr=0.000574, tokens/sec=1377339.62, grad_norm=0.2595, duration=0.38s
Step 4105: loss=3.2432, lr=0.000574, tokens/sec=1374191.99, grad_norm=0.2568, duration=0.38s
Step 4106: loss=3.3269, lr=0.000574, tokens/sec=1374997.97, grad_norm=0.2629, duration=0.38s
Step 4107: loss=3.3018, lr=0.000574, tokens/sec=1371065.56, grad_norm=0.2577, duration=0.38s
Step 4108: loss=3.3212, lr=0.000574, tokens/sec=1374110.42, grad_norm=0.2658, duration=0.38s
Step 4109: loss=3.3296, lr=0.000574, tokens/sec=1377038.61, grad_norm=0.2876, duration=0.38s
Step 4110: loss=3.3190, lr=0.000574, tokens/sec=1372006.53, grad_norm=0.2826, duration=0.38s
Step 4111: loss=3.2785, lr=0.000574, tokens/sec=1373052.52, grad_norm=0.2562, duration=0.38s
Step 4112: loss=3.3343, lr=0.000574, tokens/sec=1371801.12, grad_norm=0.2651, duration=0.38s
Step 4113: loss=3.2769, lr=0.000574, tokens/sec=1372998.52, grad_norm=0.2832, duration=0.38s
Step 4114: loss=3.2873, lr=0.000574, tokens/sec=1376911.87, grad_norm=0.2602, duration=0.38s
Step 4115: loss=3.3172, lr=0.000574, tokens/sec=1373067.10, grad_norm=0.2615, duration=0.38s
Step 4116: loss=3.2879, lr=0.000574, tokens/sec=1374746.11, grad_norm=0.2860, duration=0.38s
Step 4117: loss=3.2689, lr=0.000574, tokens/sec=1373921.54, grad_norm=0.2423, duration=0.38s
Step 4118: loss=3.2489, lr=0.000574, tokens/sec=1378361.80, grad_norm=0.2646, duration=0.38s
Step 4119: loss=3.1896, lr=0.000574, tokens/sec=1376746.35, grad_norm=0.2551, duration=0.38s
Step 4120: loss=3.2479, lr=0.000574, tokens/sec=1376938.59, grad_norm=0.2773, duration=0.38s
Step 4121: loss=3.1696, lr=0.000574, tokens/sec=1372216.29, grad_norm=0.2643, duration=0.38s
Step 4122: loss=3.2332, lr=0.000573, tokens/sec=1375315.29, grad_norm=0.2414, duration=0.38s
Step 4123: loss=3.1976, lr=0.000573, tokens/sec=1375766.16, grad_norm=0.2615, duration=0.38s
Step 4124: loss=3.2577, lr=0.000573, tokens/sec=1375617.27, grad_norm=0.2769, duration=0.38s
Step 4125: loss=3.2414, lr=0.000573, tokens/sec=1376152.73, grad_norm=0.2780, duration=0.38s
Step 4126: loss=3.2115, lr=0.000573, tokens/sec=1375163.92, grad_norm=0.2785, duration=0.38s
Step 4127: loss=3.2138, lr=0.000573, tokens/sec=1376456.80, grad_norm=0.2852, duration=0.38s
Step 4128: loss=3.2394, lr=0.000573, tokens/sec=1373067.96, grad_norm=0.2647, duration=0.38s
Step 4129: loss=3.3044, lr=0.000573, tokens/sec=1375010.01, grad_norm=0.3047, duration=0.38s
Step 4130: loss=3.3194, lr=0.000573, tokens/sec=1377181.77, grad_norm=0.3008, duration=0.38s
Step 4131: loss=3.3235, lr=0.000573, tokens/sec=1372672.84, grad_norm=0.2891, duration=0.38s
Step 4132: loss=3.3392, lr=0.000573, tokens/sec=1374865.58, grad_norm=0.3517, duration=0.38s
Step 4133: loss=3.3153, lr=0.000573, tokens/sec=1376736.01, grad_norm=0.3348, duration=0.38s
Step 4134: loss=3.3260, lr=0.000573, tokens/sec=1377410.37, grad_norm=0.3218, duration=0.38s
Step 4135: loss=3.2743, lr=0.000573, tokens/sec=1377402.60, grad_norm=0.3061, duration=0.38s
Step 4136: loss=3.3100, lr=0.000573, tokens/sec=1378437.83, grad_norm=0.3067, duration=0.38s
Step 4137: loss=3.3548, lr=0.000573, tokens/sec=1375748.09, grad_norm=0.2903, duration=0.38s
Step 4138: loss=3.4093, lr=0.000573, tokens/sec=1373575.69, grad_norm=0.3032, duration=0.38s
Step 4139: loss=3.3330, lr=0.000573, tokens/sec=1374878.48, grad_norm=0.2743, duration=0.38s
Step 4140: loss=3.3531, lr=0.000573, tokens/sec=1373658.06, grad_norm=0.2935, duration=0.38s
Step 4141: loss=3.2910, lr=0.000573, tokens/sec=1377202.47, grad_norm=0.2753, duration=0.38s
Step 4142: loss=3.2961, lr=0.000573, tokens/sec=1376747.21, grad_norm=0.2732, duration=0.38s
Step 4143: loss=3.2610, lr=0.000573, tokens/sec=1375614.69, grad_norm=0.2606, duration=0.38s
Step 4144: loss=3.2391, lr=0.000573, tokens/sec=1375640.51, grad_norm=0.2619, duration=0.38s
Step 4145: loss=3.3172, lr=0.000573, tokens/sec=1374092.39, grad_norm=0.2815, duration=0.38s
Step 4146: loss=3.2526, lr=0.000573, tokens/sec=1372367.01, grad_norm=0.2803, duration=0.38s
Step 4147: loss=3.3189, lr=0.000573, tokens/sec=1372815.94, grad_norm=0.2698, duration=0.38s
Step 4148: loss=3.3326, lr=0.000573, tokens/sec=1372424.39, grad_norm=0.2777, duration=0.38s
Step 4149: loss=3.2921, lr=0.000573, tokens/sec=1373465.88, grad_norm=0.2889, duration=0.38s
Step 4150: loss=3.2253, lr=0.000573, tokens/sec=1374226.35, grad_norm=0.2663, duration=0.38s
Step 4151: loss=3.3003, lr=0.000573, tokens/sec=1375662.02, grad_norm=0.2862, duration=0.38s
Step 4152: loss=3.3099, lr=0.000573, tokens/sec=1369768.28, grad_norm=0.2659, duration=0.38s
Step 4153: loss=3.2449, lr=0.000573, tokens/sec=1373687.24, grad_norm=0.2709, duration=0.38s
Step 4154: loss=3.1930, lr=0.000573, tokens/sec=1375363.46, grad_norm=0.2518, duration=0.38s
Step 4155: loss=3.3046, lr=0.000573, tokens/sec=1373083.39, grad_norm=0.2724, duration=0.38s
Step 4156: loss=3.3032, lr=0.000573, tokens/sec=1374090.67, grad_norm=0.2782, duration=0.38s
Step 4157: loss=3.3158, lr=0.000573, tokens/sec=1373726.71, grad_norm=0.2874, duration=0.38s
Step 4158: loss=3.3270, lr=0.000573, tokens/sec=1372531.47, grad_norm=0.2897, duration=0.38s
Step 4159: loss=3.2624, lr=0.000573, tokens/sec=1371304.10, grad_norm=0.2751, duration=0.38s
Step 4160: loss=3.2588, lr=0.000573, tokens/sec=1370914.27, grad_norm=0.2710, duration=0.38s
Step 4161: loss=3.2830, lr=0.000573, tokens/sec=1371055.30, grad_norm=0.2663, duration=0.38s
Step 4162: loss=3.1852, lr=0.000573, tokens/sec=1372569.16, grad_norm=0.2619, duration=0.38s
Step 4163: loss=3.2717, lr=0.000573, tokens/sec=1373979.06, grad_norm=0.2773, duration=0.38s
Step 4164: loss=3.2215, lr=0.000573, tokens/sec=1378103.52, grad_norm=0.2690, duration=0.38s
Step 4165: loss=3.2347, lr=0.000573, tokens/sec=1373949.87, grad_norm=0.2627, duration=0.38s
Step 4166: loss=3.2477, lr=0.000573, tokens/sec=1373350.08, grad_norm=0.2962, duration=0.38s
Step 4167: loss=3.2912, lr=0.000573, tokens/sec=1373881.20, grad_norm=0.2564, duration=0.38s
Step 4168: loss=3.2316, lr=0.000573, tokens/sec=1370783.52, grad_norm=0.2687, duration=0.38s
Step 4169: loss=3.2391, lr=0.000573, tokens/sec=1376875.66, grad_norm=0.2591, duration=0.38s
Step 4170: loss=3.1434, lr=0.000573, tokens/sec=1375919.39, grad_norm=0.2511, duration=0.38s
Step 4171: loss=3.2334, lr=0.000573, tokens/sec=1377483.71, grad_norm=0.2877, duration=0.38s
Step 4172: loss=3.2507, lr=0.000573, tokens/sec=1375379.81, grad_norm=0.2682, duration=0.38s
Step 4173: loss=3.2639, lr=0.000573, tokens/sec=1372277.09, grad_norm=0.2759, duration=0.38s
Step 4174: loss=3.2425, lr=0.000573, tokens/sec=1379390.69, grad_norm=0.2415, duration=0.38s
Step 4175: loss=3.3027, lr=0.000573, tokens/sec=1376700.67, grad_norm=0.2814, duration=0.38s
Step 4176: loss=3.2989, lr=0.000573, tokens/sec=1377034.30, grad_norm=0.3112, duration=0.38s
Step 4177: loss=3.3278, lr=0.000573, tokens/sec=1376399.08, grad_norm=0.3417, duration=0.38s
Step 4178: loss=3.2980, lr=0.000573, tokens/sec=1373183.71, grad_norm=0.3536, duration=0.38s
Step 4179: loss=3.3106, lr=0.000573, tokens/sec=1373981.63, grad_norm=0.3362, duration=0.38s
Step 4180: loss=3.3063, lr=0.000573, tokens/sec=1377539.80, grad_norm=0.3257, duration=0.38s
Step 4181: loss=3.3165, lr=0.000573, tokens/sec=1373985.07, grad_norm=0.3149, duration=0.38s
Step 4182: loss=3.3284, lr=0.000572, tokens/sec=1376736.01, grad_norm=0.3353, duration=0.38s
Step 4183: loss=3.3749, lr=0.000572, tokens/sec=1375047.84, grad_norm=0.2809, duration=0.38s
Step 4184: loss=3.3196, lr=0.000572, tokens/sec=1378997.11, grad_norm=0.3029, duration=0.38s
Step 4185: loss=3.3757, lr=0.000572, tokens/sec=1377716.72, grad_norm=0.3029, duration=0.38s
Step 4186: loss=3.3878, lr=0.000572, tokens/sec=1376422.34, grad_norm=0.2931, duration=0.38s
Step 4187: loss=3.3124, lr=0.000572, tokens/sec=1376531.76, grad_norm=0.2979, duration=0.38s
Step 4188: loss=3.3458, lr=0.000572, tokens/sec=1377320.65, grad_norm=0.2909, duration=0.38s
Step 4189: loss=3.3277, lr=0.000572, tokens/sec=1375159.62, grad_norm=0.2661, duration=0.38s
Step 4190: loss=3.3657, lr=0.000572, tokens/sec=1381100.82, grad_norm=0.2862, duration=0.38s
Step 4191: loss=3.2894, lr=0.000572, tokens/sec=1378303.05, grad_norm=0.2721, duration=0.38s
Step 4192: loss=3.3204, lr=0.000572, tokens/sec=1378018.03, grad_norm=0.3098, duration=0.38s
Step 4193: loss=3.2259, lr=0.000572, tokens/sec=1377167.11, grad_norm=0.2886, duration=0.38s
Step 4194: loss=3.3067, lr=0.000572, tokens/sec=1374163.66, grad_norm=0.2638, duration=0.38s
Step 4195: loss=3.2713, lr=0.000572, tokens/sec=1377679.61, grad_norm=0.2973, duration=0.38s
Step 4196: loss=3.2707, lr=0.000572, tokens/sec=1373046.52, grad_norm=0.2801, duration=0.38s
Step 4197: loss=3.3204, lr=0.000572, tokens/sec=1372918.80, grad_norm=0.2861, duration=0.38s
Step 4198: loss=3.2312, lr=0.000572, tokens/sec=1376028.73, grad_norm=0.3212, duration=0.38s
Step 4199: loss=3.2209, lr=0.000572, tokens/sec=1376065.75, grad_norm=0.4347, duration=0.38s
Step 4200/19073 (22.0%), Elapsed time: 1711.42s, Steps per hour: 8834.78, Estimated hours remaining: 1.68
Step 4200: loss=3.2651, lr=0.000572, tokens/sec=1375505.41, grad_norm=0.3146, duration=0.38s
Step 4201: loss=3.2399, lr=0.000572, tokens/sec=1375762.72, grad_norm=0.3934, duration=0.38s
Step 4202: loss=3.3606, lr=0.000572, tokens/sec=1373396.40, grad_norm=0.3298, duration=0.38s
Step 4203: loss=3.2134, lr=0.000572, tokens/sec=1377314.61, grad_norm=0.3325, duration=0.38s
Step 4204: loss=3.3485, lr=0.000572, tokens/sec=1373903.52, grad_norm=0.2781, duration=0.38s
Step 4205: loss=3.3176, lr=0.000572, tokens/sec=1377180.91, grad_norm=0.3233, duration=0.38s
Step 4206: loss=3.2716, lr=0.000572, tokens/sec=1375979.65, grad_norm=0.2897, duration=0.38s
Step 4207: loss=3.2746, lr=0.000572, tokens/sec=1375104.59, grad_norm=0.3077, duration=0.38s
Step 4208: loss=3.3066, lr=0.000572, tokens/sec=1375473.58, grad_norm=0.2834, duration=0.38s
Step 4209: loss=3.3183, lr=0.000572, tokens/sec=1376538.66, grad_norm=0.3096, duration=0.38s
Step 4210: loss=3.2871, lr=0.000572, tokens/sec=1379000.57, grad_norm=0.2786, duration=0.38s
Step 4211: loss=3.2934, lr=0.000572, tokens/sec=1376581.74, grad_norm=0.2779, duration=0.38s
Step 4212: loss=3.3291, lr=0.000572, tokens/sec=1378692.78, grad_norm=0.3504, duration=0.38s
Step 4213: loss=3.3384, lr=0.000572, tokens/sec=1376308.63, grad_norm=0.2806, duration=0.38s
Step 4214: loss=3.2742, lr=0.000572, tokens/sec=1374079.51, grad_norm=0.3106, duration=0.38s
Step 4215: loss=3.2210, lr=0.000572, tokens/sec=1373685.52, grad_norm=0.2961, duration=0.38s
Step 4216: loss=3.2539, lr=0.000572, tokens/sec=1376469.72, grad_norm=0.2847, duration=0.38s
Step 4217: loss=3.2050, lr=0.000572, tokens/sec=1375905.61, grad_norm=0.2737, duration=0.38s
Step 4218: loss=3.1949, lr=0.000572, tokens/sec=1376677.40, grad_norm=0.2699, duration=0.38s
Step 4219: loss=3.2208, lr=0.000572, tokens/sec=1371520.49, grad_norm=0.2694, duration=0.38s
Step 4220: loss=3.2575, lr=0.000572, tokens/sec=1375729.15, grad_norm=0.2694, duration=0.38s
Step 4221: loss=3.2331, lr=0.000572, tokens/sec=1367446.33, grad_norm=0.2702, duration=0.38s
Step 4222: loss=3.2377, lr=0.000572, tokens/sec=1369138.03, grad_norm=0.2610, duration=0.38s
Step 4223: loss=3.2815, lr=0.000572, tokens/sec=1374127.59, grad_norm=0.2869, duration=0.38s
Step 4224: loss=3.2182, lr=0.000572, tokens/sec=1374441.94, grad_norm=0.2772, duration=0.38s
Step 4225: loss=3.2412, lr=0.000572, tokens/sec=1376039.92, grad_norm=0.2822, duration=0.38s
Step 4226: loss=3.2377, lr=0.000572, tokens/sec=1375245.63, grad_norm=0.2653, duration=0.38s
Step 4227: loss=3.3088, lr=0.000572, tokens/sec=1373387.82, grad_norm=0.2604, duration=0.38s
Step 4228: loss=3.3627, lr=0.000572, tokens/sec=1375856.54, grad_norm=0.2603, duration=0.38s
Step 4229: loss=3.3100, lr=0.000572, tokens/sec=1373311.49, grad_norm=0.2719, duration=0.38s
Step 4230: loss=3.3168, lr=0.000572, tokens/sec=1376212.16, grad_norm=0.2539, duration=0.38s
Step 4231: loss=3.3468, lr=0.000572, tokens/sec=1373900.08, grad_norm=0.2929, duration=0.38s
Step 4232: loss=3.3272, lr=0.000572, tokens/sec=1369800.70, grad_norm=0.2580, duration=0.38s
Step 4233: loss=3.3090, lr=0.000572, tokens/sec=1372263.38, grad_norm=0.2916, duration=0.38s
Step 4234: loss=3.2750, lr=0.000572, tokens/sec=1375977.07, grad_norm=0.2598, duration=0.38s
Step 4235: loss=3.3098, lr=0.000572, tokens/sec=1374335.42, grad_norm=0.2806, duration=0.38s
Step 4236: loss=3.2903, lr=0.000572, tokens/sec=1372863.94, grad_norm=0.2965, duration=0.38s
Step 4237: loss=3.2800, lr=0.000572, tokens/sec=1372124.67, grad_norm=0.2916, duration=0.38s
Step 4238: loss=3.2647, lr=0.000572, tokens/sec=1372831.37, grad_norm=0.2479, duration=0.38s
Step 4239: loss=3.2747, lr=0.000572, tokens/sec=1376071.78, grad_norm=0.2579, duration=0.38s
Step 4240: loss=3.2802, lr=0.000572, tokens/sec=1374467.71, grad_norm=0.2619, duration=0.38s
Step 4241: loss=3.1771, lr=0.000572, tokens/sec=1374530.42, grad_norm=0.2859, duration=0.38s
Step 4242: loss=3.2391, lr=0.000571, tokens/sec=1371782.29, grad_norm=0.2853, duration=0.38s
Step 4243: loss=3.2771, lr=0.000571, tokens/sec=1371116.85, grad_norm=0.3020, duration=0.38s
Step 4244: loss=3.2614, lr=0.000571, tokens/sec=1374802.83, grad_norm=0.3065, duration=0.38s
Step 4245: loss=3.3236, lr=0.000571, tokens/sec=1376030.45, grad_norm=0.2816, duration=0.38s
Step 4246: loss=3.2540, lr=0.000571, tokens/sec=1375044.40, grad_norm=0.3008, duration=0.38s
Step 4247: loss=3.2806, lr=0.000571, tokens/sec=1371970.58, grad_norm=0.2867, duration=0.38s
Step 4248: loss=3.2927, lr=0.000571, tokens/sec=1376767.90, grad_norm=0.3098, duration=0.38s
Step 4249: loss=3.2586, lr=0.000571, tokens/sec=1368719.61, grad_norm=0.2550, duration=0.38s
Validation loss at step 4250: 3.7058420181274414
Step 4250: loss=3.3084, lr=0.000571, tokens/sec=151356.28, grad_norm=0.2704, duration=3.46s
Step 4251: loss=3.3091, lr=0.000571, tokens/sec=1375241.32, grad_norm=0.2735, duration=0.38s
Step 4252: loss=3.2874, lr=0.000571, tokens/sec=1372305.35, grad_norm=0.2720, duration=0.38s
Step 4253: loss=3.2940, lr=0.000571, tokens/sec=1371843.05, grad_norm=0.2744, duration=0.38s
Step 4254: loss=3.2891, lr=0.000571, tokens/sec=1374366.34, grad_norm=0.2622, duration=0.38s
Step 4255: loss=3.2824, lr=0.000571, tokens/sec=1376341.36, grad_norm=0.2654, duration=0.38s
Step 4256: loss=3.2518, lr=0.000571, tokens/sec=1373660.64, grad_norm=0.2854, duration=0.38s
Step 4257: loss=3.2623, lr=0.000571, tokens/sec=1375547.57, grad_norm=0.2879, duration=0.38s
Step 4258: loss=3.3032, lr=0.000571, tokens/sec=1375229.28, grad_norm=0.2964, duration=0.38s
Step 4259: loss=3.2774, lr=0.000571, tokens/sec=1373743.02, grad_norm=0.2810, duration=0.38s
Step 4260: loss=3.3230, lr=0.000571, tokens/sec=1373410.98, grad_norm=0.2895, duration=0.38s
Step 4261: loss=3.2105, lr=0.000571, tokens/sec=1377347.39, grad_norm=0.2955, duration=0.38s
Step 4262: loss=3.2916, lr=0.000571, tokens/sec=1375446.05, grad_norm=0.2606, duration=0.38s
Step 4263: loss=3.2226, lr=0.000571, tokens/sec=1376404.25, grad_norm=0.3065, duration=0.38s
Step 4264: loss=3.2314, lr=0.000571, tokens/sec=1375186.28, grad_norm=0.3026, duration=0.38s
Step 4265: loss=3.1943, lr=0.000571, tokens/sec=1376183.73, grad_norm=0.3077, duration=0.38s
Step 4266: loss=3.1808, lr=0.000571, tokens/sec=1376425.78, grad_norm=0.3251, duration=0.38s
Step 4267: loss=3.2300, lr=0.000571, tokens/sec=1373755.03, grad_norm=0.3236, duration=0.38s
Step 4268: loss=3.1613, lr=0.000571, tokens/sec=1380174.19, grad_norm=0.2738, duration=0.38s
Step 4269: loss=3.2152, lr=0.000571, tokens/sec=1369374.20, grad_norm=0.3282, duration=0.38s
Step 4270: loss=3.1860, lr=0.000571, tokens/sec=1369000.81, grad_norm=0.2767, duration=0.38s
Step 4271: loss=3.2212, lr=0.000571, tokens/sec=1374379.23, grad_norm=0.3091, duration=0.38s
Step 4272: loss=3.2210, lr=0.000571, tokens/sec=1375395.29, grad_norm=0.2844, duration=0.38s
Step 4273: loss=3.2530, lr=0.000571, tokens/sec=1372261.67, grad_norm=0.3335, duration=0.38s
Step 4274: loss=3.3079, lr=0.000571, tokens/sec=1372445.81, grad_norm=0.3040, duration=0.38s
Step 4275: loss=3.3273, lr=0.000571, tokens/sec=1371213.46, grad_norm=0.3212, duration=0.38s
Step 4276: loss=3.3285, lr=0.000571, tokens/sec=1371665.92, grad_norm=0.2916, duration=0.38s
Step 4277: loss=3.3213, lr=0.000571, tokens/sec=1374427.33, grad_norm=0.3829, duration=0.38s
Step 4278: loss=3.2875, lr=0.000571, tokens/sec=1376149.29, grad_norm=0.3072, duration=0.38s
Step 4279: loss=3.2913, lr=0.000571, tokens/sec=1373260.89, grad_norm=0.3513, duration=0.38s
Step 4280: loss=3.2673, lr=0.000571, tokens/sec=1375105.45, grad_norm=0.3331, duration=0.38s
Step 4281: loss=3.3245, lr=0.000571, tokens/sec=1372653.99, grad_norm=0.3062, duration=0.38s
Step 4282: loss=3.3135, lr=0.000571, tokens/sec=1376661.03, grad_norm=0.3282, duration=0.38s
Step 4283: loss=3.2813, lr=0.000571, tokens/sec=1372209.44, grad_norm=0.3265, duration=0.38s
Step 4284: loss=3.3065, lr=0.000571, tokens/sec=1377909.23, grad_norm=0.2969, duration=0.38s
Step 4285: loss=3.2819, lr=0.000571, tokens/sec=1376050.26, grad_norm=0.2915, duration=0.38s
Step 4286: loss=3.2597, lr=0.000571, tokens/sec=1378066.39, grad_norm=0.2666, duration=0.38s
Step 4287: loss=3.2446, lr=0.000571, tokens/sec=1376808.42, grad_norm=0.2875, duration=0.38s
Step 4288: loss=3.2858, lr=0.000571, tokens/sec=1373918.11, grad_norm=0.2718, duration=0.38s
Step 4289: loss=3.2167, lr=0.000571, tokens/sec=1373973.91, grad_norm=0.2698, duration=0.38s
Step 4290: loss=3.3413, lr=0.000571, tokens/sec=1372557.17, grad_norm=0.2917, duration=0.38s
Step 4291: loss=3.3047, lr=0.000571, tokens/sec=1373846.01, grad_norm=0.2723, duration=0.38s
Step 4292: loss=3.2748, lr=0.000571, tokens/sec=1375758.41, grad_norm=0.2841, duration=0.38s
Step 4293: loss=3.2273, lr=0.000571, tokens/sec=1372866.51, grad_norm=0.2508, duration=0.38s
Step 4294: loss=3.2738, lr=0.000571, tokens/sec=1376986.88, grad_norm=0.2794, duration=0.38s
Step 4295: loss=3.2644, lr=0.000571, tokens/sec=1374464.27, grad_norm=0.2784, duration=0.38s
Step 4296: loss=3.3162, lr=0.000571, tokens/sec=1371788.28, grad_norm=0.2647, duration=0.38s
Step 4297: loss=3.2810, lr=0.000571, tokens/sec=1375608.67, grad_norm=0.2438, duration=0.38s
Step 4298: loss=3.3314, lr=0.000571, tokens/sec=1378379.08, grad_norm=0.2846, duration=0.38s
Step 4299: loss=3.2946, lr=0.000571, tokens/sec=1374319.10, grad_norm=0.2910, duration=0.38s
Step 4300/19073 (22.5%), Elapsed time: 1752.72s, Steps per hour: 8831.98, Estimated hours remaining: 1.67
Step 4300: loss=3.2637, lr=0.000571, tokens/sec=1378333.29, grad_norm=0.2500, duration=0.38s
Step 4301: loss=3.2975, lr=0.000570, tokens/sec=1374624.94, grad_norm=0.2576, duration=0.38s
Step 4302: loss=3.2803, lr=0.000570, tokens/sec=1378296.14, grad_norm=0.2711, duration=0.38s
Step 4303: loss=3.2530, lr=0.000570, tokens/sec=1375699.89, grad_norm=0.3298, duration=0.38s
Step 4304: loss=3.2952, lr=0.000570, tokens/sec=1376667.92, grad_norm=0.2797, duration=0.38s
Step 4305: loss=3.2816, lr=0.000570, tokens/sec=1378385.99, grad_norm=0.2385, duration=0.38s
Step 4306: loss=3.2639, lr=0.000570, tokens/sec=1375415.08, grad_norm=0.2708, duration=0.38s
Step 4307: loss=3.2403, lr=0.000570, tokens/sec=1375626.74, grad_norm=0.2550, duration=0.38s
Step 4308: loss=3.2292, lr=0.000570, tokens/sec=1370339.33, grad_norm=0.2602, duration=0.38s
Step 4309: loss=3.1905, lr=0.000570, tokens/sec=1376152.73, grad_norm=0.2438, duration=0.38s
Step 4310: loss=3.2274, lr=0.000570, tokens/sec=1372161.49, grad_norm=0.2355, duration=0.38s
Step 4311: loss=3.1732, lr=0.000570, tokens/sec=1373888.92, grad_norm=0.2468, duration=0.38s
Step 4312: loss=3.1877, lr=0.000570, tokens/sec=1372537.47, grad_norm=0.2543, duration=0.38s
Step 4313: loss=3.1725, lr=0.000570, tokens/sec=1375471.86, grad_norm=0.2831, duration=0.38s
Step 4314: loss=3.2535, lr=0.000570, tokens/sec=1377002.40, grad_norm=0.2959, duration=0.38s
Step 4315: loss=3.2209, lr=0.000570, tokens/sec=1378894.21, grad_norm=0.2936, duration=0.38s
Step 4316: loss=3.2110, lr=0.000570, tokens/sec=1379486.74, grad_norm=0.3338, duration=0.38s
Step 4317: loss=3.1960, lr=0.000570, tokens/sec=1378455.98, grad_norm=0.3160, duration=0.38s
Step 4318: loss=3.2447, lr=0.000570, tokens/sec=1376938.59, grad_norm=0.2925, duration=0.38s
Step 4319: loss=3.2695, lr=0.000570, tokens/sec=1371903.82, grad_norm=0.3319, duration=0.38s
Step 4320: loss=3.2961, lr=0.000570, tokens/sec=1378141.53, grad_norm=0.3327, duration=0.38s
Step 4321: loss=3.3075, lr=0.000570, tokens/sec=1376352.56, grad_norm=0.3106, duration=0.38s
Step 4322: loss=3.3289, lr=0.000570, tokens/sec=1377788.37, grad_norm=0.3662, duration=0.38s
Step 4323: loss=3.2804, lr=0.000570, tokens/sec=1377003.26, grad_norm=0.3353, duration=0.38s
Step 4324: loss=3.3129, lr=0.000570, tokens/sec=1377236.11, grad_norm=0.3470, duration=0.38s
Step 4325: loss=3.2678, lr=0.000570, tokens/sec=1373967.04, grad_norm=0.3648, duration=0.38s
Step 4326: loss=3.2912, lr=0.000570, tokens/sec=1376474.03, grad_norm=0.3191, duration=0.38s
Step 4327: loss=3.3249, lr=0.000570, tokens/sec=1376295.70, grad_norm=0.3180, duration=0.38s
Step 4328: loss=3.3844, lr=0.000570, tokens/sec=1376127.76, grad_norm=0.3221, duration=0.38s
Step 4329: loss=3.3482, lr=0.000570, tokens/sec=1374222.91, grad_norm=0.3089, duration=0.38s
Step 4330: loss=3.3352, lr=0.000570, tokens/sec=1377149.86, grad_norm=0.3023, duration=0.38s
Step 4331: loss=3.2462, lr=0.000570, tokens/sec=1374814.87, grad_norm=0.2759, duration=0.38s
Step 4332: loss=3.2723, lr=0.000570, tokens/sec=1377071.38, grad_norm=0.2872, duration=0.38s
Step 4333: loss=3.2525, lr=0.000570, tokens/sec=1375301.53, grad_norm=0.2546, duration=0.38s
Step 4334: loss=3.2131, lr=0.000570, tokens/sec=1371718.97, grad_norm=0.2811, duration=0.38s
Step 4335: loss=3.3172, lr=0.000570, tokens/sec=1367618.97, grad_norm=0.2732, duration=0.38s
Step 4336: loss=3.2118, lr=0.000570, tokens/sec=1375355.72, grad_norm=0.2753, duration=0.38s
Step 4337: loss=3.2866, lr=0.000570, tokens/sec=1372431.25, grad_norm=0.2744, duration=0.38s
Step 4338: loss=3.3303, lr=0.000570, tokens/sec=1376217.32, grad_norm=0.2842, duration=0.38s
Step 4339: loss=3.2679, lr=0.000570, tokens/sec=1376536.07, grad_norm=0.2720, duration=0.38s
Step 4340: loss=3.2147, lr=0.000570, tokens/sec=1373101.39, grad_norm=0.2715, duration=0.38s
Step 4341: loss=3.2801, lr=0.000570, tokens/sec=1376575.71, grad_norm=0.2893, duration=0.38s
Step 4342: loss=3.2640, lr=0.000570, tokens/sec=1374058.90, grad_norm=0.2705, duration=0.38s
Step 4343: loss=3.2475, lr=0.000570, tokens/sec=1375012.59, grad_norm=0.2778, duration=0.38s
Step 4344: loss=3.2168, lr=0.000570, tokens/sec=1375375.51, grad_norm=0.2812, duration=0.38s
Step 4345: loss=3.2902, lr=0.000570, tokens/sec=1371574.38, grad_norm=0.2712, duration=0.38s
Step 4346: loss=3.2771, lr=0.000570, tokens/sec=1373077.39, grad_norm=0.2865, duration=0.38s
Step 4347: loss=3.2754, lr=0.000570, tokens/sec=1369160.20, grad_norm=0.2770, duration=0.38s
Step 4348: loss=3.3272, lr=0.000570, tokens/sec=1373718.13, grad_norm=0.2831, duration=0.38s
Step 4349: loss=3.2330, lr=0.000570, tokens/sec=1375867.73, grad_norm=0.2955, duration=0.38s
Step 4350: loss=3.2616, lr=0.000570, tokens/sec=1377375.86, grad_norm=0.2701, duration=0.38s
Step 4351: loss=3.2188, lr=0.000570, tokens/sec=1373459.87, grad_norm=0.2642, duration=0.38s
Step 4352: loss=3.2038, lr=0.000570, tokens/sec=1376664.47, grad_norm=0.2606, duration=0.38s
Step 4353: loss=3.2441, lr=0.000570, tokens/sec=1375839.33, grad_norm=0.2701, duration=0.38s
Step 4354: loss=3.2265, lr=0.000570, tokens/sec=1377821.17, grad_norm=0.2491, duration=0.38s
Step 4355: loss=3.2259, lr=0.000570, tokens/sec=1373192.28, grad_norm=0.2719, duration=0.38s
Step 4356: loss=3.2040, lr=0.000570, tokens/sec=1373522.50, grad_norm=0.2637, duration=0.38s
Step 4357: loss=3.2697, lr=0.000570, tokens/sec=1371673.62, grad_norm=0.2628, duration=0.38s
Step 4358: loss=3.2386, lr=0.000570, tokens/sec=1376688.60, grad_norm=0.2720, duration=0.38s
Step 4359: loss=3.1593, lr=0.000569, tokens/sec=1377427.62, grad_norm=0.2552, duration=0.38s
Step 4360: loss=3.1754, lr=0.000569, tokens/sec=1374035.72, grad_norm=0.2882, duration=0.38s
Step 4361: loss=3.2032, lr=0.000569, tokens/sec=1375313.57, grad_norm=0.2722, duration=0.38s
Step 4362: loss=3.2575, lr=0.000569, tokens/sec=1377611.42, grad_norm=0.2908, duration=0.38s
Step 4363: loss=3.2054, lr=0.000569, tokens/sec=1373539.66, grad_norm=0.2767, duration=0.38s
Step 4364: loss=3.2328, lr=0.000569, tokens/sec=1377578.63, grad_norm=0.2901, duration=0.38s
Step 4365: loss=3.2720, lr=0.000569, tokens/sec=1373993.65, grad_norm=0.3133, duration=0.38s
Step 4366: loss=3.2947, lr=0.000569, tokens/sec=1378547.58, grad_norm=0.2940, duration=0.38s
Step 4367: loss=3.3112, lr=0.000569, tokens/sec=1374235.79, grad_norm=0.3242, duration=0.38s
Step 4368: loss=3.2555, lr=0.000569, tokens/sec=1378141.53, grad_norm=0.3076, duration=0.38s
Step 4369: loss=3.3043, lr=0.000569, tokens/sec=1377233.52, grad_norm=0.3050, duration=0.38s
Step 4370: loss=3.2595, lr=0.000569, tokens/sec=1371896.11, grad_norm=0.2899, duration=0.38s
Step 4371: loss=3.3031, lr=0.000569, tokens/sec=1375091.69, grad_norm=0.3125, duration=0.38s
Step 4372: loss=3.2969, lr=0.000569, tokens/sec=1375303.25, grad_norm=0.3262, duration=0.38s
Step 4373: loss=3.3661, lr=0.000569, tokens/sec=1375869.45, grad_norm=0.2697, duration=0.38s
Step 4374: loss=3.3347, lr=0.000569, tokens/sec=1373250.60, grad_norm=0.3251, duration=0.38s
Step 4375: loss=3.3185, lr=0.000569, tokens/sec=1376233.69, grad_norm=0.3149, duration=0.38s
Step 4376: loss=3.3620, lr=0.000569, tokens/sec=1370666.46, grad_norm=0.3057, duration=0.38s
Step 4377: loss=3.3052, lr=0.000569, tokens/sec=1376961.01, grad_norm=0.3170, duration=0.38s
Step 4378: loss=3.3434, lr=0.000569, tokens/sec=1376279.34, grad_norm=0.3137, duration=0.38s
Step 4379: loss=3.3275, lr=0.000569, tokens/sec=1372271.95, grad_norm=0.2882, duration=0.38s
Step 4380: loss=3.3126, lr=0.000569, tokens/sec=1377323.23, grad_norm=0.2890, duration=0.38s
Step 4381: loss=3.2820, lr=0.000569, tokens/sec=1378903.72, grad_norm=0.2705, duration=0.38s
Step 4382: loss=3.2692, lr=0.000569, tokens/sec=1372122.96, grad_norm=0.2725, duration=0.38s
Step 4383: loss=3.2265, lr=0.000569, tokens/sec=1375221.54, grad_norm=0.2661, duration=0.38s
Step 4384: loss=3.2578, lr=0.000569, tokens/sec=1375243.04, grad_norm=0.2691, duration=0.38s
Step 4385: loss=3.2913, lr=0.000569, tokens/sec=1372356.73, grad_norm=0.2962, duration=0.38s
Step 4386: loss=3.2403, lr=0.000569, tokens/sec=1370243.69, grad_norm=0.2753, duration=0.38s
Step 4387: loss=3.2846, lr=0.000569, tokens/sec=1376632.59, grad_norm=0.2978, duration=0.38s
Step 4388: loss=3.2565, lr=0.000569, tokens/sec=1374381.80, grad_norm=0.2992, duration=0.38s
Step 4389: loss=3.1838, lr=0.000569, tokens/sec=1380650.79, grad_norm=0.3356, duration=0.38s
Step 4390: loss=3.1912, lr=0.000569, tokens/sec=1374814.01, grad_norm=0.3520, duration=0.38s
Step 4391: loss=3.2545, lr=0.000569, tokens/sec=1375835.88, grad_norm=0.2898, duration=0.38s
Step 4392: loss=3.3436, lr=0.000569, tokens/sec=1376394.77, grad_norm=0.2994, duration=0.38s
Step 4393: loss=3.2292, lr=0.000569, tokens/sec=1379923.89, grad_norm=0.3129, duration=0.38s
Step 4394: loss=3.3181, lr=0.000569, tokens/sec=1377316.33, grad_norm=0.2924, duration=0.38s
Step 4395: loss=3.3020, lr=0.000569, tokens/sec=1372642.85, grad_norm=0.3129, duration=0.38s
Step 4396: loss=3.2312, lr=0.000569, tokens/sec=1377967.08, grad_norm=0.3137, duration=0.38s
Step 4397: loss=3.2660, lr=0.000569, tokens/sec=1376352.56, grad_norm=0.3074, duration=0.38s
Step 4398: loss=3.3244, lr=0.000569, tokens/sec=1374784.79, grad_norm=0.3055, duration=0.38s
Step 4399: loss=3.2867, lr=0.000569, tokens/sec=1375986.54, grad_norm=0.3186, duration=0.38s
Step 4400/19073 (23.1%), Elapsed time: 1790.91s, Steps per hour: 8844.64, Estimated hours remaining: 1.66
Step 4400: loss=3.2528, lr=0.000569, tokens/sec=1378644.38, grad_norm=0.2960, duration=0.38s
Step 4401: loss=3.2860, lr=0.000569, tokens/sec=1375903.03, grad_norm=0.2828, duration=0.38s
Step 4402: loss=3.2910, lr=0.000569, tokens/sec=1377374.13, grad_norm=0.3670, duration=0.38s
Step 4403: loss=3.3287, lr=0.000569, tokens/sec=1377162.80, grad_norm=0.2849, duration=0.38s
Step 4404: loss=3.2597, lr=0.000569, tokens/sec=1374248.67, grad_norm=0.2720, duration=0.38s
Step 4405: loss=3.1792, lr=0.000569, tokens/sec=1374052.03, grad_norm=0.3053, duration=0.38s
Step 4406: loss=3.2478, lr=0.000569, tokens/sec=1375151.02, grad_norm=0.2844, duration=0.38s
Step 4407: loss=3.1868, lr=0.000569, tokens/sec=1375335.94, grad_norm=0.2733, duration=0.38s
Step 4408: loss=3.1767, lr=0.000569, tokens/sec=1373242.02, grad_norm=0.2704, duration=0.38s
Step 4409: loss=3.2321, lr=0.000569, tokens/sec=1374009.96, grad_norm=0.3144, duration=0.38s
Step 4410: loss=3.2130, lr=0.000569, tokens/sec=1375186.28, grad_norm=0.2667, duration=0.38s
Step 4411: loss=3.2297, lr=0.000569, tokens/sec=1376609.32, grad_norm=0.2983, duration=0.38s
Step 4412: loss=3.2463, lr=0.000569, tokens/sec=1374252.97, grad_norm=0.2730, duration=0.38s
Step 4413: loss=3.2208, lr=0.000569, tokens/sec=1374931.77, grad_norm=0.2877, duration=0.38s
Step 4414: loss=3.2282, lr=0.000569, tokens/sec=1373419.56, grad_norm=0.2732, duration=0.38s
Step 4415: loss=3.2107, lr=0.000569, tokens/sec=1372745.67, grad_norm=0.2956, duration=0.38s
Step 4416: loss=3.2011, lr=0.000568, tokens/sec=1378278.00, grad_norm=0.2515, duration=0.38s
Step 4417: loss=3.3057, lr=0.000568, tokens/sec=1376144.98, grad_norm=0.2948, duration=0.38s
Step 4418: loss=3.3422, lr=0.000568, tokens/sec=1375977.93, grad_norm=0.3072, duration=0.38s
Step 4419: loss=3.2992, lr=0.000568, tokens/sec=1376019.26, grad_norm=0.2659, duration=0.38s
Step 4420: loss=3.3046, lr=0.000568, tokens/sec=1374199.72, grad_norm=0.2928, duration=0.38s
Step 4421: loss=3.3136, lr=0.000568, tokens/sec=1373892.36, grad_norm=0.2794, duration=0.38s
Step 4422: loss=3.3011, lr=0.000568, tokens/sec=1369731.59, grad_norm=0.2685, duration=0.38s
Step 4423: loss=3.2964, lr=0.000568, tokens/sec=1374816.59, grad_norm=0.2804, duration=0.38s
Step 4424: loss=3.2886, lr=0.000568, tokens/sec=1376020.12, grad_norm=0.2715, duration=0.38s
Step 4425: loss=3.2516, lr=0.000568, tokens/sec=1379937.75, grad_norm=0.2905, duration=0.38s
Step 4426: loss=3.2722, lr=0.000568, tokens/sec=1377486.30, grad_norm=0.2905, duration=0.38s
Step 4427: loss=3.2610, lr=0.000568, tokens/sec=1374042.59, grad_norm=0.2865, duration=0.38s
Step 4428: loss=3.2579, lr=0.000568, tokens/sec=1374132.74, grad_norm=0.2953, duration=0.38s
Step 4429: loss=3.2551, lr=0.000568, tokens/sec=1374251.25, grad_norm=0.2692, duration=0.38s
Step 4430: loss=3.2297, lr=0.000568, tokens/sec=1376544.69, grad_norm=0.2720, duration=0.38s
Step 4431: loss=3.1747, lr=0.000568, tokens/sec=1377508.73, grad_norm=0.2696, duration=0.38s
Step 4432: loss=3.2150, lr=0.000568, tokens/sec=1379392.42, grad_norm=0.2802, duration=0.38s
Step 4433: loss=3.2518, lr=0.000568, tokens/sec=1377539.80, grad_norm=0.2682, duration=0.38s
Step 4434: loss=3.2548, lr=0.000568, tokens/sec=1371588.07, grad_norm=0.2937, duration=0.38s
Step 4435: loss=3.3053, lr=0.000568, tokens/sec=1370669.88, grad_norm=0.2764, duration=0.38s
Step 4436: loss=3.1743, lr=0.000568, tokens/sec=1372093.85, grad_norm=0.3058, duration=0.38s
Step 4437: loss=3.3173, lr=0.000568, tokens/sec=1375058.16, grad_norm=0.3033, duration=0.38s
Step 4438: loss=3.2706, lr=0.000568, tokens/sec=1374740.09, grad_norm=0.3037, duration=0.38s
Step 4439: loss=3.2837, lr=0.000568, tokens/sec=1371899.54, grad_norm=0.2841, duration=0.38s
Step 4440: loss=3.2879, lr=0.000568, tokens/sec=1376337.05, grad_norm=0.3145, duration=0.38s
Step 4441: loss=3.2918, lr=0.000568, tokens/sec=1375187.14, grad_norm=0.2923, duration=0.38s
Step 4442: loss=3.2588, lr=0.000568, tokens/sec=1377506.14, grad_norm=0.2862, duration=0.38s
Step 4443: loss=3.2757, lr=0.000568, tokens/sec=1380836.31, grad_norm=0.2746, duration=0.38s
Step 4444: loss=3.2584, lr=0.000568, tokens/sec=1378792.19, grad_norm=0.2808, duration=0.38s
Step 4445: loss=3.2375, lr=0.000568, tokens/sec=1373966.18, grad_norm=0.2778, duration=0.38s
Step 4446: loss=3.2834, lr=0.000568, tokens/sec=1373230.01, grad_norm=0.2718, duration=0.38s
Step 4447: loss=3.2443, lr=0.000568, tokens/sec=1375265.41, grad_norm=0.2865, duration=0.38s
Step 4448: loss=3.2724, lr=0.000568, tokens/sec=1376601.56, grad_norm=0.2775, duration=0.38s
Step 4449: loss=3.2925, lr=0.000568, tokens/sec=1376395.63, grad_norm=0.2886, duration=0.38s
Step 4450: loss=3.2693, lr=0.000568, tokens/sec=1375872.90, grad_norm=0.3083, duration=0.38s
Step 4451: loss=3.2142, lr=0.000568, tokens/sec=1374453.96, grad_norm=0.3250, duration=0.38s
Step 4452: loss=3.2609, lr=0.000568, tokens/sec=1376551.58, grad_norm=0.2611, duration=0.38s
Step 4453: loss=3.2025, lr=0.000568, tokens/sec=1374534.72, grad_norm=0.3228, duration=0.38s
Step 4454: loss=3.2322, lr=0.000568, tokens/sec=1371648.81, grad_norm=0.3214, duration=0.38s
Step 4455: loss=3.1215, lr=0.000568, tokens/sec=1377929.09, grad_norm=0.2657, duration=0.38s
Step 4456: loss=3.1853, lr=0.000568, tokens/sec=1372999.37, grad_norm=0.2905, duration=0.38s
Step 4457: loss=3.1877, lr=0.000568, tokens/sec=1375990.84, grad_norm=0.2661, duration=0.38s
Step 4458: loss=3.1705, lr=0.000568, tokens/sec=1375927.99, grad_norm=0.2938, duration=0.38s
Step 4459: loss=3.1899, lr=0.000568, tokens/sec=1374593.15, grad_norm=0.3040, duration=0.38s
Step 4460: loss=3.1484, lr=0.000568, tokens/sec=1376762.73, grad_norm=0.2921, duration=0.38s
Step 4461: loss=3.2213, lr=0.000568, tokens/sec=1376050.26, grad_norm=0.3166, duration=0.38s
Step 4462: loss=3.1949, lr=0.000568, tokens/sec=1374370.64, grad_norm=0.2893, duration=0.38s
Step 4463: loss=3.2430, lr=0.000568, tokens/sec=1375359.16, grad_norm=0.3001, duration=0.38s
Step 4464: loss=3.3015, lr=0.000568, tokens/sec=1374679.08, grad_norm=0.3046, duration=0.38s
Step 4465: loss=3.3053, lr=0.000568, tokens/sec=1371523.05, grad_norm=0.3096, duration=0.38s
Step 4466: loss=3.3002, lr=0.000568, tokens/sec=1372567.45, grad_norm=0.2990, duration=0.38s
Step 4467: loss=3.3251, lr=0.000568, tokens/sec=1377739.16, grad_norm=0.3286, duration=0.38s
Step 4468: loss=3.2210, lr=0.000568, tokens/sec=1368901.10, grad_norm=0.3027, duration=0.38s
Step 4469: loss=3.2795, lr=0.000568, tokens/sec=1371594.05, grad_norm=0.3356, duration=0.38s
Step 4470: loss=3.2515, lr=0.000568, tokens/sec=1375459.81, grad_norm=0.2963, duration=0.38s
Step 4471: loss=3.3069, lr=0.000568, tokens/sec=1376277.62, grad_norm=0.3181, duration=0.38s
Step 4472: loss=3.2725, lr=0.000567, tokens/sec=1375501.97, grad_norm=0.2976, duration=0.38s
Step 4473: loss=3.2791, lr=0.000567, tokens/sec=1376092.45, grad_norm=0.2942, duration=0.38s
Step 4474: loss=3.3049, lr=0.000567, tokens/sec=1375207.78, grad_norm=0.3268, duration=0.38s
Step 4475: loss=3.2285, lr=0.000567, tokens/sec=1375097.71, grad_norm=0.2784, duration=0.38s
Step 4476: loss=3.2382, lr=0.000567, tokens/sec=1375559.62, grad_norm=0.2832, duration=0.38s
Step 4477: loss=3.2429, lr=0.000567, tokens/sec=1373153.70, grad_norm=0.2807, duration=0.38s
Step 4478: loss=3.2600, lr=0.000567, tokens/sec=1376359.45, grad_norm=0.2673, duration=0.38s
Step 4479: loss=3.2740, lr=0.000567, tokens/sec=1374222.91, grad_norm=0.2903, duration=0.38s
Step 4480: loss=3.2990, lr=0.000567, tokens/sec=1375548.43, grad_norm=0.2762, duration=0.38s
Step 4481: loss=3.2493, lr=0.000567, tokens/sec=1374948.11, grad_norm=0.2719, duration=0.38s
Step 4482: loss=3.2634, lr=0.000567, tokens/sec=1375423.68, grad_norm=0.2823, duration=0.38s
Step 4483: loss=3.1791, lr=0.000567, tokens/sec=1375522.62, grad_norm=0.2485, duration=0.38s
Step 4484: loss=3.2942, lr=0.000567, tokens/sec=1374080.37, grad_norm=0.2782, duration=0.38s
Step 4485: loss=3.2550, lr=0.000567, tokens/sec=1376637.76, grad_norm=0.2754, duration=0.38s
Step 4486: loss=3.2976, lr=0.000567, tokens/sec=1370691.24, grad_norm=0.2524, duration=0.38s
Step 4487: loss=3.2931, lr=0.000567, tokens/sec=1374871.60, grad_norm=0.2773, duration=0.38s
Step 4488: loss=3.2931, lr=0.000567, tokens/sec=1373938.71, grad_norm=0.2643, duration=0.38s
Step 4489: loss=3.2425, lr=0.000567, tokens/sec=1374042.59, grad_norm=0.2748, duration=0.38s
Step 4490: loss=3.2823, lr=0.000567, tokens/sec=1373591.99, grad_norm=0.2440, duration=0.38s
Step 4491: loss=3.2451, lr=0.000567, tokens/sec=1376529.18, grad_norm=0.2562, duration=0.38s
Step 4492: loss=3.2547, lr=0.000567, tokens/sec=1377183.50, grad_norm=0.2556, duration=0.38s
Step 4493: loss=3.2555, lr=0.000567, tokens/sec=1376283.65, grad_norm=0.2618, duration=0.38s
Step 4494: loss=3.2596, lr=0.000567, tokens/sec=1363021.93, grad_norm=0.2648, duration=0.38s
Step 4495: loss=3.2572, lr=0.000567, tokens/sec=1372964.23, grad_norm=0.2540, duration=0.38s
Step 4496: loss=3.2330, lr=0.000567, tokens/sec=1375584.57, grad_norm=0.2766, duration=0.38s
Step 4497: loss=3.2150, lr=0.000567, tokens/sec=1370919.40, grad_norm=0.2519, duration=0.38s
Step 4498: loss=3.2268, lr=0.000567, tokens/sec=1372728.53, grad_norm=0.2586, duration=0.38s
Step 4499: loss=3.1689, lr=0.000567, tokens/sec=1372961.65, grad_norm=0.2356, duration=0.38s
Step 4500/19073 (23.6%), Elapsed time: 1829.12s, Steps per hour: 8856.70, Estimated hours remaining: 1.65
Validation loss at step 4500: 3.700763463973999
Step 4500: loss=3.2311, lr=0.000567, tokens/sec=153109.97, grad_norm=0.2512, duration=3.42s
Step 4501: loss=3.1263, lr=0.000567, tokens/sec=1370756.17, grad_norm=0.2443, duration=0.38s
Step 4502: loss=3.1620, lr=0.000567, tokens/sec=1372553.74, grad_norm=0.2508, duration=0.38s
Step 4503: loss=3.1685, lr=0.000567, tokens/sec=1372314.77, grad_norm=0.2825, duration=0.38s
Step 4504: loss=3.2284, lr=0.000567, tokens/sec=1373846.01, grad_norm=0.2739, duration=0.38s
Step 4505: loss=3.2153, lr=0.000567, tokens/sec=1374531.28, grad_norm=0.2454, duration=0.38s
Step 4506: loss=3.1886, lr=0.000567, tokens/sec=1379133.75, grad_norm=0.2836, duration=0.38s
Step 4507: loss=3.1958, lr=0.000567, tokens/sec=1371405.01, grad_norm=0.2927, duration=0.38s
Step 4508: loss=3.2095, lr=0.000567, tokens/sec=1375975.35, grad_norm=0.2697, duration=0.38s
Step 4509: loss=3.2432, lr=0.000567, tokens/sec=1376594.67, grad_norm=0.3115, duration=0.38s
Step 4510: loss=3.2743, lr=0.000567, tokens/sec=1371007.43, grad_norm=0.3098, duration=0.38s
Step 4511: loss=3.2912, lr=0.000567, tokens/sec=1373228.30, grad_norm=0.2871, duration=0.38s
Step 4512: loss=3.2893, lr=0.000567, tokens/sec=1369909.08, grad_norm=0.3125, duration=0.38s
Step 4513: loss=3.2634, lr=0.000567, tokens/sec=1376036.48, grad_norm=0.3158, duration=0.38s
Step 4514: loss=3.3023, lr=0.000567, tokens/sec=1372729.39, grad_norm=0.3618, duration=0.38s
Step 4515: loss=3.2437, lr=0.000567, tokens/sec=1373956.74, grad_norm=0.3092, duration=0.38s
Step 4516: loss=3.2656, lr=0.000567, tokens/sec=1371825.08, grad_norm=0.3393, duration=0.38s
Step 4517: loss=3.3016, lr=0.000567, tokens/sec=1376094.17, grad_norm=0.3234, duration=0.38s
Step 4518: loss=3.3994, lr=0.000567, tokens/sec=1380517.31, grad_norm=0.3211, duration=0.38s
Step 4519: loss=3.3267, lr=0.000567, tokens/sec=1376042.51, grad_norm=0.3082, duration=0.38s
Step 4520: loss=3.2916, lr=0.000567, tokens/sec=1375421.10, grad_norm=0.3099, duration=0.38s
Step 4521: loss=3.2199, lr=0.000567, tokens/sec=1377091.22, grad_norm=0.2862, duration=0.38s
Step 4522: loss=3.2640, lr=0.000567, tokens/sec=1377053.27, grad_norm=0.2770, duration=0.38s
Step 4523: loss=3.2282, lr=0.000567, tokens/sec=1375761.00, grad_norm=0.2687, duration=0.38s
Step 4524: loss=3.2133, lr=0.000567, tokens/sec=1377919.59, grad_norm=0.2860, duration=0.38s
Step 4525: loss=3.2806, lr=0.000567, tokens/sec=1375676.65, grad_norm=0.2945, duration=0.38s
Step 4526: loss=3.1793, lr=0.000567, tokens/sec=1376930.83, grad_norm=0.2746, duration=0.38s
Step 4527: loss=3.2853, lr=0.000567, tokens/sec=1375267.99, grad_norm=0.3075, duration=0.38s
Step 4528: loss=3.3089, lr=0.000566, tokens/sec=1380248.69, grad_norm=0.2952, duration=0.38s
Step 4529: loss=3.2578, lr=0.000566, tokens/sec=1376293.98, grad_norm=0.2937, duration=0.38s
Step 4530: loss=3.1958, lr=0.000566, tokens/sec=1374744.39, grad_norm=0.2649, duration=0.38s
Step 4531: loss=3.2389, lr=0.000566, tokens/sec=1372610.29, grad_norm=0.2935, duration=0.38s
Step 4532: loss=3.2655, lr=0.000566, tokens/sec=1379841.63, grad_norm=0.2711, duration=0.38s
Step 4533: loss=3.2705, lr=0.000566, tokens/sec=1376829.10, grad_norm=0.2723, duration=0.38s
Step 4534: loss=3.2077, lr=0.000566, tokens/sec=1378412.78, grad_norm=0.3017, duration=0.38s
Step 4535: loss=3.2629, lr=0.000566, tokens/sec=1378806.02, grad_norm=0.2723, duration=0.38s
Step 4536: loss=3.2393, lr=0.000566, tokens/sec=1371370.80, grad_norm=0.3056, duration=0.38s
Step 4537: loss=3.2789, lr=0.000566, tokens/sec=1376626.55, grad_norm=0.2812, duration=0.38s
Step 4538: loss=3.2946, lr=0.000566, tokens/sec=1378239.13, grad_norm=0.2843, duration=0.38s
Step 4539: loss=3.2353, lr=0.000566, tokens/sec=1375700.75, grad_norm=0.3181, duration=0.38s
Step 4540: loss=3.2001, lr=0.000566, tokens/sec=1378646.10, grad_norm=0.2864, duration=0.38s
Step 4541: loss=3.2366, lr=0.000566, tokens/sec=1379579.34, grad_norm=0.2691, duration=0.38s
Step 4542: loss=3.1777, lr=0.000566, tokens/sec=1375204.34, grad_norm=0.2753, duration=0.38s
Step 4543: loss=3.2510, lr=0.000566, tokens/sec=1367742.31, grad_norm=0.2863, duration=0.38s
Step 4544: loss=3.2195, lr=0.000566, tokens/sec=1376951.53, grad_norm=0.2511, duration=0.38s
Step 4545: loss=3.1871, lr=0.000566, tokens/sec=1377097.25, grad_norm=0.2807, duration=0.38s
Step 4546: loss=3.1836, lr=0.000566, tokens/sec=1371891.83, grad_norm=0.2618, duration=0.38s
Step 4547: loss=3.2766, lr=0.000566, tokens/sec=1381829.82, grad_norm=0.2748, duration=0.38s
Step 4548: loss=3.1584, lr=0.000566, tokens/sec=1373217.15, grad_norm=0.2540, duration=0.38s
Step 4549: loss=3.1904, lr=0.000566, tokens/sec=1372614.57, grad_norm=0.2829, duration=0.38s
Step 4550: loss=3.1454, lr=0.000566, tokens/sec=1371818.23, grad_norm=0.2688, duration=0.38s
Step 4551: loss=3.2116, lr=0.000566, tokens/sec=1375798.87, grad_norm=0.2789, duration=0.38s
Step 4552: loss=3.2004, lr=0.000566, tokens/sec=1373287.47, grad_norm=0.2948, duration=0.38s
Step 4553: loss=3.1979, lr=0.000566, tokens/sec=1376805.83, grad_norm=0.2624, duration=0.38s
Step 4554: loss=3.2035, lr=0.000566, tokens/sec=1376241.44, grad_norm=0.3346, duration=0.38s
Step 4555: loss=3.2745, lr=0.000566, tokens/sec=1373560.25, grad_norm=0.3388, duration=0.38s
Step 4556: loss=3.2810, lr=0.000566, tokens/sec=1370055.88, grad_norm=0.3124, duration=0.38s
Step 4557: loss=3.2756, lr=0.000566, tokens/sec=1375950.38, grad_norm=0.2946, duration=0.38s
Step 4558: loss=3.2525, lr=0.000566, tokens/sec=1375283.47, grad_norm=0.2766, duration=0.38s
Step 4559: loss=3.2638, lr=0.000566, tokens/sec=1377331.00, grad_norm=0.3240, duration=0.38s
Step 4560: loss=3.2459, lr=0.000566, tokens/sec=1376346.53, grad_norm=0.2689, duration=0.38s
Step 4561: loss=3.2728, lr=0.000566, tokens/sec=1375372.93, grad_norm=0.3275, duration=0.38s
Step 4562: loss=3.2906, lr=0.000566, tokens/sec=1376711.01, grad_norm=0.3024, duration=0.38s
Step 4563: loss=3.3807, lr=0.000566, tokens/sec=1374991.09, grad_norm=0.3031, duration=0.38s
Step 4564: loss=3.2825, lr=0.000566, tokens/sec=1376188.90, grad_norm=0.3327, duration=0.38s
Step 4565: loss=3.2941, lr=0.000566, tokens/sec=1377949.81, grad_norm=0.2851, duration=0.38s
Step 4566: loss=3.3581, lr=0.000566, tokens/sec=1376603.28, grad_norm=0.3215, duration=0.38s
Step 4567: loss=3.3015, lr=0.000566, tokens/sec=1377291.32, grad_norm=0.2874, duration=0.38s
Step 4568: loss=3.3455, lr=0.000566, tokens/sec=1379414.92, grad_norm=0.2982, duration=0.38s
Step 4569: loss=3.2790, lr=0.000566, tokens/sec=1374979.06, grad_norm=0.3136, duration=0.38s
Step 4570: loss=3.3048, lr=0.000566, tokens/sec=1377845.34, grad_norm=0.3019, duration=0.38s
Step 4571: loss=3.2342, lr=0.000566, tokens/sec=1376095.89, grad_norm=0.2850, duration=0.38s
Step 4572: loss=3.2776, lr=0.000566, tokens/sec=1378385.13, grad_norm=0.2997, duration=0.38s
Step 4573: loss=3.1782, lr=0.000566, tokens/sec=1377361.19, grad_norm=0.2647, duration=0.38s
Step 4574: loss=3.2795, lr=0.000566, tokens/sec=1378860.49, grad_norm=0.2875, duration=0.38s
Step 4575: loss=3.2610, lr=0.000566, tokens/sec=1374288.18, grad_norm=0.2870, duration=0.38s
Step 4576: loss=3.2013, lr=0.000566, tokens/sec=1378587.33, grad_norm=0.2906, duration=0.38s
Step 4577: loss=3.3100, lr=0.000566, tokens/sec=1378357.48, grad_norm=0.3248, duration=0.38s
Step 4578: loss=3.2187, lr=0.000566, tokens/sec=1376875.66, grad_norm=0.2927, duration=0.38s
Step 4579: loss=3.1189, lr=0.000566, tokens/sec=1371734.37, grad_norm=0.5334, duration=0.38s
Step 4580: loss=3.2168, lr=0.000566, tokens/sec=1372178.61, grad_norm=0.3126, duration=0.38s
Step 4581: loss=3.2515, lr=0.000566, tokens/sec=1377397.43, grad_norm=0.3195, duration=0.38s
Step 4582: loss=3.3703, lr=0.000566, tokens/sec=1375058.16, grad_norm=0.3098, duration=0.38s
Step 4583: loss=3.2097, lr=0.000565, tokens/sec=1376241.44, grad_norm=0.3244, duration=0.38s
Step 4584: loss=3.3136, lr=0.000565, tokens/sec=1375848.79, grad_norm=0.3221, duration=0.38s
Step 4585: loss=3.2652, lr=0.000565, tokens/sec=1378212.35, grad_norm=0.3039, duration=0.38s
Step 4586: loss=3.2310, lr=0.000565, tokens/sec=1377048.96, grad_norm=0.3230, duration=0.38s
Step 4587: loss=3.2877, lr=0.000565, tokens/sec=1375148.44, grad_norm=0.3151, duration=0.38s
Step 4588: loss=3.2966, lr=0.000565, tokens/sec=1375662.88, grad_norm=0.2783, duration=0.38s
Step 4589: loss=3.2555, lr=0.000565, tokens/sec=1377578.63, grad_norm=0.3202, duration=0.38s
Step 4590: loss=3.2484, lr=0.000565, tokens/sec=1374468.57, grad_norm=0.2812, duration=0.38s
Step 4591: loss=3.2585, lr=0.000565, tokens/sec=1380136.08, grad_norm=0.3023, duration=0.38s
Step 4592: loss=3.2865, lr=0.000565, tokens/sec=1375268.85, grad_norm=0.3402, duration=0.38s
Step 4593: loss=3.3186, lr=0.000565, tokens/sec=1378667.71, grad_norm=0.2878, duration=0.38s
Step 4594: loss=3.2228, lr=0.000565, tokens/sec=1376182.01, grad_norm=0.2905, duration=0.38s
Step 4595: loss=3.1738, lr=0.000565, tokens/sec=1374381.80, grad_norm=0.3063, duration=0.38s
Step 4596: loss=3.2298, lr=0.000565, tokens/sec=1371964.59, grad_norm=0.2961, duration=0.38s
Step 4597: loss=3.1740, lr=0.000565, tokens/sec=1377137.79, grad_norm=0.3189, duration=0.38s
Step 4598: loss=3.1938, lr=0.000565, tokens/sec=1371154.47, grad_norm=0.3072, duration=0.38s
Step 4599: loss=3.1859, lr=0.000565, tokens/sec=1369040.86, grad_norm=0.3051, duration=0.38s
Step 4600/19073 (24.1%), Elapsed time: 1870.35s, Steps per hour: 8853.95, Estimated hours remaining: 1.63
Step 4600: loss=3.2108, lr=0.000565, tokens/sec=1373569.69, grad_norm=0.3140, duration=0.38s
Step 4601: loss=3.2394, lr=0.000565, tokens/sec=1374082.08, grad_norm=0.2864, duration=0.38s
Step 4602: loss=3.1886, lr=0.000565, tokens/sec=1376729.98, grad_norm=0.2999, duration=0.38s
Step 4603: loss=3.2346, lr=0.000565, tokens/sec=1378526.84, grad_norm=0.2836, duration=0.38s
Step 4604: loss=3.1996, lr=0.000565, tokens/sec=1377525.99, grad_norm=0.2925, duration=0.38s
Step 4605: loss=3.1762, lr=0.000565, tokens/sec=1376206.99, grad_norm=0.2934, duration=0.38s
Step 4606: loss=3.2003, lr=0.000565, tokens/sec=1375072.77, grad_norm=0.2830, duration=0.38s
Step 4607: loss=3.2811, lr=0.000565, tokens/sec=1378076.75, grad_norm=0.2663, duration=0.38s
Step 4608: loss=3.3346, lr=0.000565, tokens/sec=1376108.81, grad_norm=0.2839, duration=0.38s
Step 4609: loss=3.2882, lr=0.000565, tokens/sec=1374411.01, grad_norm=0.2721, duration=0.38s
Step 4610: loss=3.2749, lr=0.000565, tokens/sec=1375335.08, grad_norm=0.2976, duration=0.38s
Step 4611: loss=3.2888, lr=0.000565, tokens/sec=1375104.59, grad_norm=0.2547, duration=0.38s
Step 4612: loss=3.2905, lr=0.000565, tokens/sec=1378659.93, grad_norm=0.3045, duration=0.38s
Step 4613: loss=3.3078, lr=0.000565, tokens/sec=1372809.09, grad_norm=0.2679, duration=0.38s
Step 4614: loss=3.2300, lr=0.000565, tokens/sec=1376753.25, grad_norm=0.2656, duration=0.38s
Step 4615: loss=3.2369, lr=0.000565, tokens/sec=1374294.19, grad_norm=0.2789, duration=0.38s
Step 4616: loss=3.2579, lr=0.000565, tokens/sec=1372275.37, grad_norm=0.2971, duration=0.38s
Step 4617: loss=3.2518, lr=0.000565, tokens/sec=1373442.72, grad_norm=0.2740, duration=0.38s
Step 4618: loss=3.2417, lr=0.000565, tokens/sec=1375953.82, grad_norm=0.2936, duration=0.38s
Step 4619: loss=3.2067, lr=0.000565, tokens/sec=1375910.78, grad_norm=0.2604, duration=0.38s
Step 4620: loss=3.2313, lr=0.000565, tokens/sec=1375684.40, grad_norm=0.2759, duration=0.38s
Step 4621: loss=3.1536, lr=0.000565, tokens/sec=1374129.31, grad_norm=0.2781, duration=0.38s
Step 4622: loss=3.1940, lr=0.000565, tokens/sec=1377198.16, grad_norm=0.2825, duration=0.38s
Step 4623: loss=3.2458, lr=0.000565, tokens/sec=1372896.51, grad_norm=0.2783, duration=0.38s
Step 4624: loss=3.2413, lr=0.000565, tokens/sec=1376228.52, grad_norm=0.3061, duration=0.38s
Step 4625: loss=3.2298, lr=0.000565, tokens/sec=1375392.71, grad_norm=0.2916, duration=0.38s
Step 4626: loss=3.2134, lr=0.000565, tokens/sec=1375065.90, grad_norm=0.3248, duration=0.38s
Step 4627: loss=3.2973, lr=0.000565, tokens/sec=1374376.65, grad_norm=0.3363, duration=0.38s
Step 4628: loss=3.2949, lr=0.000565, tokens/sec=1376273.31, grad_norm=0.2687, duration=0.38s
Step 4629: loss=3.2710, lr=0.000565, tokens/sec=1375837.60, grad_norm=0.3664, duration=0.38s
Step 4630: loss=3.2704, lr=0.000565, tokens/sec=1377594.16, grad_norm=0.2926, duration=0.38s
Step 4631: loss=3.2657, lr=0.000565, tokens/sec=1377759.02, grad_norm=0.3069, duration=0.38s
Step 4632: loss=3.2459, lr=0.000565, tokens/sec=1376007.20, grad_norm=0.3044, duration=0.38s
Step 4633: loss=3.2477, lr=0.000565, tokens/sec=1378455.12, grad_norm=0.3086, duration=0.38s
Step 4634: loss=3.2171, lr=0.000565, tokens/sec=1376977.39, grad_norm=0.2980, duration=0.38s
Step 4635: loss=3.2711, lr=0.000565, tokens/sec=1375001.41, grad_norm=0.2885, duration=0.38s
Step 4636: loss=3.2684, lr=0.000565, tokens/sec=1377861.75, grad_norm=0.2906, duration=0.38s
Step 4637: loss=3.2162, lr=0.000564, tokens/sec=1377232.66, grad_norm=0.2730, duration=0.38s
Step 4638: loss=3.2871, lr=0.000564, tokens/sec=1378689.32, grad_norm=0.3003, duration=0.38s
Step 4639: loss=3.2382, lr=0.000564, tokens/sec=1376962.73, grad_norm=0.2554, duration=0.38s
Step 4640: loss=3.2709, lr=0.000564, tokens/sec=1375200.90, grad_norm=0.2989, duration=0.38s
Step 4641: loss=3.1816, lr=0.000564, tokens/sec=1375597.48, grad_norm=0.2543, duration=0.38s
Step 4642: loss=3.2434, lr=0.000564, tokens/sec=1377619.19, grad_norm=0.2839, duration=0.38s
Step 4643: loss=3.1988, lr=0.000564, tokens/sec=1374622.36, grad_norm=0.2814, duration=0.38s
Step 4644: loss=3.1582, lr=0.000564, tokens/sec=1375402.17, grad_norm=0.2780, duration=0.38s
Step 4645: loss=3.1329, lr=0.000564, tokens/sec=1375480.46, grad_norm=0.2864, duration=0.38s
Step 4646: loss=3.1477, lr=0.000564, tokens/sec=1375234.44, grad_norm=0.2615, duration=0.38s
Step 4647: loss=3.1974, lr=0.000564, tokens/sec=1379755.92, grad_norm=0.2850, duration=0.38s
Step 4648: loss=3.1453, lr=0.000564, tokens/sec=1370372.63, grad_norm=0.2743, duration=0.38s
Step 4649: loss=3.1486, lr=0.000564, tokens/sec=1372442.38, grad_norm=0.2843, duration=0.38s
Step 4650: loss=3.1484, lr=0.000564, tokens/sec=1371468.31, grad_norm=0.2966, duration=0.38s
Step 4651: loss=3.1943, lr=0.000564, tokens/sec=1370824.53, grad_norm=0.2772, duration=0.38s
Step 4652: loss=3.1856, lr=0.000564, tokens/sec=1374403.28, grad_norm=0.2841, duration=0.38s
Step 4653: loss=3.2358, lr=0.000564, tokens/sec=1372695.11, grad_norm=0.2816, duration=0.38s
Step 4654: loss=3.2817, lr=0.000564, tokens/sec=1366966.05, grad_norm=0.3196, duration=0.38s
Step 4655: loss=3.2788, lr=0.000564, tokens/sec=1372045.05, grad_norm=0.2741, duration=0.38s
Step 4656: loss=3.3112, lr=0.000564, tokens/sec=1371888.41, grad_norm=0.3464, duration=0.38s
Step 4657: loss=3.2583, lr=0.000564, tokens/sec=1374838.08, grad_norm=0.2855, duration=0.38s
Step 4658: loss=3.2134, lr=0.000564, tokens/sec=1376788.59, grad_norm=0.3260, duration=0.38s
Step 4659: loss=3.2659, lr=0.000564, tokens/sec=1374508.09, grad_norm=0.3442, duration=0.38s
Step 4660: loss=3.2312, lr=0.000564, tokens/sec=1375113.19, grad_norm=0.3008, duration=0.38s
Step 4661: loss=3.2707, lr=0.000564, tokens/sec=1372706.25, grad_norm=0.3144, duration=0.38s
Step 4662: loss=3.2734, lr=0.000564, tokens/sec=1370460.60, grad_norm=0.3142, duration=0.38s
Step 4663: loss=3.2732, lr=0.000564, tokens/sec=1373491.61, grad_norm=0.2899, duration=0.38s
Step 4664: loss=3.2537, lr=0.000564, tokens/sec=1375546.71, grad_norm=0.3178, duration=0.38s
Step 4665: loss=3.2082, lr=0.000564, tokens/sec=1377507.87, grad_norm=0.2820, duration=0.38s
Step 4666: loss=3.2362, lr=0.000564, tokens/sec=1372749.10, grad_norm=0.2859, duration=0.38s
Step 4667: loss=3.2211, lr=0.000564, tokens/sec=1370744.21, grad_norm=0.2875, duration=0.38s
Step 4668: loss=3.3169, lr=0.000564, tokens/sec=1374294.19, grad_norm=0.2813, duration=0.38s
Step 4669: loss=3.2337, lr=0.000564, tokens/sec=1374378.37, grad_norm=0.3120, duration=0.38s
Step 4670: loss=3.2424, lr=0.000564, tokens/sec=1375428.84, grad_norm=0.2700, duration=0.38s
Step 4671: loss=3.2393, lr=0.000564, tokens/sec=1373800.52, grad_norm=0.2961, duration=0.38s
Step 4672: loss=3.2156, lr=0.000564, tokens/sec=1376255.22, grad_norm=0.2610, duration=0.38s
Step 4673: loss=3.2015, lr=0.000564, tokens/sec=1375986.54, grad_norm=0.2574, duration=0.38s
Step 4674: loss=3.2896, lr=0.000564, tokens/sec=1374633.53, grad_norm=0.2758, duration=0.38s
Step 4675: loss=3.2345, lr=0.000564, tokens/sec=1375239.60, grad_norm=0.2939, duration=0.38s
Step 4676: loss=3.3059, lr=0.000564, tokens/sec=1375803.17, grad_norm=0.2588, duration=0.38s
Step 4677: loss=3.2587, lr=0.000564, tokens/sec=1380375.19, grad_norm=0.2629, duration=0.38s
Step 4678: loss=3.2419, lr=0.000564, tokens/sec=1374416.16, grad_norm=0.2806, duration=0.38s
Step 4679: loss=3.2601, lr=0.000564, tokens/sec=1377608.83, grad_norm=0.2553, duration=0.38s
Step 4680: loss=3.2336, lr=0.000564, tokens/sec=1376004.62, grad_norm=0.2663, duration=0.38s
Step 4681: loss=3.2221, lr=0.000564, tokens/sec=1375398.73, grad_norm=0.2579, duration=0.38s
Step 4682: loss=3.2640, lr=0.000564, tokens/sec=1376636.89, grad_norm=0.2775, duration=0.38s
Step 4683: loss=3.2274, lr=0.000564, tokens/sec=1377461.27, grad_norm=0.2723, duration=0.38s
Step 4684: loss=3.2359, lr=0.000564, tokens/sec=1375436.59, grad_norm=0.2643, duration=0.38s
Step 4685: loss=3.2269, lr=0.000564, tokens/sec=1378801.70, grad_norm=0.2622, duration=0.38s
Step 4686: loss=3.2104, lr=0.000564, tokens/sec=1376312.93, grad_norm=0.2735, duration=0.38s
Step 4687: loss=3.2168, lr=0.000564, tokens/sec=1376380.99, grad_norm=0.2511, duration=0.38s
Step 4688: loss=3.2087, lr=0.000564, tokens/sec=1375683.54, grad_norm=0.2660, duration=0.38s
Step 4689: loss=3.1740, lr=0.000564, tokens/sec=1375108.03, grad_norm=0.2667, duration=0.38s
Step 4690: loss=3.1850, lr=0.000563, tokens/sec=1372319.90, grad_norm=0.2506, duration=0.38s
Step 4691: loss=3.0998, lr=0.000563, tokens/sec=1370622.89, grad_norm=0.2544, duration=0.38s
Step 4692: loss=3.1576, lr=0.000563, tokens/sec=1375272.29, grad_norm=0.2608, duration=0.38s
Step 4693: loss=3.1450, lr=0.000563, tokens/sec=1378429.19, grad_norm=0.2728, duration=0.38s
Step 4694: loss=3.2282, lr=0.000563, tokens/sec=1375652.56, grad_norm=0.2675, duration=0.38s
Step 4695: loss=3.1962, lr=0.000563, tokens/sec=1375231.00, grad_norm=0.2612, duration=0.38s
Step 4696: loss=3.1941, lr=0.000563, tokens/sec=1375264.55, grad_norm=0.3004, duration=0.38s
Step 4697: loss=3.1574, lr=0.000563, tokens/sec=1374788.22, grad_norm=0.2758, duration=0.38s
Step 4698: loss=3.1865, lr=0.000563, tokens/sec=1379716.10, grad_norm=0.3041, duration=0.38s
Step 4699: loss=3.2262, lr=0.000563, tokens/sec=1377101.56, grad_norm=0.3134, duration=0.38s
Step 4700/19073 (24.6%), Elapsed time: 1908.55s, Steps per hour: 8865.38, Estimated hours remaining: 1.62
Step 4700: loss=3.2622, lr=0.000563, tokens/sec=1371944.90, grad_norm=0.3067, duration=0.38s
Step 4701: loss=3.2594, lr=0.000563, tokens/sec=1374783.07, grad_norm=0.3158, duration=0.38s
Step 4702: loss=3.2756, lr=0.000563, tokens/sec=1376344.80, grad_norm=0.3332, duration=0.38s
Step 4703: loss=3.2555, lr=0.000563, tokens/sec=1375852.24, grad_norm=0.3390, duration=0.38s
Step 4704: loss=3.2798, lr=0.000563, tokens/sec=1377769.38, grad_norm=0.3166, duration=0.38s
Step 4705: loss=3.2175, lr=0.000563, tokens/sec=1371718.11, grad_norm=0.3353, duration=0.38s
Step 4706: loss=3.2369, lr=0.000563, tokens/sec=1378829.37, grad_norm=0.3216, duration=0.38s
Step 4707: loss=3.3146, lr=0.000563, tokens/sec=1375289.49, grad_norm=0.3133, duration=0.38s
Step 4708: loss=3.3786, lr=0.000563, tokens/sec=1372803.09, grad_norm=0.3169, duration=0.38s
Step 4709: loss=3.2835, lr=0.000563, tokens/sec=1371156.18, grad_norm=0.3073, duration=0.38s
Step 4710: loss=3.2654, lr=0.000563, tokens/sec=1375243.04, grad_norm=0.3103, duration=0.38s
Step 4711: loss=3.2123, lr=0.000563, tokens/sec=1377331.00, grad_norm=0.2889, duration=0.38s
Step 4712: loss=3.2417, lr=0.000563, tokens/sec=1374038.30, grad_norm=0.2742, duration=0.38s
Step 4713: loss=3.2289, lr=0.000563, tokens/sec=1376923.07, grad_norm=0.2987, duration=0.38s
Step 4714: loss=3.1769, lr=0.000563, tokens/sec=1375889.25, grad_norm=0.3016, duration=0.38s
Step 4715: loss=3.2479, lr=0.000563, tokens/sec=1372791.95, grad_norm=0.2819, duration=0.38s
Step 4716: loss=3.1805, lr=0.000563, tokens/sec=1373232.59, grad_norm=0.2920, duration=0.38s
Step 4717: loss=3.2645, lr=0.000563, tokens/sec=1376130.34, grad_norm=0.2818, duration=0.38s
Step 4718: loss=3.2981, lr=0.000563, tokens/sec=1379429.63, grad_norm=0.2889, duration=0.38s
Step 4719: loss=3.2385, lr=0.000563, tokens/sec=1378788.73, grad_norm=0.2873, duration=0.38s
Step 4720: loss=3.1559, lr=0.000563, tokens/sec=1375237.02, grad_norm=0.2923, duration=0.38s
Step 4721: loss=3.2381, lr=0.000563, tokens/sec=1373275.46, grad_norm=0.2703, duration=0.38s
Step 4722: loss=3.2929, lr=0.000563, tokens/sec=1371229.71, grad_norm=0.2956, duration=0.38s
Step 4723: loss=3.2596, lr=0.000563, tokens/sec=1370431.56, grad_norm=0.2935, duration=0.38s
Step 4724: loss=3.1831, lr=0.000563, tokens/sec=1371346.86, grad_norm=0.3054, duration=0.38s
Step 4725: loss=3.2213, lr=0.000563, tokens/sec=1375546.71, grad_norm=0.2788, duration=0.38s
Step 4726: loss=3.2407, lr=0.000563, tokens/sec=1375344.54, grad_norm=0.2986, duration=0.38s
Step 4727: loss=3.2480, lr=0.000563, tokens/sec=1372552.89, grad_norm=0.3041, duration=0.38s
Step 4728: loss=3.3029, lr=0.000563, tokens/sec=1377280.96, grad_norm=0.3409, duration=0.38s
Step 4729: loss=3.1730, lr=0.000563, tokens/sec=1370991.19, grad_norm=0.2921, duration=0.38s
Step 4730: loss=3.2208, lr=0.000563, tokens/sec=1370567.36, grad_norm=0.3110, duration=0.38s
Step 4731: loss=3.2097, lr=0.000563, tokens/sec=1376014.09, grad_norm=0.2660, duration=0.38s
Step 4732: loss=3.1885, lr=0.000563, tokens/sec=1375581.13, grad_norm=0.2901, duration=0.38s
Step 4733: loss=3.2431, lr=0.000563, tokens/sec=1377066.21, grad_norm=0.2779, duration=0.38s
Step 4734: loss=3.1811, lr=0.000563, tokens/sec=1372515.19, grad_norm=0.2776, duration=0.38s
Step 4735: loss=3.1668, lr=0.000563, tokens/sec=1374052.89, grad_norm=0.2800, duration=0.38s
Step 4736: loss=3.1953, lr=0.000563, tokens/sec=1375949.52, grad_norm=0.2547, duration=0.38s
Step 4737: loss=3.2013, lr=0.000563, tokens/sec=1376398.22, grad_norm=0.2659, duration=0.38s
Step 4738: loss=3.1940, lr=0.000563, tokens/sec=1370049.90, grad_norm=0.2495, duration=0.38s
Step 4739: loss=3.1608, lr=0.000563, tokens/sec=1371825.94, grad_norm=0.2586, duration=0.38s
Step 4740: loss=3.1556, lr=0.000563, tokens/sec=1374942.09, grad_norm=0.2559, duration=0.38s
Step 4741: loss=3.1572, lr=0.000563, tokens/sec=1375976.21, grad_norm=0.2790, duration=0.38s
Step 4742: loss=3.1937, lr=0.000563, tokens/sec=1375005.71, grad_norm=0.2648, duration=0.38s
Step 4743: loss=3.1677, lr=0.000562, tokens/sec=1371352.85, grad_norm=0.3112, duration=0.38s
Step 4744: loss=3.2091, lr=0.000562, tokens/sec=1372757.67, grad_norm=0.3339, duration=0.38s
Step 4745: loss=3.2606, lr=0.000562, tokens/sec=1371365.67, grad_norm=0.3151, duration=0.38s
Step 4746: loss=3.2453, lr=0.000562, tokens/sec=1376820.48, grad_norm=0.2959, duration=0.38s
Step 4747: loss=3.2770, lr=0.000562, tokens/sec=1377415.54, grad_norm=0.3595, duration=0.38s
Step 4748: loss=3.2142, lr=0.000562, tokens/sec=1376237.13, grad_norm=0.2885, duration=0.38s
Step 4749: loss=3.2513, lr=0.000562, tokens/sec=1374009.11, grad_norm=0.3354, duration=0.38s
Validation loss at step 4750: 3.699615716934204
Step 4750: loss=3.2173, lr=0.000562, tokens/sec=153503.69, grad_norm=0.2697, duration=3.42s
Step 4751: loss=3.2693, lr=0.000562, tokens/sec=1382758.68, grad_norm=0.3271, duration=0.38s
Step 4752: loss=3.3028, lr=0.000562, tokens/sec=1379175.27, grad_norm=0.2808, duration=0.38s
Step 4753: loss=3.3279, lr=0.000562, tokens/sec=1375493.37, grad_norm=0.3196, duration=0.38s
Step 4754: loss=3.2534, lr=0.000562, tokens/sec=1376554.17, grad_norm=0.3051, duration=0.38s
Step 4755: loss=3.2889, lr=0.000562, tokens/sec=1375798.01, grad_norm=0.2966, duration=0.38s
Step 4756: loss=3.3539, lr=0.000562, tokens/sec=1377281.83, grad_norm=0.3189, duration=0.38s
Step 4757: loss=3.3049, lr=0.000562, tokens/sec=1377020.50, grad_norm=0.2873, duration=0.38s
Step 4758: loss=3.2919, lr=0.000562, tokens/sec=1377195.57, grad_norm=0.2963, duration=0.38s
Step 4759: loss=3.2744, lr=0.000562, tokens/sec=1380501.71, grad_norm=0.3392, duration=0.38s
Step 4760: loss=3.2564, lr=0.000562, tokens/sec=1372908.51, grad_norm=0.2847, duration=0.38s
Step 4761: loss=3.2441, lr=0.000562, tokens/sec=1371057.86, grad_norm=0.3225, duration=0.38s
Step 4762: loss=3.2289, lr=0.000562, tokens/sec=1377559.64, grad_norm=0.2944, duration=0.38s
Step 4763: loss=3.2002, lr=0.000562, tokens/sec=1374635.25, grad_norm=0.2841, duration=0.38s
Step 4764: loss=3.2532, lr=0.000562, tokens/sec=1375354.86, grad_norm=0.2849, duration=0.38s
Step 4765: loss=3.2268, lr=0.000562, tokens/sec=1373296.05, grad_norm=0.3114, duration=0.38s
Step 4766: loss=3.2303, lr=0.000562, tokens/sec=1374550.19, grad_norm=0.2897, duration=0.38s
Step 4767: loss=3.2779, lr=0.000562, tokens/sec=1379850.29, grad_norm=0.2853, duration=0.38s
Step 4768: loss=3.1627, lr=0.000562, tokens/sec=1376728.25, grad_norm=0.5177, duration=0.38s
Step 4769: loss=3.1666, lr=0.000562, tokens/sec=1373224.87, grad_norm=0.7478, duration=0.38s
Step 4770: loss=3.2183, lr=0.000562, tokens/sec=1377496.65, grad_norm=0.4072, duration=0.38s
Step 4771: loss=3.2846, lr=0.000562, tokens/sec=1369767.43, grad_norm=0.4104, duration=0.38s
Step 4772: loss=3.3533, lr=0.000562, tokens/sec=1368901.10, grad_norm=0.3608, duration=0.38s
Step 4773: loss=3.2092, lr=0.000562, tokens/sec=1373994.51, grad_norm=0.3697, duration=0.38s
Step 4774: loss=3.2786, lr=0.000562, tokens/sec=1369793.02, grad_norm=0.3119, duration=0.38s
Step 4775: loss=3.2689, lr=0.000562, tokens/sec=1375481.32, grad_norm=0.3263, duration=0.38s
Step 4776: loss=3.2540, lr=0.000562, tokens/sec=1373017.38, grad_norm=0.3196, duration=0.38s
Step 4777: loss=3.2665, lr=0.000562, tokens/sec=1376395.63, grad_norm=0.3063, duration=0.38s
Step 4778: loss=3.2680, lr=0.000562, tokens/sec=1371419.55, grad_norm=0.3289, duration=0.38s
Step 4779: loss=3.2518, lr=0.000562, tokens/sec=1373449.58, grad_norm=0.3051, duration=0.38s
Step 4780: loss=3.2262, lr=0.000562, tokens/sec=1375685.26, grad_norm=0.3108, duration=0.38s
Step 4781: loss=3.2539, lr=0.000562, tokens/sec=1374459.12, grad_norm=0.2841, duration=0.38s
Step 4782: loss=3.2738, lr=0.000562, tokens/sec=1371629.13, grad_norm=0.3357, duration=0.38s
Step 4783: loss=3.2798, lr=0.000562, tokens/sec=1373622.88, grad_norm=0.2628, duration=0.38s
Step 4784: loss=3.2184, lr=0.000562, tokens/sec=1376093.31, grad_norm=0.2770, duration=0.38s
Step 4785: loss=3.1582, lr=0.000562, tokens/sec=1376725.67, grad_norm=0.2771, duration=0.38s
Step 4786: loss=3.2152, lr=0.000562, tokens/sec=1378640.92, grad_norm=0.2624, duration=0.38s
Step 4787: loss=3.1854, lr=0.000562, tokens/sec=1372175.19, grad_norm=0.2748, duration=0.38s
Step 4788: loss=3.1518, lr=0.000562, tokens/sec=1376790.31, grad_norm=0.3074, duration=0.38s
Step 4789: loss=3.1814, lr=0.000562, tokens/sec=1377198.16, grad_norm=0.2782, duration=0.38s
Step 4790: loss=3.2212, lr=0.000562, tokens/sec=1374002.24, grad_norm=0.3146, duration=0.38s
Step 4791: loss=3.1831, lr=0.000562, tokens/sec=1375107.17, grad_norm=0.3046, duration=0.38s
Step 4792: loss=3.2059, lr=0.000562, tokens/sec=1375503.69, grad_norm=0.3063, duration=0.38s
Step 4793: loss=3.2081, lr=0.000562, tokens/sec=1375185.42, grad_norm=0.3141, duration=0.38s
Step 4794: loss=3.1687, lr=0.000562, tokens/sec=1375373.79, grad_norm=0.3044, duration=0.38s
Step 4795: loss=3.1751, lr=0.000561, tokens/sec=1375206.92, grad_norm=0.2832, duration=0.38s
Step 4796: loss=3.1806, lr=0.000561, tokens/sec=1376829.97, grad_norm=0.2856, duration=0.38s
Step 4797: loss=3.2780, lr=0.000561, tokens/sec=1376169.95, grad_norm=0.2901, duration=0.38s
Step 4798: loss=3.3232, lr=0.000561, tokens/sec=1374905.12, grad_norm=0.2746, duration=0.38s
Step 4799: loss=3.2605, lr=0.000561, tokens/sec=1372665.12, grad_norm=0.2730, duration=0.38s
Step 4800/19073 (25.2%), Elapsed time: 1949.79s, Steps per hour: 8862.50, Estimated hours remaining: 1.61
Step 4800: loss=3.2524, lr=0.000561, tokens/sec=1369856.17, grad_norm=0.2950, duration=0.38s
Step 4801: loss=3.2816, lr=0.000561, tokens/sec=1371695.87, grad_norm=0.2646, duration=0.38s
Step 4802: loss=3.3059, lr=0.000561, tokens/sec=1375346.26, grad_norm=0.3064, duration=0.38s
Step 4803: loss=3.2519, lr=0.000561, tokens/sec=1374406.71, grad_norm=0.2646, duration=0.38s
Step 4804: loss=3.2160, lr=0.000561, tokens/sec=1374742.67, grad_norm=0.2709, duration=0.38s
Step 4805: loss=3.2198, lr=0.000561, tokens/sec=1370573.34, grad_norm=0.2645, duration=0.38s
Step 4806: loss=3.2529, lr=0.000561, tokens/sec=1373563.68, grad_norm=0.2810, duration=0.38s
Step 4807: loss=3.2355, lr=0.000561, tokens/sec=1372612.00, grad_norm=0.2534, duration=0.38s
Step 4808: loss=3.1933, lr=0.000561, tokens/sec=1373236.02, grad_norm=0.2651, duration=0.38s
Step 4809: loss=3.2089, lr=0.000561, tokens/sec=1377787.50, grad_norm=0.2559, duration=0.38s
Step 4810: loss=3.2083, lr=0.000561, tokens/sec=1375460.67, grad_norm=0.2539, duration=0.38s
Step 4811: loss=3.1318, lr=0.000561, tokens/sec=1369931.26, grad_norm=0.2673, duration=0.38s
Step 4812: loss=3.1898, lr=0.000561, tokens/sec=1371890.12, grad_norm=0.2868, duration=0.38s
Step 4813: loss=3.2319, lr=0.000561, tokens/sec=1373413.55, grad_norm=0.2802, duration=0.38s
Step 4814: loss=3.1630, lr=0.000561, tokens/sec=1375989.12, grad_norm=0.2963, duration=0.38s
Step 4815: loss=3.2707, lr=0.000561, tokens/sec=1376457.66, grad_norm=0.3027, duration=0.38s
Step 4816: loss=3.1927, lr=0.000561, tokens/sec=1373151.98, grad_norm=0.2953, duration=0.38s
Step 4817: loss=3.3236, lr=0.000561, tokens/sec=1374933.49, grad_norm=0.3136, duration=0.38s
Step 4818: loss=3.2758, lr=0.000561, tokens/sec=1376500.74, grad_norm=0.2816, duration=0.38s
Step 4819: loss=3.2554, lr=0.000561, tokens/sec=1376436.12, grad_norm=0.3532, duration=0.38s
Step 4820: loss=3.2429, lr=0.000561, tokens/sec=1378642.65, grad_norm=0.2757, duration=0.38s
Step 4821: loss=3.2521, lr=0.000561, tokens/sec=1377614.88, grad_norm=0.3102, duration=0.38s
Step 4822: loss=3.2149, lr=0.000561, tokens/sec=1373326.92, grad_norm=0.3160, duration=0.38s
Step 4823: loss=3.2078, lr=0.000561, tokens/sec=1372018.52, grad_norm=0.3110, duration=0.38s
Step 4824: loss=3.2523, lr=0.000561, tokens/sec=1376631.72, grad_norm=0.3192, duration=0.38s
Step 4825: loss=3.2571, lr=0.000561, tokens/sec=1375357.44, grad_norm=0.3145, duration=0.38s
Step 4826: loss=3.2444, lr=0.000561, tokens/sec=1380554.57, grad_norm=0.3076, duration=0.38s
Step 4827: loss=3.2367, lr=0.000561, tokens/sec=1377546.70, grad_norm=0.3128, duration=0.38s
Step 4828: loss=3.2372, lr=0.000561, tokens/sec=1376591.22, grad_norm=0.3050, duration=0.38s
Step 4829: loss=3.2450, lr=0.000561, tokens/sec=1370997.17, grad_norm=0.2873, duration=0.38s
Step 4830: loss=3.2455, lr=0.000561, tokens/sec=1373703.54, grad_norm=0.3194, duration=0.38s
Step 4831: loss=3.1641, lr=0.000561, tokens/sec=1379582.80, grad_norm=0.2752, duration=0.38s
Step 4832: loss=3.2453, lr=0.000561, tokens/sec=1375163.06, grad_norm=0.2933, duration=0.38s
Step 4833: loss=3.1313, lr=0.000561, tokens/sec=1375420.24, grad_norm=0.2753, duration=0.38s
Step 4834: loss=3.1694, lr=0.000561, tokens/sec=1378849.25, grad_norm=0.2764, duration=0.38s
Step 4835: loss=3.0956, lr=0.000561, tokens/sec=1377941.18, grad_norm=0.2616, duration=0.38s
Step 4836: loss=3.1610, lr=0.000561, tokens/sec=1378089.71, grad_norm=0.2809, duration=0.38s
Step 4837: loss=3.1787, lr=0.000561, tokens/sec=1374646.42, grad_norm=0.2961, duration=0.38s
Step 4838: loss=3.1094, lr=0.000561, tokens/sec=1374202.30, grad_norm=0.2857, duration=0.38s
Step 4839: loss=3.1500, lr=0.000561, tokens/sec=1376807.55, grad_norm=0.2801, duration=0.38s
Step 4840: loss=3.1259, lr=0.000561, tokens/sec=1377616.60, grad_norm=0.2971, duration=0.38s
Step 4841: loss=3.1889, lr=0.000561, tokens/sec=1378702.29, grad_norm=0.2799, duration=0.38s
Step 4842: loss=3.1849, lr=0.000561, tokens/sec=1379835.57, grad_norm=0.2821, duration=0.38s
Step 4843: loss=3.2202, lr=0.000561, tokens/sec=1379177.87, grad_norm=0.2914, duration=0.38s
Step 4844: loss=3.2597, lr=0.000561, tokens/sec=1377665.80, grad_norm=0.3237, duration=0.38s
Step 4845: loss=3.2847, lr=0.000561, tokens/sec=1375280.89, grad_norm=0.2758, duration=0.38s
Step 4846: loss=3.2504, lr=0.000561, tokens/sec=1378265.91, grad_norm=0.3365, duration=0.38s
Step 4847: loss=3.2507, lr=0.000560, tokens/sec=1376637.76, grad_norm=0.2953, duration=0.38s
Step 4848: loss=3.2033, lr=0.000560, tokens/sec=1377199.88, grad_norm=0.3429, duration=0.38s
Step 4849: loss=3.2506, lr=0.000560, tokens/sec=1371945.76, grad_norm=0.3522, duration=0.38s
Step 4850: loss=3.1978, lr=0.000560, tokens/sec=1379063.70, grad_norm=0.3080, duration=0.38s
Step 4851: loss=3.2718, lr=0.000560, tokens/sec=1372911.94, grad_norm=0.3293, duration=0.38s
Step 4852: loss=3.2723, lr=0.000560, tokens/sec=1372180.32, grad_norm=0.3178, duration=0.38s
Step 4853: loss=3.2249, lr=0.000560, tokens/sec=1372524.62, grad_norm=0.2756, duration=0.38s
Step 4854: loss=3.2303, lr=0.000560, tokens/sec=1375853.96, grad_norm=0.2976, duration=0.38s
Step 4855: loss=3.2077, lr=0.000560, tokens/sec=1373673.51, grad_norm=0.2883, duration=0.38s
Step 4856: loss=3.2138, lr=0.000560, tokens/sec=1371684.74, grad_norm=0.2831, duration=0.38s
Step 4857: loss=3.2769, lr=0.000560, tokens/sec=1376124.31, grad_norm=0.2922, duration=0.38s
Step 4858: loss=3.2779, lr=0.000560, tokens/sec=1375745.50, grad_norm=0.2793, duration=0.38s
Step 4859: loss=3.1789, lr=0.000560, tokens/sec=1377749.52, grad_norm=0.2807, duration=0.38s
Step 4860: loss=3.2326, lr=0.000560, tokens/sec=1375798.01, grad_norm=0.2709, duration=0.38s
Step 4861: loss=3.1942, lr=0.000560, tokens/sec=1375354.00, grad_norm=0.2776, duration=0.38s
Step 4862: loss=3.2413, lr=0.000560, tokens/sec=1373780.78, grad_norm=0.2739, duration=0.38s
Step 4863: loss=3.1952, lr=0.000560, tokens/sec=1373300.34, grad_norm=0.2642, duration=0.38s
Step 4864: loss=3.2698, lr=0.000560, tokens/sec=1374021.98, grad_norm=0.2899, duration=0.38s
Step 4865: loss=3.2467, lr=0.000560, tokens/sec=1373286.61, grad_norm=0.2768, duration=0.38s
Step 4866: loss=3.2751, lr=0.000560, tokens/sec=1375111.47, grad_norm=0.2646, duration=0.38s
Step 4867: loss=3.2087, lr=0.000560, tokens/sec=1377604.52, grad_norm=0.2469, duration=0.38s
Step 4868: loss=3.2627, lr=0.000560, tokens/sec=1376299.15, grad_norm=0.2631, duration=0.38s
Step 4869: loss=3.2109, lr=0.000560, tokens/sec=1377551.88, grad_norm=0.2734, duration=0.38s
Step 4870: loss=3.2104, lr=0.000560, tokens/sec=1373314.06, grad_norm=0.2608, duration=0.38s
Step 4871: loss=3.2333, lr=0.000560, tokens/sec=1373338.07, grad_norm=0.2670, duration=0.38s
Step 4872: loss=3.2335, lr=0.000560, tokens/sec=1371731.80, grad_norm=0.2884, duration=0.38s
Step 4873: loss=3.2034, lr=0.000560, tokens/sec=1374399.84, grad_norm=0.2752, duration=0.38s
Step 4874: loss=3.2069, lr=0.000560, tokens/sec=1376126.89, grad_norm=0.2796, duration=0.38s
Step 4875: loss=3.2084, lr=0.000560, tokens/sec=1375056.44, grad_norm=0.2834, duration=0.38s
Step 4876: loss=3.2136, lr=0.000560, tokens/sec=1376998.09, grad_norm=0.2873, duration=0.38s
Step 4877: loss=3.1999, lr=0.000560, tokens/sec=1373439.29, grad_norm=0.2578, duration=0.38s
Step 4878: loss=3.2177, lr=0.000560, tokens/sec=1372466.37, grad_norm=0.2722, duration=0.38s
Step 4879: loss=3.1303, lr=0.000560, tokens/sec=1374761.58, grad_norm=0.2665, duration=0.38s
Step 4880: loss=3.1642, lr=0.000560, tokens/sec=1375329.06, grad_norm=0.2589, duration=0.38s
Step 4881: loss=3.0982, lr=0.000560, tokens/sec=1375927.13, grad_norm=0.2703, duration=0.38s
Step 4882: loss=3.1355, lr=0.000560, tokens/sec=1377752.97, grad_norm=0.2755, duration=0.38s
Step 4883: loss=3.1459, lr=0.000560, tokens/sec=1378514.74, grad_norm=0.2748, duration=0.38s
Step 4884: loss=3.2096, lr=0.000560, tokens/sec=1373559.39, grad_norm=0.2660, duration=0.38s
Step 4885: loss=3.2031, lr=0.000560, tokens/sec=1376111.39, grad_norm=0.3042, duration=0.38s
Step 4886: loss=3.1563, lr=0.000560, tokens/sec=1377600.20, grad_norm=0.2841, duration=0.38s
Step 4887: loss=3.1365, lr=0.000560, tokens/sec=1372454.37, grad_norm=0.2833, duration=0.38s
Step 4888: loss=3.1696, lr=0.000560, tokens/sec=1376592.94, grad_norm=0.2788, duration=0.38s
Step 4889: loss=3.2124, lr=0.000560, tokens/sec=1374115.57, grad_norm=0.2919, duration=0.38s
Step 4890: loss=3.2271, lr=0.000560, tokens/sec=1377658.89, grad_norm=0.2987, duration=0.38s
Step 4891: loss=3.2455, lr=0.000560, tokens/sec=1375621.58, grad_norm=0.3173, duration=0.38s
Step 4892: loss=3.2650, lr=0.000560, tokens/sec=1377085.18, grad_norm=0.3329, duration=0.38s
Step 4893: loss=3.2380, lr=0.000560, tokens/sec=1376430.09, grad_norm=0.3240, duration=0.38s
Step 4894: loss=3.2549, lr=0.000560, tokens/sec=1375200.04, grad_norm=0.3483, duration=0.38s
Step 4895: loss=3.1960, lr=0.000560, tokens/sec=1375723.13, grad_norm=0.3303, duration=0.38s
Step 4896: loss=3.2510, lr=0.000560, tokens/sec=1378921.88, grad_norm=0.3060, duration=0.38s
Step 4897: loss=3.2973, lr=0.000560, tokens/sec=1379688.40, grad_norm=0.3609, duration=0.38s
Step 4898: loss=3.3334, lr=0.000559, tokens/sec=1375294.65, grad_norm=0.3095, duration=0.38s
Step 4899: loss=3.2598, lr=0.000559, tokens/sec=1376723.08, grad_norm=0.3199, duration=0.38s
Step 4900/19073 (25.7%), Elapsed time: 1987.98s, Steps per hour: 8873.32, Estimated hours remaining: 1.60
Step 4900: loss=3.2593, lr=0.000559, tokens/sec=1380342.26, grad_norm=0.2925, duration=0.38s
Step 4901: loss=3.1901, lr=0.000559, tokens/sec=1379472.89, grad_norm=0.2925, duration=0.38s
Step 4902: loss=3.2417, lr=0.000559, tokens/sec=1373513.92, grad_norm=0.2740, duration=0.38s
Step 4903: loss=3.1951, lr=0.000559, tokens/sec=1373462.45, grad_norm=0.3082, duration=0.38s
Step 4904: loss=3.1446, lr=0.000559, tokens/sec=1370733.10, grad_norm=0.2773, duration=0.38s
Step 4905: loss=3.2515, lr=0.000559, tokens/sec=1370990.34, grad_norm=0.3203, duration=0.38s
Step 4906: loss=3.1608, lr=0.000559, tokens/sec=1376736.87, grad_norm=0.2732, duration=0.38s
Step 4907: loss=3.2569, lr=0.000559, tokens/sec=1373451.30, grad_norm=0.3219, duration=0.38s
Step 4908: loss=3.2818, lr=0.000559, tokens/sec=1376480.06, grad_norm=0.2882, duration=0.38s
Step 4909: loss=3.1987, lr=0.000559, tokens/sec=1376737.73, grad_norm=0.3308, duration=0.38s
Step 4910: loss=3.1590, lr=0.000559, tokens/sec=1370662.19, grad_norm=0.2915, duration=0.38s
Step 4911: loss=3.2706, lr=0.000559, tokens/sec=1371912.38, grad_norm=0.3148, duration=0.38s
Step 4912: loss=3.2849, lr=0.000559, tokens/sec=1377175.73, grad_norm=0.3025, duration=0.38s
Step 4913: loss=3.2364, lr=0.000559, tokens/sec=1372690.83, grad_norm=0.3108, duration=0.38s
Step 4914: loss=3.1426, lr=0.000559, tokens/sec=1376642.93, grad_norm=0.2737, duration=0.38s
Step 4915: loss=3.2292, lr=0.000559, tokens/sec=1371455.48, grad_norm=0.2980, duration=0.38s
Step 4916: loss=3.2114, lr=0.000559, tokens/sec=1372559.74, grad_norm=0.3007, duration=0.38s
Step 4917: loss=3.2524, lr=0.000559, tokens/sec=1379305.90, grad_norm=0.2795, duration=0.38s
Step 4918: loss=3.2428, lr=0.000559, tokens/sec=1371879.00, grad_norm=0.3174, duration=0.38s
Step 4919: loss=3.1943, lr=0.000559, tokens/sec=1374542.45, grad_norm=0.2993, duration=0.38s
Step 4920: loss=3.1984, lr=0.000559, tokens/sec=1370503.30, grad_norm=0.3059, duration=0.38s
Step 4921: loss=3.2220, lr=0.000559, tokens/sec=1373188.00, grad_norm=0.2658, duration=0.38s
Step 4922: loss=3.1841, lr=0.000559, tokens/sec=1374859.56, grad_norm=0.3006, duration=0.38s
Step 4923: loss=3.2055, lr=0.000559, tokens/sec=1375601.78, grad_norm=0.2842, duration=0.38s
Step 4924: loss=3.1632, lr=0.000559, tokens/sec=1376693.78, grad_norm=0.2828, duration=0.38s
Step 4925: loss=3.1770, lr=0.000559, tokens/sec=1378880.38, grad_norm=0.2921, duration=0.38s
Step 4926: loss=3.1172, lr=0.000559, tokens/sec=1376856.69, grad_norm=0.2619, duration=0.38s
Step 4927: loss=3.2383, lr=0.000559, tokens/sec=1379532.60, grad_norm=0.2910, duration=0.38s
Step 4928: loss=3.1707, lr=0.000559, tokens/sec=1375999.45, grad_norm=0.2674, duration=0.38s
Step 4929: loss=3.1802, lr=0.000559, tokens/sec=1376100.20, grad_norm=0.2935, duration=0.38s
Step 4930: loss=3.1013, lr=0.000559, tokens/sec=1370562.24, grad_norm=0.2718, duration=0.38s
Step 4931: loss=3.1537, lr=0.000559, tokens/sec=1376706.70, grad_norm=0.2930, duration=0.38s
Step 4932: loss=3.1641, lr=0.000559, tokens/sec=1375993.43, grad_norm=0.3044, duration=0.38s
Step 4933: loss=3.1738, lr=0.000559, tokens/sec=1374975.62, grad_norm=0.3147, duration=0.38s
Step 4934: loss=3.1952, lr=0.000559, tokens/sec=1377974.85, grad_norm=0.2898, duration=0.38s
Step 4935: loss=3.2268, lr=0.000559, tokens/sec=1377386.21, grad_norm=0.3129, duration=0.38s
Step 4936: loss=3.2449, lr=0.000559, tokens/sec=1375742.06, grad_norm=0.2905, duration=0.38s
Step 4937: loss=3.2351, lr=0.000559, tokens/sec=1378578.69, grad_norm=0.3399, duration=0.38s
Step 4938: loss=3.2011, lr=0.000559, tokens/sec=1376517.11, grad_norm=0.3055, duration=0.38s
Step 4939: loss=3.2222, lr=0.000559, tokens/sec=1376760.14, grad_norm=0.3080, duration=0.38s
Step 4940: loss=3.2145, lr=0.000559, tokens/sec=1373587.70, grad_norm=0.3202, duration=0.38s
Step 4941: loss=3.2816, lr=0.000559, tokens/sec=1374925.76, grad_norm=0.3412, duration=0.38s
Step 4942: loss=3.2502, lr=0.000559, tokens/sec=1377441.43, grad_norm=0.3079, duration=0.38s
Step 4943: loss=3.3082, lr=0.000559, tokens/sec=1373413.55, grad_norm=0.3553, duration=0.38s
Step 4944: loss=3.2512, lr=0.000559, tokens/sec=1375585.43, grad_norm=0.3329, duration=0.38s
Step 4945: loss=3.2905, lr=0.000559, tokens/sec=1376291.40, grad_norm=0.3313, duration=0.38s
Step 4946: loss=3.3586, lr=0.000559, tokens/sec=1376540.38, grad_norm=0.3405, duration=0.38s
Step 4947: loss=3.2564, lr=0.000559, tokens/sec=1373818.54, grad_norm=0.3189, duration=0.38s
Step 4948: loss=3.2865, lr=0.000558, tokens/sec=1373865.75, grad_norm=0.3033, duration=0.38s
Step 4949: loss=3.2269, lr=0.000558, tokens/sec=1375921.11, grad_norm=0.3067, duration=0.38s
Step 4950: loss=3.2642, lr=0.000558, tokens/sec=1373962.75, grad_norm=0.3020, duration=0.38s
Step 4951: loss=3.1962, lr=0.000558, tokens/sec=1373605.72, grad_norm=0.2972, duration=0.38s
Step 4952: loss=3.2514, lr=0.000558, tokens/sec=1376583.46, grad_norm=0.2841, duration=0.38s
Step 4953: loss=3.1752, lr=0.000558, tokens/sec=1371657.36, grad_norm=0.2898, duration=0.38s
Step 4954: loss=3.2160, lr=0.000558, tokens/sec=1376437.85, grad_norm=0.2815, duration=0.38s
Step 4955: loss=3.2642, lr=0.000558, tokens/sec=1374008.25, grad_norm=0.3140, duration=0.38s
Step 4956: loss=3.1959, lr=0.000558, tokens/sec=1375730.87, grad_norm=0.2929, duration=0.38s
Step 4957: loss=3.2308, lr=0.000558, tokens/sec=1377945.50, grad_norm=0.6036, duration=0.38s
Step 4958: loss=3.1868, lr=0.000558, tokens/sec=1375959.85, grad_norm=0.3484, duration=0.38s
Step 4959: loss=3.1501, lr=0.000558, tokens/sec=1376455.08, grad_norm=0.3999, duration=0.38s
Step 4960: loss=3.2413, lr=0.000558, tokens/sec=1377069.66, grad_norm=0.3395, duration=0.38s
Step 4961: loss=3.2516, lr=0.000558, tokens/sec=1378465.48, grad_norm=0.3032, duration=0.38s
Step 4962: loss=3.3411, lr=0.000558, tokens/sec=1380196.71, grad_norm=0.3276, duration=0.38s
Step 4963: loss=3.1657, lr=0.000558, tokens/sec=1376173.40, grad_norm=0.2820, duration=0.38s
Step 4964: loss=3.2783, lr=0.000558, tokens/sec=1374993.67, grad_norm=0.3176, duration=0.38s
Step 4965: loss=3.2852, lr=0.000558, tokens/sec=1375879.78, grad_norm=0.2699, duration=0.38s
Step 4966: loss=3.2268, lr=0.000558, tokens/sec=1375151.88, grad_norm=0.3212, duration=0.38s
Step 4967: loss=3.2311, lr=0.000558, tokens/sec=1378913.23, grad_norm=0.2625, duration=0.38s
Step 4968: loss=3.2637, lr=0.000558, tokens/sec=1375427.12, grad_norm=0.3196, duration=0.38s
Step 4969: loss=3.2234, lr=0.000558, tokens/sec=1375303.25, grad_norm=0.2725, duration=0.38s
Step 4970: loss=3.2154, lr=0.000558, tokens/sec=1374685.09, grad_norm=0.3012, duration=0.38s
Step 4971: loss=3.2418, lr=0.000558, tokens/sec=1376231.10, grad_norm=0.2700, duration=0.38s
Step 4972: loss=3.2353, lr=0.000558, tokens/sec=1378505.23, grad_norm=0.3738, duration=0.38s
Step 4973: loss=3.2747, lr=0.000558, tokens/sec=1374093.25, grad_norm=0.2762, duration=0.38s
Step 4974: loss=3.2018, lr=0.000558, tokens/sec=1379101.75, grad_norm=0.2604, duration=0.38s
Step 4975: loss=3.1444, lr=0.000558, tokens/sec=1378595.98, grad_norm=0.2745, duration=0.38s
Step 4976: loss=3.2260, lr=0.000558, tokens/sec=1376111.39, grad_norm=0.2624, duration=0.38s
Step 4977: loss=3.1415, lr=0.000558, tokens/sec=1376344.80, grad_norm=0.2812, duration=0.38s
Step 4978: loss=3.1490, lr=0.000558, tokens/sec=1377168.83, grad_norm=0.2920, duration=0.38s
Step 4979: loss=3.1920, lr=0.000558, tokens/sec=1376148.42, grad_norm=0.2973, duration=0.38s
Step 4980: loss=3.1611, lr=0.000558, tokens/sec=1376328.44, grad_norm=0.3082, duration=0.38s
Step 4981: loss=3.1964, lr=0.000558, tokens/sec=1377762.47, grad_norm=0.3095, duration=0.38s
Step 4982: loss=3.1810, lr=0.000558, tokens/sec=1376211.29, grad_norm=0.3246, duration=0.38s
Step 4983: loss=3.1735, lr=0.000558, tokens/sec=1376431.82, grad_norm=0.2931, duration=0.38s
Step 4984: loss=3.1679, lr=0.000558, tokens/sec=1377437.11, grad_norm=0.3093, duration=0.38s
Step 4985: loss=3.1565, lr=0.000558, tokens/sec=1372907.65, grad_norm=0.2979, duration=0.38s
Step 4986: loss=3.1773, lr=0.000558, tokens/sec=1370141.24, grad_norm=0.3054, duration=0.38s
Step 4987: loss=3.2676, lr=0.000558, tokens/sec=1372064.74, grad_norm=0.3060, duration=0.38s
Step 4988: loss=3.2985, lr=0.000558, tokens/sec=1378476.72, grad_norm=0.3086, duration=0.38s
Step 4989: loss=3.2405, lr=0.000558, tokens/sec=1373809.10, grad_norm=0.2815, duration=0.38s
Step 4990: loss=3.2421, lr=0.000558, tokens/sec=1376281.06, grad_norm=0.2902, duration=0.38s
Step 4991: loss=3.2936, lr=0.000558, tokens/sec=1373215.44, grad_norm=0.2727, duration=0.38s
Step 4992: loss=3.2503, lr=0.000558, tokens/sec=1376774.80, grad_norm=0.2824, duration=0.38s
Step 4993: loss=3.2426, lr=0.000558, tokens/sec=1374693.69, grad_norm=0.2681, duration=0.38s
Step 4994: loss=3.2014, lr=0.000558, tokens/sec=1376197.51, grad_norm=0.2650, duration=0.38s
Step 4995: loss=3.2148, lr=0.000558, tokens/sec=1375380.67, grad_norm=0.2703, duration=0.38s
Step 4996: loss=3.2363, lr=0.000558, tokens/sec=1373594.57, grad_norm=0.2661, duration=0.38s
Step 4997: loss=3.1897, lr=0.000558, tokens/sec=1372192.31, grad_norm=0.2531, duration=0.38s
Step 4998: loss=3.1960, lr=0.000557, tokens/sec=1375265.41, grad_norm=0.2478, duration=0.38s
Step 4999: loss=3.1883, lr=0.000557, tokens/sec=1375384.11, grad_norm=0.2397, duration=0.38s
Step 5000/19073 (26.2%), Elapsed time: 2026.17s, Steps per hour: 8883.74, Estimated hours remaining: 1.58
Validation loss at step 5000: 3.6978399753570557
Traceback (most recent call last):
  File "/home/ubuntu/llm/train.py", line 185, in <module>
    "config": raw_model.config,
  File "/usr/lib/python3/dist-packages/torch/_dynamo/eval_frame.py", line 298, in __getattr__
    return getattr(self._orig_mod, name)
  File "/usr/lib/python3/dist-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'GPTModel' object has no attribute 'config'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/llm/train.py", line 185, in <module>
[rank0]:     "config": raw_model.config,
[rank0]:   File "/usr/lib/python3/dist-packages/torch/_dynamo/eval_frame.py", line 298, in __getattr__
[rank0]:     return getattr(self._orig_mod, name)
[rank0]:   File "/usr/lib/python3/dist-packages/torch/nn/modules/module.py", line 1928, in __getattr__
[rank0]:     raise AttributeError(
[rank0]: AttributeError: 'GPTModel' object has no attribute 'config'
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mrestful-haze-3[0m at: [34mhttps://wandb.ai/arinjay-singh-northeastern-university/gpt2-pretraining/runs/k1h7rjd0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250621_180551-k1h7rjd0/logs[0m
[rank0]:[W621 18:39:43.862464367 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0621 18:39:46.285000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17959 closing signal SIGTERM
W0621 18:39:46.287000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17960 closing signal SIGTERM
W0621 18:39:46.288000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17961 closing signal SIGTERM
W0621 18:39:46.289000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17962 closing signal SIGTERM
W0621 18:39:46.291000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17963 closing signal SIGTERM
W0621 18:39:46.296000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17964 closing signal SIGTERM
W0621 18:39:46.301000 17871 torch/distributed/elastic/multiprocessing/api.py:896] Sending process 17965 closing signal SIGTERM
E0621 18:39:50.818000 17871 torch/distributed/elastic/multiprocessing/api.py:868] failed (exitcode: 1) local_rank: 0 (pid: 17958) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0', 'console_scripts', 'torchrun')())
  File "/usr/lib/python3/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 354, in wrapper
    return f(*args, **kwargs)
  File "/usr/lib/python3/dist-packages/torch/distributed/run.py", line 917, in main
    run(args)
  File "/usr/lib/python3/dist-packages/torch/distributed/run.py", line 908, in run
    elastic_launch(
  File "/usr/lib/python3/dist-packages/torch/distributed/launcher/api.py", line 137, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/lib/python3/dist-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-21_18:39:46
  host      : 129-213-26-180
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 17958)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
